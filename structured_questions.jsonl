{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " 1) One of the projects I am working on - if connecting to internet using Wifi => would like to know what device we are having and every detail on that. + would also like to learn the difference between IPhone12 or IPhone13 etc. Would want to have feedback loops. Some statistics on models will be difficult to deliver. Some sort of annotation tool => feedback tool.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " - [ ] TECH STACK: Python/Scala => Make a package => Push that to artefactory (Frog repository - internalised) \n\n- [ ] In both of these places => 1st job had a platform to launch the model. 2nd place had data bricks to help with compute, had to do own version controlling etc and all the pipelining that will be in AWS and Azure. 2 Cloud providers. Everything will be partitioned into WorkSpaces => before prod, there will be diff stages. It did;t matter much for us. A/B Testing was not very important for us. \n\n- [ ] There are few things that are vital: Model Monitoring, Feature Monitoring, Data Monitoring.  (MLFlow or some version of that) => Model objects in place etc. Some visualisation for the same. Discrepancy between multiple records => all will be pushed into DAG. \n- [ ] We use MLFlow but not for model versioning. It is taken care by us using Artefactory. Some of the deep learning models are too big. Only use for performance tracking the model. All KPIs etc. Anything that is data intensive => directly goes into the artefactory. Docker also artefactory. \n- [ ] Version control - release process, Jenkins. \n- [ ] DAG: use airflow for triggering things. Cronjob that runs that. Data Curation to model building etc has to be in another thing. One Airflow Dag. \n- [ ] Its how the leadership deals with utility providers - earlier Azure, then AWS. \n- [x] Velocity: In my 1st job, basically created a DAG structure. Every node would be some compute => data transformation etc. Entire structure would be created in R => make the DAGs. and then encapsulate all data requirements as Class objects in R. Not utilising airflow or anything. Gave very nice performance improvements. (PROBIA)\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " * Cost Optimization \n\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * Would you help with optimizing the Cost for training? Because if the Training is non-optimal, it would eat a lot of cost. \n* Is there a way to know if the model is eating too much cost - don't want to be monitoring the UI 24*7 (- [ ] One project at Velocity - one model ran us 1000$ a night and we were not optimised. We want to know when not doing great. Can\u2019t keep on monitoring that UI 24*7. \n)\n\n* How do you guys Manage Artfacts? Important for Deep learning models. Also data logging is a problem for Deep Learning Models. WanDB has a good solution around it\n\n* If I have to switch from the current system to your, would it be a seamless switch or integration? i.e. just adding some Boiler plate code and it will work?\n\n* How much have you thought of IP? How do you protect it? When we say these are loosely coupled => we give onus on the MLOps person to be able to stitch these things together => HAVE TO EXPOSE APIs in some capacity. Because of APIs, has access to some code. How do you limit that?\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " - [ ] Would you be able to add annotations? Spacy etc. Brainless integrations. IF some model training where there is very few data points. We would not know every time what the final label is. In the inference thing if we surface the records up, 2 things can happen: * Could go into retraining * Add annotations \n\n- [ ] SERVICE FOUNDRY FEEDBACK: Its Basically - borrowing functionalities from cloud Providers. Something that is on mind for few years. Say we know there\u2019s one DAG that is supposed to run and you want to somehow optimise the cost of it. Because of running this workflow, you want to optimise the cost in terms of load balancing etc that needs to happen. If not rely on kubernetes, the problem is there is this gap. AWS - Actual compute and DataBricks to utilise the compute. I know I could use like 30 nodes, 40 nodes, 100 nodes => and i want to scale up and Spark is good at that. I also want to optimise on the code being written. But we could surface that: If you are running a Spark job utilising the Kubernetes - can tell real time stats. HOW much of CPU is utilised, how much multi-threading you are doing etc. Because of this, you reduce the cost. SURFACING that would be nice. If we have this capability, smaller companies that are in crunch of money. Get a HSTOP output and surface it in a nice UI. Grid Map or HeatMap somewhere that if your process ran for 20 mins, what is the distribution plot => First 100 processors or first X processors to tell that the engineer has to optimise.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Conversational AI Platform - automate conversations that Enterprises are having with consumers. SDK that can go inside Apps, etc. Along with the bot building platform, complementary product for Contact center agent. \nOutbound communication - Campaign Mgmt etc and personalisations => Campaigns can be delieered \n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " 100 Member Engg team, 10 member DS Team - 3 persons more on research and remaining doing both research and engineering.\nModels: BERT and fine tune it for our use case. Also create classifications model. Bot level models are there - which sentence lead to what.\nUsing Collab, have custom dashboards where pump in data into Open search. Deployments are all containerised - Kubernetes and cloud - GPU based. Support all 3 clouds - because of data localizations and loss in diff geographies, have to support all clouds.\nDevOps team maintain kubernetes cluster and developers have access to it. Healing etc are taken care of by DevOps. For CI/CD pipelines => all the code goes into BigBucket => Container registry and then Jenkins triggers the rest. Thinking of using ArgoCD.\nEKL Jobs - are based on AirFlow => data goes into blob stores for Azure and S3 for AWS. So far, have exposed spark clusters. Experimenting with SnowFlake - it can be the tool that other teams can use to extract data for needs.  \n\nThey were trying versioning using MLFlow => but it hasn't been operationalised. It was a need. Once model is built, ML Engineering team converts the model into services.\n\nFor DB Mgmt and Kubernetes monitoring => L1 alerts from NewRelic. Kubernetes metrics - go into Prometheus. Model Performance => how diff versions are working, for that, we are pumping data into Open sEARCH and have built custom logs. \n\nHave one common Jenkins to control all regions. Prometheus - data is stored separately. Thinking of making the entry points common. Open Search - they are currently different.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " They were trying versioning using MLFlow => but it hasn't been operationalised. It was a need. Once model is built, ML Engineering team converts the model into services.\n\n* A lot of 0 to 1 thing happened before I joined. Building repeatable flow would have taken 6-9 months. Automation keeps on taking the backseat. A lot of things we haven't handled => Drift analysis eg. (KUBERNETES ADOPTION - We started doing in 2019) => Developers were facing challenges in scaling up and down. Microservices side and it got propagated to ML Side. \n\n* Currently struggling to be GDPR Compliant. If they have to rely on us to expose all the controls, it will become difficult for them. \n\n* HIGH USE CASE: Real time Monitoring for a Real-time use Case. Have only real time. Batch use cases are not there.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * Is Deployment FastAPI based or FlaskAPI? You can use anything like PytoRCHServe, TensorFLowServe etc\n* Every single Deployment => you can deploy wherever you want. \n* We can expose the part of cluster creation automatically => want a cluster in XYZ. SLEEP WorkSpace => its constantly incurring cost. It will re-start the machines as well. Each Dev can also track the billing part of it. \n* Are the deployments happening in our Kubernetes cluster or TF Cluster? (Collab ++ Kind of thing => You can do whatever you want). You can get a copy of the entire Infra on your own cloud => Only your own workloads on your side. \n* 2ND QUESTION: Let's say we have currently 6 different regions: Will we need to have 6 diff dashboards or could I go to one single UI and see? There will be a clusters tab => There is a Control Plane. You deploy CP on one of the clusters. CLUSTER LEVEL Access and WORKSPACE Level access.\n* What is the current Stability of the Product say, a hem chart you have to install \n* Will MLFoundry only work in training? Or even after depoyment, you can continuously keep tracking. Real time inference monitoring will not be handled by this\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " SaaS platform => Companies can geenrate content using AI Algos. Generative kind of models - not a GAN. Very hard to get perfect fidelity.\nMultiple AI Models which do different things. We don't scale very well: 1) Narrativ Model - what's the narrative telling to users. 2) What kind of content should go into what sequence\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Predictive system => The ad which the system has proposed : Is it going to be a winner?\nOff the shelf solutions. \nEven in these, its not 1 Uber Model. There is a model from our side which works from our side. Lots of decisions coming from different things. \n\nAI Pipeline: Small and niche: 1) Pytorch 2) Tight control on what kind of models we develop. 3) Have own way of versioning the models and things. 4) How do you deploy? AWS mostly but most solutions are not asynchronous. Its not a HTTP Kind of time frame or response time. Job is created => job runs, soluton space exploration. Simple EC2 machines through Kubernetes.\n\nIs it mostly batch inference on the models: Actors come on what kind of space we explore. Models give decisons on discrete points on the space. Do a guided generation. Push the models towards certain areas in the space. \nTraining is offline, GPUs in office. System itself has a large re-inforcement loop. Re-inforcement learning at a higher level. \n\n** A lot of things we have done is manual - everything is not automated. Collectively brainstorm => we compare -> etc. What I see imissing is DATA VERSIONING For reproducibility.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " * Lots of good solutons for Model Mgmt and deployment => Hard challenge is taking it all the way from tracking, Experimentation and then documenting it. Have to do it manually and post production model including traffic etc. Still feels too much hassle.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * Where do you host the model and load it? Model stored and hosted in S3. By the time model is laoded, it seems the model is not responding to health checks. S3 becomes too slow to pull the model and load it from there. What we have done is we use EFS and mount directly on to the machines. And then you have to do a lot of jugglery to get the model to be loaded. (HEALTH CHECK TIMING is customizable - AWS will not wait, AWS API Gate will crash)\n\n* Inferencing Code: We could write anything in the inference code? There is a lot of pre-inference call that needs to be done. It runs through a pipeline of things and then take a decision.\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " * What I see imissing is DATA VERSIONING For reproducibility.  How is the data changing etc? Need to connect the loop back => how do you connect the data, how do you loop back to the model.\n\n* There seems to be a bias that every model is a classification model => Eg- generative and ranking models which are hard to fit. The thing is most teams tend to have logging and monitoring by themselves? If this can fit into existing workflow, then it becomes easy. When you have an issue, you don't want to be scrambling here and there.\n \n* If you have issues, you don't want to be keeping track of service or model. Right now, the dashboard feels a bit of flat. If you want to be dealing with 4-5 projects within it, it might be easier to visualize it. And then again you are lost. \n\n* When you say Service foUNDRY, or could you configure it to deploy in our own VPC? We can ship it as a Helm chart and you can put in your own Kubernetes cloud. Not grant public access etc. How do you do security? How would you configure those scenarios? If you are already using VPN, all endpoints are in your cluster. We are using ISTIO as a load balancer.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " * We are a Multi-tenanted system => User see only models triggered on their side of the system. \n\n* Operators see all the side of the system\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * What volume of data is your platform capable of handling?\n* Is there a dashboard where we can correlate - outputs and also comparing with different ORGS or diffeerent verticals? (Is there we can compare different points)\n* Could we compare the data statistics between models and see?\n* Is there a way to get holistic view of alerts in terms of training? \n* Purely in terms of Pipelines? AUTO-RECOVERY => infra level re-price => We don't control any of that.\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " * We have a model that does Propensity calculation => Example which customer has higher propensity. Want to enable brand users to see. There have to be a MLOps pipeline that makes it self-serve for the user? If it fails for some reason => what all feedback will go back to the user.\n\n* Prediction to show how much time the model will take to learn? User is running a long user call. Feedback to the user => Expose the APIs from our system. \n\n* Why is it a long running job? Because its not just model inferencing that is happening. But there will be training process also initiated. Whether the process is successful.\n\n* If N is running, X is failed => We can do easily.  \n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " * Building Recommender Systems => Understanding and seeing the data, how it is organized etc.\n* Track which tab to tab do they travel? Learning path could have audios, etc. Try and maintain a flow of where the user has stopped. \n* We also have Logical profile of the users => psychological metrics. \n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " DATA STACK: As I told, we are still not there - transactional data etc. But don't have Data Warehouse set-up. Trying airbyte to create different schema which could be used for analysis and creating models. \n\nData Stack: It's currently at 0. Where is the data getting stored? Its in 2 phases => MySQL and some part of data is in MongoDB. \nWhat kind of analytics are we working on? Mental Health company => people can come and do self care sessions, speak to counsellors etc.\n\n* When user signs up, we have all of these things in the app. We have an initial understanding from psychometric tests. Conclude User profile based on 4-5 parameters. 1) PERSONALISED Tab for them => Whatever goals they have chosen => these are the 1st few things you have to do. 2) MOOD Ratings of the user => Recommendation based on what user is feeling day to day. And what you have been doing about it. \n\n* Can't be build like a normal Recommendation system: If 2 people are doing a stress related path, it doesn't mean they have the similar problem. CONTEXTUAL Information in place.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Have a huge User base growing everyday => Can help the user become better ==> Little Personalised.\n* Is there any model already? Build something that is clinical efficacy. Doing research as to how physical and mental data be correlated.\n\n* From a data standpoint, what distinguishes these folks? Mood Rating.\n\n* Most companies try to think of it is the trending items? I want to keep the baseline better. User Cohorting => it helps group users. Age, Geography, state etc. 1) USER PROFILE => Changing 2) COLD START PROBLEM: Bayesian => Learning pathway is well defined?\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Intelligent Document processing - Medical invoices, Loan documents, etc. Classify, extract the information.\nMonitoring is most important. \n\nThey provide intelligent Document processing for Industry Applications. \n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " They wanted to use MLOps for Deployment and Monitoring - 100% will happen only based on data confirmed by customers. \nWe are using VMs => trying to move towards Kubernetes based solution. \nShowed a small demo where uploading an income statement, able to extract entities out of it and mark if correct or not. \n\n80-90% Use cases are NLP\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " 215 People team in iSchoolConnect. WebDev - 25 members, ML Engineers - 15 Engineers. + Huge data annotation team. \n\nIn 2020, very few companies working in inference side of things: \n1) Candidate Recommendation Engine: 2 sides - * When students come, similar to what Yocket has. Where you are likely to get admitted.\nCurrently, 5 verticals that work on ML.\n2) Search: \n3) VIA: Video interviewing analyser => Better speaker in interviews etc.\n4) DWM: I lead currently. Document writing mentor. For admission purposes, students hvae to write essays.\n5) Proctering Engine: Eg - GRE etc.  When Covid hit, all the businesses went online - ETS. 30 Mn students apply every year. The entire load came to us. Built the systems, scale them. 10Mn load. 3RD Party to Procter U. All the load that came - backend went to us. \n6) Analytics: How products are doing in market etc\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Started in 2017 and been here since the beginning of iSchoolConnect.\nMost of the things are in GCP => We are official Google partners. We don't have internal platforms.\n\nWe used to use VMs earlier, now making move to VertexAI. \n1) For Model training, we use VMs. We don't have to train models so frequently. \n2) For serving, depending on use case => * Recommendation engine => API Servers and scale horizontally. * Procter Engine =>We used to work on 6 underlying models => Orchestrated it through Docker Compose => 32GB of GPUs. Lot of data movements \n3) For monitoring, Data Drift and others => EvidentlyAI => recently tried to work with them => has a huge potential for us. APIs also allows to dump output in JSON format. Build separate dashboards for every purpose \n4) Model Cards and training: Used MLFlow but even though we went into establishing practices => not a lot of models get screened. DONT REQUIRE TRACKING MODEL VERSIONS much. Use a combination of DVC and then use GCP.\n5) Internal demoing part: Built on StreamLit. LabelStudio - did POC with them. Spent 1.5 months with them => won't waste my time. Started with streamlit and built the entire platform.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Monitoring was a big need - started using Evidently. \nBefore Evidently, was building own using PyChart etc. No point of building anything from scratch. It took a lot of engineering efforts otherwise- UI was also ready. There are places where it is not able to track Multi-variate (Milti-label or multi-classes) distributions, only binary distributions. (OPEN SOURCE PROJECT => If using for commercial purpose, you need to open source) \n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " Have had discussions with other start-ups on trying to use the platform.\nThere was this issue of Data privacy => What are the options that will be available? Just like we use GCP, we can spin up instances and we can use it as a cloud. While inferencing, code is also pushed and deployed in one of the pods in Kubernetes. \n\nWhat are the options to keep the data in particular region? Eg: We want to keep the data in USA and Europe. Can we keep data in region? 2ND and 3RD OPTION work best --  We will want our GCP. You take the infra and set-up whatever you want to set-up. Updates can be pushed.\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " Honestly, all the 3 components you are integrating - it will be very useful for us. At iSchoolConnect, we don't have big DevOps teams. A lot of time gets wasted asking for permissions and all. You have terraform and spin up things, we also use Terraform to automate a lot of internal things. \n\n\nIf we want to spin up an instance just like we do in SageMaker, I will spin up and works. Something could integrate all the things we have. My role should be solving problems and ship things faster => Solving problems. Staging, Production, Release. \nIf someone new comes, with their access keys, we can restrict permissions for resource => No worry about Cost. \n\nLast bit: UI side => it depends on project to project. Different dashboards built and gets integrated at the end. Used streamlit and built everything including annotations etc.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " - Focusing on NLP problems. \n- Just hired an MLE, 2 full-stack, 0.5 frontend engineer.\n- Good number of design partners - few between Series A and enterprise. \n- Main model that we are deploying is a huggingFace model and we are doing typical out of box deployment. \n- Not doing any training, any evaluation and we are about to start fine tuning versions of it. \n- We are about to get to a point to alleviate a lot of headache on our end. \n-\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " - Only huggingFace. \n- worked most with Amazon Sagemaker for most purposes. \n- Also have worked with MLFLow and Kubeflow as well. \n- Mostly tracking experiments. \n- Use colab for training. \n\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " 1. Soc2\n2. What if we have our own Kubernetes cluster. Its cool to be out of the box on the cluster. How does it interoperate with existng infra. \n3. Some amount of caching some result as well. We are caching results and dealing with real time predictions. \n4. We expose two endpoints and prediction endpoint does not. \n5. We anticipate that we might have some hierarchical models vs a simple sentence similiarity model. \n6. How do you add extra value from pure MLFlow and pure Kubeflow. \n7. How do we rely on an alpha product. Stability or security. \n8. Custom graph logging  \n9. Deploying of a service\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " - Ad personalisation at Spotify. \n- 8 engineer at Jupiter.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " - Worked with a few startups helping them build. \n- Honestly, I didnt get a time to explore TF. \n- What would I use right now. We are building a lot of Airflow and data pipelines. And we dont have strong monitoring on them. \n- We have data pipeliens which run weekly. We have some ML model that read those data pipelines. \n- Everything is running by Airflow. \n- Something should be outside of Airflow which will tell me here is a data that I expect to be there and if things fail report to me. I just want quick feedback. \n- Data drift or pipeline breaking is what I was going to report. \n- These pipeline would geenrate tables with statistics, predictions hapen in real time. We dont want to aggregate everything and we want to make those estimations uikc. \n- Evrry Sunday night we run something and Tuesday 3 PM user faces the order. Last statistics of the pieplien. \n0 Ther eis an ML Backend service which will read the latest stats, what the user did , create some featurs and pass it to a heuristic or ML model. \n- FastAPI based - backend engineers maintains the Terraform setup. I just create a docker container and they figure out. \n- We have a few applications. I think all of them have at least one ML model in them. Usually some model and heuristics. ML models - I am using XGBoost only. \n- We have a lot of categorical data and use XGBoost. We dont jave advanced features. \n- Get retrained every week. We dont track the versions of these models. If te mdoel gets trained every week- its going to go to a cloud storage bucket. Whenever a request comes it reads the latest model. Its a hot swap. \n- Because the models don't block the user UI. As we move towards more UI stuff. I have worked on MLOps a fair bit.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " - Tech stack- Google cloud, - Data pipeline, - Airflow for deployment\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " - What would I use right now. We are building a lot of Airflow and data pipelines. And we dont have strong monitoring on them. \n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " - Data pipeline monitoring outside of Airflow. \n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " - Deployment is not a concern currently.\n- Experiment tracking is. \n- Might be awayed a bit because of minitoring as well. \n\n\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " - Use notebooks for training. \n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " - Large language models, like BERT. Want to be able to track both for short term and long term. \n- Give explanation of how we are better than WandB / Neptune / MLFlow etc. Factors- ease of use, cost. Dont want a lot of features. Simple logging, reliable, shareable, searchable.  \n- Actually okay with public cloud. Need to mention about VAPT. \n- Dont necessary need a lot of magic. Okay to do their own logging. Want checkpoints, metrics, hyperparameters, Git sha- ideally have those things connected neatly. \n- Should work from both Jupyter notebooks & python scripts. \n- Show demo of the product in the meeting/ \n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Bengaluru: 10 people, 20 worldwide\n2018- Since 2018, ML\nMostly a backend engineer till i joined MindsAI. \nBuild a prduct similar to New Relic\nEarlier on system side.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " We have our ow product which is a training platform built on top of Ray.\nOn top of that, building domain specific product for Semiconductor fab scheduling problem.\nTaking our platform and customizing it for them. Our product is called DeepSim. \nMore work was on training side as well as inference\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Stack: RayServe => Inference. Not yet anything for Monitoring and deployments.\nKubernetes - cloud agnostic. \n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " - Big part of problem for us is that its windows. Support of all these things on Windows. Most of cusotomers tooling is on Windows.\n- Support for Ray is not great on Windows. Have to do hacks like keep minimal stack on Windows and use GRPC kind of thing.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * Do you handle fractional GPUs. Can see multiple teams using it. For a staging kind of a use case, they may want to share GPUs within the team.\n\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " * If comparing to our platform, it is nor productised and its not API first. Use case is for internal usage. \nWe don't offer the platform as an offering on its own. \n* It looks really cool. Can be used in places that are ML Shops.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Focus on Human resources Insights data. \n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Using TFServe. Want to keep models as simple as possible. TF 1.0 is a bit of a pain. Now, they have added Keras. \nWe are very traditional - mostly excel for tracking. Report all the findings. \nWe just have TFServe. Serve is as a GRPC Object. Still trying to figure out. Earlier, was not using docker. Right now, dockerizing it. We have a simple docker compose file and we run those commands.\nFor Inferencing, use CPUs. For training - we have own set of computers to train models.\nWe don't do tracking on the DS side. When engineering has issue calling our models, they call us. TFServe - GRPC report has been quite sable.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " * When we create a model, too many parameters involved and we do Brute Force manner. One pain-point is what tool to optimize.\n* The way we have data, it is not well. For new data coming in, hard to marry the data. \n* Optmizing the model is problem. \n* We are Microsoft partners and hence it makes sense to use Azure products. Cost and logs is quite easy to see. Applications Insights => quite useful to us.  \n* Validation is very tricky. We don't have the global minimum but we have the local minimum. \n\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * Code syntax in MLFoundry seems to be similar to Keras. Do you use Keras under the hood?\n* Sign-up on app.truefoundry.com and will try on free time \n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " * Major concern for us is private cloud. That will be a huge cost for us. Pipeline for creating models and deploying models.\n* Even when we ingest data, you can use our own storage ==> Control plane or Data plane. \n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " 4 Months - Buddy in HC Sector. Germany for 2 years. Before that, doing PhD.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Multiple domains: 1) Recommendation Engine, Fraud Detection etc. Other projects related to 360 virtual tool. Have DataBricks in place.\nEvery company has their own systems ==> Build models using Pre-trained models etc => Take models in Prod with AWS. We dockerize and convert models into APIs. And deploy in the form of dockers. Either in EC2 . Also trying out diff things related to DBricks -- Experiment tracking etc.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Currently, deployment is handled by our team and trying to have MLOps guys internally in our team.\nKubernetes : Not using right now. \nDataBricks: Started POC few months back, using it for 1 of the projects. DS - who has deployed using DBricks ==> there are some challenges. I am not a very big fan of AutoML Solutions.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " DataBricks provides good AutoML. They don't provide any IDE kind of stuff as integrated with DataBricks and that is something I don't like.\nAre you using Spark internally? For inferencing pipeline, we will try to use Spark in near Future. All our code is in Python. Also use DBricks for our deployment. Create Feature Store in DBricks itself. We use Feature Stores quite a lot. Can deploy APIs using DBricks. \nFor feature stores, not sure exactly how that is being done. \nPREDICTONS: Both Online + Batch predictions. DBricks doesn't handle real time deployments well. \n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * Data Drift/Feature Drift: Will it be supported?\n* Could we define the Custom Metrics in the monitoring dashboard?\n* Does it power the deployments via Docker?\n* Suppose there is a training data problem or data drift: Could I set a threshold on what level of threshold should I re-train the model? How frequently do I need to measure Data Drift ==> Alerting part of it. \n* We want to have IDE Support, support for custom files etc. We don't provide the editor part of it. As of now, you can use Collab or etc. \n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Based in India - malout. \n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " * It was about last year - discovered MLFlow. Doing POC with another company => structured tabular data. \nDidn't have time to cover the best practices with respect to reproducibility etc as well. \nThat's when we looked at tools: Lightweight, easy ===> Started using MLFlow. MaqV- incubated by MTX. Also proposed using this to the Director of AI and team leads. \n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Still defining MLOps and it will be priority in 3-4 months. \nBe ready for future when need comes. \nDEPLOYMENT: Doing manually - use KServe or Flask. We are mostly on GCP, But also using AWS for business decisions. All of the models deployed are for customers in the own cloud. \n\nMONITORING: Using Grafana - only system/resources monitoring.\n\nOnly one using it - Doing in a local instance\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " One of the concerns raised: 1) RBAC - Role based access control. \n2) Teams work on different tasks - collaboration makes a lot of sense. \n3) One of the requirements was set-up a Model Zoo. \nModel Registry compoenent of MLFlow would be a good solution. \n\nFeatures missing: RBAC, If want to use it in dev CI/CD Pipelines. Any dev can do something and it can create havoc. Others felt personally - use 1 instance for one project. If you want to do this centrally. If we want to deploy central MLFlow Server, in that case, there's no good way native to the way it is designed.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * CI/CD Integration is most important.\n* They do deployment themselves with some help from DevOps teams.\n* Could we do logging of images at a bigger Scale? Could we do something that DVC does. Note: UI takes space and hence might be difficult. Under the hood, ours is the same thing. BENCHMARK: Metrics and Images we should check as to what happens if we log a lot of things. \n* Do we support for all the integrations that MLFlow has? Eg- Optuna, H20 etc. ===> Lightning integration is on the way and we are adding integrations that MLFlow supports. NOTE: We don't have support for these.\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " * Model Registry along with Model lifecycle Management is important. Note: We will have this by end of month \n* MqaV- Building and integrating with MLOps. Eg- MLFLow provides a RestAPI for everything. Do we have python APIs or do we  have Java APIs or SDK etc? Suggestion: Python SDK for now. Haven't standardized the documentation interfaces for the REST Interfaces we have added. (INTEGRATION FOR CUSTOM MAQV PLATFORM) . We can plan to expose the other APIs but it will be a lot of work - they are undocumented. Other languages we are not considering  now. \n* Could we visualize the CSVs? We can add the visualiaitons. Helps in better visualizations. => Note: This is now available \n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Working for 5 years => Lead the efforts on DS side. Lead efforts on DS Side. How to go about product, features to helping deploy all those pieces.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " QUESTIONS: * Is it batch inference? You don't host the model as an API => We will do it when we make things realtime. \nCurrent blocker is actioning system itself is not good.\n\n* Old or new architecture? Signal processing or neural Nets? Some are Deep Learning Models. \n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " When we start an account with an idea, whatever use cases => Jupyter Hub. Has security access. \nStart with experimentation of data on Jupyter Hub. Audio needs to be brought on the server itself. Files can get corrupted.\n\nExploration: 1) Experimentation => Start with analytics => DeepDive using ML. Start using simple models and then do state of art (Hyperparameter tuning starts coming into play) \n\nBest Model: Set out to put into Production ==> How we do it? GitHub Pipeline. Use SageMaker to push model into it. Goes into Engieering team. They use spot instances to run batches of pipeline. PostGres SQL DB. Workflow starts with sometihng ==> else we pick up calls from that through sampling algorithms. (5-7 PIPELINES that are live)\n\nTracking: * Things in AWS Configured * Are the models running fine. Error messages etc.\nActionable part: Data also needs to be presentable. DS Team does the monitoring => use MetaBase for ML Monitoring. Coverage .\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " 1) As we scale => we have to keep changing the batch sizes to make it optimal. X Hours have to be maintained as we grow. \n2) As number of pipelines increase, ML Engineering effort keeps on increasing. \n3) We did explore ML Tools but never went into moving forward => Engg team will take the chalelnge\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " Questions: \n* Do we have to expose the data to our servers?\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Computer Engineer, Master in PhD\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " * Lower cost of APIs. They have a cheaper crude single end point concept ==> Data Science do not need to be backend engineers. Want a way to Productize some analytics. VOLA - reporting tool on top of jupyter notebooks. Still a Jupyter Notebook. Don't have that as a service\n\n* Data Governance \n* When you call deploy, how do you know the service is up and running? Is it synchronous or asynchronous \n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " Disadvantage for us: Is we are 70% of what you have. \nReally interested in is the Automatic wrapping from Jupyter to FastAPI Container.\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " Right now, you are priortizing control in the Jupyter notebook. Have you considered using the Cell magic support so that it can hide bit of the boiler plate. Offering both would be an interesting pitch. \n\nWe can give you images that you deploy on your cloud. Don't see before Q3 ==> Any clearance to buy software. \nMID Q3: Defining the OKRs ==> As soon as I see the ML Team ====> We can do a SandBox. We can do a Paid experiment.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Natural Language Generation - blog article generation. \nHow we use large language models - HFace and OpenAI models. We are also in a lot of their data programs\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Largely self serve. Team of engineers who do the backend etc. \nETL Pipelines - GCP, Kubernetes Cluster that runs and scheduler. Unconventional. \nGoes into the data warehouse that feeds the lot of research stuff. \nR&D and ML - A lot of predictive models around user conversion and user churn etc. \nWe also have proprietary online experimentation engine that we use to split test variance of models. Randomization and allocation of users. We have a handful of metrics that we monitor + secondary and tertiary metrics as well. \nWe use custom analysis as well. \n\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Experimentation is what is interesting for us ==> That is of core value to orgs.\nOne of the early employees.\nWe are better than most other competitors. \nIncreasing the cadence of the tests on the experimentation side of ML ==> from 1 to 10 to 100 of experiments per day. \nScheduler - built it out with KubeFlow as it supports experimentation\n\nWhen you get generative models from GPT, you handpick? Does it run offline or the end user has application? ==> No these experiments are all online.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " We limit the blast radius if things get wrong. We are in 1 Mn user base range.\nKubeFlow does operates in batch. Well designed, scalable. If it needs to be distributed, it has the capacity to do it.\nMetric is - is the user happy?\nHow do you host these models? Backend - OpenAI but support 3rd party vendors for that. We manage a lot of models overall - managed at the source control, etc. \nTypically do 2 variants for a model ==> Is it like a control plane, data plane ==> Where the control plane decides which one goes to Model 1 or Model 2 ?? Allocation to expriment groups is done randomly.\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " KubeFlow - have you put that up? Or planning to put. I just push code and it fires off. \nWhat is the blocker in going to 100 experiments? What is the blocker? It is just time constraint and in meetings all day.\n> Firing the model\n> Fine-tune the model\n> Deployment : Its not yet integrated into my application. No end point ==> Engineering resources that need to be allocated\n> Analysis => Observation\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " 1) We are more of a MSFT shop => GitHub, we are on Azure, etc. We have GitHub actions and things coming in, Deployment has become easy. Until and unless we run out of GithubActions free quota.\n2) We are working on CLoud Native architectures - Kubernetes, Docker ==> removing dependencies ==> Latest tech direction.\n3) We need more people to know about Deployments - coders who haven't been trained on deployment. How easy is it for developers to come and start using the Product.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " \"Wants to move to Cloud Native way of deployment - cross-cloud strategy is very common these days => We used to be on app services. Connect GitHub to apps service => costs significantly went up so we had to take up another strategy (PRE-SALES etc) ==> Customers are on different cloud. We allow you to go cloud native!\n(1 effort for deployment of a containers - 3 to 4 days) ==> It could go up if people don't know.\"\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " DevOps is becoming an integral part of every service provider. How are we adding value in terms of differentiation from Azure?\nMost platforms will let you do one click deployment and will allow things like Splitting between Models, A/B Testing, etc. We are trying to make that experience as fast as possible.\nWe support 14 programming languages and combinations.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Problems we are solving: 1) Recommendation problem, 2) Estimation, 3) Optimization. Market basket etc as well that is needed in the advertising industry.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Use algorithms in-house and running as a python code in different platforms.  (Stack is AWS Boxes - some other servers).\nIn our use case, it is not always needed that we have to expose the things in real-time. \nHeavy computation happening in the offline job. Can't model and predict at the same level. \nExperimentation and making the impact of it is missing - A/B Testing. \nKubernetes is not incorporated into the ML/Data Science pipeline. \nEventually we will be adopting it. \n>> DevOps team: They do initial and every engineer is enabled to do them. \n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Kubernetes is not incorporated into the ML/Data Science pipeline => Resources issues => Eventually we will be adopting it.  \nObviously that is definitely better than previous approaches of deployment. It is very easy to go there. \nIf we can get SandBox access and can try it out. \nOn Kubernetes, are you using Helm Charts to deploy? Lot of algorithms running on offline mode. \n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * If I have my inference function, how much time will it take for me to deploy? It will take 15 minutes. Notebook experience is fastest. \nLet's look at the deployment log and see how much time it took to deploy. \n(3-5 minutes + 5 minutes ==> 10-15 minutes to deploy) \n\n* How do we handle the failures in deployment? 1st thing - your pod will not come up. You will be able to see the stack trace. \n* Call went into experiment tracking a bit. \n\n* This platform is towards exposing the entire platform as real time metrics?\n\n* How do we estimate the cost of the service? Could we estimate it before hand ??\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " * Cost insights before the deployment \n* 60% is still as is how it is? I will want to try out once as to how it will help us in terms of deployment and monitoring? Splitting the experiments with training percentage\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Building a couple of CV models. Sort of a 5-6 models in a pipeline. What is the TAT of one model. What is the performance for 50% of the load etc, whats' the performance of 100% load.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Current, we need 30 minutes delay, that is fine. So we don't need real-time. \nWe batch process the listings every 5 models. We orchestrate using Prefect. Flow everything using Prefect. \nWe have taken a VM and then we use Prefect for Orchestration. \nModel runs on CPU - no GPU. You mentioned you use Docker - are you using AWS EKS. We haven't started it and moving entire thing into Kubernetes, but some cost issue came up. (GCP is used, kubernetes Control plane was charging something). \nScale - 5 minutes, 30-40 listings ==> 80*12 ==> 300 to 600 listings.\nWe are using label studio for labelling the data.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " ** If you have any orchestration set-up for active learning, that would really help us. In active leanring, we are building for monitoring the inference data. \n** We are doing everythign at VM Level, there are lot of issues. If we move to serverless, then it would also help us ==> We are using multi-processing to do it. Sometimes, the other containers are not up.  In a day, it happens twice or thrice. We don't run it as a service but run it as a batch. ==> This is dynamic CPU and memory allocation. If you ahve deployed 4th model and you are loading 5th model, we will dynamically load. \n** Cost of moving to kubernetes\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * How many services could I deploy in a single WorkSpace?\n* Let's take an example - I have a large model. If I hit a lot of load, what if it exhausts the whole 4 GB Memory. Will it auto-scale? \n* Can you go to create WorkSpace? I can only see upto 8GB Workspace.\n* Will this be Kubernetes or something? We can also attach it to your own Kubernetes cluster\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " I like how easily you have orchestrated everything in the interface. I really feel this is very useful wrt deployment.\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " We went on to introduce ourselves. IIT Madras - 2009 (13 years) ==> then in Airbus, started DS in AirBus. Then Emirates for 6 years. Emaar - owners of Burj khalifa. Lead the entire DS agenda (10 colleagues) ==> 1.5 years\n\nBossed kept changing in Emirates: Not a stable team.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " We were attempting a start-up sometime - 4 business lines (Property, Malls, Hospitality, Entertianment) . Biggest chunk is with properties and retail, which is malls. Record sales in properties ===> How do you price the properties?\n\nProperty sales team wants to know how the sales is going to look like? Property sales forecast. \nWhen want to sell properties - there's a customer acquistion channel. Which agency is likely to increase leads.\nHow to generate leads - marketing campaigns etc.\n\nDubai Mall has a marketing platform => how do we bundle offers, how do we personalize offers. How do we forecast footfalls in the malls? Shopkeepers- how do you price the shop rent. Forecasting problems.\n\nPersonalizaiton: Which offers is going to be more appealing to the users? Its' not distinctive.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " * Tech Stack: Plain DataBricks ==> We have made tech investments ==> Migrated to real cloud solutions. AzureML Studio - migrated. \n* Azure notebooks. Deployment also happens through Azure platform.\n* To save costs, instead of real time, we run it in batches. We pre-catche what is relevant to customer. \n* Do a lot of A/B Tests ourselves . Have some bit of monitoring\n* When we choose Azure v/s DataIku, Oracle, AWS are competing heavily ===> Enterprise level architecture ==> there is a bias to choose MSFT. \n* Do you use Azure Studio - Drag and drop? Yes. We use the feature store concept on Azure Platform ==> 80% time goes not in model building, but doing things around it. Feature engineering, pre-processing, etc to making models production ready for deployment. \n* We use SnowFlake and Azure Synapse too. Data Cleaning and customer 360 degree - Informatica is being used.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " * I have kind of bootstrapped - run in start-up mode. Person is able to do end to end.\n* No dedicated DevOps team. Structure is more aligned so that folks are focussed on delivering 1-2 projects. As of now, this is how it is.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * If I had to compare to an open source framework like KubeFlow => it has its own logging and monitoring. It has A/B Testing etc. It took me 3 days to deploy Kubeflow and you will spend time in understanding the system. Developer Experience. We also have model registry and things that integrate tightly with training. (Our Learning curve is very small - User journey is defined to be very simple)\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " If I am a cloud resident, will TrueFoundry help me in reducing the cost of my cloud?\nFor the parts of the platform that Anuraag showed, I will have to lean back. \nI will not be able to do away with my cloud costs ==> think it as a question. If somebody is already invested, then how would you get adoption?\n* If TrueFoundry can show a great pre-processing part, as to how do I reduce the pre-processing time, I will want to buy.\n* Models in Prod. Different versions - how do I do versioning on top of that. \n* If you are only giving me monitoring, then my main cloud costs are still there ==> the business case of monitoing then is not very useful.\n* If the Product is too close to Open Source and very complicated to the Cloud base, your set of customers will be restricted. \n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * Deployment in a Production set-up: Both Online and Offline Use cases. \nRight now, deploying a model takes weeks. And we are enabling that in 15 minutes.\n* How are you deciding the Data Pipelines? What's the boundary? Our focus is on deployment - offline training, Batch inference and real time. \n* Platform comes to the developers code as a library ==> Package from python notebook/CLI \n* Suppose you are deploying a particular algorithm - there is a model => both offline and online. For training, it takes user data as input. For exisitng users, you want offline production so its catched and online cache, you want a copy of the mode.\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " 1) Spark Pipeline \n2) Take the model as a service => Just explain within this example. \n3) We are developing ingress graph APIs - as of now you will only deploy it as a service. \n\nComplete Pythonic way -> deploy it as a service or CronJob . On top of it, if you want to take data from S3 and pass that, you can define it in a complete pythonic way. \n\nTaking code and plugging into production - harder problem is when you put into produciton, lot of new cases that need to be handled. Is it related to the transformation code? Or is it related to the input? When doing offline, there are a lot of assumptions that some input won't be 0. \n\nThe engineer needs to have a lot of cases to put that into Production. If its a live service, you still need to handle the requests. Where we see maximum Gap: Parallel Logging => Model 1 I have put a service, now I want to put Model 2 live ==> I want to comapre the output etc. Real life feed - Model 1 ==> Shadow trafficking. A/B Testing is different. MODEL CONTROLLER: You can do random traffic or you can do random at a certain stage. \n\nA/B Testing: \nIncome Prediction or CTR Prediction : To put a model out is very revenue impacting => there will be a 2nd order effect. I want to comapre a model 2 that I have evaluated in the past compared to model 1. Log inputs => compare offline. LOG => RUN => OFFLINE PROD. A convenience whcich a lot of companies do - in Online service itself, I log another model ==> Live Dashboards that will compare Model 1 and Model 2.\nI built this for InMobi (this is something around 2015) => In ad networking scenario, SMA for prediction is very small. A framework called NLeap => a lot of modelling was happening in Spark and it supported pythonic use case. Supppose XGBoost - a predictor class where you will have a lightweight version\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Based out of Bangalore - next time can meet in person in office. Started going to office. Only shut down when there was peak. Lot of business depends on operations. // Data and analytics background - primarily worked in retail for 10 years or so. I have been working on retailer side or CPG side or manufacturers etc. \nHeading DS for 2 years - 5 year old company.\nSignificant share of market in furnitures and home decor. Working to establish ourselves as a complete home solution.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Tech as a vertical is not very old - before that, the scale was pretty low and hence not worried about how to process the orders. Data as vertical is only 2 years old. Journey - adhoc requests to a lot of collusion of requests and blending into reports - tableau for the same. \n\nTYPE OF MODELS:\nForecasting and estimation models - used for capacity planning, strength planning, raw material planning. Daily models for Last mile logistic planning - how many days do we need for handling? How many inventory days are there? Daily fulfillmnet models are also there. Minimum inventory needs are also there with 90% adherence. \nIntent Model: Working with Product team to deploy - possibility of a customer to conversion.\nProduct affinity: Probability of conversion from X to Y. \nRecommendation Model: Product ranking models, recommendation models.\nImpact analytics for models that have gone live => models for this.  A/B Testing is not posisble in many cases.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Replicas created for Production - we started looking at data warehouse, resides on - RedShift. Not using kubernetes. \nThe Production: OLTP - Legacy (more of a solution etc) ==> RDS (AWS primarily) \nEvery order that comes from marketplace comes from RDS. Dashboards - tableau. \nWe have onboarded another tool called DataChannel to pull data from various APIs => allows us to download data into our system and give us a 360 degree view. \nLibrary - supports Data Engg and dev needs. \nModel building came 1 year back.\n\nDo you run models in a run-time mode or Live ? Process is there - Sales and Ops planning. That is monthly exercise. Everyone meets and agrees on a demand. Every month we project for next 3 months. Everything is stored in database.  \nPersonalization is restricted to the CRM - minor tweaks here and there. Search analytics is part of development. \n(MOST Models are offline - but pipeline is ready and it is deployable as an API etc)  \nWhat is in pipeline is Product based recommendation.\n\nDedicated EC2s and managed services like DynamoDB.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " * Roadmap I have: Driven based on needs of the organization => a lot is happening on pipeline side. A lot is happening on CX.\n* Other problem we face is optimization of budgets. Recently on-boarded a person as brand ambassador.   \n* Forecast models were not working well - all our parameters - MAE etc was too high. We used Prophet and that was giving a high RMSE. WakeFit FB Prophet. \n\nMembers in DS Team: 12 members in the team. About 4-5 months back, make it 20 members. After 6 months, all of them are data engineers.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * As of now, we don't have support for Offline. Deploying as a Cron job and deploying as a job - that feature is coming in 2 weeks.\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " All the use cases I asked is asked because team has built a lot of APIs and all of them can be deployed as a rest service, even if its not a model.\n\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Was with IHS Markit for 8 years and around the end of last year, got an opportunity to lead a team of DS. \nI am part of the SLACK group and i wanted to experiment with TF. Will be more than happy to have a walkthrough.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " A lot of cases where we don't experiment much and just try a few versions and choose one.\nSome teams use Hyperparameter testing tools. \nExperimentation Tracking: MLFlow and improves on a lot of things. We are adding integration for Minio - Kubernetes native. Internally building a pythonic library on top of MLFlow. \n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " OCBC Bank - it is South East Asia focussed bank ==> they are more like the ICICI or Axis of India. Given the domain they are in, they are completely OnPrem \nEnterprise Data Platform: Bought from Cloudera => Runs on our server. \n\nBecause its on Prem, a lot of tools are coming from Apache DataFrame // Apache Ozone - equivalent to S3 // FLink as well. Managed by IT. These tools talk to Cloudera Python Platform. \n\nDeployment Process: Data Platform is similar to a SageMaker. Does Cloudera provide direct support for Jenkins and WorkSpaces? Cloudera mainly provides the data science interfaces. WorkBench is on top of Kubernetes? Create a Kubernetes cluster - on top of this, the Cloudera Application runs.\n\nRay to distribute the overall training piece. \nThere are 2 Facets to it. For some domains, the performance of models is not high of priority. All Products don't need performance side.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * Let's say I want to publish my model training as a Service itself? As a service because it should have the ability to take an input and give out the output. (We will be able to do simple job - name your job as autojob and provide some stuff) ==>  Can I parametrize the Job? In Enterprise especially in Banks : One enterprise is FB, Google etc// And then there are orgs starting to adopt this technology. We are building HyperParameter sweeps. \n\n* Even in terms of deployment of Services - (department within OCBC: ChatBot) - The way we deploy is take a model and deploy in a  service. If you are expecting a heavy load on the service => say we have multiple clusters running on the service.  Will the infra be able to support that? \nWe have built a better solution - autoscale=\"true\" \n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " Suppose you don't have few models. You have 20 teams building the models. \n\nYou are a data scientist - do a bunch of experiments. You have the whole script of model training etc. Every week, you train the model. All models are approved through a feature training workflow. (MENTIONED ABOUT ANOTHER WORKFLOW) - We showcased the Pipeline deployment. \n\nWhich features are important? \nView will be biased: Training jobs will take priority, Pipeline should be at last.\nOnce you have a trained model, every thing becomes repetitive. People don't do Hyperparameter tuning every month. \nA/B Testing and Shadow Traffic - It is good thing and is done in conjunction with business line. \n\nFEEDBACK:\nIf I start overlaying how we are using Cloudera today => There is a disconnect between training and deployment. \n1) Training and Deployment should be in the same layer ==> Environment is different but production has a CMM that it is running. WORKSPACE : could it be generic enough that it has a jupyter notebook, connect to database and within the same environemnt, I can deploy the service. \n\n2) Distributed thing is very important if you are doing 64 GB of RAM. Using Ray for distributed training. Data processing - Spark.\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Growing organically and inorganically - Sizeable presence in Middle East.\nPersonal Employee of the company. We offer Consulting and professional services - Financials and Telecom. We do a traction in other services as well. \n\nWorking majorly as a ML Engineer - we have good presence in Data Engineering. \n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Present Use Cases that the team is working on - building solutions that are similar to Google lens. When open, access the objects => Similar kind of product for one of our clients. \nB2C Product: For that, we are trying to get the data and building a solution that can detect household objects. Object Detection Part.\n\nBuilding ML is a pain point - Lack of annotated data is the biggest Problem. We tried to use data available in Open Source. Accept somewhat okay model - we have to deploy this to mobile. We have to deploy it to applications that are built using Flutter. Considering how do we automate that.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " We are using AWS in the current project - that is the platform where we are building our solutions. Model deployed in the Mobile application - using Flutter application, deploying it. We are not exposing model as a web srvice.\nDeploying the model as a service will come much later. On the edge model - very light model. We can't have a big model here.\nWe are using PySpark and Kafka. A lot of data is tabular. \n\nWe are using MLFlow for Experiment Tracking. For Orchestration purpose, we are considering to use KubeFlow.  We deployed it in AWS. We are getting different tracking information. \nRight now, we are training on AWS GPU. That process, we want to also go to KubeFlow. We are going to make it as Cloud Agnostic as possible.    \n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " If we get a lightweight system that can help us orchestrate, that will be great. \nWe are using Kubernetes, but not for ML WorkFlow. Using it for Backend Meta Services. The way - FastAPI, Docker etc and deploying using Kubernetes. \n\nThe reason for not using Kubernetes? This is still in progress. We considered running in different CPU/GPU instances. We might have to use KubeFlow for that. \n\nWe are planning to Deploy the Model: How do we get all the information and how do we re-train the model. We are using KubeFlow but not sure where it will be useful. \n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * What are the advantages of using your Platform over MLFlow as Anuraag said?\n* Is there anything we are providing for automating labelling?\n* Someone can use the service for free? \n* Are you using platform as well? Say entire service. \n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " Wanted to recommend it to his management and team. We can get members from the team and give them a demo. \n\nWhere do you see utility? Is it possible to provide only specific Piecemeal support. We can provide individual parts and not want the infra. \n\nFeedback on the deployment System: I wouldn't be able to take the decision from my company's perspective.  \nShould we connect in 2 weeks ? I may have to push it by sometime.\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " MLOps at Holcim. I was at Unilever - DS there. Moved more towards MLOps. I have been working in this initiative - Plans of tomorrow. Industry 4.0 vision => large line of products. TrendSetters in Industry.\nSold off the Indian arm as well as Russia arm.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Some on AWS as well as some on GCP.\nRest Dockerized and not on Cloud.\nUsing BigQuery etc . Also have a global data center - we push data output to GCS/BigQuery for pushing the predictions etc.\n\nModel is in the edge. Compute is in the edge environment but batch output gets pushed to a cloud environment. \n\nFor one product, for training: using AWS. Rest, DataRobot and scripts.\n\nFlows of deployments - \n* SAGEMAKER: SageMaker pipeline => it has SageMaker experiments. \n \n* Edge Deployment: Test environment and then Edge deployment.\nOnly problem with Edge deployments - experimentation tracking is a problem.\nHow is the Edge deployment done? Sensor data collected.  \n\n  \n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " * Deployments being edge oriented. \nHow we can use the plant's infra in their virtual infra. We cannot move completely to the cloud as others are able to do. \n\n* DataRobot - being used .Sagemaker - for model monitoring\nHave had conversations with Weights and Biases, Neptune, Truera. We have been in conversations with lot of vendors but haven't found a solution that fits our needs. \n\n* Monitoring is one key aspect where they have explored solutions. Experimentation and tracking is version 2 => that's second priority.\n* Reason for Weights and biases not getting up: Not being able to be flexible. We have variety of different infrastructures. Not very PLug and Play. Good for Citizen DS. Not ready for an enterprise application. \nMonitoring: To be honest, still in talks with vendors.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * Are their enterprises using the Product?\n* How do you package the Product?  => It depends on the number of Users.\n* What's the experience been with AutoML Solutions like DataRobot. How is it different from MLFlow? => What will be the benefit over that? \n* Can I run some tests using the library? We will send you a demo account where we have logged a few things. \n* Could I integrate the deployment piece with my own cloud provider? We are using Kubernetes, so everything is dynamic. We are using Google Artifact registry - could you integrate with that? \n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " Besides the deployment, what would be the benefit for me to not use MLFlow over this?\nLot of benefits we provide on top of MLFlow: 1) MLFlow doesn't have multitenant system (RBAC first class), 2) Dataset logging, Image logging is not in MLFlow, 3) One Click Deployment\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Past context from G2 Calls there. Using this to get a progress update on how they have progressed on this.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " These are already discussed in the past. \n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Progress in ML Pipeline from last time we spoke\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Progress from last time: \n*  From the last time we met, not much has changed internally => Still deployment is using AWS Data Pipeline.\n* Stakeholder Mgmt and Experiment Tracking side: Lot of mgmt => consistent feedback. Don't know exactly what each person is doing. \n* From a deployment side, since we don't have a lot of models, it is not a problem much. Gradio and StreamLit interaction. ML Team is still using AWS Data Pipeline => there was a delay as many folks were on sabattical. \n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " Mgr is not hands on in Data Science but good at setting up metrics etc. \nIt becomes difficult to manage all the projects => Log files don't help him much => Tracking the experiments help him. \n\n>> FIGMA Design - we have done. We could have something similar. Track all the experiments. \nAll our Recommendation models are based on Language Models -> what does Hugging Face mean?\nMLFLow : Siloed activity that we are doing => Using it locally => Plan is to deploy it in G2.\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " One thing MLFlow doesn't have is Role Based access Control. \nHow is it different from MLFlow?\n\n* Not everything can be handled by the framework => whatever framework magic you can provide. \n* It becomes very difficult to understand.\n\nCurrent ML Team is very small: Experiment issues + Stakeholder issues.\nWhat kind of drift do we do in terms of NLP Models today? Just basic data drift. \n\n* We can visualize nouns etc. and Docan - Annotation platform   \n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " IIT Patna - One of the founding member at ClearFeed. Extract info from different frameworks and present insights to customers. \nAll models are NLP Models.\nData Science - only 1 person, Engineering team: 12 members\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Most models are classification, Other is text generation Model.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " We have 3-4 models in Production.\nDeployment side: We deploy to ECS. Docker image => Push to ECR Registry => Deploy the ECR image into a ECS Task. \nAlready we are using AWS - we actually took service from another company - they implemented the DevOps side of the framework. Button in the AWS Cloud build. (Key Value systems - for DevOps) .\nModel Training is on GCP and deployment on AWS side. We started with GCP and actually Software dev part shfts to the AWS.\nAll models are real-time.  We process messages in Real-time. \nData Pipeline: We use DVC => inHouse datasets we have built. S3 bucket is managed by DVC.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Main pain-point is monitoring of the models. \nWe don't have access to the customer data and we don't know what is going on the customer side. \n\nOther than storing the data, what all things do we need on top of data? Text data is unstructured - most monitoring tool is for Tabular data. What are the things you will need to make sense of the data? Model output probabilities across the distribution, Compare distribution across ground truth. Want to be able to detect if something wrong is going on. \nYou want to be able to monitor at the label as to what is predicted. \n\nCurrently, we are using DagsHub to manage the experiments and they connect to the GitHub. They monitor the training of the model. \n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * DEPLOYMENT OF COMPLEX ML MODELS: While its actually very simple function to deploy. If we have a complex Deep Learning model and it has a weights file associated., how will it work?\n\n* In this Product, do you also have experimentation side of the product? We can showcase you the demo for the same as well. \nWould this work if we manage the models through MLFlow registry?\n\n* We have test, staging and Production environment. How do we manage Dev and Production Services separately ?  \n* Does the product contain user level access control so that one one can touch the other Parts?\n\n* Could we connect TrueFoundry with the GitHub Repo? Most of our code is on GitHub repo. We are working on a feature where you give us a repo link.\n\n* Does this also support the monitoring part by itself? \n* Our data is sensitive - how will it work for us then?\n* How do we manage Keys in your system ? We have a way to store secrets!   \n\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " Deployment framework is really good.\nIs it possible to try it out? There is a lot of complication when we deploy at our end. In the way we deploy, there is a lot of steps. \nWe can manage multiple models like v1,v2, v3 etc. In current deployment side, if you want to create another environment, it is very difficult. \n\n* Could we copy the workspaces? Like create a clone of the services to different workSpaces. We can map the WorkSpace to different Environments. Other than environment level, you can also divide at team level. \n* How do we manage Code? Is their any system for reviewing the code etc\n\n* One more problem is there: How do we store/log the model as a python function so we can directly call the model as needed. We allow you to store it as a serialisation function.  \n* MLFlow supports logging the model. Does TF support logging the model/artifacts etc?\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " We have built a web applications, haven't deployed yet. \nCurrently we have 1 model - want to deploy it. \nTotal of 7 members are there in team.\nOne of the pickle format models - done for the sensor format of data. Live streaming data => Need a cloud platform and can track it. Currently its batch mode - but will want to be Real-time mode.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " - ProcFiles and all : Do we need to write our own or will it work automatically?\n- Could you show me the CLI demo? Folder structure etc. \n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Harshul - Found cool ways to integrate with DataBricks and spin off on IDE. They do allow remote pairing of Hosting clusters - POC with DataBricks went well. We are using DataBricks a lot - all Data Lake etc. \n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " We have a Project on Fraud Detection where we need Data Drift Capabilities => What is it we are trying to do in this model? API or platform on Housing.com => If we want to do credit card payments, its high fees and people try to make Fraud transactions using Stolen cards, etc. API is up and running and helped us reduce the frauds - 1.5-2% earlier and decreased to 0.3% now. We have deployed a ML Model to solve this problem. Happens for every Single transaction that is happening.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " API Call for the Fraud detection- REST EndPoint is exposed by another endpoint we have built => we then clean and pre-process and then send to MLFlow. BACKEND SERVER: Will clean up the data and respond to the user. Backend server is written in Python and FastAPI or FlaskAPI. Do you work with dedicated Customer success person from DataBricks? ///  Fraud detection: Model serving feature is supposed to be in Beta ==> Shouldn't be using Model Serving in Production. Even if requests hit 10 APIs/sec  - - > It seems like a critical application. For FRAUD DETECTION, MLLib or Skicit learn libraries. USER FLOW - when transaction got blocked or got unblocked => Red, Yellow, Green - Red is for sure Fraud. Yellow Being borderline case. Green - good. PRODUCT team decides whether to let go through or Block. 24 hours hold - Someone is recording the fraud or not. Ultimately this data is being recorded to re-train the model.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Data Drift => Why the need? // * What's the feature set we are talking about? - 20 to 30 features is when the categorical features using one hot encoding etc. No text features that you have to convert to encoding - No. Cardinality - Can be high as well, but will have to check.   * Model Type - Outsampling or Insampling?  * How much requests are we talking about? - During Training time, we did POCs to understand Data Drift etc. Re-Training pipeline after 3 months. We did it experimentally. No of transactions would be a few millions. PRIORITY:  As behaviour keeps on changing, output space and behaviour keeps on changing => Frauds have decreased. Originally trained model might not work is highly possible.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " What characteristics do you need in a Product? Have tried some open source tools - used FB algo. Its a post on Linkedin. Don't have the Data Drift Pipeline in Production - we do know how to solve the Problem. Asking to take some help from people like us and if we can integrate the system. END EXPERIENCE: Offline and Online - we did offline measuring the data and came up with thresholds - we should then re-train. Might change after few months. Online - can tell the DS the window for re-training. It gives a push notification over email and then using the Push notification. \n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " WHAT'S THE TRANSFORMATION YOU WILL SEE? \n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Ankit - What's your biggest PainPoint? Provide a way to deploy the code from the Experiment part ===> If we log the model, we should be able to log/version and deploy directly from there. GOVERNANCY: Deployment needs a change approval process. There has to be a change control in most companies!! \n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * Could I run the model several times - could I stop and resume the run?  // * Do we have the tie-up between the experimentation branch and the deployment? (What I find discomforting is - Act of deployment is different from the act of Experimentation => by the time you get to end point, you will find that something is different) * Suppose we have multiple customers- could we add a 3rd dimension and see how the features are performing across multiple customers? Could we compare how the data drift is happening across customers? Do we want 1 v/s all.\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " Never say that you don't need to push it to GIT when talking to someone who comes from such a good engineering mindset. We use DVC to pull the model /// Q) Could we annotate some of the data from Production environment in this tool?  The reason I am asking this is that to calculate the data drift, you need the ground truth. \n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Deepanshu - working with MLOps team (Model Deployment, retraining) , adding more features, improvement of features.\nPranjal - Works on Data Science problems\nSolving Client Problems \n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Model Complexity: Simpler models - collection of models => multiple simple models and averaging them out to generate results. What we have different types of data => we get different versions of Models (each version is trained on a different dataset). \nWe have multiple level of versioning in models. How many models get generated in 2-3 iterations => got dataset from client that has 10 data points. For each output, have 6-7 internal models.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " We have to configure each model - configurations related to each one so that we can train/re-train. We create configs for each structure - these are the models we have trained, these are the S3 location. If there is an optimized way to handle this. \n\nDoes training happen parallel? No its sequential. Once we have this, how do we deploy them. \nCustomer makes API calls to the models. For hosting these models, we create custom APIs? If we have 2 customers, are we going to create 2 different learners for them? \n\nModels are not hosted directly - they are not directly deployed. How many models would you have that are serving inference?\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " We are in an EC2 instance - facing scalability challenges. \nIf multiple users start training simultaneously, we have issues. \nWe also provide Optimizations - you want these sort of properties, you can use these sort of things. \n\nRight now, we are upscaling our resources. Looking for microservices etc. If we can separate out the training part separately and if inferences can happen at a separate place. Training is asyncrhonous => we let the user know that the model is trained and you can use them now.\n\nExpected Inference time: 2-3 seconds. \nHow do you do re-training?  Hyper-parameter tuning - when we train the models  . When we train, we get the logs => Single place for monitoring all the training logs\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " How do we handle if simultaneously trigger predictions?\nWhere are the models stored? It is stored in a model registry. We provide Python APKs. \nRight now, it is deployment focussed? \n\nCOULD WE UNDERSTAND WITH AN EXAMPLE: Start with a User flow - Linear Regression model trained => What are the different ways in which the pickle file is available to me?  You can call our SDK and tell MLFoundry.save . It will automatically store model in SDK bucket. \n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Sumanth has experience of 2 years, Chethan has experience of 2 years. Akhil - Fresher (June), Amal - Data Scientists (Joined in June), 4 Folks including Sydney - we can get started. Manu and others - all joined in June. ( Love to discuss a few problems you are trying to solve) . Sumanth and Prashanth have more experience in terms of deploying.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " SUMANTH - Currently, we are working on 1 project using ML - Volume Forecasts. Doing in a modular way through code itself. Pre-processing followed by forecasts. Eventually we will need pipelines, like AirFlow and DAG. \nAMAL - We try to read up and try to deploy some stuff. Its something outside of our daily duties. Its not something we want to be bothered about. \nIf we can get away by just writing DS and ML Code and not worry about how to scale it. Don't want to ML Engineer.DevOps team is also very young and we are doing it ourselves. Volume forecast is still in dev ==> Pipe forecasting ==> Prescriptive analytics (3 MONTHS) : 3 to 4 diff applications.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Hosted Notebooks - We tried using DeepNote (allotted resources are very minimal) => Will have to go premium.\nWe continue to use Google Collab. \nPrefer Local machines or VMs for those reasons ==> We are using Azure. VMs - we create it there. COMPANY IS CURRENTLY MAKING USE OF AZURE. \n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " ** Problem is data is updated everyday - you have to train the model everyday ==> Taking data into local => getting it done. \n** When we go ahead with Fraud and Descriptive, it will be close to Real-time scenarios. \n** Being able to schedule a few jobs as soon as data is there. \n** When data is changing on a daily basis, how could we build the pipeline well => Keep the margin of error low. \n\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " * Is there timeout configuration for Batch deployment? These configurations are also available to you as a YAML File\n* Self-hosting on AWS is fine\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " DS Sales team - Manage MarketShare, Other is DS Advertising. Both use same tech stack - 2nd team's tech stack is mature. \nData Scientist: 3 Data scientists => CLassification part I am the full owner. Delivery is managed by another DS. 2 Intenrs and few product analysts\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Most solutions are based on SageMaker. Created our own CLI and environments. Setting up automated pipelines - lot of things happening here and there. Models depend on a lot of other models. SageMaker instances are doing pretty well for us. \nTech Stack: Python, Queries: SnowFlake, Everything is set-up on Sagemaker\nAmazon is so dynamic - Dataset is too dynamic. Majority going for unsupervised models - mostly on Sagemaker notebook instance. Given Dynamic data, we will have issues on scalability\nDidn't have an idea on Kubernetes. EndPoint is hosted on the UI managed by UI Team. \n\nStore the models on S3 => The communication of SageMaker to S3 is pretty fast.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " What happens is the model output is not given. Sales estimate, classification => then combine the models. \nFor each client, the model changes - Do you host separate models for different clients? \nRESOURCE ALLOCATION: We were struglling. \nSet of scripts running manually. \n\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " The issues of adopting new platform is people are from engineering backgrounds and people are comfortable with SAgeMaker, unless models expnd and it becomes complicated. \n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " We are building the Product - we have to dockerize, creating the FastAPI, deploying on cloud. Around 100 DS and ML engineers\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Convert code to Python and then deploy .\nDepends on the client - if we are getting data from client. If HC or finance related thing, they share access on their envrionment. \nUSE Cases: Mainly related to Forecasts, but not on the basis of the data. For example - expert person available in HC => then from data, how to build a model. \nWe can deploy on our cloud or their cloud. Mostly batch based models . It depends on how data drift happens. \n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " We have created EndPoints => clients can interact there and we get the feedback. \nOne clarification to ask: Once the models are ready, are they hosted on Accrete's infra or client gives access\n\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Building FastAPI endpoints and dockerization is a problem. We want to give them image that they can directly use in their system. \nGenerally, end points will work. \n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " Wanted the Product to be more modular and see and do everything from the Notebook itself. \nSomeone is developing, someone is packaging, etc. If everything is in one place and one person is able to do. \nHe wanted to see eevrything in the same place ie. in Jupyter notebook, doesn't want to come to the UI Interface. \nIf UI, then everything should be on UI. If Jupyter notebook, then there itself. Could we have a Jupyter section here, would be useful. \nHe is also not appreciating the amount of effort going into building things. \n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Was earlier Senior DS at Uber - it seems they build the platform for SC. \nNexus is banking as a service platform - They partner with e-commerce companies. Bukuwarang eg. Using Nexus, other companies. 3 Members in the DS Team.\n150 Members in the Nexus - Was started within SC Ventures. Central team is in Singapore.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " In terms of growth, just launched Product in the public domain: Getting users to sign-up. With data collected, can build data products. \n\n* Churn Prediction: Given transactions of users, how will they churn.\n* Built a number of dashboard to monitor metrics for the business. \n* AirFlow for scheduling, Super\n* Customer Lifecycle Prediction \n* How will the user transition from a high value to a low value customer.  \n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Models are tested in local environments. These models will be real-time. Built as a micro-service. \nStack: Rope in the data engineers but in a nutshell - our Data Lake is on S3. We use kubernetes platform to deploy our micro-services. \nInterested in our solution and how it can be used. HAVE you worked in a bank? They are actually quite dfferent and what you can do in banks is not as easy. \nWe do micro-service in a local environment and pass it to DevOps to deploy it to Cloud. \nMostly doing the training locally - its done locally. \nFramework - ScikitLearn => currently not using GPUs.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Looking at for example - for DS ==> Modelling aspect. Data Engineers and DevOps => DEngineers and DevOps will be tied up with different other tasks. \n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * We have a real time model and we update the model in 1 week? We test and deploy the new model to Production => what kind of workflow do you have for this kind of scenario. I am fine with doing re-training every few weeks. We have both. 1st one => schedule a training job and can be done easily in our platform. \n\nBuilding a complete approval loop is something we are working on. UI for the platform and then do a real-demo with your team. \n\n* Suppose I say I see the result is good and I want to deploy the new one? The old one is already running. Standard procedure - you will want to do it using CI/CD. We have examples in our documentation as to how we are using GitHub actions. Once you committ your changes, it will automatically commit your changes. Go to UI - click a model and click deploy. \n\n* When models are running in production, do you have a tracker which models are running in Production => How many customers are on new models and how many on new models. All your instances will get updated quickly. You need to make sure a gracious exit of all models. OLD Models might still be serving some models - we don't want connection to be stopped. 2 Ways to handle it ==> 1) Adding a hook in the code meaning we will not force-kill your system. It will not be a forcekill - it will be a gracious kill.\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " Do you support multi-Kubernetes system? It could be private cloud but different locations. \nYou could have different Kubernetes clusters as you like and have Kubernetes Clusters from different platforms. \n\nMaybe you can also send the slides => if interest from them, we can get them into the call. There are various competing tools. Bank has been using DataRobot, DataIku, H20 - AutoML Platform. \n\nFew parameters where we are different: Solely focussed on Deployment and making it very deep and different. We have HC companies as clients.\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " - [x] AI Industry for 4 years => NSIT 2018. During that time, involved in research in IBM etc. After that, personally interested in start-ups. \n- [ ] Had joined SkitAI earlier -> Exciting company, working on VoiceAI. Solved a number of ML Problems, wanted to solve ML Problems and hence joined Simpplr. Solutions at Vernacular were client specific. Hence wanted scalable solutions.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " - [ ] Started ML journey 1 year back - 7 people ML Team. Trying to solve a range of Problems. Employee experience company. Intranet where company can put organisation level announcements. We were working on Problems like - Sentiment Analysis to analyse overall sentiment of Employees, trying to find topics they are talking about, What people want with the help of ML. Earlier just facilitating conversations. (JIRA, Confluence, Atlasssian, Drive, Teams, Slack) , Attrition rates - the biggest problem when it comes to scaling ML Models \n- [ ] Clients: Companies like Zoom, Nutanix. The kind of conversations for both these companies would be different. \n- [ ] 400 Customers => Individual models per client. Train, Deploy and Monitor a model is a challenge.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " - [ ] Till last year, ML trained on data from all orgs, but fine tuning the data. Data sits on SnowFlake Infra (Platform is based on SalesForce and data aggregated to SnowFlake). 7 DS, D Engineering team is different, SE Team - works on Deployment side of things. We are designing the infra. Which AWS Service would be best to deploy the model? SE Team also does integration of models onto the main platform. Need to get a ML Engineer to do MLOps work. \n- [ ] Integration: Sentiment Analysis model - they will deploy and do CI/CD Pipeline - integrate with backend service. \n- [ ] Right now - Experimentation part and building the end Point. GPUs for training - AWS Instances like EC2 Instances. Started with hosted notebooks, but better pipelines. We are using dumps - dumping something on S3. Scripts that picks the data, train and dump the model ==> DVC for versioning the model and data. (Using DVC for linking it to GitHub repositories)\n- [ ] Phase of exploring MLFlow, NeptuneAI, etc. Targets in quarter to finalise the MLOps platform (Model Experimentation) // (Deployment) // (Monitoring). Was exploring Triton but haven\u2019t adopted. APIs are deployed on EKS - Kubernetes but we are not exploiting benefits of GPU based infra. Have a person who understand Kubernetes - Engineering team part.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " - [ ] Combination of Scalability in Deployment and Monitoring. Generally, general model and then fine-tuning the model. How do we ensure we have 400 different models and how to monitor the quality of these models. ==> Alerts mechanism of the same, Improving the results of the services. KUBECTL - Data Scientists are used to using KubeCTL\n\n- [ ] Monitoring is Ad-hoc. We are doing offline evaluation of batches. That is something we are trying to roll out. Data out of the system, get the metrics etc. Small team and don\u2019t plan to expand size of team. (MONITORING - CloudWatch + Grafana for the system monitoring)\n\n\nDevOps team writes the Helm Charts when deploying. Using GitHub for mgmt. \nBENTOML: How was the experience? Yet to ship a service using BentoML . Just using the open Source Solution. Use the model registry - then programatically fetch it. 6 services in Production ==> DEVOPS: It takes them 2 weeks to deploy a service. \n\nMULTI-MODAL in a single Service : This use case will keep on coming quite Often. 400*7 GB RAM always ==> Very un-optimal\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " - [ ] We have started experimenting with Airflow but not sure if we are going forward with it. Using it for creating these pipelines - Mostly doing NLP. Standard transformations that are happening with data. We host a different service and then things flow. Looking to do parallelisation to reduce latency of the system. \n\n- [x] TOOLS EVALUATING: Want to be able to use it for next 5 years and will it be relevant. Makes the adoption process a bit slow. Most of the Evaluation has been for Single tools. Evaluated - MLFlow and NeptuneAI ==> Model Mgmt tools - how easy is to do versioning, experiment tracking. How is the UI? How is it to do resource monitoring? Follow trends a lot. Product has a continuous support - most tools are Open SOURCE tools that are community supported and how actively versions are being released. THINKING  of trying out SageMaker and haven\u2019t tried it out so far - no flexibility.\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " Features need to be fine-tuned for every client of ours - 400\n\nThere was a suggestion to try PREFECT. Was exploring AirFlow but it didn't work well. \nTreatment of ML Service is not different from that of a Backend Service. We want to explore GPUs and make use of it well. \nStarted to use Amazon Elastic service - that gives flexibility.\nWe were exploring BentoML and using FastAPI.\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Worked with Mayank in the 1st org earlier. Services start-up, end to end apps. ECommerce and personalization data space - acquired by Flipkart. \n\nHave worked on Engineering side of technologies.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " B2B SaaS company - Ecommerce store owners. Beauty is 90%. AI Selfie solution - Zero party data personalization. 150 data parameters => personalization layer on top. \n\nAI has been a strong selling segment for us. Makes us stand out and apart from competition. \n\nAI Models - SKin condition detection, some for hair segmentation, etc. InHouse Kubernetes Cluster where we deploy. AI will be a part of our product as we go forward. It will be the enabler. AI will have some share of it. 10 People here - AI Researchers: Couple of them. DevOps and Backend - I take care.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " AI Researchers put models in production themselves - how and where we should deploy? Have shared access to them and they do it themselves. \n\nWhat made you choose Kubernetes? Ideally we will keep on adding these models - some will have more load depending on requirements and we will need small microservices. Docker Swarm and Kubernetes - Kubernetes is easier to manage. We are currently on Azure and not on AWS. Helm Charts are not updated that frequently - Any new model to be trained and deployed takes months => to take it to Production Quality. \n\nWe have few test machines where we used to do any kind of scratchpad work . After we have tested things on separate machines, then do it. Push to Docker hub and then deploy on Kubernetes. \n\nWe recently made dev space and prod space. Jupyter notebooks live on the same machine? Right now, we use the normal IDEs instead of Jupyter Notebooks. Not using it for scripting. USE More of command line. GPU requirments are there and they are not fulfilled on local machines . \n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Models in Production are a combination of CPU and GPUs . Real-time or Batch? Mostly RealTime. Do you use Inference servers or use FastAPI etc. \n 2 cases: Models and services scale differently OR where they scale together. \nWe haven't found a need where we have to do it separately. Haven't found the need to scale it separately. \n\nPros for scaling it separately: Say you have GPU Model and then transformation happens on CPU. If you get a request, you have to make sure GPU is at full capacity. People will take the part that runs in GPU and extract in a separate service. (LISTEN RECORDING: 21 minutes to 26 minutes) \n\nWhich framework? Django, Flask, Fast etc. Mostly Flask or a Falcon server that is pretty light for model inference. Cloud Charge as of now - is it very small? 1500-2000$ per month. Don't expect Cloud costs going up a lot from here. Product not expanding on AI side as much. \n\nYou know you have to do it eventually - migration becomes a problem later on. We have talked to customers that you will reduce cost by 30% if you migrate. But too big to migrate. DO you also have any inference monitoring? Do you label, sample etc to improve the model in anyway? For us, the data is majorly selfies. When re-training, how do you do model evaluation?\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " 2-3 Problems - \nWhile training, we keep on tweaking these hyperparameters => if go to something older, try on that. \n\nThis is something that can be tried out at our end. Other things are not a problem for us.\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Connect via Linkedin - MLOps is pretty hot space. Look at the entire ML Journey of Turtlemint. \n\nAshish - Late last year through acquisition joined Turtlemint. Been in Software product for more than 20+ years. Earlier product was in Cloud Analytics space. \n\nLarge enterprise experience. Joined to start off Data Science and Data Engineering - was there basic level.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Data science: CV for recommendation and NLP based problems - Performance improvements. \nOpen problems around Finance - fintech (Insuretech). Open problems as well. Business Centric and highly aligned to business. \nStatistical things, Core ML Algo and Deep Learning also in picture. Reinforcement - nothing yet. \n\nFintech with major focus on Investment through MFunds - but small part. Turtlemint pioneered B2B insurance through brokers.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Primarily based out of AWS Stack - we use Kubernetes, everything is managed. Good team in DS and ML. \nPrimarily applied - lot of things in production. \n\nIntent: Can't answer or share all data. Benefitting - Team wise: < 10 Members in DS. More focus on Quality rather than Quantity. 2000+ Employees, 200 Engineers. 15+ Data Team (Small team) \n\nModels in Production: Everything is applied. Can write papers on XIV - Emphasis has been on putting models in Production. K8S for deployment, EKS => SAme thing. Deploy on Pods and can scale them. Through APIs gets into Production. Backend Data pipelines. \n\nThing could be - at juncture of company, you might not worry about scaling using K8S. Some use pHP and remain on PHP for many years. Data Scientists will come from non-CS background. Data Engineering team is well positioned to handle it. \n\nBoth real time and batch inferencing. Not using SageMaker - was tried and maybe we could use it later. MLFlow, EKS could be used. Would like to see TrueFoundry.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " You cannot imagine DS to be writing YAML. \nLooking at ZenML, SageMaker. \nWas not giving concrete answers - seemed to be fearful.\n\nK8S is not seeing adoption but based on the company, it could be different. \n\nCheckins on the KubeFlow side have stopped. \nEgress and Data Security both are important\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " Good Questions Overall- Couldn't record this though\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Skit - lot of work with ML Models. Text to speech, Speech to Text. One Product. Before 2018, doing ChatBots. Collecting a lot of data for Indian Languages. Now we are focussing on US for a while.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Base system is same, Separate system for Orchestration, Python is used by most folks and Go, Don't have preference on frameworks on ML - PyTorch. \n\nWe are using EKS - Amazon (was earlier on GCP). Most things are inhouse because of the way things have been historically. We started with Kubernetes only initially - the decision was taken early on. It has worked out well for us as well.\n\nRecently started using Kubeflow for Pipelines. Deployment - OnPrem => moved to OnCloud. Everything is real-time. In Voice, need to be real-time. We do have a lot of systems that we want to have. Systems for Anomaly and Drift Detection. \nEarlier we had a role called MLSolutions Engineer. \n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " We are not solving anything new in the ML as of now.  We have strong ML as well as ML Engineering Team. \n\nAll Models we have been wrapped around something: B2B System - have to customize it for every client and ship. There we have faced challenges in terms of mgmt of models for different clients. We have faced issues with how we manage models, how we roll out. \n\nWe also have an evaluation system that we trigger from Slack. Those things are also running KubeFlow Pipelines. \nAre you using KFServe? We mostly wrap the models in custom stuff. We used Flask, then blackship. We use C++ for ASR. The server is actually Open Source. For SLU - We have a python based system - has been optimized sufficiently. \n\nMake the Model, dump it on S3, someone else can then use it. Docker Image => Load Testing => Deploy it via the regular process\nCore ML Team - they will select the right Docker Images. This team is integrating your thing in their system.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " 1) How do we ensure that we upgrade the models well?\n2) How do we roll out the models faster?\n3) At one point, we were seeing issues with Quality of annotation => we needed to build our own platform. \n\n* What has worked for us: Gradient, DataBricks => they are trying to do everything and that doesn't work for us.. ASR kind of things. We have found Data Annotation to be a major problem and we are looking to explore solutions there. LABEL STUDIO: 200$ per member per month. \n* Where ML acceleration is possible, we have gone out and found vendors. \n* Spoke to Snorkel.AI => It would be good if there was something that works for company like us. \n* We don't do Drift detection and all - monitoring has been a problem for us => Setting up stuff around System monitoring as well as ML Monitoring. INPUT is voice and transcription \n\nWe have a framework that we have built in-house => It tracks training, testing and validation data.There is no versioning happening. QA/QC is not that great. Not good CI/CD for the model systems.\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Been at WhatFix for 10-12 months. AI Consultant to a Data Engineer. Leading team of DS and insights team.\n\nAbhishek: Senior DS with Whatfix for 2 years. Engineering side of things. Use models and monitoring - Deployment/ Retail side of things.\n\nWHATFIX\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Kind of Use Cases: We don't have Supervised learning use cases. Most use cases are Recommendation (but doing it in an unsupervised learning way) or NLP Based models. Use cases for Supervised models are minimal. \n\nToday, we don't have anything that runs on client side. Tomorrow we might have. \n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Multi-Node Hadoop set-up to do all the development. All models had Batch models deployments. Artifacts are scheduled, they run overnight and they are used in the product. \nBatch Inference - do you use Spark or something else? \n\nA lot of the use cases we have - We need a architecture: 1) CREATE easily deployment code and package it well and create service out of it 2) SERVICE should be fault tolerant. \n\nSTACK ON CLOUD: We have an infra where we do experiments (Hadoop clusters) // All the VMs are Azure (I think we would want it in Azure) // For same company, it could be different account. \n\nAvailability of the infra and engineering team - we have to wait for months to get their availability. If there was a method through which we could generate the APIs. Model goes through the changes. I want to make sure we have the ebst things avaiable at hand to reduce time to production. \n\nB2C World - this is very relevant.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Aim is to give Data Scientists an API. Supervised learning model, Python Script, Unsupervised learning model. \nInferencing framework. Have you seen any solution - almost about to finalise a vendor.\n\nIt is about the use - what other vendor?We can do a Quick POC - Azure POC was not smooth. \n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Based in Bangalore - 3.5 years. Last 1 year, leading initiative of setting a ML Platform -> we have done a few things. \nStart-up focussed on Supply Chain Financing before that. \n\nI have seen the newsletter. Its okay/good :)\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " When we started, we didn't want to go down the vendor purchase route. We wanted to understand how to solve them in the most optimal manner. AWS - Preferred partner for Cargill. If possible to enable the SAgeMaker stack itself. \n\nThere is no 1 Team. There are a lot of DS and non DS Teams. There is a Digital Foundry team - building solutions for identifying pests in grains.  \n\nFunctional Use cases as well: Lot of needs that we are unable to satisfy. Semantic Segmentation - advanced problems. \nTotal DS : 200-250 People including analysts, DataOps, DS Teams, deployments \n\nML Platform team is small: Entire team - Solutions architect from AWS assigned to us. I am leading the team. SE, ME, DS- 3 people for advisory sort of role (customer advisory)\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " 2 Use cases: 1) Proof of Value 2) Funding Stage\n\n1st one - people don't care about MLOps. 2nd - Production ready way of catering. \nProof of Value - set up SageMaker Notebook + set-up storage. If they want to explore, they are able to explore and connect to internal Cargill data platform. You can also deploy the model but its in data science side only. All of this can be done by the users themselves. They can't set up resources themselves - S3, EKS, etc. They can launch things. \n\n2nd : Funded Projects. GitHub enterprise repository - MLFlow Pipeline or SageMaker pipelines. \nWe also provide them bridge notifications. There is another repository based pipeline. Sets up Production API gateway for them. They can specify: 1) Give API EndPoint 2) Give me serverless. You can observe 3x-4x Errors. They can choose what monitors they want for the model. \n\nAPI is protected with rate limitation, firewall, security. We have set-up the Waf accordingly. API accessible from API key. \n\nEverything is single click registry Ops - With one click, it will deploy. We have used SageMaker runtime - it provides Canary deployments as well. People can specify what % needs to be changed. It also monitors Operational monitors and then it replaces. If alarms are triggered, it rolls back as well.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Tried to connect to Cloud Platform Team - tried to connect to see if we can piggyback on their solution. 2 Months wasted there. \nDid everything after that ourselves. Connectivity to Data Platform, setting up VPCs, Setting up infra in a way that people can do hybrid multitenant kind of work. \n\nVPC and infra bit is common to everyone but access control is there at the team level => They can choose compute as well to a certain level. Training jobs, data wrangling jobs etc. \n\nDEPLOYMENT: For deployment, they don't write Sagemaker related code. Someone who writes the code - that's completely abstrated out. Model building or MLFlow pipeline - that's something that they can do. In the codebase, it will invoke the MLFlow. They can use Prophet etc as well. They can bring their own kernels or if they want to bring their own inference images, they can use that as well. Model sitting on device use cases as well available. \n\n\nTraining pipeline stops with Build. We enforce Registry Ops. You have to bring your MLFlow runs, you can deploy it. \n\nBringing up Airflow as a Service - its in the pipeline. Bringing it in - its in the roadMap. Or use Managed Airflow as a service. Could be a vendor - Astronomer etc.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * Material sciences - You have to draw and detect objects in a microscopic images. \nSometimes I feel shitty that I am not doing innovation. \n\n* Do you have use cases where Model is there => there are a lot of pre-processing and Post processing use cases. We provided a solution to them. From infra perspective, we have enabled based on what is called Pipeline Model. When request comes, 1st container will do pre-processing. THERE IS NO ONE SOLUTION FOR IT. Even the SageMaker pipeline - you have to make multiple containers, docker files. We want to be able to put this solution outside the purview of the Data Scientists. \n\nMaybe its time for Feature Store. However, its not easy in a Big org. Sometimes, size of the project also matters. \n\nAt Cargill, if you wanted to build a kubernetes based app, you can easily do it. You need a deployment spec and GitHub repository - SQS, SES, etc. You can even get a database- Dynamo DB. \n\nWHAT IS THE IMPACT YOU JUSTIFY INTERNALLY? Earlier, even for deployment, they used to take couple of days. In a matter of an hour, they have model registered in registry. Now, in one hour, they will get the API out. If you go and buy Domino or DataIku, 5000$ per user per year => if have 100 => $500K $ .\n\nWant to provide Airflow with DBT as a service. DE+ ML platform or Data Warehouse + ML Platform. Managed MLFlow - we are exploring. Swap that with openSource MLFlow.\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " We decided to go over Kubernetes: \n\n* How has it been going with SageMaker? You have extracted everything and people don't realise. DS - they don't care if it runs in Kubernetes or not. \nWe have talked to a few vendors as well - Pachyderm, KubeFlow. \nIf you focus there, the end to end experience goes for a toss. \nYou need a really good onBoarding experience. \nInstead of going piecemeal, let's go full time. \n\n* Stayed away from going anywhere near Kubernetes - its going to consume Engineering hours. Whatever you do, endpoint runtime is SageMaker. Recipe is ready to put into a Kubernetes requirements. \n\n\nMotivation was end to end feature complete. As techies, its exciting that Kubeflow is there. Whatever people will be doing in platform, let's enable that end to end. \nMonitoring is coming in next. \n\nOUR Heavy work - multi-tenant, role setting, CICD pipeline. Whatever we offer as Products are also going through CI/CD pipelines. Model should be available as an API. \n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " 1) People update resume, we have to keep the embeddings. We have the model - but we have a model. That model doesn't change. But what changes is the embeddings of the explainaton. When we want to match the question to best explanation, we get a score for each. \n\nHow many embeddings are you talking about? 20-40K embeddings. Every query we get - it will go over, rank them and return. Now, on these 20K => these are updated 100 times a day. 200 of them change each day for example. \n\nWhy do you want to use TrueFoundry? COST OPTIMIZATION \nRapidAPI - We would love to sell our endpoint outside of our own universe. You have API, You have security, etc. If its a use case you already have (RapidAPI)\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " A good documentation to integrating a RapidAPI on top of our Hosted Endpoint. Could we have a paid API ?\n3 Days - 1 Week to write the documentation. Arnaud - the payment per API call is the only part that needs to be handled. \nHow do I integrate TrueFoundry with RapidAPI?\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Models are having memory issues: Right now - T3A, 2/8 Slacks. Memory was not that big before. \n2 Models - Summary and takeaways. It takes quite a bit of our memory. \nIt takes around 500MB memory. We have multiple models at the same time. It is about 16 GB. \n\nYou can put multiple models in a single container or you can put 1 model in one container. If models are getting high amount of traffic, then you put it in different containers. \n\nYou are using multiple workers and then it creates the problem. Everytime, it will multiply the memory. If we operate on one worker, we lose information. \nSpringBoot API and the AI Models. \n\nIf we want to do some form of clustering etc etc - we can do it at our end or different end. If I do some work at our end and then some at our end. \nWhen I update the code => how easy is it to modify code? YOU CAN ALSO GET THE CI/CD SET-UP\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " Deployment on our cluster for now. \nWill you deploy on your cluster? Or use ours? \nSecurity side. We are not big enough yet. \n\nAWS Machine type: CPU, T3A 2X Large, Do you need GPU? Wondering how you would do the model without GPU that I sent.\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " * Could you give us a Dummy Model? Request that is working == > We can ask them to put any of the transformer models from Hugging Face and we can replace it. \n\n* You can deploy it. We can give access to the account, we can play with it. If we are able to integrate it easily with what we are doing. \n\n* POC: We re-train our models in a Weekend. I would love if it was a single repository, but inside 1 repository would be great. \nRe-train the model via CI/CD. We have a FrontEnd, API Backend => the layer communicates to the AI Model.  \n\n* DEPLOY in US East one - Could IP Filter the access request?  We can do that. => Not needed at the second.\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Wipro Accelerating Innovation Team \n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " You deal day-in and day-out with AI Initiatives. Collateralls - Stakeholders. AI Practices - Banking and financial services. \n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " 4 folks from Data Science team including leaders. \nWe have in-house MLOps Platform. \n\nSet-up context a bit on TrueFoundry => 2-3 mins overview.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Lambda - ETL written (AirFlow is trigerring Lambda) => Time is not a constraint. \nThere is a data on S3 => ETLs.\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Bhavesh and Abhinay - PhableCare. Abhinay - KGP 2016\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Lot of enterprise application - b2b to b2c. Data Science and Data Engineering.15 member team. Working on AI Problems in HealthCare. \n\nWould like to see our offering - much value right now to talk about what our set-up is. \n\n6 Member team in Data Science team, Fresh out of IITs , Abhinay comes with HealthCare Background. Problem Statement perspective - Computer vision and NLP. There are other things like Ecommerce problems - like recommendation and marketplace\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " DataBricks + AWS Combination we use. We will look at in terms of building our processes. We only got very recently. \nDev and model training on EC2 platform. We haven't completely transitioned on it. \nDepending on PS, we will look at those opportunitues.  \n\nDo you use Kubernetes at any point? Software Engineering \nOne of the models is in Production => it will be used by Internal teams. It is Batch Mode, Real Mode.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Abhinay: Working on some initiatives. \n\n* Volume and Scale - Deployment, will look at Scale.\n* In the real sense of world, it will not be huge. Will not need heavy infra behind it\n* Eventually we will increase capabilities and features. \n* Bunch of initiatives we have on paper \n\n\nDeployment of the Model was fairly easy - it was not complicated. All the pieces of puzzle built in. \nObviously I am sure there is a lot of opportunities. \n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " EC2 Auto-scaling? DevOps helps the different kind of configurations. Based on load and traffic, they manage.\n\nHow varied our Deployment process will be?\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Use cases : We repriortised. Focus is on reporting and Business Intelligence. Did deployment for models in Text recognition. \nWe revamped some of the busienss Operations. \nFocussing on getting the things done - see need for Location based prediction. Start with BI and then go to building Models. \n\nCentral Data team - work on. Citizen data scientists.Current focus is on dashboards and BI. Trying to get some of the data that we might need for models. Customer data etc. \n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Stack remains the same - GCP over Kubernetes. \nApplication side - MySQL DB / MongoDB as well, GCP - Kubernetes (Central data warehouse) , Using BigQuery as primary data base, Use MetaBase for Dashboarding. For deployment, we do batch deployment on Kubernetes. Text AI and Custom models.\n\nVertexAI is used to build models. Why not use Vertex AI for hosting models. We have a Production cluster and hence it is easier. Kubernetes is much easier. \n\nWe are using HevoData for most of the Pipelines. For data warehouse, we use schedule before usign Big Query. \nMany platforms are targeted for large teams. Both from cost and operations perspective! Makes sense for a bit larger teams. \n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Mostly use cases will take time to evolve - Doing only based on Need basis. Some of the Products are evolving and still evolving new features. \nWe are taking any critical need. Otherwise focussed on data Ops. \nIn 3-4 months, wil see more traction on the DS side. \n\nBUSINESS: Don't understand the use cases. Being ops driven business. Constant thing between Business and tech. \n\nA number of companies invest in Platform => 9 months. ML Engineers - start building the platform themselves. \nWhen data collection is itself becoming an issue => Basic Data Platform or ML Platform.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " A little bit about the Deployment Stack: Mohan and team currently working on the deployment side as well. \nWe have some ML Engineers - kind of data analysts trying to learn ML. \nThere, some of the data tools we are trying to bring up to speed. \n\nTill now, our idea is to use off-the shelf ML Models with our customization. Next step - identifying a lot of use cases, pick up models and then start using them. DEVOPS Team - Core thing is in kubernetes. \nThere is a centralised DevOps team that handles that. We have mainly 3 frameworks - Go, Ruby, Node. All frameworks have been set-up. \nWhen it comes to ML or so, its collaborative etc. SRE Team - collaborative thing => JIRA task etc.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " * Could we use it for any deployments or is it generic to Python? (Service, Job, Model => If you select a model, library of models we will keep adding) \n\nWhat do I need to provide to deploy a Model and create a service on top of it? \n\n* How is the cost controlled for the Infra?\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " We have clients take out data from unstructured data - documents, invoices. We understand data sources and try to create Models. \nChallenges: 1) Figure out which model to choose 2) How do we re-train or orchestrate it? \n(We do our cloud, client's cloud and orchestration) \n\nWhatever learning we get from a customer while working with them and not re-discovering. \nRAHUL - Customers who are using the code in Production. How do you showcase automated training is happening etc. \nWe are aware of most recent things. Best Practices in terms of continuous training and deployment happens in UAT. \n\nOUR: Its a truly horizontal capability. We end up targeting BFSI because of the background that Rahul and Abhishek has.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " In case a client is okay to use our own environment - samples are hidden. Everything has to be done in their environment (quite often). We do entire dev of the model in their environment. Except for the code, everything remains there. \n\nHow is the Deployment Orchestration? Jyoti can elaborate better => Entire solution we deploy using Docker. We develop the Docker containers. We don't have much expertise on Kubernetes clusters. Mostly limited on model side. Mostly deployment is via docker, either on client environment. CURRENTLY - Stack for Model building: Notebooks, ETL Pipeline etc? => We have dedicated notebooks on the dedicated environment and for running the traning. Our prediction environment is nothing but our Products. Evaluated models are deployed and integrated with the workflow. \n\nFlask API is the major way of deployment. \n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " We have moved to Docker based environment. We can support all te environments. \nDo you use any tool? We have automations in our existing flow, but not 100 %. Annotations, Model training and deployment etc => not automation. \n\n\nBase models are there: How the CI/CD could be integrated using MLOps. Do you use GitHub?\nTraining code is also pushed to GitHub. \nModel Registry: Using GCP as a model registry. \n\nIt is a SaaS product - so there is a GUI. Some cases, could be through that and some through API.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " Offline and Online Training: Offline is what is happening in a batch. Online is close to real-time use cases. \nBest cases where Online can happen. \nIs the platform support both offline and online ? \nHow do we see if the model is performing as per testing. \nHow is the testing managed? \n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Connect via Deepak, who is an angel. 18 years ago, there was only Data mining. \nBased in Iran and then worked in US for a couple of companies.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " 6Sense: MidSize. In 6Sense, WB - Internal tool that we use to deploy models.\nDS Leader - 1) DS Team has a good pipeline for feature engineering. Good platform - do feature engineering, do testing. When you have this capability => installation would be 50ms, in 6sense- it could be for a day. \nNeed to have understanding about data - Descriptive analytics. Then you start to build a model. What if there is a tool - looks at the data and tells you what model is best to use.  2) Some sort of metrics in market eg ADC etc - I prefer to lose at the percentile analysis of the model. 3) Its important to monitor the model => Auto-training I don't like. Only if distribution of data is changing, then re-train the model.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Data Engineering team does the Productionization in PayPal. In 6Sense, data scientists do it. \nIndustries where SLA is more important: That industry needs complicated platform. \nMy team writes SQL on a Hive. The Vinci is the internal system for deployment. In PayPal, we have something called TypeRest. \nImbalance classification is the hardest problem to solve in Fraud detection. \n\nCanvasAI: PayPal start-up\n\nIndustry which is tech oriented is hard to target. Industry where core is not tech. Don't spend too much on complicated the model.\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " We provide all the Production loads and DS. Tooling to help with offline experiments. \n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Data Science team is about 30 folks + 11 on engineering. Analytics is doing insight driven research. Domain for Insights - Marketing, product. Engineering - a lot of forecasting. Some personalization - delivery time predictor. \nPopularity prediction etc.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " We are on Nomad - not on Kubernetes OR managed AWS version of something. A lot of what we are doing is fairly bespoke. Highly coupled into the Production or application code. \nActual application is written in GO. \nFull Spectrum - AWS Compatible. Do you have Jupyter hub? Or SageMaker? Use Sagemaker only for compute supplements. We don't run things online in Jupyter hub. Increasingly, people are moving things to Hex. Hex connects to the data sources etc? We are very conscious of what we are giving HEX access to. Hex Data WareHouse and other data warehouse. \nAll of the application loads are running on Nomad. \n\nThere is no specific need - We have adopted tools as we found them :) Realised that they solve the problems. \n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Productionisation depends on Online or Offline - If Offline, we have a framework that the DS can themselves deploy something.\nRead from Data WareHouse, write back to Data WareHouse.\nOnline one - DS Engineering on the outset. We give them the title ML Engineers. \n\nInfra team gives a server. Use anything for tracking ? We have an internal A/B testing platform - we don't use that to re-train models. \n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " How does the system integrate will all the Software Sysems? \nWe are on top of Nomad. \nMonitoring could be one thing interesting - all is structured data. \nCurrent metrics: Drift, Outlandish predictions => Would want to track what? \nVolume of predictions you get: It differs significantly => highest volume of traffic ==> Avg - 300 predictions per event page field. \n\nNo of features: 10s -100s for each model.\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " They build on top of AirFlow. This is not expected to be a very relevant call. \n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " THIS CALL IS useful to understand how they have built around the niche or getting started use case. \nFor users to get started, it is not easy => Often time, people will pick something else without ever talking to you. \n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " What is your overall thing on top of Argo or KubeFlow? \nWhat's your ICP? What is your target customer? \nWe had the advantage of the AIRFLOW Community - we backed into that. We were using Airflow as the backend and we became savvy at using it. \n\nStart with small teams/Mid Market team: Platform could be a one stop shop. \nThe kubernetes users aren't going to be final users. INTEGRATION marketing is a good channel to go around. The more specific you can be is useful. \n\n1st thing that was valuable: Multitenancy as part of our value Prop. We will help you run as many airflows as you want. Very very specifically. MULTITENANCY PROBLEM. That became the heart of our customer messaging\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " AIRFLOW - How did you get into it? AirFlow is the backend of the product we built. Hired a lot of their top committers. \nGot the person from AirFlow as the advisor. It was very much earned. It was different - we have earned the right to commercialise a product we didn't start. \n\nDEPENDING on timing - right now, only DataBricks model. It is just AirFlow in the backend. \n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " VERY GOOD USE CASE - We have to convert them \n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " We have the GOOGLE bucket, we have all the config files in the same bucket. All models exposed through GRPC in the same box. \nRight now, its just a box with Kubernetes connected. \nCPU Inference time: Depends on the type of Model. The most similar one will be similar to inception network. Most complex ones have Million weights. Inference time is roughly how much? 1000 Batch requests - half a second to deliver the output in 1000 classifications. Not really real-time as the 1000 is available offline. \n\nThere are use cases where we do need real time as well. \n\nRoughly - what is the memory consumption of these kind of models - None of the models would take more than 2GB. GPU is T4. It is a pretty normal standard machine with 16GB of RAM. \n\nEND USAGE: 1) Web Portal => Upload the data => Get a report (Support)  2) Wearables and Sensors - which you wear with our app => pushed to the cloud.\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " * What is the Problem you are facing? 1) 100 Simultaneous requests if I do, it crashes immediately. Docker file has to re-start.  We don't know how much it can handle?\nLoad testing and optimization has not been there. But its critical. If a clinical trial is running, ML Service fails, no report generated => 1000$ for someone doing the trial. \n\n2) Authentication: I did a hack using Private and Public key. How do you ensure that GRPC end point is not open to anyone outside the Dev team. \n\nAPP Team is deploying on Kubernetes. 2 Questions here - Why not deploy on Kubernetes?\nIs the ideal state? One Service for Models? Or want different endpoints for each model?\nOne AUTHENTICATION is fine. Optimize Money and Reliability - Clubbing some of the bigger ones together and then the smaller ones in another. \nThe Real time one could be a separate Box itself. There inference time matters. But in batch, it doesn't matter - takes upto 5 minutes. \n\nTraffic pattern: Some models will have morning traffic. \n\nWhy not in Kubernetes? Within the company, very few people have access to the models. Only few people in development know deployment.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " Cost: Paying a $1000 bucks for a few customers.\nFor me, it doesn't make sense. \n100 Customers - We are paying 1000$ right now. \n\nRPS: Very Few requests ==> Clinical folks will identify a lot of files together. \n\nEVERY 6-7 months, it crashes a lot.\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " I have tried Cortex - its only on AWS. It is like wherever the credits go, we follow. We got new credits on AWS, so we can move to AWS. \nWas a year and half back - it wasn't as easy as the website says. \n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " To see what we have built is useful for them or not. AWS+ Kubernetes Stack. Amit - leads the DS Team (NLP focus). 7-8 people who work full time with us. Biggest KPI is push models to Production.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Atul - ML Engineer for 3 years. ML Pipelines. DS Part + Latency of deployments (2018) // Chintan - 5 years- lot of work on NLP and Semantic journey // Praful - 2022 grad => Intern - joined Full time and have been exploring full time. \n\nDifferent Product lines: Major focus is on collections. Beyond collections, also open to other parts. Real time as well as Non real time - how agents performed in the call, etc. Recent additions: Real time capabilities to guide the agents in the call. How to navigate the conversations in the most optimal way. Transcribing it in real time. If deviations, we prompt agents on how to say. \nSummarization model - that is purely powered by ML. Auto-submitted in the CRM system. There could be model to identify 1 marker or 10 markers at the same time.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Most stack is inhouse. AWS Stack. PyTorch and TensorFlow. Everything is dockerized. Kubernetes is the only thing we attach the models to. All models are GPU Instances or Sagemaker notebooks. \n\nWe push the image to ECR and then DevOps team comes into play. We use those data points to pull to a EKS cluster. Most services in Production have logging enabled - that's where we use CloudWatch services. \n\nWe usually set a SLA for making the endpoint available in production from the time image is ready. Instance selection logic => does the DevOps team decide? \n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Everty model - test it against 6-7 machines using SageMaker notebook. The numbers are published and it gives info to the consumers of model. \nFor Non real time, we don;t care about The latency as much. \n\nDo you use FastAPI? Or use Model servers? We have the backlog items for time. You work with things until that break. \n\nGold Standard is to train a model, containerize it and deploy it. Staging and Production environment? Does DevOps come in every piece ?\nModel Side: Data Drift, Model Drift and Concept Drift. Functional aspects - we realize mostly on users.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " We had been using EC2 instances for training =>started to adopt SageMaker as well. It saves us cost by charging us only for time when training the models. Sagemaker deployments - we tried, but it is higher than EKS. \n\nWe have used MLFlow and WandB as well for tracking experiments -> they are not a big part of our platform. \nSageMaker doens't do well on the Data capturing for NLP tasks. We want to have a tool that is integrated with the entire system. Annotation - label studio. We need to label it, process it and then go to training. \n\nUntil now, we had public endpoints - Enterprise customers care about data privacy.\n"}
{"prompt": "What is the feedback on the TrueFoundry product? \n\n###\n\n", "completion": " Answers on bottleneck -  \n\n1) SageMaker - have tried out different kind of deployments it offfers => What better we offer than SageMaker. \nTheir Annotation tool and entire ML Pipeline - lot less offering on NLP side. \nFor NLP models - do we offer something where annotation is done on the same machine etc.\nAre you more Cost effective than SageMaker? Batch, Real-time, Synchronous \n\nAMIT: \n\n1) Integration of Systems or Cohesive environment to work in => Collaborative way of working in a Problem without having to share Notebooks explicitly. \n2) Maintenance of models and improvement over period of time => Doing pretty well in terms of getting them on production. \n\nCHINTAN: 1) Optimization of the Model Inference. ONNX or Trition or any route we are goi\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Have been in work for 20+ years. First was in process engineering. Then did technical mkting for Kodex. Came to know about the audio-video space. \n\nThen realised that happy to know what happens in the marketing space. Wanted to be more technical. \n\nStarted with Network company, then with Stylumia - Vision and segmentation. \nThen with Financial mgmt space - Document extraction etc.\n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Video ML - Building ML Graphs, add descriptors for the videos. \nBread and butter is cloud based content delivery solutions. There is a lot of videos viewing that has been happening. A lot of people wanted to move to cloud on content side. \n\nOver 500 channels that we deliver. \nWe don't deliver or control the final UI - Roku sons and tv ==> content owner comes that I want to monetize. \n\nData Science: Long Tail content => there is very less metadata. Looking at the video, what could you infer about the video. It could talk about a historical documentary etc \n1) Where do you want to insert advertisements given a video? ==> Digital players - don't want to spend time\n\n2) Recommendation aspect - Nature of ad, Recommendation \n\n(Current size is 5 members + offers for 3 more people)\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " We use AWS, GCP, SageMaker, VertexAI - There is a separate platform team that helps in scaling. \nDelivering containers to the Infra team. They will build deployment containers on top. \n\nCurrent use cases is batch - unless business need, not looking to move to real time. Video files -> do annotations -> add to knowledge graphs. Live ue cases - looking to start. \n\nI don't hire people who come in with mindset that I only work on Jupyter notebooks. MONITORING PIPELINE, RETRAINING Etc. Hire people with CS skillset or people who are okay with taking these things up \n\nRarely work on Jupyter Notebooks. Mostly CLI using dockers extensively. Automated training - lot of parameter search. We are cloud agnostic - we haven't done distributed training. We use MLFlow, use DVC for dataset mgmt. For training, we use something wrapped around MLFlow - launch instances. \n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Would love to know about our offering and where we specialise in. Set up the entire ML thing from scratch. Transitioned from Principal Engineer to an architect role. \nRecently also report to Chief Architect. Looking at other aspects of Platform engineering like Chaos Engineering etc. \nSet-up practices, leave it upto the Individual teams to take it forward. \n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Started from Ground 0 => No way to interact with Company's data. How many trade orders were placed in the last week? Took 3-4 days to get access to the data. Very slow process was there. \n\nThe gap was produced because of the data exposure events. We ended up setting a Lakehouse from Scratch. AWS Cloud - Native AWS Services and frameworks we have built ourselves. From ML Side - actively from April, May and June of this year. \n\nTIER 1 - \n1) Associate Partner Incentivisation\n2) Trading Nudges program - Push the user in doing things (eg: Not understand the F&O Market)\n3) Loyalty Programs \n4) RFM Cohorts - How much active a particular user is? User classification and MArketing campaigns \n5) Churn Detection \n6) Re-activation campaigns 7) CLTV Prediction \n\nSearch is painstaking. Abhishek is someone who is trading the Equities. Don't show him ETFs. \n\nUser Base of 11 Mn users - not possible for us to priortize.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Search is not Typo Tolerant. Lot of personalization related objectives. \nWe are heavy on K8s- its the Green Field Project. Migrating all of our workloads to Kubernetes ==> Application part: Tier 1 services, more than 50% have migrated. \n\nData Science Team is Serverless - SageMaker Notebooks / BI is working out of Lambda Notebooks (EARLIER they were using R Studio) . Other application services - one of the top priorties for migreation to Kubernetes. \n\nAll of the models we have hosted are Batch Models and all the workloads are hosted on SAgeMaker. Why using SAgeMaker and not general Kubernetes Infra. Only have 1 MLOps engineer and rest all the team is Data Scientists. We want to see quick time to value - doens't matter if the infra is complicated or well managed. For setting up a new tribe, you need to show the ROI.  \n\nUSER Flow and MLOps Pipeline look like? \nWorkLoads are batch processes that run for 5-10 minutes in the day. \nEverything is on SageMaker itself. \n\nRequest is Product and Data Strategy team - we sit together and deliberate on it. Exploration phase - figure out the model or data => Feature Engineering is needed or not? Once done --> Deployment strategy is pretty standard. Feature generation job that runs based on trigger. We don't have ML Offering exposed to API EndPoint. ML Offering is not integrated with Frontend. We can support easily using SAgeMaker Endpoints. There are also challenges in serving those kind of responses\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " One of the features that Data Engineering team made live - 45 Ms for the latency for Data Science team. \nAs your portfolio value grows, your net worth also keeps on fluctuating. \n\nML Engineer is the one that does it. There is no custom deployment for each of the models. You just configure the final script tht the DS team has given and then there is a CI/CD pipeline. \n\nSageMaker notebooks => convert to script => commit to BitBucket. Jenkins automation jobs for CI/CD.\n\nWe have not reached the stage - at a stage where we are proving the ROI. These kind of problems can be solved using ML. Have gone through the same journey in Data Engineering. \nCost incurred in infra? 1st part of the journey.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " NIMBLEBOX.AI => They have been in touch with us for the past year. They have also built something similar. \n\nUnderstand the shortcomings of SageMaker - they are however not even relevant to us. Data Platform also we set-up => work with the AWS Service teams => we have a say in priortization of features as well. \nWe have a leverage working with AWS. \n\nWhy would I like to use TrueFoundry? Buy v/s Build => Its always because there is a time to market. \nHOW are we different from other guys as well? What's our X factor? \n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Major Questions going into the call: 1) Role of the platform team that maintains the central Infrastructure \n2) Monitoring and Debugging Capabilities - how does the re-training pipeline look like \n3) MLFlow, DVC for Dataset Mgmt. Something on top of MLFlow for Distributed Training \n\nOverall: 500-600 Team members\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " 1) Maintaining of Secrets in an easy way - who can launch machines, who can create more clones. Broadly as it stands now, we have unfederated access to GCP Resources. Platform team comes into picture in scaling. EVERYTHING related to Scaling is taken care of by the Infra team. \nContainerisation: Itself is not a big task - whatever dev environment we have, we use. We don't have to use new containers.\n\n2)  Fully on Kubernetes? - Evaluating DataBricks right now. BROADER Data Lake - common umbrella. \n\n3) We are not too much on Jupyter notebooks. Most of the work is on CLI\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Videos is stored in cloud env. Most of it is CLI. Dev - mostly Remote. OnPrem GPU ==> Its only when work overflows the OnPrem GPU, we look to train on Cloud Env. Mostly use Spot instances for training. \nHow do you spin training? We go ahead and get started. \n\nMostly using state of Art Models. We use SageMaker and their Model API. And that works. Where we are at: If we get a 24 CPU Ram Machine - the moment it starts overflowing, we will look to have a separate server. \n\nInternal infra to launch these runs. Video files are put on S3? You mount them and download them over the network. \n\n\nThere is a Job Queue => processing the jobs ==> In between capturing metrics that are stored in MLFlow. \n\nOverFlow of the Models Memory during Inferenencing. Suppose there are several aspects we can get out of the Video. Currently the Overflow doesn't happen. 16GB Instance is good enough.\n"}
{"prompt": "What are the most common questions asked about the product? \n\n###\n\n", "completion": " Where does DBricks fall short?\n\n9902264196\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Was earlier also at MU Sigma - Decision Sciences division. Abhishek - hands on Data Scientist kind of guy.\nMaverik - working with MuSigma. Will come to what we were doing at other places. \n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " In other places, there was no platform - custom build models using python. Clean data, Univariate, BiVariate. \nHere at Maverik - Partnership with AutoML Product Dataiku => getting people up to speed. \nThat adds a lot of value to businesses - EDA much better for businesses. We work with only banks. \n\n\nProblems- 1) Predicting default for loans 2) Internally - Home Pricing prediction in the USA 3) Data gets updated, etc. 4) Cross sell, Upsell, Marketing analytics for Wealth Mgmt firm.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " Python and R \nDataiku - lot of wrappers around Python and R \nAdd a trigger in terms of deployment \n\nIsraeli company: BigPI => Help build pipelines 5-6 times faster. Visually driven. Showed a demo\n\nDo you serve the models to clients?\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Banks have their own version of AutoML they use - H20, Dataiku, etc. \nPipeline based models? Data Engineers on the team - who builds pipelines. \n\nData Pipelines are also executed on Dataiku. \nThey have recipes - Azure teams // Teams who connect snowflake to Tableau.\nData Side - will be 350 people \n\nIf you want to do heavy computation, do it on our own database. In 1 linux server, there is Dataiku for example.\n"}
{"prompt": "Give me a brief about the company and the individual? \n\n###\n\n", "completion": " Get the list of features => Library where it computes the features. Feature library is built from beginning. \n\nOffline also - we recompute features. Feast is being used. \nDockerize => ? (Go CD pipeline is there, Everything is pushed to Git) \n\nEndPoint is generated -> Engineering team integrates the APIs. \n\nDev, QA and Prod => Good promotion flows. Check for a few users and see. Even before going, Model evaluation process. \n"}
{"prompt": "What are the most important use cases and types of models for Machine Learning? \n\n###\n\n", "completion": " Model Evaluation: Business teams with statistic background. They will check and give a go-ahead. 1 month of Model evaluation phase. ==> Even before Dev => do the analysis. \n\n\nProd: Couple of days, it runs in Shadow, Monitoring and Drift. \nShadow => Roll out \n\nOnce deploy to Prod, everything is by Engg team. Once you deploy, dev effort is done. \nWe were using Drift ourselves - pipelines get triggered at end of day => performance metrics and Drift metrics \n\nLet's take use case of lending // Re-training: We have automated the pipeline for daily re-training => acceptance crtieria is set. \nWe push the pickle file to S3 => CI/CD pipeline is triggered from DataBricks => As soon as it puhes to docker => then it will download, containerise.\n"}
{"prompt": "What are the commonly used tech stack for ML deployments and pipeline? \n\n###\n\n", "completion": " USE OF GPUS: Only for training, not for inference. Distributed training might be needed. \n\nMODEL ENDPOINTS: Flask/BentoML. Few we do with Scala. Some cases, it will not work. Neural net complex model - wont work and we go ahead with BentoML. \nModel servers are not needed. \n\nMONITORING: Grafana, Prometheus, etc\n"}
{"prompt": "What are the common challenges and what solutions are people looking for? \n\n###\n\n", "completion": " Process is well set- underlying data => there is a problem or something. \nPrimary Use case: Lending use case - we will not see the results today; Results will come with a lag to us. 6 Months -12 months. \n\n6-12 Months back data: We use for model training. \n\nPipelines have to adjust to the change to ingest the data. PIT data is very important => How do you ensure this? \n\nDataBricks - Migrating to EMR Mainly. Being able to run daily jobs on EMR. \n\nRESOURCE MGMT: Some tags assigned to every resource being assigned. \nCOST VISIBILITY: Everyday report for last 7 days to 30 days => what's the cost your team has incurred. \n\nINFRA team is really good here. Visibility to things is really good. All business metrics, all ML metrics is published. Tableau, Metabase.\n"}
