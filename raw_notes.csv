Record ID,Deal Name,Associated Contacts,Title,Associated Company,Company Size,Revenue,DS/ML Team Size,Cloud,Use of Kubernetes,Industry,Location,Deal owner,Notes,Recordings,Raw Notes
12428315429,GEP Worldwide,,,GEP,>1000,>1B,25-50,,,Business Consulting and Services,"Clark, New Jersey",Nikunj Bajaj,,,
12428091307,Automation Anywhere,(11.rammohan@gmail.com),,"Automation Anywhere, Inc.",>1000,>1B,Oct-25,,,Software Development,"San Jose, CA",Nikunj Bajaj,,https://app.fireflies.ai/view/Ram-Automation-Anywhere-TrueFoundry-Multi-cloud-hypothesis-::KQG1PT3vxZnZ0M14,
12428091030,Sanofi,,,Sanofi,>1000,>1B,>50,,,Pharmaceutical Manufacturing,"Paris, France",Anuraag Gutgutia,Sanofi_Deep 07-03-2023,https://app.fireflies.ai/view/TrueFoundry-Deep-Sanofi-::rxnTDW7L741dXgYE,
12428314989,DocuSign,Gandharv Kapoor (gandharv.kapoor@gmail.com),,DocuSign,>1000,>1B,>50,,,Software Development,"San Francisco, CA",Anuraag Gutgutia,,https://app.fireflies.ai/view/Meeting-with-Anuraag::WGmFNERUuPrvrqyS,
12362100837,Max Life Insurance,Divyan Kavdia (divyan.kavdia@maxlifeinsurance.com),,Max Life Insurance,>1000,>1B,Oct-25,,,Insurance,"Gurugram, Haryana",Nikunj Bajaj,,,
12362155036,Zycus,Annu Sachan (annusrcm@gmail.com),,Zycus,>1000,100 - 500 Mn,25-50,,,Software Development,"Princeton, NJ",Nikunj Bajaj,Zucus_Annu 01-03-2023,https://app.fireflies.ai/view/Annu-Sachan-Zycus-TrueFoundry-Infra-related-pain-points::5gzAiF5Eaj,
12362154897,Intuit,,,Intuit,>1000,>1B,>50,,,Software Development,"Mountain View, California",Nikunj Bajaj,,,
12362100552,Inmobi,Kalyan Deepak (kalyan.deepak@inmobi.com),,InMobi,500-1000,500 - 1B,25-50,,,Advertising Services,Singapore,Anuraag Gutgutia,Inmobi_Kalyan 28-02-2023,https://app.fireflies.ai/view/TrueFoundry-ML-infra-Inmobi::e4GDfdZd8Lc4CkK1,
12361625352,Medtronic,Pratik Agrawal (pratik.j.agrawal@medtronic.com),,Medtronic,>1000,>1B,>50,,,Medical Equipment Manufacturing,"Minneapolis, MN",Anuraag Gutgutia,Medtronic_Pratik 03-03-2023,https://app.fireflies.ai/view/Follow-up-Pratik-Medtronic-TrueFoundry::1OosVoidEvirYJk0,
12361625221,Resolve AI,anjul anjul@cloudraft.io (anjul@cloudraft.io);ub ub@rezolve.ai (ub@rezolve.ai),,Cloudraft I/O,<50,<10 Mn,<10,,,Financial Services,"Sunnyvale, California",Anuraag Gutgutia,Resolve_Anjul 23-02-2023,https://app.fireflies.ai/view/TrueFoundry-discussion-with-Rezolve::pUN9J8dV18wnZEu0,
12361625221,Resolve AI,anjul anjul@cloudraft.io (anjul@cloudraft.io);ub ub@rezolve.ai (ub@rezolve.ai),,Cloudraft I/O,<50,<10 Mn,<10,,,Financial Services,"Sunnyvale, California",Anuraag Gutgutia,,https://app.fireflies.ai/view/TrueFoundry-CloudRaft::ErbKHN65GFX1kCDx,
12361625175,Novartis 1,Vinodh Venkatesan,,Novartis,>1000,>1B,>50,,,Pharmaceutical Manufacturing,"Basel, Baselstadt",Anuraag Gutgutia,,https://app.fireflies.ai/view/TrueFoundry-Vinodh-Novartis-::MssY42PS51Wr74Fs,
12361625175,Novartis 2,p_kowalik2 p_kowalik2@outlook.com (p_kowalik2@outlook.com),,Novartis,>1000,>1B,>50,,,Pharmaceutical Manufacturing,"Basel, Baselstadt",Anuraag Gutgutia,,https://app.fireflies.ai/view/TrueFoundry-Piotr-Novartis-::xECqUiWBH9aP5UOt,
12339635717,HighRadius,Chandim Sett (chandimsett@gmail.com),,HighRadius,>1000,100 - 500 Mn,>50,Multi Cloud,,Software Development,"Houston, Texas",Nikunj Bajaj,,https://app.fireflies.ai/view/Nikunj-TrueFoundry-Chandim-HighRadius-::IUcEp9XGQMxY7891,
12339423649,Stryker,mayank.kumar mayank.kumar@stryker.com (mayank.kumar@stryker.com),,Stryker,>1000,>1B,>50,,,Medical Equipment Manufacturing,"Kalamazoo, MI",Anuraag Gutgutia,,https://app.fireflies.ai/view/TrueFoundry-Mayank-Stryker-::aCqR6T4MYnuPyGSj,
12336403532,Vmware,dennemanf dennemanf@vmware.com (dennemanf@vmware.com),,VMware,>1000,>1B,>50,,,Software Development,"Palo Alto, CA",Nikunj Bajaj,,https://app.fireflies.ai/view/Frank-Denneman-Vmware-TrueFoundry-Multi-cloud-hypothesis::6ZXp7Y1QC7M2jXnN,
12339422352,Makemytrip,goldee goldee@gmail.com (goldee@gmail.com),,MakeMyTrip,>1000,100 - 500 Mn,25-50,,,Software Development,"Gurgaon, Harayana",Nikunj Bajaj,,https://app.fireflies.ai/view/Goldee-Udani-Makemytrip-TrueFoundry-Multi-cloud-hypothesis::NV6lzcF2iiKnh2hw,
12336402979,Quora,Lida Li (lilida@lilida.org),,Quora,100-500,10-50 Mn,>50,,,"Technology, Information and Internet","Mountain View, CA",Nikunj Bajaj,,https://app.fireflies.ai/view/Lida-TrueFoundry-Multi-cloud-hypothesis::cTWYyWphru,
12339399214,Dr.Reddy's,anishagarwal anishagarwal@drreddys.com (anishagarwal@drreddys.com),,Dr. Reddy's Laboratories,>1000,>1B,Oct-25,,,Pharmaceutical Manufacturing,"Hyderabad, TS",Nikunj Bajaj,,https://app.fireflies.ai/view/TrueFoundry-Dr-Anish-Dr-Reddy-s-::2Ip15PbGg1W1Prkx,
12339295384,24]7.ai,Mandar Mutalikdesai (mandar.mutalikdesai@247.ai),,24-Jul,>1000,>1B,>50,,,Software Development,"San Jose, California",Nikunj Bajaj,,https://app.fireflies.ai/view/Mandar-Mutalikdesai-24-7-ai-TrueFoundry-Multi-cloud-research::LzBzygxn6YjXAOmJ,
12249609647,Locus,desaiankitb desaiankitb@gmail.com (desaiankitb@gmail.com),,Locus Technologies,100-500,<10 Mn,<10,,,Software Development,"Mountain View, CA",Nikunj Bajaj,,https://app.fireflies.ai/view/TrueFoundry-Ankit-Locus-::xid91M9BRw,
12249630284,Balbix,Muthuswamy Sankarapandian (muthu.sankarapandian@balbix.com),,Balbix,100-500,<10 Mn,Oct-25,,,Computer and Network Security,"San Jose, CA",Nikunj Bajaj,,https://app.fireflies.ai/view/-Muthu-Balbix-Nikunj-TrueFoundry-::dpxyrfReHk,
12249629853,FAB Bank,puneet.gupta@bankfab.com,,First Abu Dhabi Bank,>1000,>1B,Oct-25,,,Banking,"Abu Dhabi, Abu Dhabi",Anuraag Gutgutia,,https://app.fireflies.ai/view/TrueFoundry-Puneet-FAB-Bank-::lqefPfiXKNI5QFkw,
12249629787,Lazada,goseng123 goseng123@gmail.com (goseng123@gmail.com),,Lazada,>1000,500 - 1B,>50,,,"Technology, Information and Internet","Singapore, Singapore",Nikunj Bajaj,,,
12150878758,DeHaat,Ashutosh Parida,,DeHaat,>1000,500 - 1B,<10,,,IT Services and IT Consulting,"Gurgaon, Haryana",Anuraag Gutgutia,DeHaat_Ashutosh 07-02-2023,,
12150828398,SalesForce,smeadusani smeadusani@salesforce.com (smeadusani@salesforce.com),,Salesforce,>1000,>1B,>50,,,Software Development,"San Francisco, CA",Anuraag Gutgutia,SalesForce_Senthil 15-02-2023,https://app.fireflies.ai/view/Senthil-TrueFoundry-Research-WorkFlow::U7j58c9ZAfsssYWt,
12150828398,SalesForce,smeadusani smeadusani@salesforce.com (smeadusani@salesforce.com),,Salesforce,>1000,>1B,>50,,,Software Development,"San Francisco, CA",Anuraag Gutgutia,SalesForce_Srinath 13-02-2023,https://app.fireflies.ai/view/Srinath-SalesForce-TrueFoundry::4lGTrJaCGB,
12150828333,Dashtoon,Soumyadeep (soumyadeep@dashtoon.com),,Dashtoon,<50,,<10,,,"Technology, Information and Internet","Sunnyvale, California",Anuraag Gutgutia,Dashtoon_Soumyadeep 13-02-2023,https://app.fireflies.ai/view/Soumyadeep-Dashtoon-TrueFoundry::gm5rCKzQFt,
12150845733,CVS Health,happy.jain happy.jain@gmail.com (happy.jain@gmail.com),,CVS Health,>1000,>1B,>50,,,Wellness and Fitness Services,"Woonsocket, RI",Nikunj Bajaj,,https://app.fireflies.ai/view/TrueFoundry-Himanshu-CVS-Health-::sBlS0GkhRH,
12150738858,GreyCroft,Nick Crance Nick Crance (nicholas@greycroft.com);Mark Terbeek (mark@greycroft.com),,Greycroft Partners,#N/A,#N/A,#N/A,#N/A,,#N/A,#N/A,Nikunj Bajaj,,https://app.fireflies.ai/view/Meeting-with-Nikunj::i9kE3C7z6w,
12150677061,Tata Digital,Amit Haralalka (amit.haralalka@tatadigital.com),,Tatadigital,500-1000,>1B,25-50,,,"Technology, Information and Internet","Mumbai, Maharashtra",Nikunj Bajaj,,https://app.fireflies.ai/view/Amit-TrueFoundry::FBnVWcDNpc,
12150676919,CVS Health,Amaresh Siva (amaresh.siva@cvshealth.com),,CVS Health,>1000,>1B,>50,,,Wellness and Fitness Services,"Woonsocket, RI",Nikunj Bajaj,,,
12150737763,Nykaa,Shivam Agarwal (shivam.kgp@gmail.com),,Nykaa,>1000,100 - 500 Mn,<10,,,Internet Publishing,"Mumbai, Maharashtra",Nikunj Bajaj,,,
12044422255,Novartis,vinodh.iitm vinodh.iitm@gmail.com (vinodh.iitm@gmail.com),,Novartis,>1000,>1B,>50,AWS,Yes,Pharmaceutical Manufacturing,"Basel, Baselstadt",Anuraag Gutgutia,,https://app.fireflies.ai/view/TrueFoundry-Vinodh-Novartis-::MssY42PS51Wr74Fs,
12044422040,Stylumia,Sharath Puranik Puranik (sharath.puranik@stylumia.com),,Stylumia,<50,<10 Mn,<10,AWS,No,Software Development,"Bangalore, Karnataka",Anuraag Gutgutia,,https://app.fireflies.ai/view/Sharath-Stylumia-TrueFoundry::WrqrxRzaus,
12044331076,Careem,Abhinav Johri (abhinav.johri21@gmail.com),,Careem,>1000,500 - 1B,Oct-25,Multi Cloud,Yes,IT Services and IT Consulting,Dubai,Nikunj Bajaj,Careem_Abhinav 03-02-2023,https://app.fireflies.ai/view/Abhinav-Careem-Nikunj-TrueFoundry-::ca1TtPO5NRTTYgkN,
12044330588,Wadhwani AI,soma soma@wadhwaniai.org (soma@wadhwaniai.org);yash yash@wadhwaniai.org (yash@wadhwaniai.org),,Wadhwani AI,100-500,10-50 Mn,25-50,Multi Cloud,Yes,Research Services,"Mumbai, Maharashtra",Anuraag Gutgutia,https://cheem.notion.site/Wadhwani-AI-17dbab3664e242c7b012aaeb35485c14,https://app.fireflies.ai/view/TrueFoundry-WadhwaniAI-Optha-Demo::m2BN1dc8D04r8xvY,
12044330588,Wadhwani AI,soma soma@wadhwaniai.org (soma@wadhwaniai.org);yash yash@wadhwaniai.org (yash@wadhwaniai.org),,Wadhwani AI,100-500,10-50 Mn,25-50,Multi Cloud,Yes,Research Services,"Mumbai, Maharashtra",Anuraag Gutgutia,https://cheem.notion.site/Wadhwani-AI-17dbab3664e242c7b012aaeb35485c14,https://app.fireflies.ai/view/TrueFoundry-WadhwaniAI::eVlJuDQgYG,
12044330588,Wadhwani AI,soma soma@wadhwaniai.org (soma@wadhwaniai.org);yash yash@wadhwaniai.org (yash@wadhwaniai.org),,Wadhwani AI,100-500,10-50 Mn,25-50,Multi Cloud,Yes,Research Services,"Mumbai, Maharashtra",Anuraag Gutgutia,https://cheem.notion.site/Wadhwani-AI-17dbab3664e242c7b012aaeb35485c14,https://app.fireflies.ai/view/WadhwaniAI-TrueFoundry-Introduction::elFVzeDoTJ,
11984272003,HDFC Credila,Shashank Agrawal (shashank@hdfccredila.com),,HDFC Credila,,50-100 Mn,>50,AWS,,Financial Services,"Mumbai, Maharastra",Nikunj Bajaj,HDFC Credila_Shahank 24-01-2023,https://app.fireflies.ai/view/Shashank-Nikunj::68dHrs0j9Z,
11984271522,Legato,Amrish Kumar (amrish.kumar@legato.com);harsha.bs@legato.com,,Legato,>1000,10-50 Mn,>50,Multi Cloud,Yes,IT Services and IT Consulting,"Indianapolis, Indiana",Nikunj Bajaj,,,
11942287761,Soroco,george george@soroco.com (george@soroco.com),,Soroco,100-500,50-100 Mn,Oct-25,Multi Cloud,Yes,Software Development,"Boston, Massachusetts",Nikunj Bajaj,,https://app.fireflies.ai/view/George-TrueFoundry::QfK1FsW01K,
11942347136,IIFL,Arihant Jain (arihant.jain@iifl.com);Lakshay Malhotra (lakshay.malhotra@iifl.com),,IIFL Finance Limited,>1000,500 - 1B,<10,AZure,No,Financial Services,"Mumbai, Maharashtra",Anuraag Gutgutia,IIFL_Arihant 31-01-2023,https://app.fireflies.ai/view/IIFL-TrueFoundry::JOAGpUcy26,
11942346630,GAP,Ojas Nivsarkar (ojas_nivsarkar@gap.com),,GAP,>1000,>1B,<10,GCP,Yes,Retail,"New York, NY",Anuraag Gutgutia,,https://app.fireflies.ai/view/Ojas-Gap-TrueFoundry-::Rno44An12e,
11773896093,PayU,Yusuf Firoz (yusuf.firoz@payufin.com),,PayU,>1000,>1B,25-50,AWS,Yes,Financial Services,"Amsterdam, North Holland",Nikunj Bajaj,PayU_Yusuf 18-01-2023,https://app.fireflies.ai/view/TrueFoundry-Yusuf-PayU-::ULdnX2UfeFap38z1,
11773895754,Quantiphi,Harshini Infanta (harshini.a@quantiphi.com);Ritesh Patel (ritesh.patel@quantiphi.com),,Quantiphi,>1000,>1B,>50,AWS,Yes,Information Technology & Services,"Marlborough, Massachusetts",Nikunj Bajaj,,Not Recorded,
11656830826,Procore,Vaibhaw Chandel (vaibhaw.chandel@procore.com),,Procore Technologies,>1000,500 - 1B,25-50,AWS,Yes,Software Development,"Carpinteria, CA",Nikunj Bajaj,Procore_Vaibhaw 24-01-2023,https://app.fireflies.ai/view/TrueFoundry-Procore-Vaibhaw-::HB17cVFNGd,
11656830826,Procore,Vaibhaw Chandel (vaibhaw.chandel@procore.com),,Procore Technologies,>1000,100 - 500 Mn,25-50,AWS,Yes,Software Development,"Houston, Texas",Nikunj Bajaj,Procore_Vaibhaw 10-01-2023,https://app.fireflies.ai/view/TrueFoundry-Procore-Vaibhaw-::rqVrpfBNKP,
11656829007,HighRadius,soudhakar@gmail.com,,7701145756,>1000,>1B,>50,Multi Cloud,Yes,Renewable Energy Semiconductor Manufacturing,"Fremont, California",Nikunj Bajaj,,https://app.fireflies.ai/view/TrueFoundry-Soudharkar-HighRadius-::2ygHJhXBDL,
11656830384,Enphase Energy,vvenkata vvenkata@enphaseenergy.com (vvenkata@enphaseenergy.com),,Enphase Energy,500-1000,50-100 Mn,<10,AWS,Yes,Software Development,"Palo Alto , California",Nikunj Bajaj,,https://app.fireflies.ai/view/Vishnu-Emphase-TrueFoundry::WsGZOfKXwd,
11655737135,Uniphore,Vinod Muthukrishnan (vinod.muthukrishnan@uniphore.com);balaji.raghavan@uniphore.com,,Uniphore,500-1000,10-50 Mn,Oct-25,Multi Cloud,Yes,Financial Services,"Bangalore , IN",Anuraag Gutgutia,,https://app.fireflies.ai/view/Vinod-Balaji-True-Foundry::RnDIG489Ne,
11655736993,Navi,gujju.chanakya@gmail.com,,Navi,>1000,>1B,Oct-25,Multi Cloud,Yes,Retail,"Dubai, AE",Anuraag Gutgutia,,https://app.fireflies.ai/view/Chanakya-Navi-TrueFoundry::X7EshktRez,
11655736739,Landmark Group,Mohan Sb (mohan.sb@landmarkgroup.com);kunal.kumar kunal.kumar@landmarkgroup.com (kunal.kumar@landmarkgroup.com),,Landmark Group,>1000,>1B,<10,GCP,Yes,Financial Services,"Mumbai, Maharashtra",Anuraag Gutgutia,,https://app.fireflies.ai/view/TrueFoundry-Landmark-Group::hs2cMDB13Q,
11655736677,Kotak,deepak.m.sharma@kotak.com,,Kotak Mahindra Group,100-500,100 - 500 Mn,Oct-25,AWS,No,Software Development,"Santa Clara, California",Anuraag Gutgutia,,https://app.fireflies.ai/view/Meeting-with-TrueFoundry-Optimize-Data-Science-Machine-Learning-Deployment::ewH42UEi3n,
11655736593,EightFold,Tushar Makkar (tmakkar@eightfold.ai),,Eightfold AI,>1000,100 - 500 Mn,<10,AWS,Yes,Financial Services,"Bangalore, Karnataka",Anuraag Gutgutia,,Not Recorded,
11655716664,Slice,Nitin Basant (nitin.basant@sliceit.com),,Slice,,,Oct-25,AWS,No,,,Anuraag Gutgutia,Slice_Nitin 25-01-2023,https://app.fireflies.ai/view/Slice-Truefoundry::Kkh1qqB0PJ,
11655716664,Slice,Nitin Basant (nitin.basant@sliceit.com),,Slice,,,Oct-25,AWS,No,,,Anuraag Gutgutia,Slice_Nitin 29-12-2022,https://app.fireflies.ai/view/TrueFoundry-Slice-Nitin-::9x8TKKbOcw,
11577360522,Advata,Vikas Kumar (vikas@advata.com),,Advata,100-500,10-50 Mn,Oct-25,AZure,Yes,Software Development,"Bellevue, Washington",Nikunj Bajaj,,https://app.fireflies.ai/view/Meeting-with-Nikunj::ruRSecxGN4,"Hello? Hey. Hikarsh. 



Hi. 



Yeah, I'm able to hear you. Are you able to hear me? 



Yes, I can hear you. 



Thanks a lot in time for the call and very Happy New Year. Vikas. 



Yes, Happy New Year too. And I'm not sure if Nickwind is on the call but Happy New Year to Nicole's too and your team. 



Nick, Andrew is not able to join today. He's on the road unfortunately. I'm taking the call in place of him. 



No worries. 



Yeah. So Ricashi, I can maybe give a quick introduction for myself as well and just to also set the agenda. The purpose was to learn from you as to how AI functions in healthcare where you have a lot of experience and specifically with respect to what I would love to know about the current ML stack, the use cases, the challenges you are seeing or what exactly you are aiming to achieve in the next six months. Also will share a brief background of what we are building at Cook Boundary. 



Sure. Yeah, I'm looking forward to hear about that. On my side, I can probably accordingly, as you said, can do the introduction and can share a few stuff where we are probably looking forward in three to six months. And even here probably just given some of the focus area we are looking at. 



Sounds perfect. Cool. So just to introduce Amanda, one of the co founders at Tru Foundry. Nikkunj and Avishek are my other co founders. Nikkunji is based out of the Bay Area generally and Vishek and myself, we are in Bangalore india. So basically our history goes back like 13 years because were bachelorettes at college in AIT Kharagpur and then Nikon and Abhishek went towards different path. Nikkunj was on the ML side for like seven years with his last team at Facebook and Abhishek on the other hand let the software engineering and the systems team at Facebook also let the videos are there. So he brings experience on the infrastructure side of things. I on the other hand, used to work for a hedge fund called Worldwide. It's a US. Based fund. 



So initially I was india office building trading strategies across global markets using a lot of data. And fintech is another area where there's a lot of compliance and other things just like what you see in the healthcare domain property. So have some experience on that side and then went to us as a part of the CEO of the various strategic initiatives as well as was leading portfolio management for them for one of the asset classes. And then in 2020 we all left our jobs. We wanted to build something together, came to India and we built our first startup in the talent space which we sold to Infoge which is one of the big talent companies india. And during that time were building models and when we kind of built models the productionization of the models was very difficult. 



Like we had to assemble a lot of open source, learn about the cloud and so on. And it took us almost 30 days to reach a good stable state of production with reliability. And were discussing as to the kind of infrastructure Nikunjana Vishak saw at Facebook and that is when, you know, we thought that if we can bring that to every company that will be great. And then after selling entire we basically started True Foundry with that goal that will help companies move fast with data in terms of testing out and deploying models to production as well as monitoring them in the right way without much efforts needed from developers. So that's like at a very short level as to my background as well as a brief about Truefoundry, happy to answer questions. 



Oh Ivan, that's great and a great journey. Looks like you guys have gotten some good experience to actually lay out for the True Foundry. And probably I'm hoping that the company is doing well and you guys are able to attract some of the kind of right users and customers for the product itself. 



Thanks. We are in very early stage, like a team of 20 members and we are looking to actually work with companies like we are working with around five to six different companies across them and one on the healthcare side itself. And we wanted to kind of talk to folks like you to learn more and potentially even see if there's a possibility potential to even work together in solving some of the problems that you are. 



Yes, I'm sure probably about healthcare. It can go beyond 30 minutes, but hopefully I can reflect back a little bit on what some of the specific challenges on healthcare are. Okay, I can go ahead with my introduction quickly. I'm at Advertiser. It's a healthcare machine learning company. The focus for the company has been in multiple areas, but right now it is primarily around pre operative care and revenue cycle management, which is the post operative care. So these are making about what should happen with a patient before any service or care is given as well as once the service or care is given. In us especially, that problem is like how account receivable, which is basically how the payment for the patient has to happen and that the entire area is called revenue cycle management. 



So it is not necessarily a clinical decision making, but much more from a CFO finance perspective of how all the money can be recuperated. So that is where Advata focus is from my perspective. I am leading the research and machine learning.org at Edwater. It is actually five years now. Joined Kensai, what we used to be known as in 2017, located in Seattle area. I am also based out of Seattle area and as part of that experience, of course, healthcare is very wide and deep both. So I have worked in both the pre operative care, the post operative care, as well as intra, which is like what this needs to be taken while a patient is in the hospital or given a care. 



So given all those different journeys right now my focus has been around mostly the joint replacement as a domain area and especially thinking about what and how we can actually enable the clinical decision making to better for related to joint replacement. But over a period of time, as you can imagine, similar problems, similar challenges and whenever we talk about ML, we face the same challenges. But healthcare comes with its own unique flavor and the flavor especially includes one. The diversity of users is very high. Like as you can imagine in healthcare, the depth of knowledge for a particular domain is usually pretty good. The users are pretty well read, pretty well acquired, whether it's a math side of the thing or whether it's the literature side of the thing or including the patient people perspective. 



So clinical side of the world is pretty well aware of all those challenges as well as the depth that is needed. The problem that happens is that communication or the language is not the same. As we would like to communicate data in terms of tables and let's say even through GitHub codes for reviews, it does not work with the users who still have to review certain data elements to ensure that the clinical insights are true. One cannot send it to a doctor hey, can you review this code in a GitHub? So it creates unique challenges on how actually one can create a platform. In other words, journey of data from input to output in a way where it is both well managed but also well validated because validation is very important part of the data. 



It's not a consumer data, it's not something we can understand either. There are multiple codes that get used in healthcare, very domain specific parameters that exist and those are the specific, I would say unique characteristic of health care that presents very unique challenges and over the last few years have been working on various challenges and now I think we are at a pretty good position. But still I would say far from saying that there is one flavor of healthcare ML platform that can work, it is still far from there. Yes, apart from that, just before I joined what was known as Kensi before so we got acquired and hence the renaming and rebranding happened. I completed my PhD at the University of Minnesota in the area of recommendation system. 



So I had a chance to work with all the kind of multimedia domains music, movies, images, geosawan actually was one of my first open data that I worked with along with the Savan team that used to be based in the Bay Area, actually in Mountain View. So yeah, that is where my recommendation system journey as such it started and it was very beautiful. I enjoyed it. But at some point I was like, healthcare kind of got exciting. So I kind of moved a little bit of application area into healthcare. 



One question and one point before that. Nicole actually would love having conversation with you. I think he actually specialized in the recommendation system side as well when he was working with Reflection, which is an e commerce company. And they were trying to build recommendation systems for a lot of different ecommerce and kind of worked on ML for data to like 600 million users. So I'm sure you'll have a lot of good things to do. 



And prior to that, I was india. I was not very far from Karakpur campus. I was in NIT Rawkilla, my bachelor's there. I remember went to Raw Kila. So in 2007, I believe, when String came there as a band to perform incorrect. We went to see that band in your culture fest and whatnot I'm not sure if you guys were there at that time or not. 



Not in 2007. We used to call it SpringFest, by the way, the culture I joined 2009. 



Okay. 



But I'm sure it was a very popular thing. Every January. 



At least the east side of the thing of India. If spring fest are happening in Karakwood and if some big celebrities coming, it would usually attract a lot of distributors around there. 



Yeah, yeah, that's it. I would love to hear more on Advertise and the kind of use cases you already mentioned. But when you mention from the challenge perspective, I do understand a lot of challenges leaders to the model building side of things. How about the downstream? Like in terms of once the model is testing it out and then deploying it and monitoring it, how does that happen? 



A lot of it is still through. Like you can imagine, versioning of the code is pretty well and as you can expect across the industry of the code and the testing of the code is pretty well done because GitHub allows that to do. But the burdening of data and the testing of data is where I would say we are in like, let's say if there are three stages and stage three is a very mature stage, then we are in stage one. And the industry is probably, I would say in health care. I don't think I have seen anyone beyond stage one either. Stage one is where basically foundations are being led by associating certain quality of data by after certain preprocessing processing and whatnot. But it is still very non standard. 



Like I said, everyone has their own way of doing and dealing with the data. But the code side, as you can imagine, using GitHub as your pipelines and GitHub workflows together along with Docker and Kubernetes, you can get all the versioning testing, deployment pretty much in place. I saw the Truefoundry video and I was just looking at the documentation side on the website, it definitely makes sense given how probably easy it is to integrate it. It gives me a little bit of reminder of how ML flow generally has been designed. So I believe that is also I'm not sure if you would say so, that it is very close to what the ML flow looks like. 



So I think just to add there like ML flow basically primarily what I have seen at least with us, and correct me if you have seen something else, that people use it more from the experimentation perspective. Basically building models like comparing different versions of model while they are still in the training stage and seeing which metric is good and so on. Accordingly thinking that this is so our platform is more focused on the deployment side. Like as you said, you use probably Kubernetes for deployment. So for example, a data scientist or an ML engineer, if they are building a model or training a model, how do you orchestrate that training very easily over Kubernetes clusters and you have actually completed the training, then obviously comparison of the experiment is also there. 



But then how do you take any version of the experiment and then deploy it in a testing environment and from there move it to the production environment while having complete monitoring. So we try to build that layer where everything is orchestrated over Kubernetes. And a lot of the things that you will need to worry about on the infrastructure side are taken care of by the platform. And all you need to do is write like few boilerplate code lines and that's it. And that is very standardized and it works for pretty much all the frameworks. So we are more travelers on the site, but would love to hear your thoughts. 



I probably will have to of course continue exploring or understanding Truefoundry a little bit more. And if you have anything to share, demo or any other documents to share through email, I would love to hear it. And I'm saying this especially, and this is something Nikkunjo also wrote on the message when he connected with me on the LinkedIn. The reason this is interesting is I know these are the very common problems and many people are approaching it like every other day there is some message that you receive on your LinkedIn or an email that someone is reaching about ML ops, right everyone is trying to solve. 



So I think one best way I have figured out is to first of all see and understand where everyone is kind of understanding is going from and seems like the foundation understanding is same across the industry, that deployment is hard, tracking is hard and actually deploying multiple persons is even harder. It's definitely, I think, in line with what a product of such goal is expected to do. I think I found it very much aligned. At the same time I haven't used it so I can't comment a lot about it. One thing I can say about from the unique perspective like let's say from a data science perspective where as you can imagine a lot of time gets to spend on the experimentation. 



And the last part when the deployment happens, if the foundation was laid out right then foundation of deployment is not big knowledge gap. Once you have the pickle file then there are thousand ways to actually deploy it. And as of now, what we are using is KF serving along with Kubernetes. So just deploy it as a model as an endpoint for the KF serving and deploy it in a Kubernetes server. Have a security layer, have a user access layer and your APIs are good to go to be accessed by your users, right? But the problem that happens is when let's say API based models which is a stateless model versus model. Let's say that is based on certain data that gets pre processed every day. 



And then it kind of like imagine a case of a hospital where someone wants to get length of the state prediction. Then we are not talking about stateless model, we are talking about the data that comes every day about the entire hospital. You preprocess it, you prepare it and then from the trained model now you actually define like you build the features for the model from the data. So it's not like a model that has a very specific feature API that one can use. There are like 1000 features and that is where healthcare becomes a little bit unique. It's very hard in certain cases to define stateless model because models usually end up with large number of features and if it is not large number of features then the person on the other end is clinical. 



Let's say they are not going to say like okay, I mean one needs an It team basically on the other side and not every customer has a strong It team. Which means that at the end of the day the kind of the pipeline you are talking about is data comes in and data goes out irrespective of what is happening in the blackboard. So that is how we have seen the industry. It's very hard to suggest that hey, we don't take your data. Here is our model specification API. If you just request it, you will get the responses. Very good to have such thing. Problem is in healthcare on the customer side, you rarely talk to It teams because It team is managing healthcare infrastructure, their hospital infrastructure. 



And the conversation that you are having about most of the machine learning is not the infrastructure team on the healthcare side. They are the clinical team or they are the finance team and they are like oh, are you saying that we have to go and engage our It and infrastructure now into this they're like okay, this looks like a two year project to us and it just doesn't work from a company standpoint perspective. And then you are like okay, well, don't worry about it, give us your data. So data in, data out rather than feature in, prediction out. So that changes a little bit of a scenario and that is where we have found that a lot of ML ops become harder to adopt as is similar to ML Flow. 



We have tried ML Flow in the past for of course tracking purposes but it did not give us the leverage in the way that we wanted to have because that part of the deployment is very small part of the entire journey. So the leverage is not that huge. 



Got it. I would love to understand from these are great pointers and the point you mentioned about stateless models versus the non one. Curious, how do you currently implement it? Love to know about the overall pipeline today like starting data scientists using data. I'm guessing you're using a feature store or something for storing features but I'd love to hear how the thing starts from there to training to testing to actually deployment to monitoring. If you can give an overview because that will be really helpful. 



Yeah, if you follow the very traditional way. I know we are working on something much more different for our upcoming months and year but what we have traditionally worked on is as we all probably know you have either Python code or notebooks to explore and understand the features. You write two types of code, especially. One is for the training purposes, which results into the experimentation and one you write for the coding purposes and for each of the code and given if you have approached from a feature based models versus like, let's say deep learning models, which in some cases we also have like, imaging based models, so we don't really need features, definition and whatnot. So that differentiates like how those pipeline are looking like. 



But if it is feature based model then you define feature metadata, you define actually explicitly what those features look like. So similar to a feature store internally we call it feature bank and then you define and store all the features, their definition, their ranges, their expected values and whatnot. And then that becomes a foundation or as a metadata across your pipeline to be utilized and using those feature metadata which of course during the training time or during initial phase of learning or development metadata will keep updated very frequently. But let's say once you are in a stage of the end of a model development then they will be rather much more fixed for probably several years also at times. 



And that then results into either data bricks specific which where we have most of our feature based models which means we get the data from customers in a data bricks. I mean of course that part is managed by our own infrastructure which basically gets the data turns up the data bricks, runs the infrastructure, puts the right user access and the data is available in data bricks. You write your codes in the databricks to basically convert that data into models and then you write a scoring pipeline in there itself in the databricks and the data bricks. Along with Azure. We use Azure machine learning a lot. So along with Azure data factory is what is that pipeline? 



If we are looking at non feature based which is like let's say image based deep learning models, then we don't use database, we use directly Azure ML pipelines in our case. And Azure ML as you can imagine also provides a lot of similar kind of constructing pipelines, similar structure of burgeoning data, experiment tracking, including scoring itself. Because Azure ML itself has all these features available very in a standard way, so it's much more cloud native in a way, but it is still very zoodependent rather than independent of specific cloud. So that's how most of our pipeline is. 



At the end of the day, once the training is or training of the models are done, we usually end up with a pickle file or a job lift file which we use as either as a data factory if it is related to that kind of model. I was saying where we get data in, data out, not necessarily featured in prediction out then it is usually Azure data factory where the pickle file is with just one part of that data factory somewhere or on the other hand it is stateless model. So we do have some stateless model and stateless models are supported by KF serving and Kubernetes service right now. 



Got it understood. And this entire infrastructure is it managed by the infrared team or like yeah. 



So everything that is needed for the training as well as the scoring purposes if just the resources are required. So of course it is terraformed for respective customer if it is needed or respective even development environment that is needed. And then there are certain deployment automation for the KF serving Kubernetes if needed be and similarly on the scoring pipeline. So we generally separate the training and the scoring pipeline or in other words development and production. And each one has the infrastructure that is available as a code, can be deployed multiple times and can be of course utilized multiple times with the multiple customers. It's not as probably easy or simple as I am suggesting. It usually still requires some manual work just because of the differences that we start observing customer by customer and data by data. 



And that's what the messiness of health care is. The same data like let's say blood pressure as an input can differ from one customer to another just because of the metric they use or the formatting that they use. And that creates a lot of messiness, that creates a lot of customization which becomes harder and harder. And as you can imagine what is the right word? I'm looking at like chaotic way of keeping all the code where multiple versions of the truth starts existing and which is something we have been trying to address. 



Got it. And I'm curious to know like in this entire pipeline, once the pickle file is made and you have to deploy, who does the dockerization and everything? 



Infrastructure team or basically there is an ML engineering team. So we have infrastructure which does the Terraforming helmet, all those management and deployment. And then we have an ML engineering team who does most of the packaging for the models and deploying of the models against KF serving right now or the data factory if it is a kind of a batch processing rather than a straight place. 



So one question you got like all of these models are serving internal use cases or are there also models that actually have to be taken and deployed on customers infrastructure or mainly everything is deciding on your infrastructure. 



So until it is a stateless model, everything is actually deployed in a customer environment. And that's why infrastructure as code is required. Because data in healthcare cannot come out of customer environment. Right? So we go to their resume, we deploy all our resources in their Azure. Once everything is set up, run it, execute it, produce it, get out. 



Okay. 



And if they like, let's say relationship, as long as exist, as long as the contract exists, the interest exists. If the relationship does not exist, in some cases when it has, we either choose to leave the infrastructure as is or we destroy it if they don't want any of it. 



Sorry. 



Go ahead, go ahead. 



What happens if a customer is not using SEO or you are using Kubernetes for some deployments? What if the customer is not on Kubernetes and using a zero? Then you ask them specifically or do you kind of generally support multicloud? 



So in that case we provide the infrastructure then because we kind of create a specific infrastructure environment on our side, like tenant on our side and then allows the customer to be part of that. Now the foundational layer behind this all is because we are ideally to certain extent a Microsoft partner and hence we have a lot of Azure exposure as well as Azure infrastructure in place. And the moment you say to a customer that hey, we will help you get an infrastructure on Azure, more than often either they are not in cloud or they are a customer or someone else, then Microsoft also gets interested and it is Microsoft often who is willing to COVID the cost of it. Because as you can imagine, you are bringing a partner to a Microsoft as your right. 



It's a bigger Microsoft story than our story actually, let's put it that way. 



It's pretty amazing to know. Do you have anything to monitor these models from a drift perspective or from an explainability perspective? 



Yeah. So of course, most of the models that are deployed, whether it is in the data factory side of it, which means data bricks, pipeline or the stateless side, they all have certain monitoring properties around it, including the Drifts and we use Grafana for any visualization of those monitoring. But we use just actually trying to remember if we have used specifically on the Kubernetes something else. But on the Azure side we use most of the Azure logging metrics and then Grafana is connected to it and it picks up all those metrics and then it shows which includes both the system performance as well as model performance. 



But model specifically health perspective or model monitoring I'm not sure I would still trade even across the industry I don't think it is yet there to say that it is a matured way because it's generally about hey, model generated a prediction, it's up and running. Here is the prediction and it generated five previous day, five next day. It is not telling us like probably the qualitative differences of the monitoring. It's probably a lot more focus is still now on quantitative and this is probably true across the industry on the model monitoring side, maybe a few models that we may have from some of the bigger ML focused companies, but I am seeing at least mostly it is still quantitative monitoring. 



Got it. And when you try to like you have seen other ML platforms in the past, I would love to know if there's anything that from a goal perspective that you would want to meet if you would want to ever adopt an ML platform. And the reason for this question is the following it will help me understand where you might potentially benefit or where you might potentially think of even a two like two Foundry. And what I would like to do with us is also do a follow up call, post this where I can actually dive you through details of the platform, also show you overall demo as to the overall pipeline as well as for the Use case you are telling maybe also showcase that part in more detail and learn your feedback and learn from the system you have built at sure. 



Yeah. So I'm also realizing time again, thank you for taking the call. I think it's midnight for you already. 



I should be grateful for you to take the call. 



One case that as I was saying, what happens is multiple versions of the truth starts existing and that is a major challenge. It's not about that hey, whether we are able to deploy monitor and version models, that is solved. The problem is they are being done in multiple ways. So that's where when I see things like Proof Foundry or any other ML ops platform is a question of like hey, is this interesting to us because this will help bring consistency in what we are doing. Rather than, let's say if someone. Is in Data Break, someone is in Azure ML or using Data Breaks, using Azure ML or using ML flow, they all end up with different ways of doing the same thing. 



So idea is how we can bring consistency in both not only engineering side of it, but infrastructure, the quality, the quantity, monitoring everything around it. So that's where I see Truefoundry or any other ML Ops platform to be such. And 2023 is something we have discussed as a focus for that to kind of expand on, especially around the ML Ops and bring some consistency around it. We do have like as you can imagine, we do have a good number of ML engineers who has worked to provide the consistent framework. But imagine like let's say a five engineering team working to provide consistent framework versus I mean just even think about Kubeflow itself, 100 engineers supporting it. The number of features that keeps updated is very high. 



So we are debating that, hey, why we are building on our own, we should adopt something and then actually expand on it if we need to. So that's the discussion we are having and that's the consistency we are looking for. 



Got it understood. That is very helpful. And this is exactly what we also try and optimize for. A truefoundry like even if you are using tools, how can you standardize the overall flow so that even a new person coming in can use the same flow. If anyone leaves, then no one has to worry about knowledge transfer and everything is in one place, which is like a base layer that can sit on top of other layers if at all exist or make a common workflow as well. So that's very helpful. One more question I would like to know. 



Can you hold on? Someone is knocking on the door. Let me just take the call. 



Hello. 



Thank you for waiting. Okay, what we are talking about asking. 



About this entire pipeline because you mentioned about what will be there for adoption. So I wanted to understand one thing, like in this entire pipeline, which would you rather prefer? Like one tool doing that entire floor, you'd want it to be composed of different tools along the journey. And one more question is around Kubernetes. How comfortable is the team using Kubernetes as the main layer? Like everything orchestrated say on top of Kubernetes. 



The team managing and deploying Kubernetes is very comfortable with it because it is not new to us. We have been using Kubernetes from its beginning actually. So about three years now. So the team is pretty comfortable in deploying Kubernetes clusters and services on it. For example, the learning phase is already done. In a way, I would say from angle of adopting Kubernetes for stuff, we are at a pretty good stage. The difference is adopting Kubernetes for ML development is not there. Adopting Kubernetes for jobs, for data, for pipelines, because there is a lot that happens even before model kicks in. A lot of that is pretty well managed, deployed and versioned. I would say the ML side of it is not yet there. 



Got it. 



So for example we don't use yet cubeflow in our production pipelines. In other words we don't orchestrate models that is like deploying on multiple Kubernetes for training purposes. However we of course deploy models on Kubernetes for scoring purposes just to provide it as a stateless API. 



Got it? Understood. I think this is super helpful because I know we also set this meeting for 30 minutes so very of your time as well. But what I love to do is, based on this information, I'd love to kind of take a next step wherein I'll show you the overall platform as to how we have built it. And along the way, during that journey like discuss about some of the use cases that you think and your thoughts and at the end love for you to act as a champion for us. Like, if you like the platform, then guiding us on what part would be essentially useful within advata or how we could approach it. And if you feel really it's worthwhile, then we would love to work with you and your team potentially in order to take this forward. 



Sure, yeah we'll look forward to that and let's figure out some time for a follow up and then we'll accordingly follow up on that. I would say that let's figure out a time. I was going to say that probably by the end of the January we'll have even a better picture overall. Happy to have a follow up. But again, we'll have to be both mindful of each other's time given the outcome that we are expecting. And if the outcome is expected, then I would say right in the January we can have follow ups but we may not have outcomes yet, which will happen probably a little bit later "
11578091092,Aviso,Raghav Nadella (raghav@aviso.com),,Aviso,100-500,10-50 Mn,Oct-25,AWS,Yes,Software Development,"Redwood City, CA",Nikunj Bajaj,,Not Recorded,
11577326177,Tide,Suryanarayana Ambatipudi,,Tide,500-1000,100 - 500 Mn,Oct-25,AWS,No,Financial Services,"London, England",Nikunj Bajaj,,,
11575824756,GroundSpeed,alexander.findlater alexander.findlater@groundspeed.com (alexander.findlater@groundspeed.com);Yue Ma (yue.ma@groundspeed.com),,Groundspeed,100-500,<10 Mn,<10,AWS,Yes,IT Services and IT Consulting,"Ann Arbor, MI",Nikunj Bajaj,,https://app.fireflies.ai/view/Alex-Groundspeed-TrueFoundry::4JODcSHqS2,"1 second. Sorry about that. 



No problem. How are you doing, Alex? 



Doing well. 



How are you doing? I'm doing very well. Thanks a lot for taking time again. Something like drastically changed in your looks since the last time we spoke. Did you grow a lot of long hair in the last one week or so? 



It might've been pulled back a little bit. I can surprise some people with that sometimes. 



Yeah. 



I invited you. She's the team lead of the ML engineering team here. She should be joining shortly, but yeah, she's definitely another interested party in an ML ops platform. 



Got it. I see. So actually one thing I remember from the last call you mentioned that the ML engineering team is like, whatever, 78 people team size right now. And is there like a separate intra team or that part I'm missing. Alex. Hi you. How are you? 



Hey, sorry for being a little late. My previous meeting ran over a little bit. 



No problem at all. Glad to meet you. 



You too. Thanks for move the meeting because I had a conflict earlier. 



For sure. My pleasure. Where are you based? 



You well, it's us like alex I'm in Michigan as well, also in Michigan. 



Okay. 



Yeah. 



Nice. Amazing. Do you have context about this call from Alex? We can do also a brief introduction if you want. 



Yeah, that might be good. 



Sounds good. Yeah. So let me maybe briefly introduce myself and True Foundry. I would love to learn a little bit about your work as well. And then I have a couple of questions that I would love to ask you before we move on to the demo of the platform, if that's okay with you. 



Yeah, sure. 



So you I come from a machine learning background myself. I have worked at Facebook in Conversational AI and have led the machine learning team at a startup where we built out a lot of recommended systems as well. Currently I'm a co founder at Truefoundry, and before this I built another startup in the HR tech space that got acquired by one of the largest HR tech players india right now. The way we got introduced to Alex and now you as well, was via Vishal, whom we got connected to earlier a couple of months ago and mentioned to him about what we are building and then he found interesting, he found our product quite interesting. So he said that I should talk to you all, basically. So that's the general context. 



I had a brief call with Alex last week where we discussed a little bit about the data science and the ML engineering teams and the entire document processing OCR work that you all are doing at ground speed. So I have some context about the work and I remember, Alex, you had mentioned a couple of problems that you are facing around taking your models from the Jupyter notebooks where the models are getting built to actually deployment that was number one. And then also some things around like unsupervised monitoring of the data and the models as well. So those were the two problems that I remember that you had called out. Please do let me know if I'm missing something. 



I would say the first one, the wrapping of the models, that's something we've worked on internally and are sort of alleviating. So I would say that's not as big of a one. Obviously we're open to different ways of doing things, different methodologies. But yeah, mostly I think our immediate concerns are with model observability. Ways of detecting drift in an unsupervised or maybe semi supervised way, not needing a ground truth signal always coming in, something that can take the place of that. If we're doing ground truth auditing on some cadence periodically, something to fill the gap there on an unsupervised signal sort of thing. 



Understood. Okay, awesome. Thanks at all for clarifying that, Alex. So you would love to learn a little bit about your work as well. 



Yeah, so for me, well, I'm managing what we call the Document Processing Automation team. So it's more machine learning and well, it's more engineering than I guess data science research. So we do overlap with Alex's Use case. But we also have some additional things like we want to be able to which actually are currently covered by data dog monitoring because like errors, model errors and through logs. And then like latency of the model, we will want something that's more tailored for the model because the model is a different type of service. We want something that's more specific. Right. Like in terms of latency, then we want that to be more related to the model predictive performance other than just the normal how the model is handling requests. 



And also which is similar to Alex or Use case is we want to make sure that once we finish developing the model and we release it to the production environment. Does the model maintain its promised performance? Is it still giving us good predictions? And is there a dashboard that we can just go to and check, for example, some of the metrics like number of files that were number of requests of the model process and how many were successful, how many were unsuccessful and like what's the cause of the model being unsuccessful and track like predictions and the competence score and then try to plot those as well. 



So things like that which kind of give us more insight and then can serve as some we can even set up alerts like, hey, the model performance has been declining over the past weeks or a month or so, then it's time for a retrain or something like that. Or the model has been handling a large load recently and it hasn't been as efficient in processing requests. Is it because the packages are old or stuff like that? So these are some of the things that kind of do overlap with Alex's use case, but might be more engineering related. 



Thanks a lot for clarifying that. You one follow up question here. Actually a couple of follow up questions. That is, model monitoring also the biggest pain point that your team is experiencing? Or there are other parts in the pipeline? Like if you think about, I don't know, like tracking your experiments, deploying your models, training or retraining your models. Any of that concerns that your team is facing? 



Not really. We can handle that on our own. We can do the development and then transfer the code from the notebook to a code base, wrap it up and deploy it. We can't handle that internally. It's not really a pain point per se. 



Understood. Got it. One thing there is roughly after a model is built out, how long does it currently take to put that to production? 



It depends on if we just want a model. Like if we just release a model itself, it's really not that long. But we have to integrate a model into the pipeline, and that would take some time because there is like, routing. And so our models are usually used for either document automation or document routing. So that would involve some additional logic. Now we need to follow either how to route files to the model, how to route the files out of the model, stuff like that. So that would take some time, but deploying the model itself is actually pretty simple. 



Got it. Very nice to know. And then the second question, which I think holds for both you and Alex, which is if model monitoring is a pain point, like, have you all evaluated any other vendors recently? Is that any active work that you all have done or considered any other vendors? 



Yeah, we're talking to other people as well. Just kind of seeing what's out there internally, I guess, for monitoring, not so much. As far as monitoring goes, we've pretty much just been relying on what the rest of the engineering uses. Data dog in this case. Now we're trying to get out from under that and evaluate other platforms. As far as, like, observability goes, I don't know that we have. I mean, we kind of use our monitoring solution for that in some cases, but we're looking for more specialized, I guess, for ML purposes, basically a monitoring observability, but with that extra layer that's more custom tailored to doing this for ML models. 



Got it. 



Understood. 



Anything that you would like to add there to you? 



Well, pretty much agree with what Alex have said. I think in the past what we've done was like manual auditing of the model results, model outputs. So nothing automated at the moment. And we are definitely evaluating different service providers and see which one would fit our needs, which one will be the best fit. Yue ma groundspeed. 



Got it. Understood. Awesome. Thanks a lot for giving that background. So let me give a little bit of context about Truefoundry. So I think Truefoundry, as a platform, tries to solve two main parts of the ML pipeline, right? One is the deployment component of machine learning models. That is, how do you quickly take a model and put that to production? And then the second is the monitoring component. Each of these two components, monitoring and observability, by the way. So each of these two components can be used as a standalone solution or can be used together as well. Or you could also plug any individual part into an existing system and use it with your existing stack. 



So, for example, in your case, given that you guys are focused on monitoring itself, you can use our monitoring with the rest of the stack and basically integrate it in the existing setup, essentially. So in today's demo, what I'll try to do is I will first walk you through like a developer experience of how you would integrate the monitoring system of Truefoundry into your existing stack. And the second part of the demo, I'll show how the dashboards itself would look like, what kind of debugging that you can do in terms of the monitoring. In this case, I'll focus more on the machine learning monitoring side and less on the data dog type of monitoring, although we also do support that. So if you're interested, I can cover some of that as well. Does that sound like a fair workflow for the demo? 



Yeah, that sounds good. 



Good. So let me start with the introduction here in terms of the developer workflow. So right now, by the way, I pulled through foundries documentation. So here you would notice that we have a monitoring section where we talk about how do you get started with our monitoring, right? So a lot of the initial code is basically a boilerplate code that just required, that is required to just explain monitoring. Where you're building a model and you're making some predictions. Like the model itself is making some predictions. This is where monitoring is integrated. So imagine that you have implemented like one predict function that computes certain features, gets the model predictions, right? And this is where the integration with Truefoundry would happen, where you would make an invocation to our API called Log predictions. 



So essentially, you start logging your predictions, which is your features, your prediction values as well. If you wanted, you can log your raw data. If you want, you can skip that. So basically, you would make one API call to our log predictions. And later, whenever, if at all, you have access to the actuals, you may log that or you may skip this also. So this is an optional step. So long as you're logging the predictions of the model, we will be able to generate your model monitoring dashboards for you. And I'll explain how our dashboards look like. But practically, this is the most important point of integration for your real time monitoring. Oh, by the way, this is an important point that right now what I'm explaining is a workflow for a real time monitoring. 



That is like if you're making inferences over API calls, if you're doing more inferences using batch monitoring, we also support that and I can walk through the workflow of that as well. I'll take a pause here to see if you have any followup questions. 



So this client, the server here, is some managed solution on your side. Or we would also host like a kind of reminds you like the ML Flow API. So in that case, you're hosting a web server that's backed by a database and artifact storage, I guess. What's the other side of it? Of it here? 



Sure. So we actually do both. You can actually deploy so you can use like a SaaS offering where all you do is you sign up on our platform, get an API key, and start logging your predictions and you will start seeing dashboards getting generated on our platform. The second thing you can also do is you can deploy the entire True Fonter platform on your cloud, where like, you know, we would like everything will be hosted on your cloud. None of your data ever leaves your cloud. So that is also available. Okay, right. And like ML Flow, this is very similar to ML Flow, where our installation is also backed by a postgres database, an S, three bucket, and the equivalent systems in other cloud GCP, Azure, et cetera. 



So the database could be bread shift and we could pretty much host it in AWS. We basically have a Kubernetes cluster in AWS. 



Exactly. We basically provide you a hem chart of True Foundry and you install that on your existing Kubernetes clusters. We actually manage the entire installation from our platform as well. I will not focus on that part because it seems like the monitoring itself is the more important one. But if you guys are interested, I can always walk you through the Kubernetes set up and all on our platform as well. 



Yeah, I just want to make sure were clear in the hurdle that we could have everything self contained on our infrastructure. 



Yes, some of our customers are already using us in that mode. All right, any questions from your side? You. 



No, I'm just going to take notes and then ask questions at the end. 



Sounds good. Okay, so basically, when you get to the model itself, like the monitoring bit of it, you can basically track all your models in one place, where you can obviously get the Model FQN, but you also have the model schema like what are the inputs? What are the expected outputs of the model? And you can always see the model files and associated metrics and stuff like that. Right. Now let me explain you how the monitoring dashboard itself is arranged. For example, what you would end up doing basically like our monitoring dashboard is arranged in a hierarchical fashion where you start with the top level information about the model and then you get to the next level, which is like your distributions of your data and all. And after that you can get down to a single row level. 



So you basically get from the absolute aggregate level down to a single prediction level. And it's designed in a manner that it helps you debug when things go wrong very quickly. So let me explain how this entire thing functions. And by the way, these charts are like the default charts, but each of these charts are configurable. So you can actually put in your own custom metrics if you wanted to log, depending on the type of model that you're building. Or sometimes you may be interested in logging some business metrics and also you can do all of that. But let me explain you the default charts to begin with. So here what we are doing is we are logging the number of predictions, logging the number of actuals being reported. 



As you can see, the number of actuals is actually lesser than the number of predictions, which is an expected outcome because generally you may not have all the actuals that are reported to you. Post that you can always track different kind of metrics like log loss, or in this case, I'm tracking precision across different classes of this model, et cetera. Or whatever metrics f one score recall, et cetera. So you keep tracking all those metrics over a period of time. Now, let's say if you notice that your metric drops at a certain point in time, and by the way, these are demo jobs, so you don't see like a continuous time series graph. It's more like a step function that you're seeing here because we ingest the data for the demo at certain intervals, basically. 



So that's why the nature of the graph is like this. But let's say you notice that the performance of your model on your F one score drops at a certain point in time and you want to debug what's happening between, let's say, 16th and 19th November. So what you can do is you can actually zoom into your data a little bit and then check the data distribution. So let me actually remove this for now and just show like just explain this one chart to begin with. So here what you're seeing is you're seeing the distribution of your predictions, you're seeing the distribution of your actuals, and you are seeing the distributions of every single feature that was input to the model. Okay? And Alex, by the way, to your point, like this entire chart will work even if you don't have actuals reported. 



All that will happen is this particular chart will practically be empty or like very sparse, depending on how much actual has been reported. But most of the debugging will happen anyways. Now here you're seeing that this graph itself is actually color coded, where you have some bars which are darker in color and some bars which are lighter in color. So darker in color means that it has high log loss. It can be FN score or whatever metric that you want, and lighter in color means low log loss. So now you know that. Which class of prediction for which class of prediction is your model underperforming or performing better? Right. So in this case, you can clearly see that there's a difference in the predictions that's happening in class number six versus class number seven. 



So you already have some insights just by looking at this graph. The other thing that you can do is, like, similar analysis at a feature level, so you can see the distributions of your features, and you can see that certain colors are darker and certain colors are lighter in color. Certain bars are lighter in color here as well. Right. So again, the same thing that for certain values of your features, your model is performing better than certain other values. So a concrete example where something like this could be useful is, let's say your model is performing really well for the state of California, but performing not so well for the state of Michigan, for example. So that those are the kind of information that you will immediately notice by looking at this graph. 



I'll take a pause here before explaining a little bit more in detail here to see if there are any follow up questions. 



Can you expand real quick one of those again? 



Sure. 



What is the actual value here? It kind of looks like a feature importance plot, but you're basically just correlating. 



So this is actually a feature distribution plot where basically the histogram of the feature. Yeah. 



Okay. Then the color corresponds to the accuracy when that is the value for that feature. 



That is exactly right. 



Okay, thank you. 



And how do we get these features? Is this something that do we have to configure the model so that these information are being passed to the dashboard? Because all I've seen so far is just logging the prediction. Is there a features in there? Okay. 



Yes, we also have features in here. Yeah. 



Cool. 



The features I guess it's something that's required. If we didn't pass any feature into this, then will we just see empty everything in the dashboard? 



I think features is actually a required argument because without the features, I think you practically have, like, very little that you're tracking in terms of actual debugging of the models. 



Okay, sorry. 



I was just going to just follow up a little bit. So our model summary, it would also be completely blank, or are we still going to see some stuff in here? 



No, you will see some stuff here. Like, for example, if you're logging your predictions, you will be able to see these number of predictions graphs. You will be able to see if you have logged actuals and predictions only you will be able to see some of the performance metrics graphs. So you can see some of these graphs if you want to, but I think a lot of debugging basically goes out of the window if you have not logged the features. 



And then so data distribution will be fairly limited if we don't pass any features in. But the raw data will still be able to access that, right? 



Yes. 



Okay, got you. 



In the case we're using like a heuristic or something where maybe features aren't clearly defined. Like if you're using a deep learning model, for example, you wouldn't want to just know what the value of this dimension of bedding was. In that case, could you basically just say, hey, the features are actually just all the words on the page? And then maybe it would tell you it would be sort of a proxy for the features, and then it would just show you what you just showed us for, like, acidity, except it would be like number of words on the page or something. You could kind of get what I'm getting at. 



Yes, 100% what you can also do. And by the way, I think this might feed into your question as well, that if you don't want to log the actually computed features that are getting fed to the model or you're working with heuristics based on some data, you actually log the raw data. And basically, then the dashboard will use that as your main debugging entity, if you wish, right, where you could have different columns, where if you have categorical columns, we will accordingly arrange these distributions. If you have like numerical columns, we'll then arrange the distributions accordingly. And then these columns would also change depending on the type of data that you have logged. 



Essentially related to that, we might also have things that we're not allowed to have in the models for PII reasons, but we would still like that dimension exposed in a dashboard, for example, like our client name and things like that also use of that raw data feature. 



Yes, you could do that as well. That's the other thing that you can actually log with or without features as well. So sometimes this use case is like one of the other customers also ask for the same use case where they want to log some other data just for analytics purposes, which is besides the features of the model. So that's actually the reason why we had initially introduced this raw data field. 



Got you. Basically put anything in there and you'll correlate it with how the predictions and metrics are going. 



Okay. Yeah. Any other questions on this so far? 



Just one quick question. Is there more on your roadmap as far as like this is almost getting towards like explainability, are there any additional sort of features like shapley values or anything else you see kind of come into the platform. 



Yeah. There's a couple of things that we are going to build on in terms of explainability. So one is like the vanilla shapley values, right, that you log just the data set. And with that we automatically either compute the shape values or by the way, our APIs already allow you to log your shape values. If you are computing your shape values, you can log that and we will display those dashboards. We are also building out features where we can automatically compute shap values in the back end. But this one more interesting thing that we are doing as part of our roadmap in terms of explainability, which is the surrogate models. Are you familiar with that? 



Yeah, I guess. What would the application be? 



What would the application of the surrogate models be? 



Yeah. Where are the concepts sort of training a model, like on the output or back end of another model. 



I see. So basically explainability would be the application. So imagine that you have built out like a deep learning model or any model that's not very explainable by design of that model. Right. So what you can do is you can use the predictions of that model as your actual labels and train like a simpler, let's say, tree based model. Right. Now that tree based models model is basically like a surrogate model. That only helps you do the explainability of the main model essentially because the features of the tree models are the same and you can just get that by design is explainable. So we will also build that out as part of our dashboard here. 



Got you. Yeah, that's like fitting random forest to a cluster. To a cluster. Okay, sorry, continue. I'll try to maybe ask those questions till the end so we can get through it. 



No, these are great questions and like we are anyways focusing on a very focused part of the platform right now. So I think we should have a lot of time for questions. All right, so I'll jump in and explain a couple of other things here. So we talked about being able to see the data distributions that you have logged. But sometimes what you're interested in is comparing how is your model doing in production on your live traffic compared to how your model was doing during your training time. Right. So you want to compare your training data set with your inference data set. Or you also want to compare the performance of your model over a period of time. That is any data drifting over a period of time just week by week, any data drift is happening. Right. 



So those kind of comparisons are also useful. So you can actually do that here as well or across model versions. You have a lot of different types of comparisons that you can do here. The idea remains the same, where each of these graphs are color coded. But now you can actually see the distribution of the data in whatever time range or model version or training data set versus inference data set that we have chosen here. And you can do the same thing for your feature comparisons as well, that is feature distributions as well. So you immediately get a view and obviously once you select this comparison view, you also get like a drift column here as well. So now you can check which of your features are getting drifted. 



So you pretty much get a quick view on what are the potentially bad features which are causing your model decay, essentially. Does that make sense? Do you have any questions on this? 



I would just be interested to know how you could filter this more. Like if I was interested in comparing two different versions of the model or at different times, but comparing a specific customer. 



Yeah, so you can do the do it, you can add whatever filters that you want here. So for example, like if you have like one raw data that you have logged, you'd have something like raw data customers and then you can say equal to, and then it will have whatever value that you want. You can do that. So what this will do is it will filter the data to that subset of the data, essentially like this entire dashboard to that subset of the data. 



Can you show us what are the filters that we can access? Because I have seen features are there any aspects in the raw data that we can filter with? For example, in this case, I have. 



Not logged any raw data. If you had logged raw data, you would also see those columns getting populated. 



Okay. And then you said we are filtering by something equals and what are the other options? 



So you have other things like greater than list, than not equals, et cetera. 



No, like list membership or anything like that. We do a lot of that. 



Got it. Yeah, we can add that. So right now most of the customers have been using these features because we have been dealing with a lot of numerical data sets. But that's a fairly simple addition that we can make to the platform. Any other follow ups on this? 



Yes, if we selected, for example, a number of different things. So we're just going to see those reflected down there, like in all features, or all features is just always going to show all features and only the graphs up there, like the comparison graphs are going to be filtered. 



No. So this will actually change for all features as well. So, for example, if you decided to select data distribution for a certain customer or a certain state or whatever, the distribution of those features for that segment of the data, right, for that slice of the data can actually be different. So think of it as like this is almost like a data frame slicing operation that you're doing. Here. Right. Imagine that you have logged this entire data frame from which we are generating these dashboards. When you put these filters, you take only a slice of the data and then you do the same computations on that size of the data, essentially. So computing your feature distributions, computing your prediction distributions, et cetera, basically. 



Okay, got you. Yeah, I just want to verify if everything on this in this view is affected by the filter. 



Yeah. And the other thing that you can do is basically once you have done, let's say you have analyzed that, okay, this particular segment of my data is not performing very well. So you can always use this, go to the raw data and filter out for that particular segment of the data, essentially that you realize, which is not performing very well. And then you can visualize row by row that what's happening in your data set. What was the actual that was reported? What was the prediction that the model made and what were some of the features that was actually passed as opposed to just looking at the distribution and losing out some information there. So that's what you can do. And then later, obviously, you can set up things like alerts and stuff like that, basically. 



On the right hand side, did this also include the confidence score of each of those classes? Is that or is that something different? 



So if you have logged the confidence score as part of your raw data logging, you will also see the confidence course. Any other questions here? 



Are you going to talk about the actions maybe you already talked about? 



So I think in terms of hello? Yeah. So I think in terms of the actions, we are still building out certain features here where you can basically mark it's not completely built out as a feature where you can mark first of all, you can copy in a certain row as a JSON object and then you can play around with that. If you wanted to if you wanted to download some of these subset of the data, you can do that, like export that to like a CSV file or something. And the third thing that we are doing is basically you can actually take certain row values, basically, and add another column, which is like mislabeled data set or something. We will end up integrating this part of the monitoring dashboards with your closing the loop of the data itself. 



That something that you manually notice that your data itself was mislabeled and you want to mark it correctly so that it can feed into your model training loop. So those are a few things that we'll be building out, but this feature is actually not built out currently. None of these things are built out yet. 



Yeah, that was going to be my next question. Was there any sort of data set version in management component to this where you would then like, if you had a specific record, I really should make sure that's annotated correctly and retrain on it. Is there any sort of workflow built into the platform for that? But it sounds like it might be on the roadmap. 



It's on the roadmap, yeah. It's not built out yet. There's one other thing in this context that could be interesting, is let me just quickly pull up a model just. 



While you do this. There's also what's it called? Model repository functionality. 



Yes, that's exactly what I was going to show. 



It looked like it. 



Yeah. But actually, for some reason in my demo account, I don't have any models logged. So I'm just trying to see if I can log into another account and just quickly show you. Yeah. So for example, in this case, I have this model logged and the model itself can have a bunch of different versions. And for each of these versions you can track things like how you want to use a certain version. What is the schema? Any metadata that you might have logged with the model, the actual model files that you might have logged. So you have those functionalities as well. 



How do you typically relate this back to the source? Like, if we had source code in GitHub and then now we have the sort of model page that's kind of holding our artifacts, is there any sort of because we're going to deploy the source code and then have to load the models in. I guess that's just handled from wherever you're deploying, you ask this API for the version of the model. 



Sorry, I'm not very clear on the question. What's the use case here? 



So say you're doing CI CD with GitHub. You have a Jenki"
11575824756,GroundSpeed,alexander.findlater alexander.findlater@groundspeed.com (alexander.findlater@groundspeed.com);Yue Ma (yue.ma@groundspeed.com),,Groundspeed,100-500,<10 Mn,<10,AWS,Yes,IT Services and IT Consulting,"Ann Arbor, MI",Nikunj Bajaj,,https://app.fireflies.ai/view/Alexander-Groundspeed-Nikunj-TrueFoundry-::fgpzsGtpRb,"Hi, Alex. Good afternoon. I'm good, how are you doing?  



Well, nice to meet you.  



Very nice to meet you. Where are you based?  



So I'm in Fenton, Michigan, which is about 40 minutes north of Ann Arbor. That's where Ground I think headquarters is now. Officially remote now, so I'm not sure we have official headquarters. I'm sure we have a PO. Box somewhere. How about you, too?  



I'm generally based in San Francisco.  



Okay.  



Right now I'm traveling to India.  



Oh, yeah? You're at the airport? Like, mid travel?  



No, I'm in one of the cities called Udaypur.  



Oh, okay.  



Yeah. Great to connect with you. Thanks a lot for taking time.  



Yeah, for sure. Definitely interested in hearing about the platform. Yeah. Just a little bit about myself, I guess. I'm a data scientist here at ground speed. Do a little ML engineering, also a little ML apps. Also kind of in all three camps, really. Just end to end sort of model development, conceive it, develop it, deploy it, monitor it. We use some stuff like ML flow in the past, like limited scope. And I kind of get the ML flow vibe a little bit off of a True Foundry. So I guess just excited to hear what's being offered. If it could be framed from how it deviates from ML flow, maybe, or how it's similar, how it deviates. That'd be a great framing for me personally, just because I'm used to ML flow.  



Sure. Absolutely. I can give you some context on what Truefoundry is doing compared to ML flow. Right. So can certainly do that. Let me maybe start with a brief introduction about myself.  



Yeah.  



And then I can tell you a couple of things. Just an overview of True Foundry and how it differentiates from memory flow and all. And after that, today, the majority of the call I would love to spend talking to you about the type of data science problems that you are trying to solve. What's the machine learning operation Stack that you have built out over the last few years that you have spent at Groundspeed? How is the team organized and structured and what are the next few challenges that you are trying to solve? I'd love to spend some time understanding trying to understand that.  



And in that context, after hearing what we are building a True Foundry and what problems you are trying to solve, if you feel like there's, like, relevance, then maybe we can set up, like, a follow up call where we can do a demo that's personalized to the existing problems that you have. Does that sound a reasonable plan?  



Yeah, sounds like a good plan.  



Cool. So a little bit about myself. I come from a machine learning background myself. I used to work at Facebook where I led one of their conversational AI teams. Have you used heard of a product called Portal?  



Portal? I don't think so.  



You know about Alex or Google Home.  



Yes.  



So Portal is like an equivalent voice assistant device, virtual assistant device that comes with a video calling functionality. So that's the device that I was working on before they launched it. Prior to Facebook, I spent three and a half years at a startup called Reflection where I built out a lot of recommended systems for the ecommerce industry. So there I got a chance to pretty much do what you're doing now, where take a hat of data scientists and build the models. Take a hat of an ML engineer and deploy the models, and then take a hat of an ML ops engineer and build out a horizontal platform for the company, basically. And between Facebook and the starting True Foundry, I did one more start up in the HR tech space that got acquired by the largest HR tech player india called Info Age.  



So that's a little bit about my background. And at True Foundry, our goal is to basically build a platform that acts or supports the internal platform teams within companies, right? Internal ML platform teams within companies help them operationalize their machine learning models from training to deployment to monitoring. That's the part of the stack that we are focusing on now coming towards ML Flow. So the way I think about ML Flow is it's actually very useful. Like ML Flow's core strength is in experiment tracking and project packaging, right? So the core strength is in when you're doing the prototyping of your models, you track all your metrics in one place. You put those models in a model registry, right? I think that's where the core strength of ML Flow lies.  



They have a little bit of a support of directly deploying a model as an endpoint, but that's not like a flagship feature yet, more like yeah, it exists as well kind of thing. For us it's the exact opposite. So we do support some level of experiment tracking where you can track your metrics, track your experiments and prototypes and all of that in one place. But we focus as the core on the deployment bit of it, which is you can actually run real training jobs. And by the way, we manage infra. So like ML Flow is a Akshay, Truefoundry is a platform as a service, right? So we actually manage your infrastructure. So we will actually manage an entire Kubernetes cluster where you can run your training jobs, we manage clusters where you can deploy your model as an endpoint.  



So we do all of that in from management as well. And then we take it all the way to the inference monitoring side of things, right? That is, once you deploy model to production, how do you make sure that the model is working well and stuff like that? So from that perspective, actually we are fairly different from very different from ML Flow. Basically.  



A better comparison then after hearing that is probably more like data bricks or something more managed services that leverages something like ML flow as the centralized landing pad.  



Right? So I think data bricks would be a little bit closer than ML flow itself. Right, but actually, even with data bricks, what ends up happening is data bricks started by Spark folks, right? And by the way, data bricks is super close to me because like, I was at Berkeley, so these folks who started Databricks are practically the same lab and stuff, right? So Databricks, it's built on top of Spark, right. So it really excels in your data pipelining jobs, model training jobs as well. But I think once you get to the online inferencing side of things, I think that's not necessarily a core strength of database, but they're like excellent in your data management and your job management. For us, we have a core focus on job management as well, but we really treat online services as a first class citizen.  



So if you really had to think about the closest analog to what we are building, a truefoundry it would be something like a Sage Maker from AWS or Vertex AI from GCP, that would be the closest analog of what we are building. Okay, yeah, I would love to learn a little bit. Maybe I'll start with asking this one question, which is a unique thing that I noticed in your profile is you have actually played a role of a data analyst, ML engineer, and a data scientist at ground speed. So what's the role difference and how is the team organized between across these three role titles? Right? Like is there generally a data science team and engineering team or how is that structured?  



Yeah, data analyst was just entry level that I wasn't in for too long, but there was a lot of shift going on because that was early stages of the company. So there's probably only three different job titles at that time, so that was just an artifact of that. But as far as the ML engineering and data science goes, currently we have a healthy ML engineering team. I think that's probably like eight people on a couple of seniors and then MIDN level ML engineers. And those teams are focused on supporting our document processing models. So we have two families of models that live in our pipeline. What lives earlier up is taking these insurance documents, doing OCR and then doing named entity recognition or different ways of labeling the data points in these documents. And then after that we have enhancement models.  



So say that something was missing from the actual data found in the document, maybe like the coverage type of the insurance policy was missing. We have models to sort of impute that type of stuff and those are data science concerns, the sort of enhancement stuff, whereas the name entity recognition and the extraction using various models from the documents as our ML engineering team. So that's sort of how that's set up. If that answers your question.  



Got it. The ML engineering team is focusing on the OCR and Er type of models, and then the Data Science team is working on enhancements. But actually, I did not quite understand that. What kind of enhancements are we talking about here?  



Yeah, so a big one is just imputing things missing from the presentation of the document that we ingested. So if someone sends us a bunch of insurance claims, for example, and depending on the client, it's coming from the presentations very drastically. So there could be a field in that document that is Coverage type, or it could be something very similar to Coverage type, or we could think Coverage type is just completely missing from this page, and it's something our clients would like to see. So we take all the other data points on the page, all tabular at this point because it's post extraction, and then train models to impute these various fields using the context of the other fields. So, for example, there's a field called AI Transcription that appears in claims a lot, which is a verbose description of the accident.  



So based on the reading of that, you can infer, okay, this is probably an automotive claim, or this is a general liability or something like that. So it's basically an enrichment on top of the broad data that was extracted.  



Understood.  



Okay. And this is the data science role. On top of that is like different, like, regression models for trying to predict claim outcomes and things like that, but basically things on top of what could be extracted from the documents.  



I see. And how large is the Data Science team?  



So it was larger. Right now, it's just one person. It's me.  



Oh, I see. Okay, understood. Got it. So I'm curious, why is there this, for example, these Data Science models that you're building? Does it get deployed to the end client like this filling things? Or is it more like an internal batch processing job that will fill in the missing data? How is this consumed?  



The way it works, like, operationally, is we have all these SLA contracts with our clients, what we call flow. And these SLA's are like under an hour. So they'll send in a few documents, and then these documents will have the data points extracted. And this is all like in an event driven system we have in Kubernetes and also using AWS step functions and lambda. So it's very much like an online sort of flow environment where the document comes in, goes through the OCR part of the pipeline that notifies the next part of the pipeline to pick up those results to extraction. Once the extraction is done, then the enhancement pipeline picks it up, runs these various models, which are all hosted as services on Kubernetes. We use like SQS and SNS to kick off those events so that the data is ready.  



Perform your function on the data, publish your results back up to the redis queue, and then once at the end of the pipeline, that's turned into, like, an extract, which is based like a CSV, and that's what kind of gets sent to our clients.  



I see. Okay. What did I see? So basically this is actually part of the actual, like, the end deliverable to the client, basically, these imputation models that we're talking about. I see. In that case, is it hosted as, like, an API endpoint or does it still run as a bad job?  



Yeah, it's an API endpoint because the clients have a few different options for getting their materials to us. There is an API endpoint and then there's various email triggered. I'm not exactly sure how they work, but like, an email gets sent to an address and then the attachments get scooped out of that email. But it's very much like an automated process where there's some sort of API in all cases.  



Got it. So you mentioned about AWS. Is that the only cloud that Groundspeed currently uses or there are multiple clouds that are getting used?  



We're working on getting away from GCP. We still have a foot in that for a few things, I think, maybe just OCR leveraging their Doc, AI, Google Vision. But AWS is our primary thing. A lot of our services are in Kubernetes, and then things that aren't in Kubernetes exist as lambda or step functions. But the models I'm talking about specifically usually are hosted as services on Kubernetes.  



I see. Okay. And who manages this? Is it do yourself manage it or the ML engineering team has set up something like the entire Kubernetes management. Right.  



Say that again.  



Who manages these models?  



I guess it depends on the definition of manage. Like, the environment is all managed by DevOps. We have a helm repository where we have all these helm charts, and there'll be like a helm chart template for these enrichment models because they're all fairly similar. So we'll have a template for a helm chart to just deploy one of those models. And that's all managed by DevOps. I guess you could say they manage the deployment process and the infrastructure, but that's it. The Data Science Engineering team is actually making the PRS that then go into that home repo.  



Oh, I see. So basically the models that you're building is each model eventually deployed as a separate endpoint and then hosted on top of Kubernetes? Is that how it is working?  



Yes.  



Got it. I see. And are you all using any containerization, like dockerization of every model or something?  



Yeah, everything's containerized and then deployed on Kubernetes.  



Okay, understood. This entire pipeline of containerizing, the model, creating the hemp charts, et cetera, still falls within the scope of the Data Science Family team? Basically.  



Yes. So we're working towards a place where, for example, one thing we've been doing recently I've been doing recently is developing my models as we use an internal pipe repository for certain things. So just having the code of the models be like a pipi package and then those get installed by sort of like a service template basically that just installs them and knows there's a predict method for this library makes a call. So it's kind of similar to what you were trying to get to somewhere where it's similar to kind of what you're describing for ML flow, but where they have this sort of loose definition of a deployment and you can kind of package it there.  



But yeah, we're trying to find a way to get rid of these pain points where we just have to concern ourselves with here's our model, it works, we tested it. Now we just want to wrap it and then deploy it and have as few sort of like pain points and touch points along that path as possible.  



Got it. What are these pain points that we are talking about here basically today? I guess once you build a model, how long does it take to deploy it and I guess what pain points do you experience in that process?  



I guess yeah. So the biggest pain point was, and this has been sort of alleviated by this pipe approach, but the biggest pain was what we used to do is develop the model. Data Science team would have a notebook. We would develop this model using Sklearn.  



Here's our SK.  



Learn pipeline. We fit the model, we evaluate the model and then we would use to just hand it over to another team and they would take that and deploy it. But what that ended up meaning was ripping code out of the notebook and translating it into PY files and then putting that code into the service. So we tried to put as much of that code into objects as possible, like the fitted objects. So like trying to pickle a whole class which corresponds to the model so that they would just have to unpickle that and then incorporate that into this SQS service we are using that we call that the model and deployment.  



So that's the pain point I would say is getting from we're developing the model locally, like in a notebook or whatever, and then getting that wrapped up into this actual service that gets deployed to Kubernetes. And a lot of this is probably maybe not sound quite right, but a lot of this has to do with the fact that there was, like, this infrastructure put in place for delivering, like, typical software stacks or typical applications that we sort of I don't know if it was the right fit for ML models, but we tried to shoehorn it into that sort of paradigm where we have these services deployed to Kubernetes and then that was causing these sort of pain points.  



So I think that's kind of where the pain comes from is the fact that were taking this approach that was being used for other types of applications and trying to deploy ML models using this paradigm.  



Got it. I see. And this approach that you described about, you have these models in Jupiter notebooks that you want to package and deploy on Kubernetes. You would hand off the Jupyter notebook to someone who would create like a PY file and actually understand the code and stuff like that. Right. So how long was that process taking and how long is it taking now that you have switched to this Python approach? Basically like the library approach.  



Yeah. So the problem with the previous approach was just it would get done and it wouldn't take too long, but then we would run into all sorts of problems because of the different environment. So assumptions were made in the notebook that were probably no longer necessary true. In the production or development environment. So the way the preprocessing functions were, there's differences that then had to be diagnosed by us. Eventually we would throw it over the wall and then have to go back over the wall and figure out what was wrong after they tried it. So we figured the best way is to handle all the logic stuff, the preprocessing of the data, the inference, any post processing on the inference that needs to be done, have a very clean API that we can then interact with these teams that are responsible for deploying it.  



So now that we have this sort of pie pi approach, it's a lot faster. Because once we ensure that this library is working and returning the correct predictions and stuff, and then we put it on this internal pipe repository, then it's just a matter of rebuilding the service image, because now it's just pulling the new version of the model, installing it, running some tests and then deploying that. But the main, the difficult part was repackaging the model from one environment to the other.  



So there was some background noise, no problem. Understood. So when you say that you're creating this PyPy repository, are you also packaging the model itself or is it like the post processing logic and then model reside somewhere else?  



Yeah, so you're right. It's sort of the like we take in the input data, the pre processing, post processing, but then the actual fitted model. So right now those are being stored using the ML Flow model registry, but we just use those to sort of publish up to S Three. And then once the model is being built in this sorry, the pipe repository holds the source code, which is the preprocessing steps, post processing steps, and then the actual source of the model class, we'll say that has like, the predict method and everything. The artifacts are being stored on ML flow using the S three back end. When the model service is being built, which imports the library.  



At that point, while building that docker image, we reach out to S Three and pull in the fitted objects that then get baked into the final service image.  



I see. All right. I see. And, like, if you had to change the model version at this point, what would be the processes for doing that? Or if you had to create a new feature or change some feature which requires a change in the PyPy version, et cetera, how would you test that out? Basically, like, these two things I'm very curious to understand.  



Yeah. Then I would just go to the repository on GitHub where it contains this library. We'll say model library. Yeah. I would just go into that model library, make my changes, do my tests just on that library. And then we have a Jenkins CI CD pipeline. So once I merge my changes for that library into main, it'll just automatically redeploy that pipe. I package up, but the build for the actual service is pinned to a specific version. So I'd have to then rebuild.  



That.  



Service after updating the library version.  



Right.  



So it'd be two builds. Yeah.  



That makes sense. And how about updating the model version? You know what I mean? You might have a separate training pipeline. Right? Right now we're talking about the actual inference pipeline.  



Yes. So you're saying if I wanted to train a new model with that process? Looks like, yeah.  



So let me give an example. So imagine that you have one model that keeps getting retrained with new set of data coming in, basically, right? The model remains the same, the hyper parameters remains the same. It's just new data, new version of the model. So nothing in your code is changing, but you're getting upgraded version of the model every single day, let's say. So how do you make sure that you're using the new version of the model? Do you handle it? Using ML Flow latest or something?  



Oh, no, we don't update our models on that sort of cadence. It's less often than that because before we want to deploy a new model, we don't have annotated data sets. We don't have new annotated data sets always flowing in. We have to make an effort to decide what records we want to annotate and then retrain the model on that and see if there's an improvement relative to our previous evaluations. And then we would consider updating the model. But it's definitely kind of flipped in that regard.  



I see what you mean. Okay, this is actually an important question. How many models would you get are currently in production, and how frequently are these models updated?  



So there's a lot so they're not updated very frequently. And there's two reasons for that. One is we need to improve our monitoring, our sort of like, unsupervised monitoring to get an idea of when we're experiencing drift and things like that. But right now, we basically only retrain things when we know there's an issue, and a lot of that is driven by client feedback, unfortunately, at this point. So right now, we don't retrain things unless we see that there's an issue just because of our limitations. Currently with just capacity. But as far as how many models, I'd say somewhere between ten and 20, depending on how you define them. A lot of them are like the same model, but fine tuned on specific cases and stuff like that. But somewhere between around 20, I would say.  



I see. Okay. And do you end up using primarily, like, the classic ML types models like Cyclone and XGBoost, etc. Or do you end up using a lot of Pythons TensorFlow type of models?  



For a lot of our stuff right now, it's just ScikitLearn and then, like, Heuristic stuff, too. For example, our ML engineering team to extract some stuff out of some documents. These are something called Sensible, which is almost like an API with a sort of high level regex language around it, where you basically kind of just very descriptive and like, okay, if this word is to the left of this word, you label it as this word sort of thing. But yeah, we're experimenting with some stuff, like doc, AI question answering models and some stuff like that. That's more deep learning. But, yeah, for right now, it's all classical ML.  



Got it? Okay, understood. And then I guess one thing I'm very curious about is what are some of the current top challenges that you're trying to solve? Put in other words, why is this call interesting to you?  



Sure, yeah. So we've tried a few different things, and it's just hard to ML flow, and some people might use it personally, but we struggled to get because it's not, like, centrally supported. It's hard to you can't just call somebody and ask for ML flow support. There's plenty in the community and stuff. So we are definitely looking for more of, like a managed, more holistic platform for that reason. But I like MFL because I like the centralized logging as opposed to something like DVC, where everything's stuffed into your GitHub repositories and it's very distributed across all that. I like the idea of central logging. And then we definitely need some sort of solution for monitoring. And that seemed kind of from where I forget where I read this, but I thought your platform was mostly focused on post deployment and the monitoring stuff.  



I mean, it seems like you do a lot of stuff. There also.  



Model pipeline. What's that we call it post model pipeline.  



Okay.  



So we focus on our once you have built a model, like, on a jupyter notebook, how do you deploy that? How do you monitor that? That's the part that we focus on the most.  



Yeah, I think that's we've kind of found solutions to that. But, yeah, we're sort of looking for the holistic solution, but we want to find the right workflow to fall into sort of for all these things. And it seems like this platform kind of hits each of those points we're interested in.  



Got it. I see. So here you mentioned about.  



I think.  



The idea here is that you tried ML Flow and because ML Flow was not managed, is that why you face the problem?  



That was part of the problem. Just because then we had to lean on, like, DevOps and other parts of the company that are also super busy and then it's just hard to yeah, just lack of support, I guess, because I guess when you go to something like that, you're assuming someone internally is going to support you, right?  



That makes sense. That makes sense. And then you also mentioned about monitoring solutions. Right. Have you seen any monitoring solutions like ML? I'm assuming here you mean by ML model monitoring and not like the data dog type?  



Not like data dog. We want, I think, weights and biases maybe does some of this, but like, we want to get good indications of, like, drift in our confidence scores. We want to know we want outlier detection on what we're sending to the model, what we're getting out of the model, ways to sort of empower that sort of model monitoring, getting indications of like, oh, we're starting to see things that are considering outliers. Maybe we want to get proactive in retraining this model and target some of that data so that we can extend to it before getting the ticket from a client saying, we're starting to see poor results on this new segment we've been sending you.  



Got it. I see. Understood. How, like, if you had to rate these two problems, right? One is your workflow for deployment, right? Like, how do you get it from your prototyping to your end deployment stage? And then you think about your monitoring pipeline. While I understand that these two are not really separate, like, there is some interconnection between the two, but like, your model monitoring, if you had to rate the current problem intensity on a scale of one to 10 for each, what would that number be? Though?  



The monitoring stuff definitely is more of an issue just because we barely have any sort of solution in place for that. We're very reactive at this point. So I would say definitely that's a ten. And then the deployment stuff that's slowly getting better as we go to this as we separate concerns more. So I'd say that's four, probably.  



Okay, I see. Okay, understood. Got it. If monitoring itself is like, ten or ten right now, are you actively evaluating other platforms right now to figure out which monitoring solution might work best? What are you looking for? Let's say if I gave you a magic wand, that okay, get whatever you want for monitoring, what would you pick?  



One thing I liked about ML Flow is or one thing I didn't like about ML flow? I saw this addressed on your website somewhere, but you could publish all these, like, atomic or scalar metrics to. ML flow, and they supposedly had this sort of way you could dive into those metrics and group them and basically perform analysis on top of them. And I found that very limited, and I think this is even said directly on your website, but you don't need to publish the PNG image of your confusion matrix or classification report. You can just back that stuff out from the atomic data points.  



So I like that a lot, not having because what we found with ML flow is that we wanted to publish this new type of metric or do this new sort of analysis that involved coding that up so that now the model is publishing that every time and then having to distribute that to all the models. So it would have been nice just to be able to just dump all this atomic raw prediction information, then be able to do analysis on top of that. But I liked the API sort of there and how dumping the Maxwell and stuff actually worked. I just didn't like the level at which you could do the analysis. So that's one thing I would say. And then even just further, I guess, just more analysis on top of just atomic data points.  



Then if you could do outlier detection on your ins and outs from there and that sort of stuff, I guess maybe that's approaching probably a new model that you would develop internally on top of that. But I guess that's sort of the direction I would I'm hoping to go towards.  



All right, I see. So how soon are you looking to solve this problem that we're talking about, which is like an outlier detection and just being able to do more analysis on your model predictions?  



Yeah, so there was a pretty big push. We started to get concerned about this earlier this year and then some priorities shifted and I think we're targeting like Q One next year as we're really looking to evaluate how to move forward with this and then start putting in solutions towards the end of Q One, I think. So we're just now sort of picking back up on looking at what it's out there as far as more managed services go and Vishal had recommended. We speak to you guys.  



Got it. Okay, for sure, yeah. And what's the general philosophy at ground speed? Do you all actually end up using a bunch of external tools to solve for these problems or the tendencies typically to build some things in house? What's been your observation there?  



Yeah, it's a lot of in house stuff, especially for the ML teams, just because it's for historic reasons, I guess, but these deployment systems existed for the standard software stack, and then we found ways to get the ML tools also deployed in that fashion.  



Okay, understood. Which is what I'm hearing as well. Based on the stack that you described, it sounds like the team actually intends to build a lot of solutions in house. Right. So why for monitoring specifically, like model monitoring specifically, you are currently looking for solutions outside. Is there any specific problem that was harder to solve internally?  



No, I don't know that we've, I guess necessarily tried to solve this internally. But I guess the assumption was that there are probably great tools for this specifically out there, so that maybe we shouldn't focus at first anyway on developing an internal tool for this.  



Got it. I see. Okay, understood. And one other thing is even in model monitoring, you can think of it as in two different directions, right? One is where you mentioned about being able to compute things like confusion matrix from your raw data, right? Like model predictions and actuals that you are logging. And then the other direction could be that you figure out and by the way, which is analysis on the output of the model. Right. So you have model predictions, you have actuals and you compare that and you do something there. But then you could also do things like drift tracking, which is frequently done on the input of the model, which is the features, are your features drifting and stuff like that. Right. And then there could also be just a general model performance monitoring.  



So like tracking your F one score of the model over a period of time, or accuracy, precision recall, whatever you want to track. Right. Between these three, is there a certain thing that catches more of your attention right now?  
"
11564066567,LendingClub,ragupta ragupta@lendingclub.com (ragupta@lendingclub.com),,LendingClub,>1000,500 - 1B,Oct-25,Multi Cloud,Yes,Financial Services,"San Francisco, California",Nikunj Bajaj,,https://app.fireflies.ai/view/Ravi-LendingClub-Nikunj-TrueFoundry-::vSTWRWsZHW,"Hi.  



Nicole.  



Hi.  



Ravi.  



Hey.  



Hi. Jimmy. Hi Ravi. How are you doing? Good. Nicole.  



Hey, Jimmy. I see. Other than note taker, how are you guys doing?  



Very good, very good. Have you used this Notetaker or similar other apps?  



Ravi yes, with some of our vendors they do take notes as I'm familiar. It doesn't spook me out anymore. Let's just say that. Okay.  



We have been using this and it's serving us quite well since the last few months. So we are happy about it. I gather your India trip was great.  



Yeah, India trips are always fun.  



Where were you india?  



I traveled all over. I am from Jharkhand, my wife is from Haryana, so those two places. And then went to Bangalore with family in Delhi and Calcutta. Pretty much all over the place.  



Oh nice. You have family in Calgary?  



I do have some family in Calgary.  



Okay. I'm from Calgary actually.  



Oh, you are? Okay, thanks. So you grew up in Calcutta?  



I grew up in Calgary. I'm born and brought up in Calgary actually. Almost close to Bharabazar. Do you know show up as I.  



Have know a few names. I'm just a visitor in Calcutta. I don't know the geography very well. Okay.  



But they are close to Rawazar, like just a kilometer or so away from it.  



Yeah, I mean your last name sounds like a Marvadi guy.  



Totally. Yeah. One of the very stereotypical Maravadi guy who moved from Rajasthan to Calcutta.  



Yeah, I'm from the same plan, but we are from Haryana. Marwari is from Haryana.  



Okay.  



Jinma is feeling a little left out, so let's stop that part. Where are you from?  



So my family is from Bihar, but I've been born and brought up in.  



Common. I'm the Marvadi from Behalf.  



Nice. Awesome. Ravi thanks a lot for taking the time to speak today. I think it sets some context in my LinkedIn message. But I guess I'll just give you a little bit more brief about the purpose of this call. So basically we are building True Foundry which is a startup in the machine learning domain, helping companies build out ML platforms. So like a lot of what we are building right now is what we saw at Facebook where ML platform teams built out a lot of internal tools that made our machine learning developers super productive. Like being able to deploy many models updates like launch on frequent updates and kind of stay at the cutting edge of it. Right.  



And we noticed that doing something like that without a platform, getting that level of productivity without an actual internal tooling basically is super hard. So that's a problem that we are trying to solve a Truefoundry. And currently we are at a stage where we are working with a few companies like Early Enterprises that we are working with and cobain the platform basically. Right. So in that context it's super helpful for us to talk to a few folks who are building and like, practicing machine learning at scale in some of the largest companies because that's a segment that we are working with.  



And in that context, I had reached out to you primarily to understand what type of machine learning problems that you all are trying to solve, what's top of mind for you from a modeling and an infrastructure perspective, and getting any business outcomes from machine learning, basically. So just wanted to discuss that a little bit.  



Sure. I'm happy to give you what we do. I don't know if you work with any fintech. Fintech is a little tricky and I'll explain in more detail what that means in terms of we have a lot of limitations around how much data science you can actually deploy. You can analyze all the way to glory. Nobody stops you from that. But from protection, it gets very tricky and I'll get into that in a little bit. I can answer that question. So do we just jump in or do you have anything else you want to feed me before we jump in?  



I guess I can just give you a very brief background about myself as well. And maybe I can also set some structure to how some of the questions that I would love to get answers from. Of course you can. Also briefly about the fintech issue that you just described. Right, so my personal background has been in machine learning. I worked at a startup called Reflection where initially I built out a lot of ML models myself, like recommended systems, personalization algorithms. And then the last year and a half, I got a chance to build out an ML platform for the company because we had like five, six teams building out machine learning models. So it made sense for us to build out a horizontal platform. And then at Facebook, I did not do any platform work.  



I just did conversational AI, led one of the conversational AI teams, but I got to use their platform. And that actually opened up a lot of insights for me. I guess that how much Delta there was based on what we had built out at Reflection versus what I saw at Facebook. And the interesting part, one of the realization for me was while I was at Reflection, I never realized that all these interesting things can be built out basically. So I thought that what we had was amazing. Only after I got exposed to Facebook is when I realized that, oh my God, there could be so much more.  



That's what we're trying to bring.  



But that's practically my background. I would love to learn a little bit about you and then jump into understanding a couple of things, which is what are some of the machine learning problems you're trying to solve? How is your team structured from a machine learning data science perspective? And how do you like what's your process of putting the models to production. What are some blockers there? These are the three main that I would love to get an insight on.  



Chida is part of your team.  



May you want to also briefly introduce yourself?  



Yeah.  



So I've been part of the two foundation since the last four to five months. Before that I was working with McKenzie as a management consultant and I graduated from It in 2001. During college I worked mostly in natural language processing and machine learning, where I also got a chance to work on CD in the internationalization team. That's a bit about me.  



Based india. I'm just trying to get a more context of where based.  



I am based in San Francisco. Shenma is based in Bangladesh.  



So you guys have offices in both places?  



We have a couple of folks operate out of Europe. Time zone, basically.  



Got you. Okay, so good background on me guys. I've been with LC for four years. I've been exaggerated probably when you were in diapers from It. I've been in the fintech industry mostly. Fintech industry itself is like ten years old. Before that I was in banking at HSBC and consulting for price for the house. And since for the last eight, nine years been very much in the fintech space. And I span mostly in between analytics, product and pricing, risk management. Those are domains that kind of move around in depending on the need of the company. Currently I delete the data science and I had fraud on my plate and then I transitioned that to somebody else. I have taken on more of a business role now. So that's my current role at Lending Club, jumping into your questions and on data science front.  



So when it's not fintech, when it's a pure big tech, you don't really have a lot of regulations in that space. So you can use any kind of personal information from a consumer perspective and build any kind of models. You could show a white guy versus a brown guy versus the black guy, different recommendations, and there is no issue around, hey, you're having this current treatment because you are responding to their needs and what they are looking for. When it comes to lending, any fintech product, insurance, payment, probably not so much, but mostly insurance and lending, you need to have a very clear explainability of your ML. And because of that, our hands are very tied to we don't go anything beyond the classical MLS like neural net or a gradient boost or a random forest, which is all classical ML. Right?  



ML has expanded to a lot more than that. But at fintech companies it's been a hard sell in terms of, hey, we have this AI we can use to improve the pricing or approval rate or blah, blah. It's just not there. Like it's just not allowed by the industry because when you have a black box ML, it's difficult to explain to the regulators why I gave a certain Apr or why did I force somebody or decline somebody? So that makes deployment of ML very tricky in the fintech space, the sustainability part of it, right? So that's 1 second is if you think about the state of the art is you start with your historical data.  



You do a bunch of testing in Python across all these models you host, raise these three, four models and then you deploy a version of it in your Java code from production environment. It could be no, Java one of these languages. You just deploy the code and then you have to wait for three to six months before you can see performance in terms of responses are perfect one month. But credit quality of your portfolio, it takes six months because that's how long people take to charge off on the loan. So it's very difficult to charge off. If I give you a loan today, it's only after six months I realize you're going to pay me back or not. Right?  



Oh, I see. Okay.  



So that also increases the lead time of iteration, so to speak. But let's say you're at Amazon. You're recommending a blue packaging versus the red packaging. You'll see a response in two weeks. So you can really trade and do a lot more AB testing there compared to in a lending environment. So because of that, a lot of the lending companies, they keep talking about ML, but they don't really a lot of ML because the returns are kind of not there. Because around what you can do with it and how long it takes to iterate.  



Understood. Just a question on that, Ravi. Are there use cases besides the one that directly go into decision making, let's say something that you would need for additional data point in order to target your sales effort or something like that, or to push a certain product? Are there such models where the regulatory hold is not that great? Does this kind of use case exist?  



So, that's a good question. There is. When it comes to marketing, you have less amount of regulatory scrutiny because you're just telling people to come and apply for your product, right? And bringing in conversation AI could be one. If you have chatbots on a portal or on a third party website there we could apply the whole GPT AI, the latest ones, to say, hey, this is the right language to use, this is the right way of reaching a customer so you can increase their response rate. In that case, you're starting to do a little bit of like my team started to build again very basic models, right? Which is doing great in boost nothing because we don't even have an ML platform today.  



So we just do a bunch of work in Python and if it works, we just deploy it into our production environment. Like we've been talking to Amazon H Two to try to see if we could test their platform, but there's not enough use cases even on the marketing side today to actually deploy it.  



Okay.  



And is this considered like a priority for the organization in terms of its business impact or something? Or is this still on the sideline?  



It's mostly on the sideline because it's cute, but it doesn't really drive the business. They use the word cute. That is still physically that's how they look at it. The bread and butter is the risk models, which is mostly decision trees. Actually, it's not even a model. Right. So you have models that generate these risk scores and these models are mostly GBT models. That's about it. So it doesn't really go any deeper than that. So ML and data science in lending space and insurance space probably not. Like a buddy of mine from my badge who launched it's called Rocket ML. He might have based out of yeah. So the founder is my classmate and he and I have been walking, talking through he was able to get into one of the Amex vendors with something on the marketing side.  



But when it comes to actual lending decisions, it's not worth the effort because the Cam is not that big and the problem doing its statement doesn't lend itself to ML. Very well understood.  



I see. Understood. This is very helpful context. Ravi, one question that maybe I would love to understand is you mentioned about people building out these Python based models and then quickly deploying it to production, et cetera. Right. So, in this context, how is the team structured at LC? Right? Like, the people who are building out the models, are they the same people who actually deploy these models and maintain these models of production? Just like separate teams?  



No separate teams. Right. Because we have data engineers and then you have a data scientist. Right. We basically have people with stats, PhD majors who come in and basically build these models and build the next generation six months, twelve months out once they have more fresh data. So recency of data is just so the data quality is the biggest thing in lending as opposed to the model itself. So being able to generate the next version using recency of the data and recalibrating those basic models, that's the bread and butter of statisticians in many companies. So what we do is the build and the pass on the requirements to the data engineers.  



They do the deployment, the testing, the launch, and then the maintenance of there's only one version in production, so they manage that while the other group of data scientists, they just look for more data and keep refreshing the buyer in progress. I see. This is also very different. Right. If you talk to a data scientist who don't know how to deploy a model, if you talk to a data engineer, they won't know how to build a model. Makes sense. Yeah.  



And these models that you're talking about. These are the typical ML models. Are these for risk or this is for which use case?  



So it's mostly for risk. On the marketing side, we have just started to play with some of the basic modeling. So far it's been very brute force. It's also a function of lending them, not being the biggest lender. Like in terms of there's not a whole lot of optimization to be done. The way the channels work, they're very straightforward. But when you go to a big company like Amex or Capital One, they have economies of scale where even a 0.1% improvement helps them actually get that incremental topic up on the left. So for us, it doesn't make sense to invest that much on the marketing side and on the risk side, there's not a whole lot to do. Which is why ML is something which it always almost looked at is like a good to have.  



One area which we are exploring is alternative data, right? So how do we bring in non bureau data into the room and how do we pull signals out of it? So I'll give you an example. So were just talking to a company who uses they haven't given us the signal, but they're using bank cash flow data to come up with underwriting models. Now that is one space where you could have some wins if you're able to pull out signals using ML from the bank transaction data. Okay? And that's one area which is very nascent. It's up for disruption. A lot of companies have tried it, but nobody has gotten a success. Like they were pitching a solution to us, but the runtime was like 45 seconds.  



So they said give us your bank data and we will be able to give you back a risk signal, but we will take 45 seconds to generate the signal. That's too long. Today's game was people are talking in milliseconds, right? Like one or 2 seconds in the worst case scenario.  



Interesting.  



Makes sense if you guys can figure out that could be one area I would love to stay in touch with. If you guys have a solution where you can say look, I'm going to take your cash flow data and I can generate a signal for you in 300 milliseconds or 400 milliseconds because patient time is not that much. I don't know where is it? Just maybe I don't appreciate the problem that much. Or maybe those guys are just behind the curve. I don't know which one it is.  



Okay, but is this like a processing problem or is this something else related to the pipeline? They haven't revealed that. I understand.  



Yes, they haven't revealed it and I don't know, but that if you're able to crack that code, that could be really big for learning investment.  



Makes sense. Makes sense. One other thing, Ravi is like you mentioned about this handoff of these models that are getting built by the data scientists to the data engineering team to get deployed. Right. And you mentioned that skills between these two teams are not very transferable. So I assume that given that the skills are not transferable, this handoff might be a little bit tricky, that people need to sit together or.  



Machine, because it's been happening for ten plus years now, all lending companies. So what they do is they come up with the productivity tools. So the way it works is you build your code in SAS, for example, right, in Python, and they have these converters, which converts that into their in Java code. And they look at the code, sanitize it and deploy it, and they come back with the results. And we do the validation, the test comparison of what I gave you and what he came back with. So that part is a very well oiled machine in the banking industry because it's been like that way for the last ten years, right? Like you have these two different groups of the other thing is your production environment is not suited for testing and sorry, for analytics.  



So you need to have a modeling tool like a Python or a MATLAB, where you can actually do all your coding and do the what if analysis and run your models. The production environment doesn't need that functionality. It's a waste of computational power.  



That makes sense. Yeah.  



And then you can't do your testing in Java and you can't do a production in Python. Makes sense. This is more statistics and analytics. This is more pure production and engineering.  



I see. And what is the team split between these two? Like, how many data science people do you have and how many data engineers?  



I would say for every data scientist, they probably have three engineers. If you have one dev, one, two dev and one QA.  



Interesting. I see. Got it. And one of the things is, let's say you had suddenly like two X the amount of resources, right? Like, let's say you need data science, you need fraud, and suddenly you have two X the number of data scientists. What are some of the interesting things that you would want to do with these data scientists, extra data scientists that you have on your team.  



So I think if you have additional horsepower, you will put them towards the marketing side of things to find ways for things like multitouch attribution. So you've done a bunch of Google Ads, you've done a bunch of Facebook ads, and you've done Direct Mail, you have done Gmail Now, which made the customer come to you. How do you find that out on the marketing side? If we can figure out multi touch attribution solutioning, that could be one area I'll put the customer base in. But again, not at Lending Club. It will be at other bigger companies. LCP doesn't add a lot of value, which is why the data science teams in these companies are not that big. In the last 15 years, it has not grown has been that much.  



It has always been like a team of five or six modelers for the whole company.  



Got it. Interesting. I see. And how frequently do they actually end up like updating the model itself? Is it like in the order of many months?  



I want to say like at least annual view. In some cases it's every six months. On the marketing side, recently makes more impact. So it's more like six months. On the credit side, it's more like annual view.  



Interesting. I see. Okay, got it. Okay. So basically the frequency of building new models and the frequency of pushing updates to the existing models is fairly low, basically.  



Yeah. That is the nature of the industry you belong to. Compared to that at Amazon, you're probably turning to models every month.  



Right? That makes sense. Yeah. And how about the customer service side of things? Like, do you have a lot of these chat bot and question answering that type of modeling?  



There is a lot of that, but I guess, again, it will depend on the size of the company. So if you go to bank of America, if you go to a cap for and if you go to US bank, those places, they'll have a lot more return for investing in like chatbots and conversational AI and streamlining your communication process. Compared to that at smaller companies like LC, it's not worth the dollar to spend there because they're not that big.  



I see. Understood.  



Also mentioned that there is a lot of testing, et cetera, that you guys have to do. How do you facilitate that? And also like the model explainability part, is that something you have tools for or is it like handled by some vendor?  



How does that work? Testing is straightforward. Right. So basically I give you a simple regression model. You deploy it and there's some results which you expect back from it. The results are given back to us from the engineers, our data scientists compare the results and that's how you test, basically.  



Go and deploy it in production.  



It's a standard, it's a mock environment. Okay, and then what was the second question?  



Yeah. For explainability of these models, do you use any tools or how is that.  



Going to be explainability? So because you're not using like black box ML models right. It's mostly like gradient boost m, so it's easy to trace back towards score deploy. What's the point? In production chipma is only data from the credit bureau. So using a credit bureau attributes, you come up with some scores. Using the scores, you build a decision tree and you end up in a decision tree node and those nodes have approved the client outcomes. Basically, it's very easy to explain it to go after the attribute.  



Basically. In general, like tree based models are like by design, they are explainable, right?  



Yeah, exactly.  



Got it. So do you guys end up using any external tools for your entire modeling pipeline from whatever your experiment tracking to deployment monitoring?  



Experiment tracking? Yes, we'll use optimized for our A B testing, for example. Right, for your session tracking. I think those are two key tools we use from a testing optimize the heap. That's about it. Everything is on the cloud. It's redshift. AWS, redshift is where all the right stores we have these production tools, which you need, but specifically for ML, it's not like we go and we have a big H two account or whose databricks account. No, that's not what we do.  



Makes sense. And how about model monitoring? Has that been a concern? Like, for example, tracking things like data drift, concept drift performance of the model in production, like in real time?  



Yeah, that's a good question. Again, depending on scale required, you might want to say, I want to use the tool outside in. But right now that's all done using the SQL queries, you know, the scores and the model. Right. So on a monthly basis, the same data scientist, what we do is look at the production output and see what is the drift every month. And if the drift is outside of your range, that's when you kind of say, hey, I need to do something about this.  



I see. Okay, got it. So actually being able to monitor this in real time when I say real time, not like real time, I mean like near real time is not a concern that for whatever reason, let's say data format changed, right, and the model started breaking. You want to be alerted for that, right. Or some data drift happened because of, let's say, an event like COVID. Right. Being able to detect that and respond to those things, has that been not a concern? Like a big concern in general?  



Not really. Because the amount of testing which grows in is pretty huge. So once you put something in production and when you make changes to it, there's a lot of logic, which happens after that. And there are enough splunk logs and alert systems in place which trigger any kind of deviation, has enough root cause done by engineers. When event like this happens, more often than not, it's mostly a big bug in the port.  



So you're saying usually like these.  



So.  



I was just saying that in general, the degrading of the model has, because of at least manual errors, has not been a concern at all.  



Basically, yeah, and it's obviously not. There are nonzero errors. But the point is, there's enough feedback loops to catch them early enough that your loss of business is not big enough. That's a concern.  



I see. Understood. And the drift and all. Again, given that you check for drift, like every month, generally, you don't see data getting drifted in this business, like more frequently than that.  



Yeah. And then you have all the older instances of your code base.  



Right.  



So let's say you see something which is off, you suddenly revert back to the older version, deploy it and then go to the root cause in the meanwhile, because that sort of butter of the company. So you can't really afford to have errors, unchecked errors go on for too long. It really impacts the bottom line of the company.  



Got it. I see. Okay, understood. This is very helpful background. Jimmy, did you have other questions here?  



No, I think.  



Got it. Understood. We are also almost on time, Ravi, so we wanted to keep a few minutes to answer any questions that you might have for us as well.  



I'm glad that you guys are I'm just curious what made you guys jump off from the corporate and pick on this challenge?  



Sure. I think the story for me is basically having experienced the problem personally. So I guess the story for me is like a two part story. Like I had started up another company before starting True Foundry and that was because of the problem that I had experienced in the hiring space. I decided to solve for that problem. So I spent about a year building that and that company got acquired by one of the largest HR tech players india. And while building that startup, I again came across this machine learning model building and deployment problem, which resonated quite a bit with the story that I told you about building a platform at reflection, not realizing. So those three things basically came together, the trigger point for us to jump and start building Truefoundry. Basically. That's the story for me.  



Actually, I'm also very curious to know your answer about how you decided to leave your cushy job at McKenzie and join like an early stage seed startup. Actually.  



Yeah, I always wanted to build something of my own. I had always been interested in machine learning. This was something. And having been at McKenzie, I got some view of the business side of things. I think this was the best of my interest, combining the best of both world machine learning as well as business aspect of things that I get to work on. That is all. That's my motivation.  



Awesome. Good luck. I know this has not been the most answers which you are looking for, but at least tell you what not to do, what not to focus on.  



Absolutely. This is very helpful, rabbi, I think. Always good to hear. Basically put guardrails around where you're moving it's.  



Great. Cool. There's other things which come up. I'll keep you guys in the loop. If we have needs from ML platform, things change right in the Fintech place. When you get more tech, that's when ML becomes interesting and useful. So we stay in touch. If things evolve, we can keep talking.  



Yeah, for sure. Actually, one thing that we can actually use your help in is at this. Point, I think. I'm sure that your peers, like from your undergrad days and stuff, are leading similar teams in a lot of other companies and startups and stuff right. Who are probably not in the fintech space, which is super guarded. So if you can maybe introduce us with one or two of your peers. Not asking for a lot of introductions here, but given that, you know, the problem that we are trying to solve and the background with which we are trying to solve this problem, maybe if you can make one or two introductions there, that would be very helpful to us.  



Yeah. So what you could do is if we can identify some of my LinkedIn network is public. Right. So if you can find people in my network who think might be useful to you, that would be easier way for me to because otherwise I'll just do. Guesswork that makes sense.  



Yeah. We can totally do that. So maybe we can take that action item, scan through the network of Ruby and then maybe send out a few folks, maybe four or five folks, from which Ruby can choose one or two introduction that he feels comfortable making. Yeah.  



So at least make a list of five or seven, because a lot of the leads are cold leads.  



Right?  



For sure.  



Yes.  



We have to talk to things undergrad, for example. I know. Yeah. From that, I'll be able to find a few which are little bit more warm, and I can make the connect 100%. Awesome, guys. Cool. Good luck.  



Thank you.  



Thanks. You have a good one.  



Bye.  



Talking to you. Thank you. Bye. "
11488154639,CarGurus,,,CarGurus,>1000,>1B,<10,AWS,Yes,Internet Publishing,"Cambridge, MA",Anuraag Gutgutia,CarGurus_Jonathan 05-01-2023,https://app.fireflies.ai/view/Nikunj-TrueFoundry-Johnathan-CarGurus-::wE3adOjt7e,"Please click the link for call notes

https://cheem.notion.site/Cargurus-e9069ac32b674473bb2f401c5bf437a9"
11488124534,Outreach,carlos.garcia carlos.garcia@outreach.io (carlos.garcia@outreach.io),,Outreach,>1000,100 - 500 Mn,Oct-25,AWS,Yes,Software Development,"Seattle, Washington",Nikunj Bajaj,,https://app.fireflies.ai/view/Nikunj-TrueFoundry-Carlos-Outreach-::ctPMSDsGqL,"I can. Yeah. 



Yes, I can. 



Good to see you again. Been a really long time. Carlos, how have you been? 



Good. Yeah, I can't complain. How about you? 



I'm doing very well. Am taking a bit of a travel right now, currently india. 



Nice. 



You're based in Seattle, right? 



Yeah, we have snow right now. Snowing in redmond. 



Nice. 



Oh, you're in redmond, okay, understood. 



Nice. 



Do you have any plans for the Christmas break? 



We're going to stay around here, so plans with some friends and going skiing, stuff like that. 



Nice. Sounds fun. 



Are you an avid skier? 



No, I'm an average middle of the run snowboarder, ice snowboard. 



Okay, lovely. 



Just enough to have fun. 



Great. 



Are you keeping in touch with Facebook folks from our team? 



So Kushal works in my team. Do you remember Kushal? 



Yes, Kushal Lakodia. 



Yeah. 



Krishal works in my team and then Go Away is kind of my peer. So he manages the back end team. I manage an ML team, you remember? Go away. Go away. 



She yes, I haven't worked together, but yes. 



I work with it kind of on a regular basis. 



Oh, nice. 



So you have a couple of folks from the team like your coworkers again. 



Good. 



Tell me what you've been up to through Foundry. 



Yes, truefoundry. We took this, I think since quitting Facebook, actually. I've been on this entrepreneurial journey, Carlos. I actually started up initially in the HR tech space first and spent about a year building that startup and that got acquired by one of the largest HR tech players india. And now with the same set of co founders. This is the second gig that we're doing together through founder. We're fairly early stage right now, maybe like 1011 months in our journey of building out the product and seed stage. Currently at a seed stage. So we raised some money from Sequoia and working with a small team to build out the platform. And the idea is to build out something similar to the predictor service at Facebook. I don't know if you remember. 



Okay. Yeah. 



So think of it as almost like a continuation live. If you think of the machine learning pipeline from left to right live, that is the data and feature store. And then model training, and then model deployment, and then the model monitoring. Think of it as going from towards the end of training to deployment to monitoring. So like every learner, like end of every learner to the next predictor service. I guess that's how we think of ourselves. Or at least I communicate to Facebook folks. So it's not like full EPI learner, but it's like you're able to launch your training jobs on our platform, you're able to deploy your models from there. That's the part that we end up focusing on, actually. 



Okay. 



Yeah. 



And the idea here is that like, different. Like there are live basically like two. When we talk to companies which are not like very large tech companies like Facebook, Uber, Airbnb, et cetera, companies still end up spending a lot of time building and deploying ML models. When you talk like this, people who are building models and then they hand off models to other people or Live, this is a separate intra team and there's generally not a very large team who has bandwidth to build out. Live a full fledged ML platform. So that's the segment for which we are currently building out for companies that have enough ML models that they want to build but don't have enough resources that they can build out every learner like platform or a predator like platform, basically. 



So that's live the sweet spot that we are essentially trying to target. 



I see. Okay, is this for online inference scenarios? 



Yes, primarily we do both. We do batch inference and real time inference as well. So people can actually run batch inference jobs on the platform, or they can use we actually convert their models to API endpoints that they can invoke for. 



Online inference as well. 



Got it. Okay, cool. 



How about you, Live? What part of outreach are you currently leading? What's the work that you're focusing on? 



Yes, so primarily my team is anything that is NLP or text processing within outreach. That's kind of what my team owns. So we have a couple of scenarios. Some are around email processing. So we block some entity extraction out of emails. We do some sentiment analysis on emails right. From these pipelines that your sales representatives have with potential buyers. There's kind of a lot of processing that we do over the emails and then I know you're familiar with. So outreach has this voice assistant that joins your meetings and provides live transcription. And then there's some models, some NLU basically cash buyers and extractors that run on top of that. 



So for example, if you have a meeting, we have something that is running and the texting action items, we have something that is kind of think of it as a small Q and A engine. So you have a little database of questions and answers that your reps need to be able to answer. So if your buyer asks you a question and we detect we have a match for that question, we'll pop up on we call a content card for the rep to have little bullet points to answer the question. So that's some of the online kind of real time inferencing that we do. So yeah, primarily, as I said, email and then conversational kind of real time on top of transcription. 



Got it. I see, understood. 



So Live, the first use case, that is the email analysis and the sentiment analysis entity extraction kind of use case, I'm assuming that's primarily batch, right. Live this, there's a little bit of both. 



So we do have a semi online. So as emails come in, they go into a catch up queue and we're processing this out of this catch up queue. So it's not real time, but it's close to real time. So that as you go through your emails and we can kind of provide you in as soon as possible, but we do have a batch process also that catches anything that the online piece missed and serves us as a backup, as a backfill, all of those type of scenarios. 



Got it. 



I see. Understood. 



And do you guys use a lot of hugging face models? Is that like the bread and butter of doing most of the MLP now? 



Yeah. So basically we have some sort of a small PyTorch framework on top of that can consume and load kind of hogging phase models as kind of the base model. So it's basically a little bit of python on top of hogging phase to build. Most of the models are kind of in that reality. 



Got it. I see. Because live the PyTorch wrapper helps fine tuning the models a little bit better. 



Yeah. And just kind of some options for configuring live kind of the last layers and doing some managing the rounds. And there's a little bit of a framework that we build on top of that because she'll build most of that for us. 



Interesting. Okay. 



I was actually supposed to talk to Kushal also at some point. I will ping him and connect with him once as well to understand this. Yeah. So one of the things that I wanted to chat with you, Carlos, was I guess two different parts that I wanted to understand live, get your perspective on, which is number one in the NLP world, the team organization bit. 



Right. 



Do you see more and more people who are building the models? Are they the same people who are deploying and maintaining the models live end to end, including the infrastructure? Or do you see that mostly people are split across the model builders and would love to hear what are you guys doing at outreach? And then the second question is that if you think about the overall stack that we discussed right? Like feature engineering and deployment and monitoring and stuff like that, where are the biggest gaps that you are seeing in this world? I actually get fairly different answer to these questions when I talk with people who work with more structured data set versus people who work with more unstructured data set. So that's why I want to get your perspective specifically on the NLP world. 



Yes. On the team split, there's been kind of a shift. Originally we have originally there were two teams, there were an email team and we call the Voice team, which is like any kind of prescriptions, those teams are now merged. The Voice team was very much from the team. Everybody deals kind of everything into end. Right. So the people building the model was the person live, deploying the model and kind of doing everything. Whereas on the email side it was a little bit more of you had a scientist building the model and you had like an ML Ops engineer doing the deployment and the pipelines and that side of things. Now we kind of have this little reorg where we merged the two teams and then we split off some of the machine learning Ops engineers into a platform team. 



So now there's kind of a dedicated machine learning platform team. But to be honest, I think that's still a bit of flux exactly where that boundary is of who does what. In general, the criteria has been if there's a piece of infrastructure of machinery or something that applies across multiple models or across multiple teams that will be owned by this machine learning platform team, but anything that is kind of specific to a particular model or a particular team, the team will build it EndToEnd. So right now for the most part, we're kind of doing everything like my team is owning from data to deploy model, because in particular because this MLS team is kind of new, is ramping up and so they're still trying to find exactly what pieces they're going to support kind of as a general thing. 



But yeah, for the most part right now it's more kind of single team managing or building things end to end. 



Got it? Understood. Understood. 



And how about the intra location on this bit? 



Live? 



I understand that the deployment, like the actual maintaining the pipelines and stuff is something that team is owning, like getting the machines live, scaling up the clusters. Is that the same team that does live your regular application infra? Or this is a separate ML infra team? 



Anything that is specific to our live, let's say one of these models. Right. So we'll basically do the only training and offline stuff is on data bricks. So there's a team that manages the databreaks instance and they make sure that it's running smoothly, but just kind of the general infrastructure. They take a look at costs and all of that, but like running the specific jobs or building the pipelines that's on the individual team. 



Right. 



So we build those pipelines within Kinetics. That's kind of for online and training for online. Right now we deploy to Kubernetes. So we do our own Kubernetes deployments. We roll those out. We are responsible for having the Pots running, for doing kind of the monitoring of those. 



Interesting. I see. 



So do you end up using any Argo workflows or something on top of it? 



Right now our deployment is on DevOps, Azure DevOps. And it's fairly simple. On the voice side, you also have this split thing where anything that is on this voice and transcript stuff is on Azure and anything that is on email is on AWS. So that's where the things are. A little bit of split on the email side, I know they do have some margo stuff that I tried to migrate out of, but I'm trying to remember what they're moving towards. I'm lacking on, but there is a little bit of more additional workload. The email side is a little bit more mature. So that's kind of the original piece of outreach that was built first, and Kayak, which is the voice assistant, was kind of a second. So it's a bit of a newer product. 



So the interest is not as mature as on the email side. 



I see. 



What you mean. 



Okay, got it. 



I do have a follow up question, but I want to take up the second question that I asked you, which is, in this pipeline live, where do you see things are the most broken currently or things are the most time consuming that you would rather not have your team spend time on? 



That's a good question. I think, for example, for online, for monitoring right now, to be honest, it's a bit of a manual process, right? So the monitoring itself is happening mostly on the calling side. So the service that is consuming the model right now, it's doing kind of effectively the login the metrics, for example, computing like distributions or logging live metrics for distributions and things like that. And that is to get an impact to Data Dog. And that's where we put the dashboard and alerts and everything. So that's a little bit kind of a manual type of thing. I feel like there could be a little bit easier way of just saying like, okay, lock me and monitor distributions. Right. 



And where evaluating I lost you. 



I'm saying that process is a bit manual in terms of in the Go service that calls these models, we're writing code to publish this metric into Data Dog. Then we build the dashboard in Data dog using TerraForm. So there's a bit of a manual process here that I could see there could be something for right. To support this a little bit better. So I'll tell you, we're evaluating a vendor for some of the monitoring right now. I don't know if you've heard of robust intelligence. 



Robust intelligence? Actually, I have not heard of them. I've heard of a lot of vendors. 



In this space, but yeah, okay, robust intelligence, yeah. Okay, interesting. Okay, hold on, I'll take a look at them. Yes, understood. 



So I have one question here that you mentioned. So when you're talking about monitoring here, do you mean to say the machine learning, model level monitoring live things like, is your data drifting, is your model performing good? Because I'm assuming that the system monitoring and the API health monitoring, et cetera, is well taken care of by datadog, right? 



Yes. And that so our platform team, like Goa's team, actually, they build some sort of, I guess a little bit of infra layer on top of data docs to do that. So most of that is pretty simple for us. It's a couple of API calls and a couple of statements here and there to enable that. That is mostly there almost for free for us. So the availability, latency, things like that. 



Yeah, right. 



So the part that is missing is the machine learning model monitoring side, right. 



The distribution shifts. 



Yeah, right, okay, understood. I see. 



And on this one, like Carlos, actually, have you also evaluated some other vendors like Arise, AI, Fiddler, True Era, et cetera, or Robust Intelligence is the first. 



Platform that you're the first one I know a different team was evaluating Wild. 



Okay. 



But I don't know how far they've gone with that yet. 



Got it. 



I see. Understood. 



You also mentioned about the AWS and Azure part. Right. So is there any technical reasons why the company is using two different cloud providers? For business reasons. 



Yeah. So the company itself is primarily on AWS. And the reason for you going on Azure is we have a strong partnership with Microsoft for the speech recognition. So we have a both, I guess from a business perspective, our relationship with them, and also from a technical perspective, they give us the ability to customize the models in the way we need and that has been a close partnership with them. But primarily it's because we run the Azure Spatial condition is that most of that, all of the voice processing and transcription and everything is an Azure. 



You got it. I see. Understood. I see. 



One other thing, Carlos, is in the part where you mentioned and I'm asking some follow up questions from my first question that I asked you about the team structure. 



Right. 



So, for example, if a developer needs more machines to train their models on, like at Facebook, we used to have this UI where we can go get more machines or live in Apple and we launch a job and it automatically figures out whatever machines that it needs to run on. 



Right. 



Is that part also currently automated for developers or they need to interface with some DevOps team that is going to allocate the machines? 



Yeah. 



So basically right now we use data bricks and there you can provision your cluster right on site that you need. So we kind of do it on your own. You do get that sometimes where you'll meet the quota of whatever your subscription or your subscription is set up for. So a couple of times we've had to go back to Microsoft and ask for a quote increase. So that's where you can get a little bit of friction. But for the most part, yeah, it's us. Just provisioning clusters as needed. 



Got it, understood. I see. 



And for this part, have you guys not considered using AWS, Sage Maker or Azure ML, which is, take a note, managed ML of the platform? 



Yes. 



So build up something on top of Open, it seems like, right. 



Azure me for deployment, for the deployment side yeah. 



For deploying your machine learning models. 



Yeah. 



So for deployment side, to be honest, we didn't explore too much on the Azure ML side. We did try Azure ML for a little bit, primarily for training. And there it was not a great experience because we have a requirement that the data does not leave basically a VPC. Right. So we have like a cluster within Azure that all the data should kind of remain. And the way we had to deploy Azure mail to be able to be part of that BPC basically required that to interact with Azure mail to the front end, you have to use this kind of this VM to hop into that VPC. And that was very painful. Right. To have this additional set of hop into effectively a remote desktop or a remote shell to do that. And then that was the main reason. 



There were a few little quirks here in HML, but that was the main reason to move to the networks because databreaks, they have this data plane control plane. So the data plane we could host within the BPC, but the control plane could be kind of outside and that allows us to interface directly with databricks without having to kind of hop into these grid configurations. Plus the rest of the company on the AWS side was already on data breaks. So that was also kind of a reason to go with them. But that's primarily for the training. 



Got it. 



I see. 



And for this one, like you talked about, Azure ML Live. Is Sage Maker similar experience or Sage Maker something that you never evaluated? 



I've never evaluated. The history here is that I primarily have worked mostly on the Azure side of outreach. It was relatively recent that the email side kind of got merged into became a part of my team. So I personally have not worked a whole lot on the AWS side, even though now some people in my team kind of are on that side. But yeah, I haven't gotten as familiar with that. 



Got it. 



Understood. 



And the other thing is the choice of deploying it on Kubernetes as opposed to one of the more managed ML Ops platforms, was that taken, like, before you joined? Was there something that you were part of that decision making process? I would love to understand any rationale behind that. 



No, that was kind of there before and it just has not risen to the top of the list of things to reevaluate. So we haven't really gone much further down route. 



Yeah, got it. Understood. Okay. 



So it just works that way and people are fine with it. So you never hear complaints about managing Kubernetes clusters and debugging on Kubernetes from ML developers of the sort. 



Right. 



Is it not really, no. Actually Go's team manages most of the Kubernetes cluster and yeah, so it was set up before I joined. So it was relatively low friction to just add a couple more deployments into kind of something that was set up. So it was not too hard for us to kind of expand on that. In terms of runtime and debugging, it has not been bad. I can't remember any kind of particular difficulties around debugging there. 



I see. Understood. 



And here one of the other things, is that with NLP model, specifically for that matter, all the unstructured data sets live, being able to debug what's happening live, what's wrong is fairly hard generally because it's not like you can look at live, identify which particular segment of your text hello? 



Yeah. 



You broke for like 10 seconds. Yeah. We're talking about the kind of the difficulty debugging text models. 



Debugging? Yeah, debugging text models and writing test cases, maybe for these models. Do you think that the team is already writing a lot of test cases for the models itself? 



Not as probably not as many as we probably should. That's one of the areas where this robust intelligence offering seems appealing. They have a framework for doing stress testing on NLP models. You can look at the roughing, but that's one of the areas we do have kind of our typical test set on BBT, but that could be a little bit more comprehensive. 



Got it. Understood. Okay. 



I think this is very helpful. I think at this point, I'm actually spending quite some time talking to people and understanding where are they seeing problems. It turns out, actually, monitoring is one area that stands out quite frequently, as you mentioned as well, that people are facing a lot of challenges in the model monitoring side of things. So something that I should look a bit more deeply into, like as a platform, we have not focused too much on specifically NLP monitoring. I think we've been focusing a lot on the structured data monitoring, like figuring out data driven on individual features, because a lot of those algorithms actually don't work so well for NLP use cases. So we are doing a little bit of that. 



Actually. 



On that note, Carlos, it would be very interesting for me to kind of at some point get some feedback from you on the platform that we have built out in terms of how to set up the infrastructure, how to deploy models and monitor the models and stuff. If you would be open to whatever, spending 30 to 40 minutes with me going through a platform demo and giving me some feedback there. 



Okay, yeah, I could do that. 



Maybe we can reconnect after the holidays, or if you prefer, during the holidays as well, given that you mentioned that you'll be around in the city itself. So if you feel like you have more time there, that works for me too. 



Yeah, probably after, because I will take some time off. 



Okay. After the holidays, maybe I can pay you again on LinkedIn or something. We can set up a time to chat. 



Sure. 



Yeah. 



I'll be curious to see what you've been building. 



Yes. 



And besides that, do you travel to the Bay Area frequently? 



Not really, no. The company's seattle based. We don't have a small office in the Bay Area, but I never have a need to go there. 



Got it. 



Do you guys generally work live mostly remotely or do you guys work mostly from the office. 



So we're in a once a week scheduled right now, and in February, were going up to two days a week and then the rest from home. 



Oh, I see. Okay. Wow. Okay. Nice. Yeah. 



So that's fairly comfortable. And for the once a week, do you guys keep live the entire office space reserved throughout the year? 



Yeah, so I don't know how the lease works, but the same office we've had for a long time, and yeah, it's been empty for quite a while, but more people are coming back, and some people that live in Seattle live closer, just decide to go more often. Right. I have one person on my team that usually goes three times a week because it's convenient for them. 



Okay. Yeah, understood. All right. 



Very nice. Super nice to connect with you again, Carlos. 



Yeah. 



I would keep in touch with you live. I'll reach out to you for the feedback call that I mentioned. And yes, we would love to stay connected. 



Yeah, sounds good. Let's chat in the new year. 



How's? 



Happy New Year. Happy holidays. Have a good one, Carlos. 



Happy holidays. Bye. "
11488077643,Crowe,Thomas Callaghan (thomas.callaghan@crowe.com),,Crowe,>1000,>1B,Oct-25,Multi Cloud,Yes,Accounting,"Chicago, Illinois",Nikunj Bajaj,,,
11487646267,Zulily,Priyank Gupta (priyank@zulily.com),,Zulily,<50,100 - 500 Mn,,GCP,Yes,Retail,"Seattle, WA",Nikunj Bajaj,Zulily_Priyank 27-12-2022,https://app.fireflies.ai/view/Meeting-with-Nikunj::MkFW8I01q3,"27-12-2022

Intros: Done

Business problems and use cases:

Personalization and recommendation; Real-time

Forecasting sales of a product; Real-time

Supply chain side - how much qty to hold in the warehouse, how much do we store

New ideas  reverse image search + NLP (use some kind of similarity)

Moved from decision tree based models to deep learning models. As of now a healthy mix but moving towards deep learning models

What cloud are you using?

Kubernetes

Is everything dockerized

Cloud provider = AWS & GCP

What was the motivation to go in two different clouds?

This decision was made before he joined

Leverage the best of both worlds

Google - great on BigQuery

AWS - great on real-time productionization of models

Would model training have an on GCP and productization on AWS?

Yes - correct

Decided to build their own platform by using multiple open source

Feast = feature store

Kubeflow

Built something custom for the server layer

Model DB for experiment tracking

Maintaining all of this themselves

We use EKS and GKE

Reason for not looking at Google Vertex AI / Sagemaker

Was not mature enough about 3 years back

Hows your experience managing Kubeflow?

Parts that work well

Can have pipelines defined there

Issues = certain bugs, some features were not available

What are some of the features that Kubeflow was not supporting?

How is the team structured?

Which team manages the infra layer = platform team manages this

Who are the people building the models = Data scientists

Who are the people deploying the models = Applied MLEs put the model to production

What is the interface between data scientists and ML engineers?

Toughest part - Capacity estimation for real-time features.

What is the server layer you use?

Nf

Do you use a lot of AutoML?

How many DS = , MLEs = , ML platform team = 6

In the entire stack are there any problems that you are facing? (see 35 mins onwards)

Ease of onboarding of the data scientist on the platform

Ease the hand off process between the data scientist and MLE

Some features missing around Kubeflow

Infra provisioning  ~ Making is seamless for MLEs to provision resources (depends on use cases, ofcourse)

What are you using for monitoring?

We dont offer anything on the platform. Upto the data scientist to look at this

Next steps: Will do a deeper diver around challenges & get feedback on the platform + see video before the call

Link to watch before the call https://feast.dev/resources/a-behind-the-scenes-look-with-matt-ziegler/

https://www.youtube.com/watch?v=6JWbxGBmQC4
"
11354404209,Pega Systems,mattareddy.mannem@in.pega.com;Arijit Mitra (arijit.mitra@in.pega.com),,Pegasystems,>1000,>1B,25-50,AWS,Yes,Software Development,"Cambridge, MA",Anuraag Gutgutia,,https://app.fireflies.ai/view/TrueFoundry-Pega::6zq2fEoIxV,"Yeah, hearing you.  



Yes, able to hear you. Is Mata also joining?  



No, Mata got a conflict today so he won't be able to join.  



Okay, cool, awesome. Thanks a lot for taking out time for the call, really appreciate the same. So I think from our side we wake and myself will be there, so we'll dive right in. So just for context, can I get.  



A bit of introduction before we dive in?  



Yeah, of course. I'll go first. So I'm one of the co founders at True Foundry. Prior to this, like I graduated out of IIT Corrupt 2013 batch. After that I worked with a hedge fund called World Fund where we used to build trading strategies out of global markets and then spend my time between India first four years and then three years between us and Singapore. That time I was a portfolio manager managing around 600 million in assets for them. At the same time, as a member of the CEO of Looking After Growth Initiatives I used to angel invest as well as into a number of startups. So I was able to make like 22 investments and then post that like in 2020 quit my job. Avishek and Nikon who were my bachelor from It correct.  



They were on the same boat, they were working with Facebook we Shake on the software engineering side, Nikon the machine learning side. We all started our first company called Entire where were helping companies hire technically with a talent. We sold it to Info Edge and then we started to found a year back with the goal of helping companies speed up their machine learning pipelines. Sure, yeah.  



Go ahead, sure. So I work with Anada very closely with Anada, joined Roofonry I think five months back and before that I am into this Indian startup ecosystem for almost four years. Previously working with a startup and connected worker domain for Frontline workforce in manufacturing and then before that I was in consulting and graduated sometime in 2012 from Issamar. Work very closely with Anaracare. Nice.  



Actually Nikunj is not able to join. That is basically the note tickets for us to take notes.  



Here. And I basically had the AI from Pegasystem. So we are basically deployed at literally all 1400 companies. The AI for marketing, for customer service, for NLP, for process automation, etc. And before that I had a startup and the startup got acquired by Pega and that's how I joined it.  



Okay, what was it called?  



It was called Mesh Lapse. It was the NLP startup. We got acquired in 2015 or times.  



Okay, very interesting. Early days of AI.  



Been in this case for a long time now.  



For a long time. Basically the context is the following so I think we got connected by Ankur and he's one of the seniors from KGP. We are like very quick before diving into more about Truefoundry, we are working with companies to help them enable machine learning deployments faster. So basically when were at Facebook, we saw internal platform that Facebook had which made it very easy for building out the models and once the model is built out, the testing and the deployment and the monitoring of models and when we came outside we saw that this ecosystem was quite broken. Companies were facing internal tools, open source tools or building internal platforms themselves to kind of get this to happen. And we thought that it could be made much better and much seamless with a lot of flexibility for developers and the aid.  



And that's where the truefoundry comes in. We are a platform built on top of Kubernetes multi cloud from day zero and we enable our developers to kind of deploy models fast like whether it's a training code or whether it's like a production ready code and it comes automated with like monitoring so that you have debuggerlity right from day zero. And the main part is while this system is being built with making it easier for developers at the same time it has the entire right security framework etc. So that the entire team does not have to bother the right liability systems. So that is basically at a very high level what we are building. And the goal was to kind of dive a little bit more into Pegasystems as to what the current ML pipeline looks like.  



How do you kind of deploy models, how do you test it out? If there's an internal tool we use, like we got some context that probably there's usage of Sage Maker like how do you manage multicloud? Because if you have Fortune 100 clients, I'm sure they'll be on different clouds. So when you ship it to them, like how is that? So I wanted to dive into that a little bit for today and based on that happy to also do another follow up call where we can share more about our platform. So you also demo and potentially see if there is a synergy in which you want to try it out or it helps in any of the use cases that you are trying to solve.  



Sure, yeah. So if you wanted me to talk about how Pegas AI infrastructure right?  



Yeah. So I think we can start maybe with the use cases. I know that Pegas solves for a lot of customer relationship like CRM and business process and AI on top of that. So maybe we can start there and then we can dive into the developer side of things.  



Yes. So the biggest use case for Pegai is marketing. So all these types that are there, whether using GPMC City, all the top banks, all the top insurance companies, et cetera in the world, they in Pega under the hood. We are the gartner leader in real time interaction management which essentially I use would see those annoying ads like you talk something and that starts popping up with Facebook, that happens with third party data, we do it with third party data. So essentially we connect with the CDPs customer data platforms, we have our own version of the CDPs, but we can connect with external CDPs. We get in the data, we get in the customer analytical reports, et cetera. And then sort of we have this model factory which is called the Adaptive Models. The entire intention being that zero data scientists are required.  



So I just had to look at proof and read that's very similar to like Domino Data Labs. That's a project that we sort of crack. But what we do essentially at this point of time is sort of data comes in, the model gets built literally every five minutes. The model picks up the best predictors. It's basically an online learning model. It picks up the best predictors, it deploys itself automatically, no help required from any data center. That's why is the market leader in this segment. Having said that, there are certain models like customer models, Churn models, fraud models, et cetera, which are basically not these online marketing models. From there we are looking at a couple of options. The first is that since we are required by Fortune 100 companies and Fortune 500 companies, they have their own data science team, right?  



And their data science team are using either Vertex AI or they're using, let's say Sage Maker, et cetera. And they have this entire pipeline ecosystem. You can experiment, you can build models, et cetera. Also has these what is this? We sort of integrate with that. Also we are partnered with H Two because H Two ML libraries which are lot of our customers and we are always looking to add partnerships with providers who are deployed in Fortune 100 or Fortune 500 companies. So we can end up process being that since the execution is happening at Pega, we care not so much about the experimentation side of things because for the marketing models are the main models that we care about. Many other models that you want to bring in the scores or you want to bring execute the model, et cetera.  



We want to sort of get those models either into there's a format called P MML. H two has their mojo format. For example, you can bring in Python models as a PKL file and we'll deploy it using our MLS pipeline there into a stack and we should be able to execute it. So that's what we do currently, right? And the tools that we have mainly encountered out there are basically Sage Maker, Vertex AI, H Two for a lot of places. All of these are basically building out this entire experimentation pipeline model, building pipeline, monitoring pipeline, notifications pipeline, so on and so forth. And they are also investigating these days integrations with data robots, domino Data Labs, et cetera. And all this. But looking at your website, it's a lot similar to Domino Data labs that we have.  



Yeah. So I'll give you a little bit of context there. So the website is not like the best representation but I'm just not focusing.  



On experimentation and building out and all those things. Right. Data science pipeline, the ML Ops for the data science process to the deployment and the monitoring the renter cycle that we talk about.  



Yes, primarily there. But the more focus is actually outside of experimentation. So basically once you have built out the models so while building out you might need distributed training or you might need resources for training. So we kind of enable that and then once the models are built out the deployment in a scalable way. So you start deploying say with 1% of the traffic and that scales 200% of the traffic or if you want to do batch real time, all of those. But the experimentation part we do less of actually. So that's more like a support system, but primarily more on the deployment side, making it easier there. We can dive into that a little bit.  



So I just wanted to understand so I'm guessing you would be also building a lot of these models internally and testing it out before it goes into the client side. So there will be analog pipeline.  



Yeah, we are starting to do that. So this is one use case. The other use case that we have is in customer service where again we are the sort of partner leaders there. If you see let's say customer service salesforce is one, but they guys do and there as part of the AI also we have the highest we even beat out salesforce on the AI. Okay, I see there we basically have NLP and we have Cushioning. So there we have models like the search voice. So if you're talking to the call center, the data gets converted into text and anything model kicks over recommendation and then from there yeah, so these decisioning models come in, the predictive models and then they start giving offers back to the customer. So that entire loop is there from voice, from Chat, from emails, et cetera.  



And since Pega is a business process automation company, we are also into process AI. Process AI is basically optimizing processes. It's a very unique template. All these are basically done mostly with internal models that our data science team builds and then that's basically a recipe that we deploy with customers and then go okay, but we also want to activate customers this thing. So from there, that perspective we are also building out, we are also connecting to different sources.  



Okay, so most of these will be deployed on customers.  



Cloud is it like no, it's Tiger cloud. Yeah, customer cloud instance. But we basically we call in Pega Cloud which is essentially a customer instance. You have to remember these are large companies. Yeah, banks are data sensitive, there's a lot of governance rules and all those ensure they'll probably give you an instance.  



And then you will deploy the system on.  



XYZ compliant.  



Okay. So then they are fine with it. Okay. Understood. Okay. Got it. One more question. So when it's your cloud, like, do customers tell you that, okay, we only want GCP or we want AWS or.  



Is it fine, we are targeted both on GCP and AWS.  



Okay.  



What about no azure? We don't.  



Okay.  



By the way, GCP you as a startup now, what is happening in a lot of companies is basically Google is having these budgets given to GCP, targets given to GCP. So we are basically like for example, a large telco provider in UK just spending $50 million just from shifting from AWS to GCP. So those kinds of things are essentially happening. So yeah, you can target GCP and say that GCP is giving a lot of room to companies to actually switch. Right. A lot of budget is basically getting pre allocated because Google is helping them.  



Interesting. Now, love to understand like how big is the data science team or is it like the AI team in paper?  



So it's basically engineering and data scientists together. Yeah, together it could be around 50 people.  



50 people primarily. Okay.  



And the decisioning got it.  



When you are deploying on this customer, these models are hosted on your interest. The ML Ops is something that you look at primarily the entire taking the.  



Model, we don't care about the experimentation of the model.  



Yes.  



The operationalization we look at primarily because we need a latency of seven milliseconds on this.  



Fair enough, that makes sense.  



Because for example, City runs at 6000 decisions per second. That's just not the models, that's the entire so again, there is a leader in decisioning as well. So the entire decision piece comes in where basically all the customer logic of who should see what offer, et cetera, and all those things comes in. And basically the model is called at the last stage it recommends an offer, then it closed back up. So that entire thing should happen within 200 milliseconds. And even though we connect with Sage Maker, right? The latency there itself is around 200 milliseconds.  



Right.  



So a lot of usage is not happening for real time marketing use cases. But the point is, because we are deployed in Sage Maker, we are trying to see if we are deployed in the US. I mean, we're trying to see if we can sort of provision Sage Maker into a VPC and then have that latency reduced down to sub ten milliseconds. But that's a limit for most of her, basically.  



One quick question before that, like the data scientist, do they do the deployment themselves or do they pass it on to the engineering team to do the deployments? Currently, yeah.  



So, all good questions. So essentially, as I said, we are 90% of our algorithm is basically this online learning marketing model, right? So that model essentially is being built in house, right? And basically they're getting deployed into the ecosystem. That's 10% of the model. So just for pure predictive model, we have a very outdated stack, I would say. So we are looking to modernize that stack. But if we modernize that stack, that essentially would be something similar to H two, I would say for example, because and that might be a directional guidance for you also because we really don't see data scientists deployed. Yeah, so we want to move data scientists honestly out of the picture.  



Yeah, that is true.  



We want to basically sort of enable business users to say that this is my data, this is what I want to predict. And the UI is actually that simple. And the data set is coming in from 500 different sources, all getting collected in Pega, the master of record. And they just point and say for this part of my journey, they use the journey, this is what I want to predict and this is the data. Right? And what we do internally is we basically go down the data, figure out the best predictors and then deploy the models, basically build it up.  



Now that works sometimes well with an online learning mode, but we are also trying to build out an offline learning mode where we are basically partnering with each two at this point of time because we don't have enough time to build out that pipeline. But we will go towards the AutoML kind of a solution.  



Fair enough. Yeah, it seems very much like an AutoML kind of route that should be there.  



Yes, because I agree data scientists are important. But if you want to have I'm just talking from the perspective of your startup. If you want to have a lot of growth that is coming in, it would better to target business users because data scientists honestly speaking, in my experience of last 1015 years of a stumbling block, they essentially are the ones who take a lot of time. They essentially are the ones who basically build models which don't get operationalized, right? So probably a bit better moving towards a business user saying this is my data, this is what I want to this is my business problem. Forget the data, this is my business problem, this is what I want to do. This is the data sources, this is the output that I want you to predict.  



You sort of run under so many other site AutoML. There's salesforce transform, we have so many online open source libraries that are available out there that can help you build an AutoML model. And then the game is basically to figure out once you operationalize those models, you monitor it, right? You just monitor it for drip data response, all the different monitoring that you do, lift, blah. And then you sort of start getting notifications and then you sort of trigger a new AutoML bill, let's say if it doesn't meet the yeah, if there's.  



An alert and you kind of just go to the retraining loop or something in case of AutoML will automatically do for you, is it? One major question that I have is you are using Google for AWS and GCP and potentially maybe later on you might have to use as your or something depending on the clients you are serving. So why is the stack you have built off the deployment around Sage Maker? And the reason why I'm asking this is because Sage Maker is tied to the AWS cloud, right? Like you cannot use it over multicloud. And I was seeing that internally Pega uses Kubernetes. Right. So why not build this ML deployment layer over Kubernetes? Was there a decision making there as to why?  



Or was it more like because Sage Maker was very easy to use and therefore wanted to move to switch Maker and then later on the plan is to build this deployment layer over Kubernetes.  



So that's the thing I was trying to explain. So our models are typically internally built. But as I said, Fortune 500 customers that we deal with have their models, right? Yeah, their models are where we also need to connect. Right. So probably 50% of the guys are basically in Sage Maker these days and they expose endpoints to us, which we try to call or they give this post back to us, et cetera. 20, 30% are basically in age two and we are seeing GCP pick up rapidly, very drastically at this point of time.  



Okay.  



It is to enable the data scientists on the customer side of things that we actually connect to these tools. Right, but that is purely execution. Right? That is execution. They give us execute this, get this inference in and then we will sort of inject it process, flow or decision and then make it work.  



Right, but what about the internal part? Like the internal models that you are deploying on, where are you deploying those?  



So currently we are building out the entire gradually not in our priority roadmap, but we are basically building out the entire end to end ecosystem for model building, model deployment, experimentation, et cetera.  



I see.  



But not in our priority because as I said, 90% of our models is a better and better online learning models, which is our marketing models. We have also very good set of models for NLP. We are shifting that to the deep learning infrastructure. But the main thing that I'm trying to communicate is Pega does not give data scientists too.  



No, that is well understood.  



Right. So we have our SAP, you use our if you have a better model in your site, you're welcome to challenge, you're welcome. If your model is better, we don't care. Right? Yeah, but yeah, what we try to do is we try to eliminate this out of the picture.  



No, that is completely well understood. Fair enough. That is well understood. While I understand that, my question is see, I'll tell you, I think I'll give you a picture and then maybe the reasoning or the question will become clear. So we do not hear whether the model is being built by a data scientist or is being built through say an automated process. Our system, what it does is it just takes a model and it deploys it reliably a built model, deploys it reliability with the right scalability, with the right latency requirements and ensuring that the cost is minimal. So now suppose your system, whatever like the team or the AutoML process build like 50 models which need to be deployed.  



Then either you will be deploying it on Sage Maker or you will deploy it on a Vertex or ML or you will build your own system and over Kubernetes and deploy it.  



The fourth option is what we are taking currently.  



The fourth option, yes, exactly. So that part that you are building, what we are saying is instead of building that, what we provide is a platform that's already built and that is not a black box. So Domino Data Labs. The difference what comes in. Domino Data Labs is a black box. You cannot build on top of it. Our system is built on top of it. It connects to your Kubernetes cluster, it basically sits on your cloud. You get access to the entire code base and then there is a UI layer on top of which your models can also be deployed. You get a full visibility and then on top of that further, if you want to do anything, you can do it. So it's basically the internal development process that you will do to deploy your models.  



Instead of building that platform, we kind of make it easier for you to do it via our platform. So that is the point I was getting at key I'm sure like given you are deploying it, you'd be building something internally. So that is where our platform can come in. So I would love to kind of maybe schedule one call where I can walk you through details of the platform because that will give you a much better picture. And if by chance that is something that there is a plan or there is a roadmap, it might be worthwhile to even just try it out for one use case where you use this to kind of deploy if you like it, rather than trying to build it out. But that's just kind of a potential route.  



But yeah, what I would suggest is we would basically be anyways doing this in house, right? Because the team that is there. So if you want to schedule a demo, you can sort of look at it with respect to maybe taking feedback from us that can be done. But from our perspective since we have, let's say so many compliance issues because we have to be compliant with, I think 32 these things across us right there's model governance rules and all these things. We typically don't go for anybody who is not basically falling within Pega cloud, meeting these things for Pega cloud etcetera.  



And then second of all, even if you want to do this, we'll probably acquire somebody if you want to do this thing will probably acquire a startup president partner because we don't want our clients paying additional license fees to somebody else, right? Basically it would probably go in that direction. So I think that if you want to schedule a demo, I can sort of give you a feedback of if, you know, like what we have seen across our Fortune 500 clients, what they are doing, how your system might work, might not work, maybe some tips on how to improve those things. So that definitely I can help you out.  



That will be helpful. I think we can do that actually. We can show you the platform. We can take feedback and even learn as to what can be done to enable it or like, even try to understand a little bit as to when you are building it, what are the challenges you are facing, which might be the challenges others might be facing. That would be great to do.  



What's your growth story that you have told the investors as such?  



So for investors primarily it's the following a lot of companies to build internal platforms to kind of deploy ML models. Ideally it takes like a good time, like a year of a few developers to kind of build scalable platform and then every company is not able to do the best job because ultimately they'll not be able to cater to all the use cases. So what we are building is a platform on top of Kubernetes that allows very quick deployments along with the flexibility for developers so that they can build on top of it and with the right security and other things for the Internet. Because we already work with a few clients. Like, I'll give you a few names that we work with, like a global silicon chip design manufacturer, a multi counter, $100 billion plus company.  



And then another company in the healthcare, which is again, $50 billion plus company that we are working with. So we kind of make it so we are not like a black box. I think what we do is we provide you something that works and then on top of it you can do your own customization. So that way you reduce the time to value for your ML models and you are able to get a system that is scalable. Because we have seen this at Facebook, we have already built it at Facebook and Facebook has a very scalable system like the things you talked about like latency and all. We already take care of that in the system so that you don't have to worry about it and it's very easy for anyone to use. Getting started on it is like 30 minutes or so.  



We try to make it sure that it easily deploys on your infrastructure so there is no data leaving out of the system. The entire compliance is met, VIP testing and other things of the system is done and then it comes with a very good UI that is something that gives visibility to the intra team as well as the MLT in terms of what cost is going in, what resources are being used. So at no point they are overshooting costs. So building such a system internally is generally not easy. We hope that a lot of companies, instead of building internally, will potentially use. But I mean, obviously there'll be companies that will want to build this internal and that's completely fine.  



Yeah, so I think if a sales process is you shouldn't target companies like Pega or Salesforce or these kind of because they have a lot of internal tools that probably like a chip manufacturer or a health manufacturer, et cetera, which are basically not hardcore It companies. Right. I would be able to target them and sort of see that there are model deployment process. But I'll sort of bring in my job to save you costs. And if you ever grew up big enough, then essentially companies like Salesforce and Peggy would be very interested to partner with you and then sort of say that okay, you have 100 clients on our side. Right? On your side, basically. Right.  



And that's what we do with Ishto, by the way, Isto is a joint partnership that we have, but Ish two is a $1 billion company at this point, but still so this is it. You have so many clients on our side. I have so many clients on my side. Let's sort of do a synergy and say, but for the initial growth stages, I think you should sort of go and sort of partner with companies who basically are not at the level of, let's say, salesforce or a service now, or these kind of companies which are basically selling software to other companies. Right. So probably that would be the best way. And the other thing is, seriously think about getting data scientists out of it, which your growth story will be much better because literally just hiring a data scientist.  



Just getting a data scientist, I hear, let's say, the CEOs of these big companies they voted on, and I'll cry about it, like literally hiring a data scientist, managing the data scientist in their expectations, ETCA. It's basically a long drawn out process. Right. So a lot of people are saying that if I have a business requirement, maybe you will not give me 99% of accuracy. That's my model. Maybe your accuracy will be ten to 15% later. But yeah, that's offset by the cost by not maintaining a data science team on my side. So basically you're. Sort of pulling the experimentation pipeline and all those things. If you have another pipeline parallel coming in this is my AutoML pipeline and you are enabling business users to basically run there and then your operational story basically makes a lot of sense in that.  



Let me connect to your database noflick whatever. Right? And we basically get the data out and we love the model and then we operationalise it for you. Then your entire story of Kubernetes and cloud agnostic and new monitoring and we will trigger the next model. You just leave the worry out to us. Right? Yeah.  



Fair enough. I think I think that's a fair point. No, I think that makes sense.  



Right. I'm definitely interested in any demo that you have. But again, probably the feedback perspective because partnership is not possible with a company like that at this point of view unless you grow a bit big. Sure, no problem.  



Let's schedule a demo. Let's take you through the product. I hope you're it will be good to kind of learn, get feedback and maybe it just presses your mind like if there is a need like that later on and at least not to find it. Yeah, sounds good. Thanks. Have a good one. Bye.  



Bye bye. Thank you. "
11354404149,Prove,answol.hu answol.hu@prove.com (answol.hu@prove.com),,Prove,100-500,50-100 Mn,<10,AWS,No,Software Development,New York,Anuraag Gutgutia,,https://app.fireflies.ai/view/TrueFoundry-Prove-Intro-call::HOBf7s9tJN,"How's it going? 



Hey. Hi, Ansul. How are you? I'm good, doing well. 



Nice to meet you. On Rag. Did I pronounce your name right? 



You pronounced it right. Did I pronounce your name right? Ansul? 



Yes, exactly. Exactly as it spelled. 



Awesome. Ansel, where are you based? Are you in SF area? 



I am. I'm in the Mid Peninsula. 



I'm currently india, by the way, so it's pretty late for me, but. 



Sorry to keep you up. Delay. Are you normally based there or are you traveling? 



Yeah, generally right now I'm based india. I used to be in the US and Singapore before this, but a lot of our tech team is currently india, so I'm currently here. One of my other co founders, Nikkunji, is in the SF area. 



Cool, very nice. Well, thanks for taking the time to chat. 



Thanks a lot. Like, my pleasure. Really grateful for you to kind of spend this time. I'll give a little bit of context and so I would love to hear from you as well what I wanted to do on the call. So I can tell you a little bit about my and my co founders backgrounds and what we are building at two foundry at a high level. I would love to dive into a little bit of how Proof uses ML, what is the current infrastructure, what are some of the challenges you have seen along the journey and if there are any pain points or challenges that you're seeing right now that you are looking to solve for. And then if time permits, can dive a little bit more into what we are building. 



Otherwise I can set up a follow up call to show you a high level and a detailed demo of the platform as well as to what we are building and hopefully see if there is a use case where I can work with you and take your help in the journey. 



Sure, yeah, I'd love to definitely find out. 



Okay, cool. So my background, I did my bachelor's in electrical engineering from IIT Corrupt india and then post that I worked with a hedge fund called World Quant, which is a part of a Linear Partners based out of US. I was working as a portfolio manager and a quant there. So initially I was building trading strategies using a lot of data sources, and then later on I was a portfolio manager managing around 600 million assets for them over global markets primarily. We used to trade equities at the same time, I used to angel invest into startups like starting in 2017 and have invested in more than 22 startups, a lot of them being Indian, South Station and US startups primarily, and always wanted to build something of my own. 



And my friends and my current co founders Abhishek and Nikkunj, their background, they were my bachelor's at Undergrad and then Avishek, went to Facebook as a software engineer, was a senior staff software engineer when he left, led the Videos team, the in priority design team and the Cashing team there. So he brings a lot of experience on distributed systems. And Nikonj, who is my other co founder, he brings experience on the ML side. He was the lead ML engineer at Facebook and prior to that led the ML for a form called Reflection where he built the ML system grounds up and prior to that did his MLS from UC Berkeley. So all three of us, both of these were on the same boat where we wanted to build something together. 



We all left our jobs in 2020, moved to India, built a startup in the talent space, sold it to Infoge, which is one of the bigger tech players, and then later on started Truefoundry over the last one and a half, a little more than a year. The goal of building True Foundry is to kind of bring what we saw at Facebook in terms of the ML platform there to every company around the globe. So Facebook has an internal platform and on top of that there's an ML platform that allows ML developers to quickly kind of test out and deploy models to different environments and do that with the best engineering practices, including monitoring, auto, scaling the right authentication and the security systems and at low cost. 



So our goal was to kind of build something like that and provide it to every single customer company where they also have the flexibility to kind of build on top of it. But at the same time, the developers are able to increase the speed at which they kind of test out and deploy model. That's like a very short background. Love to hear if there are questions and then I'd love to hear a little bit about you and Prove. 



Yeah, no, definitely interesting. I mean, I think the problem you described I think is very prevalent just to kind of think through the target customer of True Boundary, or maybe I should say the target user. Are they primarily the ML engineers that you're targeting who are focusing on building models? Is that kind of your target customer, if you will, or how would you describe your target customer? 



Yeah, I can talk a little bit. So the target user who will use the platform, there are different users. So one is the primary user who would be the ML developers who are building the models. And then they would want to ideally test out and deploy these models either in a depth test environment or in a production environment and so on. Or while training out the models, they would want to kind of do virtual training, right? Like over machines or over shared classes, et cetera. That is the primary user. The secondary user is the intra engineer or the DevOps team or the platform teams in companies who would want to have a full view of what is going on in the resources, in the infrastructure that they have allocated. Which part of the infrastructure is allocated to which ML team? 



How many models or services are being deployed there, ensuring that there is the right reliability and security system. So that is the secondary user. And then obviously it's the ML engineering lead or the manager who wants to have an ROI faster for the NML models. That is like the ultimate decision because that's how I think of the user profile. 



Got it. Yeah. Basically it's really making it overall more efficient. 



Yes. 



Essentially what you're targeting. Right. That makes a lot of sense. Okay, so this is very concentric, right? Because everything is really about infrastructure. Deploying the models in the cloud and being able to iterate that on pretty quickly is kind of the goal here. Right, okay. Yeah, that makes a lot of sense. Just very briefly, definitely we have struggled with this and I think every group struggles with it to some extent. We have built quite a bit of stuff, kind of we used to do a lot more custom stuff, if you will. Like, we set up an ML Flow server and all sorts of stuff on tracking and things like that. We've recently tried to move towards leveraging Sage Maker a bit more to kind of take a little bit of that maintenance work off our hands. 



Because I think the main thing really is that a lot of the stuff that our team is building right now is, I would say more kind of early development and RMD. So we haven't put like a model in production in the cloud. And to be honest, it's not clear that we would necessarily do that because a lot of the stuff that we're building right now is actually targeted for OnDevice ML. 



Okay. 



And so that introduces a different at least from the way I see it introduces kind of a different dimension. Right. A lot of the things that we're testing and training is not meant to be directly deployed to the cloud and like serving customers through APIs and whatnot it's really about. Okay, we need a test framework that is essentially kind of replicating, if you will, what we are going to be doing on device and optimizing that. And some of it's, like it's not all going to be the traditional, let's say, cloud native software, right? It's like, oh, we're going to go into the world of C or whatever because it's going to be optimized. But right now we leverage a lot of Sage Maker everything's containers to kind of run in Sage Maker pipelines to kind of run a lot of this stuff. 



But I definitely think that the general problem is always there on how we become more efficient. I don't know if you guys ever thought about kind of an on device kind of angle to this. 



Yeah. So on device is an interesting thing. We have recently been talking to companies that actually deploy models on device. And the problems are very different from the normal deployment problems. Right now, we are not building or focusing on that as well. But I'd love to hear, given you kind of care about that when you think of like an ML deployment on a cloud native infrastructure versus on a device. I'm sure there are a few challenges that come in respect to size because ultimately every device will come with a limited hardware. But I'd love to hear at a high level, what are some of the other challenges that you end up seeing that will be great to kind of hear. 



Yeah, and I'll be honest, we are working through those right now. Right. Like, as I said, it's still very much in an R and D phase. And I think at a high level, it really is what you described. Right. Basically what we are when you talk about doing the, let's say the research and the development aspect of the model is one problem, but then it almost is like basically with that, you're trying to optimize for speed of improvements, iteration, and I think that fits more like what you're describing with what you found you optimized for. Right. It's like, oh, I want to be able to train, I want to be able to really quickly deploy model and try it out and things like that. And so we definitely have been viewing that as kind of one kind of angle of looking at it. 



And then at some point it becomes an iterative process of, okay, now we know algorithmically what we want to do. Then we have to take a step back and also say, okay, now if you were to try to put some device, it's like, is it even feasible? What do you do? Is it like, oh, do I use TensorFlow on the device, TensorFlow lite or whatever, right. Is that the right solution? Or is it still too big? Because then you have to bring in the TensorFlow dependencies and it's like, okay, is that the right way to do it? There are different ways to get around that. And I'll be honest, we don't have a perfect solution for any of that right now. 



But it is kind of an iterative process where it's like, we think about the on device part, but at the same time, you won't really know what the bottleneck is until you've kind of figured out what model or algorithms you want to run. And then you kind of have to go back and forth. And we haven't figured out for sure the right kind of optimal way of doing that. 



Right. 



Because I would imagine it's going to be related to something like you tried to find a way to essentially build the on device thing as much as you can in the cloud to be able to iterate. But at some point you're still going to have to get closer and closer to like, let's say SDK, right? And so where did you get that? 



Do you end up using any sort of distillation techniques or for reducing the size of the model? Like, let's say you train the model and it's up to 500 MB size, and for device you need maybe a 50 MB. So do you use something at currently or you try to train models right. While at the point of training to be of an optimal size? And I'm curious to know before this also like this kind of models, what are these use cases when you have to deploy a device so that I have a sense of what the actual size of the models, et cetera, could be that you are building, actually. 



Right. I think the general problem statement is really how do I say the model sizes vary, to be clear, because I think it's obviously use case dependent. The way that we think about our problems is like if we're trying to, as you probably know, prove is trying to do a lot of identity authentication work, right. And a lot of it is trying to use signals like, let's say from the device to help figure out how much risk on this device there is, things related to whether or not it's still this person using the device or some behavioral biometrics related stuff. And so, depending on the specific, let's say, factors that we're looking at, the size of the model will be very different. Some of them are very small, which case it's not really a problem. Some of them are not even a model. Right. 



We're like logic. That's great. On the other end of the spectrum, you have stuff along the lines of we're trying to use a gate as a way to determine whether or not it's a person or not. Right. That is a much larger model. But the key thing is that a lot of it is privacy driven. Right. We don't really want a lot of this data to leave the device. And so that's why the goal is to try to do a lot of it kind of on the device. And in terms of model size, I think we are currently kind of at a point where it's like we want to build the models that we think we need to build in order to meet, let's say, a certain performance capability. Right? 



Yeah. 



So we recognize that may lead to bigger models, but as we said, it's iterative it's like, okay, let's try to establish upper bound on how well you can do and then start iteratively going down and making the trade offs likely. We could use techniques that directly whatever quantities, the weights, still, whatever, all sorts of kind of techniques for compressing the model. That would obviously be one approach that we would take. Right. Because it's kind of not out of the box, but kind of out of the box. Use standard techniques, but also just redesigning the model to make it smaller than what we learn. So I don't think from our perspective, very practically all of that is on the table. Right. 



Yeah. 



But I think maybe what you're getting at is if there were the more tools we have that could easily try a lot of that stuff for it, the better. Right. It's like, here's a model, you can now go and compress it, which there are capabilities out there. That's kind of the thing that you would probably try first because of course okay. Right. Fair enough. 



I get a good sense. A few things I wanted to ask also on the infrastructure side, I was seeing you all end up using AWS, and you mentioned also that you use Sage Maker. When you're using Sage Maker, you'll not be using a case, but do you have Kubernetes in house in the company that you use like an EKS or something? Yes. Okay. So you mostly deploy all your software on EKS, I'm guessing. 



That's correct. 



Okay. Is something that is owned by the ML team. So I'm guessing the DevOps team is not getting involved and it's completely managed by the ML team. 



Right. So for all this R and D effort, the ML team owns it. So that's why we kind of chose to go something simpler like Sage Maker, just to be as simple as possible. Right. But that's actually part of the reason we did it, because we didn't have a lot of dedicated ops support. Like we could do it, but it wasn't kind of the best part of the team's time. 



And then this Sage Maker, which component of Sage Maker does the team end up using the most? So, like, Sage Maker has this hosted notebook, sage Maker notebooks. And then you have a way to kind of obviously deploy in different environments, do multi model loading and testing and so on. Which component does the team maximum use? And that is one question I had. 



Right, yeah, no, we definitely use the notebooks. Right. That gives the easy kind of prototyping interface. Right. We used to actually have our own Jupiter hub that we maintain and that was just a lot of work. So we definitely use that. We also use Sage maker pipelines. As we start building more of the as we start kind of solidifying some of the code, then we basically package it up and use pipelines to run and we run experiments with that regression. We use it as well. I think that is definitely one of the key things we use. Just allow us to kind of simplify the running of a lot of these things. Okay, got it. 



When you are using and building models, you mentioned earlier that you use the ML flow as well. Was it the experiment tracking piece of ML flow you used, or was it more the ML flow server for quick hosting and everything? 



Yeah, we've been trying to move away from it because it was the experiment tracking piece. 



Okay. 



We found that pretty effective. It worked pretty well. Again, it was something that we hosted ourselves and so again, it was more overhead. 



And what was the reason for moving or wanting to move away from it? Because from what I understand, Sage Maker also does not have a very good experiment tracking interface as such. Is it like the need is not there or you are trying out some other tool in that regard? 



Well, I'll be honest, I think we're still trying to figure out what is the best way to do experiment tracking. ML Flow definitely worked well to a large extent. We do definitely like to use it for a very long time. I think a lot of it was kind of the maintenance aspect and I think generally were trying to say the more of a kind of a managed service we would move to, it would help free our time. So that's why we tried Sage Maker. We haven't yet fully figured out the best way to fully replace I see. ML Flow. Right. Because Sage Maker has some capabilities for that. It's a little bit clunky, right? Yeah. And so we're thinking about building other potentially just setting up our own databases to kind of track it and potentially optimize it for our usage. 



But I agree the decision to not use a malflo came with trade offs. 



Got it. I have a few more things to ask and I would love to take some time in this call to actually give you at least a high level picture of what you are building so that you have more context rather than just asking questions. So just one last question is how do you take care of some of the other good practices from an SRE perspective which would be important for you as well, like things like the CI CD or authentication or say secrets management ensuring that none of the secrets are written in the code? Like do these things matter at this point or is it a little too early in the journey? 



They do like the CI stuff for sure. We run everything through GitLab and so actually we post up Sage Maker with GitLab, we do CI and we basically just use from a regression testing perspective. For example, we basically have the CI pipelines kick off the Sage Maker pipelines to do regression. And so that's kind of how we've managed to address a lot of that stuff. 



Got it. Cool. One thing anchor I would like to kind of mention is for us we are still very early in the journey and really I'll be brutally honest is we are working with a few companies and our silly to work with a few other companies who could actually be early customers or design partners as we are building out this platform and releasing it for mainstream usage. And during that time we are also happy to build out custom features for people we are working with. So I'd like to take this opportunity just here, maybe a little bit about what the platform is currently doing in terms of easing out the testing process for developers. 



I'll just try to walk through it, but obviously I would love to set up a follow up call to do a more detailed live demo and also probably explore with you if there's any possibility of any problems or pinpoints that you are facing which we can potentially address to our platform, if that works well for you. 



Sure, yeah, absolutely. If you wouldn't mind, I did glance at your website and it definitely seems quite nice. 



Thank you so much. The website is a little outdated, I think we never revamped it well. But yeah, I think I'll give you a picture through. 



Cool. 



Are you able to see my screen? 



Yes, I can see it. 



Okay. The core thing that we are focusing on from a perspective of truefoundry is the deployment layer. And when I say deployment, it does not have to necessarily mean a production. Production, deployment, it would be simple. My connotation of deployment is anything that you have to run on a virtual machine, right? So it could be a simple training that you are running and you want to test out different models and then track their metrics and compare what is doing well. Or it could be actually a deployment on a test server so that you can expose it to someone who can test it out and say it's working fine before you finally do the deployment at the corresponding scale that you want. So it could be anything in terms of the deployment. So we have built out a deployment platform as a service over Kubernetes. 



We use Kubernetes as the base layer, but obviously we can obviously work without it as well if there's a need. But so far we have seen that option to be maximum, so we try to use and build it over Kubernetes and so on. The aim is to speed up developer workflows while providing them flexibility to build on top of it as well. So I'll just show you what that means and then enable doing that with full security and control for the intra team. So we believe that the intra team in itself is a very important layer. Like while most companies fill shortages of intra teams, like, they are still an important layer. And at least if you are able to enhance the visibility, you can actually make them much more effective in the overall journey. 



The way the platform works is it enables you to deploy very quickly, like single click deployment, either through the UI or through CLI or through the notebook, enabling it to be faster. We make it simple to learn for the ML developers, basically in terms of exposing the Python libraries that they easily understand and ensure that they don't have to depend on DevOps or intra people to kind of deploy models. So we kind of try to enable full independence for them automatically. The Platform ensures that the SRE best practices are followed and inculcated into the team's behavior. So things like auto scaling, security management, authentication, secrets management and so on automatically come with the Platform, like the CI CD and all of those stuff. 



And finally we try to ensure that the Platform runs on top of the same infrastructure you are using for software, so that you don't have to run parallel to two set of infrastructure for your company. So for example, in the old case, if you're using AWS and EKS for your software, we would ideally want that your ML also runs on EKS without bothering the infra team and your developers are labeled to access and use it. So that's the thought process behind building it. So if you look at the Platform Idly consists of the following parts and I'll walk you through a short demo and you will see as to how that comes into play. 



By the way, I'm showing you the deployment but we also along with deployment, as you do the deployment, we have a layer of tracking out your offline experiments as well. So basically what ML Flow does, we actually use ML Flow behind the hood and we kind of augmented further as well to improve the tracking of metrics, the logging of metrics including system monitoring and so on. So you can potentially then use that part of the platform and I can dive into that a little bit more in the follow up. But primarily the workflow is as a developer. The first part is you create a secure development environment for your ML team. So this part is either managed by a DevOps or the head of ML or the engineer, like the ML lead engineer and so on. 



What they do is they connect to your own cloud, we make the connection easy and then you are able to provision and allocate resources to your ML team. And once you do that, basically the system ensures that in no way those resources are exceeded anytime. The system is controlling for the costs and the right access controls are there. Here you also integrate with your existing systems like GitHub or GitHub in case of yourself, whatever docker registries you are using and so on. Once you have done that, then the interface is open for the developers to keep on trying out. You can start trying out simple functions to complex models real time as well as batch models. You can use the best rate practices by default like the CI, CD authentication, auto scaling. We also use and support model server. 



So if you want to optimize your model in terms of size and an OnX server feeds better, you can actually do that. More customization. Here is coming, but basically at a high level the model server part is supported as well and as you deploy we make it easy to do it via the UI as well as from the CLI or the Jupyter notebook as well for developers. And once you deploy, we kind of automatically show you things like your system monitoring us to CPU memory usage and so on. And if you are using structured data, we also automatically showcase you the model performance and data drip so that you can easily debug your system. 



Can I ask you a question? Hello, just a quick question. I'm just thinking through like this model deployment part, right? How does your system generally manage the, let's say reproducibility? Right? Let's say I have some ML developer, he's got some model he's figured out on the notebook and he's like okay, I want to really try this thing out. How do you track that model to kind of ensure reproduce it? 



Yeah, so basically in the model like that two things like suppose you have an option to register the model with us as well and you can actually define your schema. So we provide the model registry component so that way every model is version like v one, V two and so on. And if you're directly deploying event from notebooks without the registry, you can actually track your versions in the deployed tab like I'll show you that and you can actually roll back to any older deployment. So when you are deploying, basically the system creates the docker image automatically and then it will deploy so that way again, it preserves the reproducibility aspect of the platform. 



Cool. 



Does that answer your question? 



It does, yeah. It sounds like it was the registry kind of the probably the recommended way to do it. 



Yes, I believe the registry is the recommended way. Yes, absolutely. 



Yeah, that makes sense. 



But by the way, you don't have to necessarily use our registry if you are already using a docker registry like AWS docker registry or any EMR registry or anything, you can actually load models directly from there and deploy and the system will still enable versioning. So it will basically tie back and ensure that is version and you can easily roll back to an older version as well. There. 



Cool. Okay, do a quick touch, a couple of minutes and I do need to run at once. You said do you want to set up a follow up call? I'd be totally open to that. 



Yeah, would it be great? It would be good if we can do a follow up call. 



Yeah, I'm totally open to that. 



Okay, cool. What day or time will work best for you? 



Take a look what times work for you? 



Do you want me to email with you? 



Yeah, we can definitely coordinate our email. Yeah, we can do that. Not a problem. 



I wanted to quickly show you a high level glimpse of the platform. Sure, okay, I'll quickly do that. So. I'll not take more than two minutes. So this is the part of the platform until where you kind of integrate with your docker registries, GitLab, secret store and so on, and you actually integrate new clusters. Suppose you have an ETS, so you automatically kind of create that cluster. It could be in any cloud. Like in case of your AWS, you have the control, you can configure your monitoring as well. And in case of ML, you sometimes want to configure what kind of machines you want to provision or blacklist. You can also control that. 



So once you do that, your cluster gets connected and your DevOps infra team or the ML leads can actually create workspaces which are smaller groups for your developer teams to work on. Each workspace basically comes with resource limits and proper access control. Again like admin editor viewer. So with the resource limits you can actually configure how much GPU, GB, Rap, etc. You are allocating so no one can exceed that resource limits. So this is the first part of the infrastructure. Once that is done, the developers are ready to actually start deploying models and they can go to this deployment tab and you can deploy like preprocessing functions, services, jobs, like training jobs, inference jobs, as well as models directly. 



The new deployment is as simple as just you click the service or whatever, you select the workspace and then you can deploy from a source code or even from a docker image if you have a docker image. If you are deploying from GitHub, you can just do that here. GitHub is connected, but you can easily connect GitLab as well. Similarly, if you are deploying using a Python build package, just give this one. You don't have to do much, we automatically generate the docker image. And if you want to configure this, you can do that. But otherwise you just click the submit button and literally the model will deploy at your end and you will see the endpoints, et cetera. 



So you can deploy to a test environment if you want developers to quickly test out and you will have access to metrics logs, et cetera as well, so as to quickly debug things. You also asked about the versioning. So I'm not showing the model registry here in this recorded demo, but basically there's also a model registry which I can showcase in a more detailed demo. But here the versioning happens automatically and you can redeploy any old version here without having to go back. All the spec files and everything else you see are there. 



Cool. 



So similarly you can deploy jobs, deploy models and then there's a monitoring which I'll skip for now in the interest of time, but I can showcase it to you later as we talk further in the next call. 



Okay, yeah, you mentioned that the website is a little outdated. Do you have an updated slide deck or anything that I can look at offline as well. 



Yes, I can send you a brief slide deck and also include this video in it as well. Hopefully that will be helpful. Okay, cool. 



Yeah, no, that would be helpful. Just give me some. 



Also offline set up a follow up call in respective to kind of go over a more detailed demo and yeah, sounds good. 



No, definitely appreciate your time. Sorry we had to run, but look forward to talking more later. Thank you. Thanks you. "
11354392885,Cars24,abhay.kansal1 abhay.kansal1@cars24.com (abhay.kansal1@cars24.com);swapnesh.khare swapnesh.khare@cars24.com (swapnesh.khare@cars24.com);naresh.mehta naresh.mehta@cars24.com (naresh.mehta@cars24.com),,CARS24,>1000,>1B,25-50,GCP,Yes,Motor Vehicle Manufacturing,"Gurgaon, haryana",Anuraag Gutgutia,,https://app.fireflies.ai/view/Meeting-with-TrueFoundry::O0FKhsqAPC,"I'm good, catching up again after a few weeks. 


Swapnesh Khare
·
00:03
Yeah sure. Let me check with Abe. 


Abhishek Choudhary
·
00:08
Yeah. 


Anuraag Gutgutia
·
00:09
Abhishek will also join. Who else is joining in? Subnets from your side you are here. 


Swapnesh Khare
·
01:00
I think that's it Nadesh. I'm not sure he will be joining. 


Anuraag Gutgutia
·
01:07
Okay. And I think you mentioned Aksha will also be joining in. 


Abhishek Choudhary
·
01:13
Okay, cool. 


Anuraag Gutgutia
·
01:18
To connect with you again. 


Abhay Kansal
·
01:25
How are you? 


Anuraag Gutgutia
·
01:28
Yeah, we are good. I think you both didn't meet Abhishek. Abhishek, maybe it will be good if you can quickly give an introduction for yourself as well. 


Abhishek Choudhary
·
01:37
You mean my introduction Mandrak? 


Anuraag Gutgutia
·
01:39
Yeah, I think we all have met in the office. 


Abhishek Choudhary
·
01:44
Yeah. Hi abhiyan sabnesh. I'm Abhishek. I graduated from It correct for 2013 in Computer science and then worked in Facebook for around five and a half years. During her time at Facebook worked on different teams like the Distributed Caching System team was leading a mobile performance team and was eventually leading the business organization there. And then I did indicate my job came back to India and currently working on Trip Foundry basically where we tried to solve the ML deployment platform. 


Abhay Kansal
·
02:13
Yeah we looked at the solution you guys are offering and building like last month awesome. 


Anuraag Gutgutia
·
02:29
Awareness of nestle the ML side of things has built out the ML platform. Aware was earlier on the data science side but then I think he realized that it will not work without the engineering and then in cars 24 and even before that he has been involved in a lot of work in building out the ML platform and overseeing it. So we had a good discussion. I think you have seen the notes from it. How would you want to take this call forward? I know last time you had questions, I do have notes around that. At the same time, maybe it will be good to kind of quickly recap your system once and we can again revisit the demo quickly and then go into the questions if that works or is there some other agenda you had in mind? 


Abhay Kansal
·
03:21
You are in mute, sorry. So it is up to you, like not have any specific agenda. Last time were discussing in general how were working on it and I think you had few questions on that. We are happy to answer those. Yes. 


Swapnesh Khare
·
03:43
Cool. 


Anuraag Gutgutia
·
03:44
So let me give a quick context of whatever we know of the past 24th. I think the infrastructure is mainly GKE based. You are using Caseerv and Knative on that like using Feast as the feature store for things native. You use for the scale to zero things to make it easier and then I think you ensure that inside the code like people are not like things like Pandas et cetera so that the speed is faster and things like that. And monitoring you are using within Google itself, right? Like you're not using separate anything for monitoring. And then I think other context, I think you also have right. 


Anuraag Gutgutia
·
04:28
In terms of there were questions that had come around usage of I think triton or the flexibility that is there earlier, I think you are using GPUs, but now, from what I understand, most of it is CPU based. A lot of services are mostly real time. From what I understand. Again, correct me softness or anything is wrong there. 


Abhay Kansal
·
04:54
So only models based on images are on GPU, Rest or Rest dollar CPU. 


Anuraag Gutgutia
·
05:02
Okay, got it. And there was one thing you had tried on the Cuban native log site, which was 30 days I think you are able to retrieve. But beyond that, I think. 


Abhay Kansal
·
05:14
We realize we do not even need it. You do not, because most of our APIs are real time and if something has to go bad, it will go bad in 30 days. We are not maintaining audit logs as per se. Our APIs are stateless. All the audit logs are being maintained at check in. Our APIs are stateless. They are family for request response that says if some outlier comes in and the code is not supporting that, against that we will have some error. And we usually bind those deployments against alerts, again supported by GCP. So the respective teams get emails or we use custom Slack as SDK to shoot messages if they are in bulk. 


Abhay Kansal
·
06:18
So we have some services running in Airflow there for a data engineering use case and they consume Gcpins in another way, but they send requests on Slack and then Slack adds acts as our so we have Slack Enterprise enabled in at cars, so we have no limit on history of messages. It works fine for us. 


Anuraag Gutgutia
·
06:55
Did you have any other questions? 


Abhishek Choudhary
·
06:56
So, are you on the Airflow on Kubernetes? 


Swapnesh Khare
·
07:00
Not right now. It's on VM right now, but we don't see much of the traffic coming on Airflow VM. So right now it's working fine. But yeah, in the future if we get more Dags, then we'll have to move to Kubernetes because that also has to scale. 


Abhishek Choudhary
·
07:19
I see. And so all jobs, if data center wants to run a training job or anything like that, it's completely on Airflow today. Or they run jobs. 


Swapnesh Khare
·
07:27
No, we don't use Airflow right now for those. Because these trainings, these data centers, they do by themselves, they have VMs specific teams. Every team has a VM, so they would do it on that. But at jobs they don't have any. So they basically get the data, they have notebooks, they run the training on that. 


Abhay Kansal
·
07:58
They are different teams, basically. So when we started this practice internally one and a half years back, we decided categorically to focus on the ML Ops and Mln side of the thing, not the DS side of the ecosystem, because that was coming with too much baggage for us too. It requires everyone to move to something else and then that will have its own opinion architecture over and everyone will the backlash was too heavy and we are literally 2.52.25 bandwidth kind of the operator. So we are really lean. So we did not want to waste time on that. So Ashworth saying lately we have realized, okay, rather we have focused on how we can accommodate all those granularities on infrared side and scale that instead. 


Abhishek Choudhary
·
09:24
You'Re saying training job locally they are running. So our model will save, they save it on a three key. Where do they GCs basically GCs. 


Swapnesh Khare
·
09:34
After they have done with the training, they approach us that we have to deploy this service, then we get them. So your service will either go on GKE or if it's like a normal slow or small service, then basically cloud function or cloud run. But your models would be stored on GCs rather than having it on the docker image itself. So we fetch it from GCs and during runtime it's loaded once like case of has this functionality. So in the load function it calls the GCs, downloads the model from GCs and it's loaded. So that way we also maintain history in GCs. 


Abhishek Choudhary
·
10:24
Okay, I'm guessing you are also putting history on Kubernetes, right? Because you are using KSR. Yeah. 


Swapnesh Khare
·
10:31
So every deployment has a canary deployment. So we have the deployment numbers 1234. Every time we do, we redeploy things. We have logged for those also. 


Abhishek Choudhary
·
10:45
Okay? And data scientists log, they ask for this thing like Immodel V one predictions can be right, it's being written somewhere. I can go and compare logging in case that comes with that logging. But is it flowing to some bucket? And they can analyze v one versus V two, maker difference and all that stuff. How do they know v two is greater than v one or equal to v one? 


Abhay Kansal
·
11:05
So that is something as I said, our APIs are stateless, so they do not manage Kia haircani. So whenever the tech integration with our endpoint happens, it happens in between. There is an intermediary cloud function over the request forward case endpoint function throughput control kirpan for it to not choke like Kubernetes chokenake. We use Pub sub enabled cloud function jokey scale just to send requests to cases because we do not have API gateway in between. So rate limit, we do not have anything, but you have. 


Abhishek Choudhary
·
12:04
Limiting. Understood. So basically request carcondra the clients, no client directly internal. 


Abhay Kansal
·
12:22
We have all the requests that are internal. 


Swapnesh Khare
·
12:26
Directly exposing URL data science, your project zone, everything is driven by tech. 


Abhishek Choudhary
·
12:35
Request goes to cloud function. 


Swapnesh Khare
·
12:54
And that cloud function will call the case of endpoint. 


Abhay Kansal
·
13:02
Pub sub say actuate karna is not straightforward in at least GKE either way. For us, the easiest and cheapest way was 128 MB node kernel and eventually translated into this practice endpoints. And they are generic, they are efficient and they are generic. And our front ends which are cloud functions or cloud run, they are application specific. So this was again done to separate the Mln and Data DS responsibilities, cloud function. 


Abhishek Choudhary
·
14:52
When they wait for the response to come back through the pub, they're also subscribing to the response channel, right? 


Abhay Kansal
·
14:58
And post request for the slot. And we usually they respond in few milliseconds, so it has been working fine. 


Abhishek Choudhary
·
15:23
Understood. 


Abhay Kansal
·
15:26
Also we cache inferences, heavy inferences on cash. So we do not take same inference for same model on same image twice. 


Swapnesh Khare
·
16:07
Instead of running the model, just massive results. 


Abhishek Choudhary
·
16:40
Distribution, all that stuff. 


Abhay Kansal
·
16:54
Product development, more data. I also manage the data warehouse. The recency of data is more critical than archive. 


Abhishek Choudhary
·
17:41
Drifting, model performance, downgrade horizon, all those things. 


Abhay Kansal
·
17:47
Whenever a model is new after six, eight months, internal generated data. Okay, so data internally generated a vino key data and then zoomatrics. Usually business KPIs may be reflected. For example, there are KPIs that are tracked at business level as well. For example, if you go on our website, you will check for the price for web quotes for your car. Eventually appointment and final price quota error. Business KPI is monitor at a higher level. Higher level. Sometimes that delta is by design to increase footfold. Sometimes that delta is not expected. And that is where the DS monitors that API via Bi route at cars frontival DS is not working in Silos, it is always paired with the Bi team. 


Abhishek Choudhary
·
19:30
Okay. 


Abhay Kansal
·
19:33
Bi team is catering to KPIs, all the KPIs that is responsible for business. 


Abhishek Choudhary
·
19:40
Understood. I think that answers all my questions. Or knight. 


Swapnesh Khare
·
21:06
Load pre process, predict post process, but at least navigation. 


Abhishek Choudhary
·
21:21
Data scientists. Understood. We can show like it's actually very similar. We are also building on top of. 


Anuraag Gutgutia
·
21:55
CASER only take just few contexts while you are walking through, it will be good. I think awareness, softness has seen the platform at a high level, but I had shown them a little bit. But while you're walking through, if you can actually talk to the little bit on the architecture side as to how we have done it. And then there were a few questions that had come in last time. So I'll also interject you in between to answer both. But yeah, I mean, they have seen it once, but as you walk to talk more in depth rather than just the higher level. 


Abhishek Choudhary
·
22:24
Okay, if you've already seen the platform, then let me know which parts are clear. 


Anuraag Gutgutia
·
22:35
It was at a very high level. It's good to walk through anyways. And I think it's been 30 days before us. I'm sure the pressure would help, but that time it was very high level. The more in depth things will help you. 


Abhishek Choudhary
·
22:52
Basically, this is where you can add all the Kubernetes clusters in the company. You can add the cluster and you can add like which users have access to what cluster and all permission control ingress URL star API 24 com is cluster. So people can then start creating subdomains of that. So you can map keys cluster p added A and then you can also decide his cluster type a machine supported what are the instance that are supported on this thing? 


Abhay Kansal
·
23:26
So every machine will create a different. 


Abhishek Choudhary
·
23:28
Node pool PJ will create a different note pool. The kind of recommend using is auto node provisioning. It's both there on AWS and GCP mandoe note pool. Just because even if you mark it's not creating the note pool right now. It just means that somebody, the moment they try to deploy on C six, a node will be automatically be joined to the cluster. Dynamically configured but node pool cluster by default. You can add all the docker history that you're using ECR, GCs, Get, Repositories, add Cursor, standard stuff. Or Secret Manager. 


Swapnesh Khare
·
24:26
Secret Manager. 


Abhishek Choudhary
·
24:28
We actually work with secret Manager. Also Google cloud secrets. The moment you add all the clusters, you can start seeing all the namespaces. 


Abhay Kansal
·
24:40
Here. 


Anuraag Gutgutia
·
24:43
And then you can upload those cluster. That is the level at which you will operate right now. And then was there separation of access as well? 


Swapnesh Khare
·
25:02
But deployment last time. 


Abhishek Choudhary
·
25:33
Basically namespace level between different teams and access teams data science could be around size limit so they can play around with it if they want to. Deploy something quickly and test out. And then you can also give them access to, like, service accounts, data science, three bucket access. There's no way they can screw up the other intro. And they can further limit instance type. On a namespace level, GPU machines, something along those lines. We also want to show the cost and all that. We have names level costing and all that things. And then on the deployment side, the deployment, we kind of try to make it very easy. PoeB Company Maker Paiga basically at least for testing out stuff and everything. Or you can also do the production deployment. So you just choose the workspace where you want to deploy on service. 


Abhishek Choudhary
·
26:39
Java model is actually powered by Kserv. 


Anuraag Gutgutia
·
26:42
Underneath, I'll show you that eight questions last month. I just remember your workspaces in general. Like workspaces when you set the resource constraint, whether it's a soft or a hard limit, exactly what's coming in. 


Abhishek Choudhary
·
26:58
It is a hard limit. It will not allow you to go beyond that other deploy carousel services. 


Abhay Kansal
·
27:10
Requested. 


Abhishek Choudhary
·
28:10
Xdiff or docker image already builder you can also give us a docker image uri public DNS you can do this and this is coming from the ingress URL we configured on the cluster you can do that. 


Anuraag Gutgutia
·
28:48
It'S not binding, right? This was also a question I think of where you had like binding or. 


Abhishek Choudhary
·
28:52
Not binding. 


Anuraag Gutgutia
·
29:05
Basically it's a compulsion like is it binding or is it a non binding thing? 


Abhay Kansal
·
29:11
So basically we use internal IPS so. 


Abhishek Choudhary
·
29:17
Internal IP and cluster internal IPC cluster credentials on the IPI directly. 


Abhay Kansal
·
29:45
Okay. 


Abhishek Choudhary
·
30:15
Port 443 in a lot of cases below I automatically be able to environment variables. You can add the environment variables here. It is as simple as you can add it directly from here. 


Abhay Kansal
·
30:54
But you don't get. 


Abhishek Choudhary
·
31:01
Secret ad is secrets. I'll quickly show you basically secret. 


Swapnesh Khare
·
31:31
Config map or secrets config map or secret skype section. 


Abhishek Choudhary
·
31:50
But the values just values google Secret Manager you don't rely on Kubernetes to store the secrets permanently. Secrets Storage so you can add the secret values. This is the key using which everybody can use the secret in their code. So whoever has access to use the secrets this value and your value after Google Secret Manager may store. We don't store the value also. It's actually going and creating in your Google Secret Manager behind that. 


Abhay Kansal
·
32:31
Link. 


Abhishek Choudhary
·
32:33
This is just the link request memory and all that stuff. Like standard stuff? You can directly just by default. 


Anuraag Gutgutia
·
33:00
If. 


Abhishek Choudhary
·
33:00
You want to run it only on TC machine. It can also do that. A replicas, a service account add canon. Service account, constructive liveness probe, redditness probe, most of standard stuff. And the moment you click Submit here, it will automatically build the image and automatically deploy. And at the end you can see the service running, basically. And you can see all the previous versions of the service cupcap cupcup deploy and previous version revert back connected as simple as you just click here and revert. 


Abhay Kansal
·
33:31
So basically hamari approach what we do is basically we reuse our hardware again and again. Last time I think maximum time we spent on multi model serving correct and that is basically we are reusing our hardware for different use cases the reason being every time we provision that hardware we incur cost and we have realized GCP pay on demand provisioning is not reliable at least for GPUs. So GCB asked us to reserve that hardware. 


Abhishek Choudhary
·
34:20
Okay? 


Abhay Kansal
·
34:21
The way out for us is basically to do multimodal survey where we consume same hardware. Last time, if I remember maximum triton, he discussed why we are using triton because it is the only open source multimodel server available, which is giving us efficient deployment plus inference graph. Recently OCR model deploy current like past three weeks. And we are working on that only to efficiently deploy OCR model on our infra. And we do not want to incur that extra GPU cost. 


Abhishek Choudhary
·
35:16
Multi. 


Swapnesh Khare
·
35:41
Just be project. 


Abhishek Choudhary
·
35:44
Understood dynamically the amount of memory you allocate to triton thing is to be some of memory of all the models. 


Abhay Kansal
·
35:56
That you load, no right? It manages memory, but it manages it efficiently, pretty efficiently. We try to deploy these model in standard deployable formats onxia Tensor ID like we do not deploy them in native formats of any info. So OnX runtime efficiently down credit plus all these support FP 16 support. So basically mixed precision says further synthetically Ram double. 


Abhishek Choudhary
·
36:50
Understood. 


Abhay Kansal
·
36:50
So that's how we are managing it basically high availability not to just keep on deploying one model over another images model we do okay, understood only for images, not for tableau, but frankly Xgboos not for GPU, but CPU entrance and it was significantly faster as well. So basically the different workflows that we worked in our iterations may we looked at approach where if the model is deployed as a function at Kserv to scan her incrementally copy one three of it to multimodal server, which frighten and use GPU back end there. And XGBoost provides the inference file format, which is supported by triton. And recently OCR model without inference graph of triton with inference graph of trident and ten times OCR model requires multi people called. So every row is detected, then every row is detected for Yoski rotation. 


Abhay Kansal
·
38:48
Every row then subsequently recognized gRPC request. So basically throughput backup, I think as the criticality of our services will increase, we will probably add another but recently on demand availability India region. I do not know if you guys face the same issue GCP early this year on demand GP availability. So we had to move it to Taiwan part of our code. 


Abhishek Choudhary
·
40:12
Understood. 


Abhay Kansal
·
40:14
So we have two type of jobs fewer real time and fewer batch for images. So when were doing this that time, our primary use cases were batch oriented to Taiwan Petuladia. Then we realized my real time use cases we enough forget reserve. And then basically we are monitoring USA consumption gas and we are realized compute level pay. It is not even 50%, because I'm Q managed here. 


Abhishek Choudhary
·
41:00
Understood. Basically the model deployment tenor. This is actually powered by CASER only the model deployment. Basically what we do is models the company model saved. You can pick anyone. So versions we automatically store of every model. So just say like fraud detection 6.76.6 Yovi version up to deploy Kanye. You can deploy CPU memory resources. Certain instance could target Kana. You can also do that. Those things will come. So that is something the moment you click behind the hood, it's creating an inference service and all those things the same thing that you're doing. Basically, you just enter the name URL of the hugging face model the pipeline name and then you can just deploy it model Java. This is how you basically show it. Data scientists basically trying to make it easy. Behind the hoodie we know the model framework. 


Abhishek Choudhary
·
42:25
So TensorFlow server Python service model up TensorFlow server for every framework we choose the different this thing. We try to standardize the same inference protocol for everything. So we use the V two inference protocol. So by default we track to kind of make the choice like TensorFlow server Python. So all of that is by default. 


Abhay Kansal
·
42:53
Taken care of how much is triton. 


Abhishek Choudhary
·
43:02
And then probably TensorFlow. 


Abhay Kansal
·
44:01
Shared memory. 


Abhishek Choudhary
·
44:57
Understood? Okay, understood. 


Anuraag Gutgutia
·
45:21
This is very helpful. 


Swapnesh Khare
·
47:05
Because of this annotation is not present. 


Abhishek Choudhary
·
47:12
Request knative skill to zero as a request during the day sometimes. So that scale to zero is activated knative we'll put scale to zero option, but you can run it in both options. Either you want scale to zero or you don't want scale to zero. 


Swapnesh Khare
·
48:06
Scale to zero. Knative knative functionality. 


Abhay Kansal
·
48:13
So in our workflow, we end up spending time more time on figuring out how to take model into inference server rather than deployment. I have realized model go inference. So were trying to deploy debugging that and taking that out of methods for our detectron model. Otherwise we'll convert. So there are a number of issues that I think we are stuck in. Ideally, you guys will never stuck in those because we want to GPU. 


Abhishek Choudhary
·
50:16
Look to deserve it doesn't matter how. 


Abhay Kansal
·
50:25
The reason I was not able to reply was beside the things that are getting into production basically repurposing models for different use cars and things cost wise automatically tightly binded. 


Abhishek Choudhary
·
51:01
You can throw money at the problem. 


Abhay Kansal
·
51:02
For example, detectron t four pay and media t four pay three requests per second. When it got converted to tensor RT, the request fitnama 24 requests per second. So latency we automatically resolve. 


Abhishek Choudhary
·
51:45
How are you testing this thing? 


Abhay Kansal
·
54:15
The thought process of Tech team was very different from it used to go in the red in this direction, correspondingly in parallel. These case projects came up as well. There is another good project that we will soon try. For similar thing but for wider use cases. Once we settle this case, we will start working on Ray as well. For similar use cases, the use cars, they are more broader. Like we can have more wider application again. Instance, graph is available there we can do scaled up data processing and things like that. 


Abhishek Choudhary
·
55:23
This might reason why at the end of the day, it's both deploying docker images. 


Abhay Kansal
·
55:32
From developer point of view, the thought process is very different like at least the tech we have at cost 24 days. We then resort to a post request these are two ways they can directly communicate with us and we will reply to them with language established. For example, they do not get the idea of inference store, and we are using inference store for many things, inference. 


Abhishek Choudhary
·
57:46
Understood. 


Abhay Kansal
·
57:48
And then unparalleled my GCP ecosystem use fees, update karna so basically what people do is they put their data in BQ. They put their data in GCP GCs then GCs major data put author service trigger and then feast. Cars load update. For many workflows we are using Gcpk in built triggers. 


Swapnesh Khare
·
58:32
Feature storage copy used. 



So basically hamada and patio tech APIs with So tech do not does not share their these data endpoints live databases endpoints with DS the government request memo data, then data access connection postgres by security policy. But we can only access data warehouse which is snowflake in our case. So we can sync snowflake at 15 minutes frequency. So snowflake is more or less near updated data for us. And over that we build this. 



Basically. 



Just like thread layer. Over the data store, get that kind of thing. 



Fees was our replacement for Firebase. Basically. Initially we started with Firebase, but Firebase scale Fire, story of Firebase after a certain number of requests, then we realized, let us move. To this tech. So mostly codenam, static data by policy. 



Features, load functions. 



GCs server. So DVC we use for model versioning. 



Okay, understood. 



This is super helpful question. If put a platform, I think which parts together. Plus, I don't know if you would want to discuss the architecture from our systems. I'd also like a call. Or we can walk over our architecture and again upload already? But I have a coffee. Coaching, but still, if you want to try our system for deploying and seeing how it works would be happy to kind of expose the system to you. And we will try to make it easier for you to also update your cluster. 



I think we surely want to at least try if not consume it for our use case because these guys are moving to inference graphs which are even more custom thing we call those you guys will implement it for sure once you will integrate that feature in your ecosystem. So you will figure out a standardized way to connect different API endpoints for sure but we have tried it in a few use cases in QM there are a few use cases that we can easily build on those insurance graphs if you have looked at it. If it is possible to connect two clusters in different two different zones actually issue case of resources available I would want a particular service same case as the Du sole. 



Cluster. 



Level communication linked checks and balances build Nagaro the whole idea of using case internal language zone on a defined career. Checks and balances. We did not want a copy of the same service. 



It is not going to do inter. 



Cluster diagram 10,000 model plus problem. It is there. So I was assuming beyond cluster for simple reason document. 



Did you guys not try out Vertex? What was the reason this way and not doing that way? 



Main reason. 



That is what we are doing already. 



Triton recently. 



Because GCP is selling that 19 9% MSD and was 99% 400 MSP. 



Okay? 



For that small piece that we had cost multiplier as I said hardware across multiple deployment per deployment multiplier which was and then we realized even GPU availability to India. We thought but vertex India may be scaling or other underlined GPU available whole purpose got defeated. For us it is expensive. And even that is not scaling. Plus I frankly do not want to bind myself to any particular cloud as per se comfortable is liquor but other cost effective they come to the lambda. So Lambda cloud is really cost effective but management approval to go there. 



But what the cloud run is more efficient. 



Cloud run be equally inefficient. 



Cloud. 



Run is basically managed knative API application sometimes compute shared hardware. 



Basically routing. 



Specific payload specific. What we have done is we have minimized the generic. 



As we keep building. I would love to sync up with you if you have time and show you and get feedback. 



Yeah, sure. Actually it will be helpful for us as well. Mutually. So there are very few people who are actually working on the same problem. Actually startups of. But it has not been easy for us to work on this. Plus changes. As I said, Consumer. What is the right approach? T get in. Bye. 



Thank you so much. Bye. "
11354403948,TigerGraph,Bill Shi (bill.shi@tigergraph.com),,TigerGraph,100-500,500 - 1B,<10,AWS,Yes,Software Development,"Redwood City, CA",Nikunj Bajaj,TigerGraph_Bill 13-12-2022,https://app.fireflies.ai/view/TigerGraph-and-TrueFoundry-Introduction::2hv23yB5NG,"13-12-2022

https://cheem.notion.site/TigerGraph-bb691a09ce264b499ecabf5085c1be9d"
11354403912,TabSquare,vishal vishal@tabsquare.com (vishal@tabsquare.com),,TabSquare,100-500,<10 Mn,<10,GCP,Yes,IT Services and IT Consulting,Singapore,Anuraag Gutgutia,TabSquare_Vishal 10-01-2023,https://app.fireflies.ai/view/Tabsquare-TrueFoundry-Follow-Up-Call::KxH6IPKTtH,"All Notes

https://cheem.notion.site/Tabsquare-ad4bae6258f8419dbd075c24e04e6782"
11354403912,TabSquare,vishal vishal@tabsquare.com (vishal@tabsquare.com),,TabSquare,100-500,<10 Mn,<10,GCP,Yes,IT Services and IT Consulting,Singapore,Anuraag Gutgutia,,https://app.fireflies.ai/view/TabSquare-and-TrueFoundry-Introduction::BmBa3F3T6X,"You are going to lead? 



Yeah, if you want, yeah. 



Happy to. 



Kind of see as to how you go about. Try to lead through the floor. 



Okay. 



On the user flow. 



Let's see. 



We don't have to give a demo. 



Hi Michelle, can you hear us? Hey. 



I was wrapping up my call. 



Where are you based? 



Auto. 



I'm from Bangladesh. What about you? 



Yeah, I am also in Bangalore. Our entire engineering team. 



Glad to meet you. 



Which part of by the way, I live in South Bangladesh. I'm in Managata Road. 



Our office actually is in Core Mangla, too. 



Nearby? 



Yeah, nearby. Nice. 



Just to give you a short introduction. 



So. 



Hi, Michelle. I look at products and customer development at Truefoundry. Previous to this, I was working as a management consultant at McKenzie and graduated out of It for chemical engineering. For that. 



Yeah, I can give a quick background as well. We shall also be tech from It for electrical engineering and then spent six years working with a hedge fund called World Quant, where were using data to build trading strategies across global markets. Spent three years in Mumbai and then three and a half years between us and Singapore as a portfolio manager, trading a portfolio of 600 million in assets for them. And was a member of the CEO office as well. And my co founders, Abishek and Nikkunj, both of them were at Facebook. Abishek on the engineering side, he was a senior staff software engineer there at Facebook and Anikunj was a lead ML engineer. All three of us are, and we have been like each other for 13 years. And that's basically been the basic foundation of starting Proof. Truefoundry. 



Awesome. 



Okay, my introduction. So I'm working at Times Square as the head of engineering. Now we got acquired by Delivery. So we are more of a delivery. And I'm taking care of entry and engineering practice over here. And I haven't worked with the company like Facebook and all because for my career I work with startups only. So I started my career with Inpibium.com, which was an e commerce company. It was a startup. When I joined the company, I was Pivot engineer. If you think about any startup, I have joined them either first or second or third engineer. 



I see. 



In premium. Then I moved to Logic, first engineer over there. Then I took him on Commitment for ten of years. Then I moved to Curate. Then merkel Science and then merkel science to here. 



That sounds like an interesting journey. 



Yeah, Merkel Science is the one that was being run by Subhanker and all. 



Right, which one. 



Was with the Singapore based company? 



Or is it ma'am is a founder, CTO founder. 



Got it. I think this is different. 



Okay, understood. 



Thanks a lot for the introduction, Michelle. I'll give you a short, brief context of what we want to do with this call. So Truefoundry is building an ML platform, which is focused majorly on ML model deployment. We want to ease out this process for enterprises. So much so that the data scientists themselves can do most of the stuff for which currently they would have to rely on an infra or a type of team so that it really reduces the kind of back and forth that happened there. In this regard. Currently we are in a very early state. 



So what we are trying to do is to partner with companies like TabSquare and what we try to do is to understand more or less of what kind of problems they are facing, what kind of context is there, how is your experience currently in terms of model deployment and associated steps of ML lifecycle? And try to understand what kind of problems are there potentially that we could better help you solve? So with that context you want to just understand what kind of ML use cases do you guys work at and brief context about what kind of stack do you use, what the developer experience like right now for a data scientist and something like that? 



Is that fine? Like Michelle, if we can use this call to learn a little bit about the workflow. And also basically one thing I had is you mentioned you kind of got acquired by Delivery. Is it the delivery, the Indian delivery? 



No, Delivery hero. 



Delivery Hero. Okay, got it. So one context is like does the engineering and all integrate with them or is it like you run separately, still. 



Continue to run separately, but we are working with them to integrate our products with that product, with our products. Okay, so even before that, before we have already integrated with Pool Panda and few other companies to process their orders on our system. We have that order in our system. 



Okay. 



So I'm not the right person to explain it because we have a dedicated data science team, they are taking care of it and we have San Luke who is VP of Data Science and Engineering. So he's the right person for it. But from high level, what I can tell you is like we are analyzing each and every data. Okay? So whenever you click on a particular product in a restaurant, then I'm just telling you the way. I'm not ML Kai, so I can't tell you proper way. 



No. 



So what we do is we are tracking each and every event and then we are sending it to Cloud Firestore and then Firestore triggers an event to Google pops up, send it sensor to Google dialogflow where we are doing ETL jobs. And then we are storing data to BigQuery and we are also storing data to this one elastic search and for recommendations, purposes and all. And we are running some AI and process on the top of data. And then we are providing what is it? Cross selling and upselling recommendations along with analyzing personal food, eating habits, so that algorithm I am also here to understand because I've never got a chance to get into that particular part. 



But we are running ML engines over there where we are analyzing persons for eating habit only through the order data, whatever order that we are getting from for the particular. And we are analyzing each and every event. Like what are the ingredients, what are the taste level, everything. What kind of poor weather is vegetable, non vegetable. So there are around 600 to 900, I think even more than that list set up battery against which we are checking the order details. And then we are giving personalized recommendation for each and every user. 



Okay. 



And you mentioned you work with your clients would be like big conglomerates. Is it like food panda or something? What you mentioned. 



Restaurants are our main clients, but we have created a product called Smart Connect where what we do is you don't have to whenever you have a restaurant and you want to register your restaurant on Biggie dometo and Uber, it managing menus categories, orders in all the applications, whatever they have provided. So if you want to update a menu item, then you go to update in all the three applications. If you want to market out of store or update the price, whatever you want to do. So we have created a centralized system. We manage everything and create categories, manuals, promotions, whatever you want to do on our system. It will be a full sync all this information with four pounder or whatever system. 



And then whenever somebody will place an order on this particular platform and if that restaurant is registered on our system, then four ponder will send those orders to our system and we will start processing those orders. So you don't have to accept orders and pool print application, accept it on sorry Tesla product and then manage everything in one go. So offline orders, online orders, everything will be managed at the center of location only. 



Okay. 



So one question that I had, what is the team size for the engineering and the data science all and also for the is it separate or is it like generally the platform team that manages it? 



So we have separate team for infra and separate team for engineering. So Data science, we have around 15 people in data science and engineering. And when it comes to only product engineering, then we have around 120 people. 



Okay. And this is just TabSquare, I'm not speaking about delivery. Okay. And why separate infra for data science and ML, sorry, for ML and software? Is there a particular reason? What is the stack like? If you can given you had the engineering side wanted to was curious as. 



To what it is. Totally isolated application. Right? There is no connection between product database or their database. There is no connect because they are going to store data in their system or sorry, in BigQuery or they're on Elastic search. So through pop up queue we are just pushing it through their system or either the system, we are just pushing that. There is no relation. So we have given them separate product sorry, separate Google project and they are going to take care of it from there. 



And what is the base layer? Do you use Kubernetes or do you use any particular cloud? 



No, we are using TCP and we are also using Kubernetes. 



Okay, got it. Understood. And I'm guessing the data science team will also be deploying on GKE then. 



Yeah, a few things are there on App engine and few things are there on GKE. 



Okay. 



There are Dataproc and all these systems are also there, right? 



Yeah. 



Okay, cool. So shiny. Like you had questions. I also want to ask more on the engineering side given Michelle that is where a lot of the experience of your site and then maybe try to. 



Kind of try to get somebody on call from data engineering but not replaying might be busy income other call. Okay, we can set up a separate call with them. 



That's fine. 



If you can just link us through, that would be really helpful. But on the engineering but we are just going to understand the journey. So as a restaurant, if you are catering to individual restaurant, they just integrate with, let's say Omato Sweet and Food Panda. 



Right. 



And how do you install your system on there? Like do you give them hardware? How does it work? 



We are providing the hardware. Then if they have already purchased Path machine, then we are integrating with Path machines as well. Like we are doing all kind of integration whether it is offline or online. Okay. 



And does your application like run on these machines or. 



We have applications running on kiosk machines, we have applications running on tablets and we have QR codes managed with the application. So then the curcode application will come, the application will open your mobile specific to that particular restaurant. Or if you are sitting on a table, then it will be specific to that particular table only. 



Okay, so it's like integrating both the live dining experience as well as online that the restaurant might be catering, right? 



Yes. Okay. 



So like a unique channel for both of these orders. And now let's say this data comes in. This data, where is it stored? Like let's say a user or a restaurant would have to create their own database on menus et cetera. This would be stored there. Is this also in GCP? 



Yeah, everything is in GCP. 



Okay. 



And there is no privacy concern et cetera. 



Right? 



I mean you get access to every customer's data. 



Yeah, because like customer data is the core thing for us. So one customer with one TabSquare customer can access Tabsco application. In any restaurant you just have to enter your phone number at the time of registration. We are taking the permission or consent like we are going to use your phone number and we will go forward we will give you start recommendations wherever tap score is available you will use the same credentials okay? 



And now just to connect the flow let's say a customer places an order on any of your apps let's say the QR code this event goes when, how is it processed? If you can just shed some light again on that. 



Once you open the application, scan up your application, go through menu items and then move them to card, then confirm that particular card. And if there is any promotion and apply that particular promotion, or if you have any reward points and you can also apply reward points and then go ahead with order process. Place the order and then order will come to your system. We will go to merchants consoles like merchant console will get an order. You have an order. Okay? So that's one flow. And along with that, whenever you confirm the order, along with that, we'll send trigger one event from front end only this data. And you receive this data and you analyze whatever you want to analyze. 



So now this data let's say this order comes to the console. Now this data is sent where? 



It's sent to your cloud it's sent to our cloud okay and analytics on. 



This, is it like aggregated like a batch mode kind of analytics or now. 



How does this order get processed after. 



It'S sent to your cloud? 



Once it's sent to our cloud. Okay, so we are triggering one event. There are two ways. If it is front end sorry, web application. Then if you go to Google Analytics we operate our own analytics. So from front end only we are sending that data to our analytics database and other option is we are sending whenever we confirm the order we are sending that order detail to that system through Google pop okay? And there is one more platform where we are supporting offline systems as well where we are sending match order because there is a possibility we are then offline set up in some restaurants where there is a possibility there won't be any internet. 



Connection, but we have installed it, so without internet connection also you can place an order and restaurant person will receive that order in their merchant console and they can process the order but everything will happen offline so once the system will come online. Theoretically, we are syncing data with them. 



Okay, understood. 



And let's say this data comes in, let's say 100 people order from this restaurant. All this data comes in to you. So do you have like, where does analytics come in? Where are the ML models for each customer? Like there is a separate model and is it like a batch inference or a license? So you didn't get per customer you have different models. 



Per customer I need to check that I need to confirm. 



It okay, but basically this data comes in, you have analytics dashboard which you populate with this data, right. And they can go. So are there any predictive or classification or something kind of modeled as well rather than the statistical analysis? 



When do they I'm just sharing whatever information is public right now. 



Okay. 



But whatever possibilities that you can take up, everything is there in the system. 



Okay. 



Got it. So few things vishal I would love to know from an engineering standpoint, I wanted to understand what do you use for are you using Data Dog new relic or some other tool for the monitor using Data Dog? And how is the engineering workflow? Like, suppose an engineer and this is say the reason why I'm asking this, I'll give you context is while we have built an ML platform for the deployment and monitoring for the data science teams, we kind of have built it on top of integrated to the software engineering workflow. So for example, in your case you are using say, GKE. So what it will do is the system will connect to your GKE and it will create spaces for your data science team to work on. 



And then within those workspaces, then the data science teams will be able to deploy. So basically we try to do it on top of the software engineering systems. And one of the things that we have heard from companies is even for software, some kind of this system could be useful. So while diving into that, I wanted to understand once the engineering workflow, two developers themselves test out the services they are building. Is there a visibility of all services that are there in the organization? How does the engineering workflow look like? Who writes the YAML Configuration? Is it the DevOps infrastructure? So a little bit of understanding on that side will be really helpful. 



Okay, definitely. So YAML file, if you think about YAML file and definitely DevOps team is taking care of it. But when it comes to docker and all the files, then it is engineering team who is taking care of it. 



Okay, so basically engineering team will write the code somewhere, then they will dockerize it at their end. And then this docker file will then be sent to the DevOps file who will write the YAML configuration responding to the Kubernetes and then they will do the deployment. Is that it? 



Yes. 



And is the rollout generally like one shot or is it based on traffic? So it's rolled out say 1% of the traffic, 2% of the traffic and then skill or is the rollout yes. 



Following Canary development. 



You follow Canary? Okay. Who does the canary like? Is it again like the conditions for the Canary, is it set by the development team or is it set by the DevOps team? And how is it? 



Right now we are just setting up proper DevOps practices in the company. Previously there were limited engineers in the company. So devo engineers in the company. We are doing it through DevOps team only slowly. We are improving all the things. 



When you say proper DevOps practices, I wanted to ask a few more secrets management. Do you use anything for secrets management? 



Yeah. 



Okay. Is it the Google Secret store? 



Yeah, for some service we use it. We use something else. 



Okay, got it. And then the authentication layer on top of this, like it's built in house or in house? In house, currently. Okay. And then how about I'm guessing in software, the hardware would not be a challenge like which hardware to use? Mostly in most of the cases. 



We have with specific binders. So whenever the requirement, we are looking for kiosk in Apple tablet or Normal and a Samsung tablet or any printer concept to your device, then through those vendors, we are just purchasing them. And then we are setting up each and every country. There are different devices depending on the availability. 



Okay, got it. Okay, that's understood. And then how does the CI CD is taken care of, like CICD and. 



What do you kind of Jenkins right now. 



Okay. 



So for a developer, just try to understand. So do they do the training, et cetera, on their own laptops through a Jupiter notebook or something like that? What is their experience like currently in. 



The for the data centers training? 



No, let's say they're developing some models or trying out different stuff. Experimenting? 



Yeah, they are doing it on Jupiter. We are done set up for Jupiter Instruments on cloud. 



Okay. 



Also on GCP. 



Also on GCP. 



Okay. 



And then from migrating this code, let's say this Jupiter code to creating images out of them or writing the Python files are the same. Is this also done by the developers? 



Yeah, that is DevOps team. Everything is done by developer. And just sorry, database team is everything is done by developer and then help in deploying all the services. 



Okay. 



And just trying to understand in terms of issues that you might be facing with deployments, what takes up like most of the time of DevOps team when it comes to interacting with the data, serving the data science team? Are there like any particular invoice that. 



You setting up pipeline and all the things setting a pipeline of creating ML file or monitoring system with infrastructure set up and all this? Only those things fall under the op string when it comes to deployment. So you just have to merge your code and it will be deployed automatically to the production. 



Okay. 



And this pipeline, resource provisioning, et cetera, everything has already been templateized or something or is that a case to place? 



Everything is in place. 



Okay. 



Understood. 



One more thing like visual. As we are moving to this basically more DevOps practices, I wanted to understand, are you developing something in house from a platform team's perspective? Like basically what's the platform teams at hotstar. There is a kind of develop where they basically make it very easy to kind of put services and it automatically goes through Canada and then there's full control at the hands of the developer in terms of visibility, in terms of cost, as well as promotion flows like currently. Do you deploy it to different environments and do you kind of promote from one environment to other or is it directly into a product environment? How does that part work? 



We do promote staging to you, to. 



Production and is that easy from a developer standpoint or is there a lot of bottleneck? 



No, there is nothing to coordinate. You don't have to coordinate anything. You just raise your peer against a particular branch and automated pipelines are already there in place to measure and then. 



Okay, so do you kind of create a copy of that? Basically suppose something is running in the test environment and now you have to move it to a production environment. So are these in the same clusters or are these in the same workspaces or are there different clusters that manage the product or test? 



Different clusters. 



Different clusters. So now if you have to take this and deploy a different cluster so do you create a copy of this code in the different cluster? 



Yeah, we need to because everything different branches and everything like staging may have code which we are not supposed to deliver deploy. So we can't promote staging branch to UAT or UAT branch to promotion. Definitely UAT and promotion. Other production are identical, but staging to UAT is not possible. 



Can you use GitHub visit or like BitMarket? 



Both are fine. 



Yeah. Okay, so I got a sense of the engineering side of things. 



Well, what we do, I'm telling you, we are creating a release branch and that branch we keep matching to different systems. So one, staging can have multiple release branches, but that specific relief branch will help code which we are going to read on that particular day or that particular time. 



Understood. Which are like two things. Like first of all, thanks for sharing the context on the engineering side and also whatever context you are on the Data science side. What I would really request is. 



Somebody from data science team. 



Yeah, if we can get someone from the Data science team it will help understand that flow a little bit better. And we'll also love to take the chance to showcase to the platform in that and specifically for the work that you are doing on engineering in terms of setting up the best DevOps practice. Actually the way we have built the platform, we have tried to build it agnostic to software engineering as well. So I'll just tell you at a high level, what happens is basically first you can easily connect to your cluster like a GKE account, you know, including authentication, access control and choice of machines. Once you have connected within that you create different namespaces or workspaces for your different teams. 



This come with resource constraints like fourGB tenGB, memory, whatever and then you can have control of allowing people access to it, not access to it with full access control. And once this workspaces are created, then developers individually can deploy any service to this workspace which could be a service or a Cron job. The Cron job can be scheduled or you can trigger it at your own well, the deployment to the workspace is very simple like you can do it from the UI or through a simple YAML configuration. You don't have to take the help of DevOps or infringement for the Kubernetes YAML. Basically everything is orchestrated through the platform and then the DevOps comes and is more like a security layer wherein if anything break or if you change in the final approval, they come into the play if you want it. 



If you are okay with your developers directly pushing then that is also fine and develops as a visibility of the platform. So that's the kind of abstraction we have tried to build out into the platform. And then once you kind of do this deployment you automatically see monitoring logs which you can push to Data Dog which you are using and then at the same time you have secrets management automatically ingrained build. So if you are using Secrets Manager you can use our interface to kind of generate secret keys on top of this so it acts as a layer to sit and then all the other best practices like CI, CD integration to GitHub bit bucket, all of that. Your authentication systems, your rollout to production in a canary shadow testing manner, all of that comes by default with the system. 



Okay, so that's how we have built it. And the same workflow is replicated in data science. In data science like instead of TML, it basically exposes the Python way so that data scientists can do it. And then there are certain more complexities that come into the platform in terms of doing monitoring at the level of data science rather than just system level monitoring. 



Okay, got it? 



Yes. 



I would love to kind of have you on the call along with the person and actually give you a walkthrough of the platform. I would love if there is a way in which you feel this could be useful given you are already transitioning to something more usable on the developer side in terms of the best practices. I would love to see if you think there could be any value from there. 



Sure. 



So as an except we'll write an email, but it would be great if you can then include the corresponding person and we can try and take it forward. 



I'm just checking if we can join right away, but I think I'm also running out of time because I need. 



To we're all out of time, but another day is definitely. 



Let'S do that. 



We will also take the time to show you the platform that we are building. 



Okay. 



Can we do things? I can set up a call with you guys tomorrow. I'm not available. Thursday around 04:00 p.m.. 



Yes. 



Thursday 04:00 p.m should work. 



Do you also check with the Data Science VP? 



If he also Data Science VP, I think he will not be available. But Data Science Engineer who is the main person who is working on it. 



Yeah. 



Okay. 



Then I'll send you a minute for that. 



Thanks a lot. Thank you. 



Thank you. 



If you can stay for a minute yeah. "
11354392663,Flatiron Health,guy.amster guy.amster@flatiron.com (guy.amster@flatiron.com),,Flatiron Health,>1000,100 - 500 Mn,25-50,AWS,Yes,"Technology, Information and Internet",New York,Anuraag Gutgutia,Flatiron Health_Guy 09-12-2022,https://app.fireflies.ai/view/TrueFoundry-Guy-Amster-Flatiron-Health-::80bEUbf44R,"Hi Guy, 
Hope you had a great start to your week! 
I understand you might have missed my last email, so sharing a recap of our conversation and the next steps that we discussed: 
ML Pipeline at Flatiron Health: 
Team of 40-50 Data scientists. The ML infra team consists of 5-10 people. Big team on in-house capability on label generation which is done manually. 
Use Cases: - In the Oncology space, HR system - patient care, screening, Research data to pharma client.
Model: NLP - extract data from patients chart (85% of overall ML effort). Inference: Batch process (majority)
Current ML stack: 
Prototyping - Jupyter hub, GPU & Machine on AWS (considering Databricks), XGboost, pytorch. Also, use published models sometimes.
Feature store - AWS 
Deployment - in-house solution
Monitoring: in-house for model performance and keeping model up to date
Challenges: - State of prototyping, ML Infra to some extent 
Next Steps: 
A follow-up call with ML Infra team to get feedback 
Understand and explore if TrueFoundry can help Flatiron Health in solving existing pain points 
I understand you must be busy with planning work and a roadmap for the Quarter (and Year). Checking here if you could get a chance to speak with your ML Infra team? 

Another meeting with you and Flatiron's ML infra team will help us in validating the challenges that we discussed and deeply understand the end user's needs. 
We look forward to meeting with you and the ML Infra team at Flatiron :) 

Thanks
Vivek"
11354403489,Netradyne,Rashmi Pulgam (rashmi.pulgam@netradyne.com),,Netradyne,500-1000,50-100 Mn,Oct-25,Multi Cloud,No,IT Services and IT Consulting,"San Diego, California",Anuraag Gutgutia,,https://app.fireflies.ai/view/Netradyne-and-TrueFoundry-Introduction::zUOEpX6Tbc,"Prompt.  



Let's see. As a junior then the junior is not very junior, but I'm not a decision maker. So you can dive into the ML side a little bit. Join me.  



Yeah, thanks. She's here.  



Okay.  



Hello. Hello. Hi, Rashmi.  



Hello.  



Hi, Rashmi, nice to meet you.  



Yeah, I can hear you now.  



Okay, great. Nice to meet you.  



Yeah, hi.  



How are you doing?  



I'm doing great, thank you. How about you?  



Good, actually. Great. Where are you based? Out of ashram.  



So basically I'm based out of Maharashtra. So I've been working in Bangladesh for about four years now. Initially I was working in Hyderabad and then it's been four years. I joined Netradyne about a year before. Okay, so could you let me know more about True Foundry and yourself as well?  



Okay, I'll just give a short introduction of myself and then andravin appreciate so basically about myself, I did my chemical engineering from It Khadatpur post that I was working with McKinsey and Company as a management consultant and I joined through Foundry about four months back. I'm looking at product majorly here, so that's about me. And why would you want to give a shot?  



Hi Rusty. First of all, basically at Two Truefoundry, we are helping companies speed up their machine learning deployment process, whether it's training deployment or deployment of production traffic. So we'll dive into more details later on, but just wanted to set the context there. Right now we are a team of team membershake, myself and Nikunja. We are the co founders. We are looking to work with companies that are scaling up their ML and trying to help them partner with them to kind of see if we can help them in adding certain values to the pipeline and understanding as to how the MLS and that's why this discussion was set up. Brief about me. Graduated from it. Correct 2013 batch. After that, spent six years working with the hedge fund called World Point where I was looking after portfolio management for them.  



So we used to work with a lot of different data for building trading strategies. Spent three years in Mumbai and then three years between US and Singapore. And then after that founded our first startup together again with the same team for the entire we sold It Twin Foudge and then finally we are now Abhishek C truefoundry over the last one, a little bit more than a year. Just a quick background about him. He was at Facebook for like six years. Posted he led the software engineering site there from a software engineer to later on when he left as a senior staff software engineer. And he led the videos team, the intro team and so on there. And they couldn't just be on the ML site. So that's like a quick background.  



Yeah, that's good to hear.  



Yeah, I think now since the context set so that's why basically since we are like a young startup. Right now we're looking to partner with different enterprises and basically trying to understand what kind of problems companies like yourself, which heavily use data science and machine learning are facing in the machine learning pipeline. And just trying to explore if there is any avenue where we can add value possibly to this. In that regard, I would love to understand what parts of the ML pipeline do you look at and how in a broader sense is machine learning use at metadata.  



Yeah, so as you know, Netradyne has its own products which is basically installed in fleet, wherein it has a different capability of monitoring the road as well as the driver. So it has multiple features. So for example, if you consider the on road, like you have to detect the signals, you have to detect the lane, you have many other detections. And if you come towards the driver detecting the drowsiness of the driver, whether he's obstructing the camera or whether he is sleeping or some other facial expressions. So based on that, there are multiple features. So this comes mostly into deep learning aspects that we consider the data science and machine learning. So here, wherein we do experiment with different deep learning models, we come up, we have our own data set.  



So that is a strong point of Netradyne where we have developed a huge data set till now. So based on our data set, we keep on training the models, we keep on improving the models. And that when it comes to deployment. And we have different kind of products. So like, we have three different products, okay? One is the NVDI based products and another one is Qualcomm based product. So I just wanted to know, is this being recorded?  



No, it isn't.  



Okay, so with deployment side, when we try to deploy the device end, we basically try to fall back with the devices wherein they provide us the deployment pipeline. For example, if you consider the Qualcomm deployment, we have Snappy, so we have the Qualcomm processors being used, so they provide us the pipeline for Snappy. So using that, we deal with the deployment. So that similar thing when it comes to Nvidia, we have Tensor RT. Okay, this is the big.  



I think the thing was whether it was actually the fire size note taker does kind of take the transcript notes from the meeting. If you want to kind of if you're not comfortable, just remove it from the material.  



Yeah, that would be great because since it's. "
11354392103,Sugar CRM,Neven Sumonja (sumonja.s.neven@gmail.com),,SugarCRM,500-1000,100 - 500 Mn,<10,AWS,Yes,Software Development,"Cupertino, CA",Anuraag Gutgutia,,https://app.fireflies.ai/view/TrueFoundry-Neven-SugerCRM-::6xdSV4LFTD,"Just send me the LinkedIn profile. That's fine.  



I'll anyway set the context of before anyone. Hi, how are you?  



You're on mute, niven, we are not able to hear you.  



Yes, we can hear you.  



Hey hivin.  



Hello guys. Hello. I'm sorry, first time using this up here on my iPad. So I had some little problem with all this configuration and everything because I mostly use the slack. So I have my working configuration somewhere else. So I needed to put it in the work. Okay guys, nice to meet you. I'm sorry. Also one more thing. For some time last time I just forgot of the old obligations I have. So I mean most of the time I'm almost twelve or more hours working something. So I'm sorry for that.  



No worries. So I'll quickly set the context up. Unfortunately, Abisha could not join. But I have with me today anurag he's a founder as well with the firm. So Anurag, Abishe and these are the three founders batchmates from their graduation college. So they'll introduce themselves. But I'll just quickly set the context up. I took a download from Abishek just to ensure the session is productive. So basically Naveen has kindly agreed to help us in understanding the current process that he follows at Sugar CRM. Some of the challenges that he and his team might be dealing with day in, day out in terms of deploying the model into the production, what are the learnings that he has? These are some of the things that we'll be discussing today, focusing our conversation around that Nevada.  



Basically we are building a solution to enable the deployment of machine learning model really fast onto production with inbuilt monitoring into it and something which I think and Rag will talk a little bit about more. I am Vivica and I look after business development growth at Truefoundry. We work very closely with Anurag and Bishop on growth initiatives. Nice meeting here.  



Me too. Yes.  



First of all, I really appreciate you taking time for the call. What I would love to do is spend two minutes giving a brief introduction of myself and two foundry, including my co founders. And then I would love to spend some time learning a little bit more about you about Sugar CRM and the data science use cases plus what the current entries. So does it sound good from the context?  



Yes, of course. Yeah, please.  



Glad you had something.  



No, I just wanted to say as I already mentioned Abhishek, I cannot speak about anything that is yes, I cannot disclose any type of this level of information. This is something that I already say. I can told that this is okay. So I can speak about my experience in the sugar and outside of the sugar, in the science and so on, but only in not so specific cases. No, this information cannot be disclosed.  



Yes, yes, that is completely fine. So Niven like I am on Rock, one of the co founders at Co Truefoundry. Like my background is I did my undergrad in electrical engineering from Indian Institute of Technology Krakpur and then post that, spent six years working with a hedge fund called WorldPoint where I was using a lot of data for building trading strategies and trading models. During that time I spent between Mumbai, US and Singapore and I was trading around 600 million assets for WorldPoint. So got a really good sense of how to use data for building strategies. We used a little bit of data science but more things around data engineering and basic strategies. During that time I also started investing into startups myself and invested in 22 startups so far. Post that like when? In 2020.  



Me, Abyshek and Nikunj we're all in the same boat. Nikonja Abyshek were senior software engineers and lead machine learning engineer Facebook to bring a lot of experience on the technology side of things and how ML platforms are built together. Like Facebook internally uses something called an FB Learner for speeding up their machine learning pipeline and what we are trying to do is bringing a version of a bill Learner to every company around the world. The goal is to make it as less for companies globally to adopt ML and deploy ML models either to a test or a production use case just like it is in Facebook and with the best practices that are followed. So yeah, I mean that is a quick summary.  



We built and sold a startup in the talent space as our first startup to one of the big talent companies india and through foundry we have been building for a little more than a year. We are right now a team of around 18 members full time most of it being the engineering team and primarily we are based out of our headquarters is in US. We have a team india and we have few folks in Paris as well.  



Nice, this is nice story. I guess then you have some experience with all these people especially for the Facebook. This is glad to hear. I would definitely like to know more. Okay yeah.  



Naminga please go ahead.  



Thank you for introduction. So on my side, I work in the science for some time, more than eight years working on some first research problems, multiple I mean different ones from buying from artists perspective protein interaction, structure, rotation, genetic and so on. Then after my finalize my PhD I go out of the science. It was the last how much? Three, four years. I did some different job. One is some of them are high profile like one of those is for some company that work with Hollywood studios one that we try to predict how the movies will basically how much money they will make but only using the information. Even this information is disclosed to the public in any level. So even before you start shooting and so on.  



Different stuff then yes, I first started to work with note startup, then we go to the sugar working on the different levels. So I mean not to go in many details, there is almost very little thing that I didn't do in that time. Only thing that I didn't produce. I had the opportunity for working with the pictures but almost everything else I had some level of experience with it.  



Got it. Just quickly to a quick follow up question. So a lot of your work has been on the research side. What has been your experience in terms of putting the models that you are developing either into hands of a real user, either in terms of like maybe a test environment deployment or a real pro deployment or even for training like say bigger models like when you have to use virtual machines. I would love to hear a little bit about that.  



The problem is two level. First is that be research lab that I work on and at the time I also work on the server infrastructure basically maintaining all the things mostly with some help of the DevOps. But in the science this is a little bit different. The level of the distinction between the different role is much more blurry than you will see in the companies, especially big ones.  



Got it.  



So the two problems are always there. The scale and position of something. Okay, it was the time before Rves. So right now Averse definitely changed many things and many people are just doing the two things. Do you want to do something on the premise or do you want to do in the ives and that change industry as you already know and most people are just using that. I mean if you're a small company you're using probably OBS prices and everything. You all know that story. But I was from the time where you need to do something on the premise, there is multiple reasons for it. There is my work on also very projects that are connected to something that are of the national basically security issues.  



So it cannot go anywhere outside of the country and even if you provide some information it needs to stay on the protected servers and so on. So there is a completely different story because you need to build your infrastructure from ground up security to the outside of the user who will use it, with which tunneling, how that will be introduced and so on. Of course that is the problem of the nodes and basically the problem can be scaled. Are you using just how big and how complex of your survey infrastructure are you using? Single node, multiple nodes and so on because most of the things that smaller companies and research labs can do is mostly smaller stuff.  



If you do not have the high level of the HPT computing and so on and your labs are not using that. That mostly is single or probably two or three very much connected nodes. In small labs, the bigger ones you will have the infrastructure. I work in the France where we have basically all the people imposed their infrastructure. But then is the changing, do you want to train on it or just deploy? And that is the first question. Then there is the question of course how you will use the prediction and that sometimes they're even the more problematic than having the model itself. I had a different situation. I had the situation when we had 40 million predictions per day. Oh wow, that's now yes, that is the problem because it's not a problem.  



Training, even training is slower, you can faster the train the model that you need for the prediction. So then is the question how you will do that? Do you need to produce multiple models and then predict the power so the infrastructure on server if you put it how you will handle it without another question is maybe you want to put it on a mobile device. And then the whole, of course, different game changer because your model needs to be shrink. Neural network need to go completely floats. Need to change everything need to change because you don't just have the memory you don't have. The capacity for very long prediction timelines. And then also, if you're working in iOS, you have the gateway problems. 32nd predictions depending on which infrastructure you're using.  



Sometimes their gateway is just not enabling because all the process of deploying the data, transferring the prediction needs to be done under some timeline depends on what you're using. So it's completely different story now, I understand that it's easier now because of the OBS. So most of the people I heard some and I heard something, people are starting doing things and they're just using Rs. I'm creating specific configuration for you that can be done. But like I said, it all depends on what you are targeting. If you are targeting big companies, they will probably have their own infrastructure. The biggest one, the middle ones, another story. The small ones, maybe they have money, maybe not. So that I would do if I go that route.  



Awesome. No, I think that is fair. I think in terms of the way the infrastructure is built, I think bigger companies definitely build most of their infra with the authentication layers, with the right secrets management, with auto scaling and everything enabled, which then makes it simpler for them. But for midsize companies, I think it's still time where a lot of people spend time to build it, but are not able to build it or at the same time. Sometimes what happens is they do not have enough resources to build, like, you know, building. It needs, like, a combination of data, science knowledge, infrared knowledge, engineering knowledge. And sometimes it's not easy to find those engineers. So a lot of times we have seen people trying to either use something that is open source or something out of the box.  



So one question here is in terms of like the challenges you saw.  



Did.  



You also end up using sage maker by any chance or was it mostly AWS that you used?  



No, one of the things that I can say, we have the completely custom infrastructure. So right now in the sugar everything is built from scratch, everything. So there is no sage maker inside there and even, I mean outside from the even the libraries sometimes are completely rebuild if they're needed. So that is one of the side that I do not know what your target is, but this is the story when basically you just need to have your teams, you need to have data scientists, data engineers, architects, DevOps and this is all we have. So we basically for our going from the scratch. I mean if you need assembler, you'll go there, you build libraries, you build everything.  



So this is like another side, this is on the left side and the right side would be if you are a smaller team and you want to produce something fast, you are startup and you have some idea and maybe in that shoes then there is maybe help. Okay, I have the data science but I do not have the needs or level of the knowledge to completely do my own infrastructure. Then maybe that is the place that I understood you guys want to be positioned.  



Yeah, ideally I'll talk in more details of the product side, but the positioning is for two cases. One is people who don't have good infrastructure and they want to make it better and they want to accelerate machine learning faster. And then the second is people who have built systems. But probably a lot of the build out was fast and therefore not taking into account all the considerations led it to Cost or latency or secrets management or anything like developer speed. And they now want to either revamp it or integrate with something so that they can still speed up their pipeline. So these are the two targets primarily and we are still in very early stage of the journey.  



So we are working with a few companies even and at the same time looking to work with a few more with them of working very closely on specific problems that they might be facing. For example, one of the companies we are working with is facing specific problem around support of different frameworks and then having their entire system deployed on someone else's customers clouds. So working there. Similarly there's another company that was facing issues around building the right authentication system and then ensuring that the secrets are not exposed to the data center. So trying to work with them to improve the system and at the same time add value. So that's how we kind of operate.  



I wanted to take two more minutes to understand a little bit like whatever you can share on the without sharing proprietry as to how big is the team currently at super and then on the infrastructure do you end up using Kubernetes? Is there any challenge you have seen or you foresee seeing and what is the reason of buying versus building versus trying to adopt something out of the box? So this will be really helpful for me.  



Yeah okay so team about size of the team, I mean Sugar is the company how much? 700 something people. Team that is connected to AI I think more than 30 or more people yeah, sometimes even bigger than the company scale so like I said, the reason why we go with that level of the solution we go completely custom is the things that we do okay? Okay and the things that we do and I cannot disclose anything outside of something that you can find on our company is the protect team is doing is basically integrating level of the information of outside of the world different kinds of information, building specific pipelines just for that information and code them in very specific ways. Okay? Some of them are trade secrets. While we are doing that, feature engineering and everything.  



And then integrate them to provide our customers like. You can also find out basically different level of the use cases for our customers in the level of the predict. And most of the use cases are for the market, for the leads, for opportunity, for the advanced forecasting and so on. So all that use cases have completely different pipelines on the different infrastructure because you are not having just one source of information, even not from the one single database of information. Now there you can guess how many how? Fast and how things can complicate become very fast complicated then you have the data protection from the customers everything that is done, how it's done so it goes on and then there is basically solution. So we build from the scratch our own AutoML modeling predicting everything else.  



So everything is done from the structure. There is multiple reason complete control of the system needs for the customization some things that we are doing are just on the level of getting your PhD on that. So we are researching things, but we don't have the time for PhDs. And some of these things are never published, but on the tutorial. Okay, so another thing is about cooperatives. Yes, we use them. I mean, companies definitely using that technology is using also Docker. I use Docker for some time also in my labs some of the people are providing that so yeah, most of these technologies are used I cannot go in more details there are multiple teams in the Sugar, they are using that for the multiple reasons I mean most of the DevOps and so on.  



I'm not directly involved in the why that is chosen, but I can say yeah, definitely.  



Okay, cool. I think I get a sense of, you know okay, one or two last questions and then I love to kind of share with you what we are building. And because I don't have a lot of information but maybe by sharing what we are building you can potentially see if there is any part of the product that might be useful or maybe give feedback on how we are building it as well. Either of those will be super helpful for us. So I wanted to understand if you do any sort of monitoring for your models in terms of like when I.  



Say monitoring, of course, everything and everything. One part we are having UI teams, multiple ones for different parts of the project. And also, of course we are having UI teams and our UI team is basically taking care of everything. So from MLS side there is the pipelines, they take care of every steps of the production and everything is being monitored show to the customer or you have even different ways, I mean, you're not showing to customers. There is internal reporting, there is external reporting, there is stakeholder reporting, different level of the knowledge to understand exactly what you are seeing. But like I said, everything is custom and everything needs to because the solution, I mean I can go with machine learning or technologies from the little bit broader side.  



But the problem is the problems we encountered doing this road are very specific and the solutions was it's not something you can go and find on the net. That is the problem. But most of the things is not you just implemented, you cannot implement it because it's not work. Your data are specific. I mean you can apply some of the rules, but you need to devise many, I mean many different and that is a little bit goal to the customer. Of course it's bothersome, it's problematic, you waste so much time. But then it is what it is. That is also the reason why we have so big team.  



Yeah, so I totally understand the reason for custom, but I have one question that is when you say custom, an event like for the model building site, the custom I totally understand because the kind of models you are building are very different. But once a model has been built, after that the operationalization process I'm guessing will still be common to a lot of other artists. Right? Because ultimately after that it's basic engineering.  



Yes, that is correct. That is correct. With some cabins also because there is multiple cabins there where you will store your predictions. And what we are providing, we are not providing single solution, we are providing out ML solution or ML solution to our own customers. So we are company that is providing ML solution not only for us but for other customers. And that is why we are leads in the CRM world because right now currently as the middle I mean in the size but still our AI solution to other customers basically providing them as our state of the art and on the part at least what anyone else is doing. And that is the complexity also there. So not only we are building that, there is no single pipeline.  



So for every customer we are providing this resource and there is the storage, there is the show where it goes first what you want to show and there is different use cases. It's not single thing. You are not predicting single things. Okay? You're not building chat bots. Now you want to output chat and where there is a solution and prediction are used by the multiple different scenarios. So do you have the storage and then you have the shared storage between your own storage and the customers and so on. I'm just trying to explain why complexity I totally understand.  



I think I get it. So one clarification niven is we as a company are not building the model side of things. We basically plug into anything that other companies are building. Like say for example, you might be building different kinds of models and we totally understand that it is a need to build those because we cannot build the models. The amount of data access that you have is completely different. What we do is only once the model has been built, the entire infrastructure for productionizing. The model which involves say shipping it maintaining shared, ensuring that the cost is lower. So in case of yours, I'll give an example, you might have a case where you might be deploying the same model but the same model is trained for different customers on different data and then there are thousands of customers.  



So there are two options. You can deploy thousand models as 1000 services or you can do a deployment into a single service and then do dynamic loading of that. So how do you choose between the two and ensure that okay, cost wise it's minimum but at the same time there is no interaction between data of one customer versus another. So that's probably an optimization problem which is more on the operationalisation side and something that a lot of other companies also face, for example. So just trying to tell you that maybe, I think maybe there was a mismatch in the understanding. So we are just trying to be on the other side.  



Yeah, okay, perfect. Okay, cool.  



So Devil, if you have five minutes, what I can do is I can quickly show you a high level of the platform and then please yes, please.  



How much? 30 minutes. Okay, I have I.  



Let me try to speed this and something quickly. So are you able to see my screen?  



Yes.  



Yeah, this is at a high level still, but at least it will give you a sense of what we are building on event. Okay, so basically we are a machine learning deployment platform built over Cuba networks with the aim of speeding up development workflows while providing them full flexibility and with full security and control for the internet. So one side suppose there are developers like Data scientists, ML engineers and on the other side there is infrastructure to ensure that both the sites have the right access to the system. So what that means is basically for developers we enable single click deployment to make the deployments fast. We make it easier for developers to learn by providing Python SDK which is something that they understand.  



We ensure that across the stack SRE best practices are followed like the CI, CD, Auto, Scaling, secrets Management, Authentication all by default without you having to build it or without developers having to care for it. And finally, everything is on top of your software engineering workflow that is built on top of Kubernetes. So that way your infertility is just maintaining a single pipeline rather than maintaining double pipelines or increasing their work. So if you look at the product has three different like has the following workflow. So first is for the infrared, the DevOps team where they create a secure development environment for the ML team by easily connecting to their own cloud, ensuring that they can allocate resources to the ML team and ensuring that the platform has guardrails that prevents them from doing mistakes or ensuring that costs are not overrun.  



And I'll show you how this happens through the video here. Basically the main thing is it makes it very easy to connect to your own cloud and you are able to provision workspaces clusters for your team to work on. Then the next part is model deployment which is by the developers. So once a developer has tested out a model and now they want to deploy it, they can actually deploy very simple functions including preprocessing functions to postprocessing functions, they can deploy models which are real time batch, pipelines, anything. Again ensuring that the best practices are followed. We also enable like traffic splitting. So for example, if you have to launch a model to production, you don't have to launch the entire model, you can launch it to say 1% of the traffic and then after that scale it automatically based on performance.  



So that functionality of traffic splitting or Canary testing and federal testing is available by default as well. And then once you deploy your model with a single line of code, you get access to system monitoring, model monitoring which is model performance drift in the model and ensure that you are able to debug and have full visibility to what you have built. Any questions so far?  



Yeah. So how you are detecting drift of the model?  



So drift we use like a proprietary algorithm. Basically it tries to earlier we're using KL divergence etc. But now we have shifted to a more proprietary algorithm. It works primarily for more structured data. It does not work very well for unstructured data, but we are able to predict if there is a drift in the data as compared to say wait.  



How will even detect drift in unstructured date?  



In unstructured we do not do because that is something that is still a topic of research. So we are not doing like monitoring is not the main thing our platform is built for. Our platform is built more for the deployment monitoring. We are enabling for structured data only as of now, for unstructured data as of now. At least we are not aware of how people have sold it and we are not doing research on that right now.  



Okay, so when you said diversions of what you are looking for the so you're monitoring divergence of what exactly?  



Yeah, we are looking at the feature drift and the data drift. So features like you might be calculating a lot of features on top of your existing data. Right? So we measure the drift in the distribution of those features.  



Okay, yeah. Why do you want to do check if any divisions or what?  



Because if there is a difference between what the input data input feature distribution was, which was at the time of day.  



Yeah, I understand that, but this is exactly the reason why I'm asking for that solution. So divergence, you can do without different models in their predictions, but distribution is something else. That is what I want to clear out. I didn't understand you completely on the start what you exactly want. So outside of the distribution, what else you're looking for in the model?  



Outside of the distribution?  



Yeah, because you have at least four forms of the date, some of them are or can be detected by the distribution. So distribution is just one thing and your model can drift even if your distribution doesn't change. So how will that.  



So actually for that to be honest, we'll have to ask the technical team that is implementing this and I'm not aware of the full details, but I can get back to you. I think we measure concept drift as well, but I'm not aware of the details of the implementation. So I'll have to go back to the technical team and ask. I believe you are referring to something like a concept drift, right?  



It depends. The problem is even in the literature the terms are a little bit of the messes maybe. Yes, but what I'm referring is there can be situation, at least there is the four different situation drift. Some of them include distributional changes. But even if distribution changes happens, it doesn't mean that your model predictive performance will be lower and then that means you need to time series of it and so on. So when you said that I wanted to hear about more of the drifter detection because that might interest me. Okay, but I get some of the answers.  



Yes, I think we can go into a little bit depth of that. I can have a shake or someone to kind of sense their more details on that. That is completely fine. Basically. Now let me show you like take three minutes to showcase you the platform once are you able to see my screen?  



Yes.  



So basically I'll quickly kind of show you and this is just a kind of a small version, but basically we can dive into details later on in another call. So this is the place where the insert team or the DevOps team comes and connects their cloud. So you will be working on probably again, I don't know which cloud account you use, but suppose you are using Kubernetes and behind the hood you are using AWS. Then in the Kubernetes you can create a cluster on your AWS account, you have admin member privileges, you can configure your base domain, you can create your monitoring dashboards, you can configure the type of machines you want to provide access to our blacklist or whitelist. And then you can add the cluster. And then the cluster will be added.  



Once you add a cluster, the infrastructure can create smaller workspaces for the data science team with resource constraints and limits, so that the data science team never exceeds it. It can also be different environments, like a test environment, fraud environment, depth test environment. And then you have resource limits like CPU limit or memory limit. You can also further have cost restrictions here, so that no one exceeds the cost here. The cost tab is not there. We are going to add that and then collaborators. Again, full access control is there for the developers. So once this is said, basically the infrastructure is allocated to the developers and then the developers are ready to start working on a deployment model. So how that happens. Any questions so far? Otherwise I'll go ahead.  



No, I cannot.  



Okay, so I'm showing you the deployment via the UI, but the same is possible over a CLI or through Python as well, using either Python or YAML basically. So you can deploy services, jobs, models, even pipelines is something that is coming. Suppose you have to deploy a service. All you do is select a service. You select one of the workspace in which you want to deploy it. You can deploy from your source code directly using GitHub, or you can deploy from your docker image. Suppose you are deploying for your source code. You give the GitHub URL. If you have the docker file, you can give it. Otherwise you kind of simply select Python built back and give this override command. If you enter the part two requirements, we automatically create your docker file.  



You can select the ports, you can select the resources, you can configure your environment variables like secrets management, etc. And this is the ultimate YAML spec file that is generated which can be used later on. Once you click the submit button the model is live along with like your metrics which is the system metrics, your logs so that you can debug if something is going wrong. And then if you go deep into the model further you can see different versions of the model that are deployed and if you want to go to an older version you simply click redeploy button and then it will move to an older version. So literally you don't have to go back to the infrastructure and everything.  



And similarly like this way you can also deploy your training job and you can deploy like a model here, even deploying model we don't have to deploy like from a source code. You can deploy from an existing model registry that you are using by simply adding a line of code as there. And once you have deployed, the monitoring is like this once you basically add a line of code to your model you are able to see your metrics like predictions, actual precision recall, you know, log loss, etc. You can also measure the distribution of the data and how it is changing between actual and prediction. You can compare it across two different timestamps so that you can see if there is any change in distribution again. And then further you can see drift.  



If there is a drift, it will throw an alert to you. You can calculate drift by features as well and then at the same time you can go back to the audit and then you can dive deeper and you can also kind of configure and see alerts et cetera. So roughly like this is at a high level the platform. There are a lot of things underneath it but I wanted to just show you an overview so that you"
11317793541,Dream11,,,Dream11,500-1000,10-50 Mn,25-50,AWS,Yes,"Technology, Information and Media","Mumbai, Maharashtra",Akshay Siroya,,https://app.fireflies.ai/view/Dream11-Meeting-Discussion::x9NrQdRBDZ,"Control plane installation control plan installation. 



Service. 



Deployment. 



Basically, you are there in the policy remember Joe, remember the company but there is a possibility environment like models deployed they have more than, I think, 10,000 models, obviously, because they use a lot for predictions. Key concept players like infinite number of use cases they have most of them real time potential. Bharat is the person. Joan KVPA, backend Engineering this is how we have built platform. He's an instagram. Or he's what he is an intra guy. But he has understanding of the ML side. So he's building the entire infrastructure from it only you can go. But dream Eleven actually coffee Buddy company million users daily request 10. Million users arrangement obviously, whether it's not like something but the goal is to give them a very detailed overview like from a technical perspective, from a product perspective, including AB testing. 



So that is one part that they are thinking of building, contemplating everything flowing through the CI CD. How do you make that system really robust from an engineering perspective and for scale while telling and then focusing the platform? Then it will be good concept model along the course of the testing. 



I. 



Don'T remember anything him knowing anything about the Kubernetes side of things. Either he did not know or I don't remember. 



Recap and we talked. We know that you were using some part of data bricks for the data pipeline and there was some team that was working on the day for distributed training or thinking of that we'd love to learn. Maybe five to ten minutes overview once of the stack as well from your side and then we can show you the platform. I think high level they already are doing everything so we'll need to ask very specific questions then distributed training development we'll have to wrap it up. We cannot take the call for try and wrap that up in 510 minutes, ten minutes or so, okay? "
11189912081,Merck,jeffery.wu jeffery.wu@merck.com (jeffery.wu@merck.com),,Merck,>1000,,>50,AWS,Yes,Pharmaceutical Manufacturing,Darmstadt,Nikunj Bajaj,,https://app.fireflies.ai/view/Jeff-Merck-TrueFoundry::GCNq32oZpl,"Bye. Hello.  



Hello. Hi, how are you?  



Hi Jeff. I'm good, how are you?  



I'm good. Thank you for choosing this.  



Yeah, of course. Are you dialing in from New Jersey?  



Are you in which location, where located?  



Sorry, right now I'm traveling to India. My co founder and Rag is based india.  



Oh hi Ragla. How to pronounce him ragna.  



You can call me ANU.  



ANU. Nice to meet you.  



Nice to meet you.  



Jeffrey so your company is in San Francisco or india?  



San Francisco, basically like our company headquartered in Delaware.  



Okay. I need to meet with you guys india.  



He just meant for us to take this call and figure out some good next steps in terms of like the ML of Set, merck and stuff like that. That's the context. I can give some more context. Do you work very closely with Suman?  



Yeah, I work under Suman's organization.  



Okay, understood. So the context here is that Summer and I had a chat about one of the platforms that we are building. I will give you some background about our team as well. Jeff this is around operationalizing your internal machine learning processes. So imagine like deploying models, deploying batch and print jobs, monitoring your models and stuff like that. And someone really liked building based on the discussion. So he was like, we should definitely do a POC so we can figure out how can this help merck in general in his process. Right, so that's where someone connected us with you. Let me share a little bit of my background and rock and also tell you some details.  



So I personally come from a machine learning background jeff I did my masters at UC Berkeley and since then I've mostly been working in the Bay Area in ML. So initially I worked at a startup called Reflection where we built out a lot of recommender systems for the ecommerce industry. So there I got a chance to scale our machine learning systems from 100 users to like 600 million users a month across hundreds of ecommerce websites. Most of I joined Facebook to lead one of their conversation AI teams. So if you have used their virtual assistant product called Portal, which is like the Alex or Google home of you, that's the team where I was leading out of the conversation AI efforts.  



And between quitting Facebook and starting True Foundry, actually Anuraga, Vishek and myself, the three co founders here also did another startup that got acquired by the largest HR tech player india. So that's a little bit about my personal background. Also telling you about one of my co founders, Abishek, who's not joining this call right now. Abhishek actually joined Facebook and spent like about six years there. Three years out of New York office and three years out of the Bay Area office. And before he quit Facebook, he was leading the entire videos.org as an IC basically. So actually one of the fastest growing engineers at Facebook. He has also led some of the Caching and Preoptimization teams as well.  



Okay, I can add maybe briefly just like again batchment of anikunjana Vishek from Karakpur. After that I worked for a hedge fund called Worldwide. We used to do asset management across global markets. I was managing around 600 million in assets for them primarily again across global markets trading equities and during that time I was also a member of the CEO of Is looking after various strategic initiatives for the firm. Spent part of my time in Mumbai and then around three and a half years between us and Singapore myself. Used to also angel invest into a number of startups and then finally that Mercedes into our journey of finding our startup and then now building through foundry with them of optimizing ML models.  



Okay, I see. Yeah.  



We would love to learn a little bit about you as well Jeff before we start.  



Yes, sure. So I need engineering team here and also have a couple of projects like MBX and Business Engine and mostly so I'm on the engineering side, I manage the product development deployment like going out to the market. My background, I jumped different health care company mostly in the healthcare AI space. Before Merck, I worked for clarity and voxo cloud. They all healthcare AI startup. And also before that I have spent half year in Cisco. I work as a software engineer and I graduate as a commercialized degree. So would love to hear more about your products for sure.  



Thanks a lot for sharing your background Jeff. Actually in this call we will obviously share a little bit about our product as well. But we would love to spend some time trying to understand a little bit getting more context on some of the messages that I think someone sent out. Like he mentioned a few names that obviously we don't understand because they're kind of internal names right now. NBX and all that's one thing. And the second thing is we also wanted we had a few specific questions about how is the team at work structured in terms of building and deploying models, what are the tools, technologies, cloud on prem, et cetera that you all are using. So we had a few questions as well, if that's okay with you.  



Yeah, maybe before that it might be good to kind of give a two minutes overview so that context then you can focus on telling problems around those areas.  



Can you share me some of your solutions, what you are selling like the product or service?  



Let me give you a little bit of a background very quickly. Okay.  



Just give me 1 second.  



I'm going to pull up one slide and show you that. I think that should clarify and Rogue, you have that slide right where we talk about what roof on Is building. Do you want to quickly share that and give him some context on what we're building.  



Okay, sure, I can do that. Are you able to see my screen?  



Yes, we can.  



Jeff. So basically, like, the ML workflow, as you know, it involves like data and feature engineering to kind of get the data from different silos into one place. Then there is the model building where data scientists are building their own models. Then you need to deploy it, which could be in production environments, and you need to scale this deployment system. And then at the same time, you want to be able to monitor it in the way that you can ensure that there is any data. Then you can retrain if something is going wrong. So we are in this part of the pipeline. So first you have built out the model, the entire deployment, the entire monitoring piece.  



So basically the thing around operationalizing ML, if you think of our platform, we have built a platform to speed up the developer workflows with full security and control for the Internet. We make it very easy to kind of deploy models for data scientists or ML engineers. We make it also like the platform works in a way that it is very simple to learn for anyone in the team. Like even if they do not have knowledge about complex infrastructure, or if they do not have engineering skill sets, it works. And even if they have engineering skill sets, the platform kind of speed set up, it kind of ensures the best si practices by default, and at the same time, it ensures that everything is done on the same infrastructure that you are currently running. So roughly, basically, there are three major parts.  



One is related to setting up your infrastructure in a secure way. The second is making the model deployment of different types with the right scaling, with the right practices in the right way, and then finally being able to do basic monitoring on top of that. So, just wanted to kind of like this picture so that you have that. And then we are working with companies and customers to build around this as well as at different solutions around this, so that we can help them speed up their overall deployment or the operationalization challenges that they are facing. And that is where someone had connected, as Nicole mentioned. Probably. I would love to hear about the kind of challenges you are seeing at Merck and then very specific to also some of the things that okay, so.  



Are you trying to provide service or this is like a platform. We can use it's.  



A platform you can use, yes, you can use platform. And if you want, you can build on top of this as well. Or we can help you build on top of it, working closely with you as well.  



Okay, how to use this platform? If there is a data scientist they want to deploy the model, how to use it?  



Yeah. So basically what will happen is that we provide like one of our client side library so that you can install do your data scientists frequently use like Jupyter notebooks or like Python? Okay, so you can actually directly install that client side library that we provide on your Jubilee notebook itself. And imagine that you have like written a training script that you wanted to run on a cloud machine or you have written like an inference function that you wanted to expose an endpoint for. So for each of these we provide you APIs as part of that library. You can invoke that API directly from a Jupiter notebook and it will quickly deploy your model on a remote server.  



Basically let's see which infrastructure you require, what kind of infrastructure do you need.  



So we work with pretty much overall any infrastructure right now. The platform supports Kubernetes and can work across multiple cloud on premise as well. But we are open to working where people are using specific clouds as well.  



Plan with the infrastructure inside work you need kubernetes, I'm not sure, do you have like infrastructure? Basically we need to deploy your package or deploy my system right here.  



So you will generally deploy the package in your infrastructure. So we make it, we work with your infrared team to make that happen. And that's why we wanted this call to understand a little bit more about the entire pipeline and we didn't want to give you the full product because a lot of times there are nuances in the product that is on your deployment. So I think just wanted to give an overview so you have a sense of the problem. But if you can tell us a little bit more about the ML pipeline at more about the specific problem areas, that will help us also understand your problem and then we can probably give you a better suggestion to how you can use it seamlessly within more as well.  



Okay, yeah, I can give you some overview of the team setting here. So, basically, we have multiple products like MBC stands for Next the Best Experience. Next Business Experience. So It's recommendation platform, it's already been built and served for different countries. And we have piercing inside engine. We have business. Engine. Those are pretty matured products. Another part of the large group, they are technical analytics and they do like ad hoc stuff and they analyze the patient journey or market size or just commercial business analytics and they provide the insights back to the stakeholder so they can decide how to sell more job. And there's a data strategy team, they onboard the third party vendors. So we have a lot of vendors like Ikea, Komodo, those are the data vendors.  



They provide the claim data to us and we need to onboard those data set and standardized data set. Yeah, that's basically the settings here and there's the It team, so we need to communicate with them. It's called HSID, they provide AWS environment. So I assume if we use your product, we have to deploy your product. Because we manage the Python package, we have the internal pipeline, and we only can install from internal pipeline. We cannot download from outside. I need to think about how to sort of onboard your package and deploy your system. Because we have limited sort of budget, if your system costs too high, the item won't allow this. So that's why I need to understand your infrastructure by battery so I can get sense about how to talk with the It in order to deploy your system.  



So I'll give you one context here. Just like we have been working with another really large company, two very large companies in the enterprise space similar to yours in different domains, and they also have like a pretty stringent It team. So we have been able to work with them to deploy on their infrastructure pretty simple constraints with regards to VPC or with regards to downloading an external package. And all of those will not be there. Like, we'll work around the constraints of your It and will kind of work seamlessly with them to make that happen. So that is something you should not be worried about. But Nicole, do you want to share more around that so that concern at least is allocated?  



Yeah, for sure. So like you mentioned about the internal pipe package, of course there will be some security. And by the way, this is all when we get to that stage of deploying truefoundry on the platform, right? So far, we still have to validate if the fit in terms of the project that you guys are working on, what we are building, et cetera, is all good. So we have to spend some time there. But imagine that we get past that then exposing our package to be deployed on your internal pipe by going through the security reviews, going through the compliance reviews, et cetera, something that we have done a few times before. Our platform has already gone through stuff like VIP testing and stuff like that.  



So basically we have already gone through a bunch of the hoops in terms of It, security, compliance, et cetera. And also our team is flexible to work with your constraints as well. So if there are some other things that we need to take care of, happy to work together and make that happen. So I guess that's where we stand. If you had other specific.  



I need to understand your solution better. So can you share some details about the solution architecture or something like that? Because even like, if I on board you guys, I have to socialize with the It team. So they will ask me what's the solution? They need to understand what you guys are using with service like that.  



Okay, let me maybe spend like five minutes and give you an overview of the product itself. Okay. Just so you have a good mental map of what we are doing, how we are doing it, and if you have any more questions on that, we can actually take some time today to answer those questions and then we can get to some questions that I'm ragging I had for you. Is that okay?  



Okay. Yeah, sure. Okay.  



I'm going to keep it fairly high level given that this is a first call that we are taking. You get an overview of the product itself, right? Do you see my screen?  



Yeah.  



The way you should think about the demo is imagine that Merck already started to work with True Foundry. What would happen first in terms of like whatever installations and all. And then what would the data scientists or machine learning developers within the company, how would they interact with our platform? Right? So like, imagine recommended system, how would they interact with the platform? Those are two parts that I will show. So first thing is, let's say if you have already a Kubernetes cluster running within your company, that's what you want to use truefoundry on that is deployed through. The way that works is like you connect that cluster with our Truefoundry dashboard. Now, connecting up cluster itself does not do anything. It just tells us that there is a Kubernetes cluster that exists like this.  



But then what you would do is you would actually install a couple of helm charts on your Kubernetes cluster. And that's basically going to set up the infrastructure that is required by Truefoundry to run your machine learning models. And at that point, you will start seeing that this cluster is connected on the Truefoundry platform and it's available for the data scientists to be able to deploy their machine learning models via the Truefoundry platform. So that's pretty much like the setup. There's a couple of other things that you can set up. Like if you wanted to connect your docker registry or get and stuff like that, you could do that. But I'm going to skip all of that for now. Now imagine that this place is set up and I'm going to give you a quick overview of what your developer experience would look like.  



Right. Imagine you have some data scientists in the company that either want to deploy any services, which are like, imagine any API endpoint for preprocessing or postprocessing of your machine learning models. Or they want to deploy some jobs which could be like a training job or a batch inference job. So something that runs once spins up a machine would execute that piece of code and then kill the machine. And lastly, if you wanted to deploy a model where let's say you have a model file saved anywhere, like an S three or something, you just provide us the Uri of the model and we will just deploy that and give you an end point of the model. So these are the three things that a machine learning to represent deploy on the platform. They can do it directly from the UI.  



So, for example, if they wanted to deploy a model, all they do is on the UI, they tell us the name of the model, they will tell us a Uri of the model itself, and just hit Deploy.  



Okay, if they want to deploy a model. So can you show me an example of the model?  



What an example model looks like? Is that the question?  



Yeah.  



So in here, basically, we support different types of models. For example, let me actually show you how a model looks like. So, for example, let's say we have, like, whatever, a basic Churn prediction model here. So, like, this model itself, if you think so. These are all the models that are logged on our platform. We have version control of each of the models.  



Here.  



You see that the way this model looks like is it takes like a few of these inputs as arguments, right? Like, these are the features that this model takes as argument. And then the prediction that it makes is like a categorical prediction, for example, right? And then if you wanted to understand how this model looks like, you can actually look at the actual, whatever. This is a psychic learn model. It is saved using a pickle format, for example, right? And then you can obviously download the entire model if you wanted to and stuff like that. Basically, we can also track some important things like metrics and stuff.  



I'm just trying to understand the workflow. So data scientists can write anything in jupyter notebook and then what they need to do. Okay?  



So I think the way this conversation is going is we are practically giving, like a full demo of the platform in some way. I'm happy to do that, but I think usually the only problem in giving, like a full pleasure demo without us understanding some of your requirements is we end up covering, like, all functionalities of the platform that may or may not be relevant. That's why we generally like to personalize the demos for the problems that people are facing. But in this case, because you're curious, I'm going to give you a little bit of this workflow. So, for example, let's say a data scientist wrote, like, one training script in their Jupiter notebook. This training script is very simple. They read their data, they instantiate a classifier and the trainer model, right?  



Now, let's say they want to run this particular training script on our platform, okay? So there are two different ways in which they can do it. Number one, they can directly have this training job deployed from their Jupiter notebook. So basically, like, this service boundary that you're seeing is true as our SDK that a data scientist would put on their machine. And then they write like, these five lines of code. So they need none of these, actually. They just write these five lines of code and when they hit a job deploy it will directly deploy this particular training script on our platform. And they will be able to track that on the dashboard here. Okay, so they can actually see this on the dashboard here.  



I see. Could you go back to the notebook? There's this.  



Like, be the same script.  



That the data scientists wrote here, and they just.  



File.  



There are also ways on the platform where they don't need to dump it to a file and they can directly deploy like a function as an end point as well without actually writing it to a file. We also have that API.  



But how about the data if they want to connect with a database?  



Yeah, for sure. So remember that this is like a vanilla Python script, right? So in this case, they are getting the data from this read CSV, like a CSV file that's hosted on GitHub. Practically, you can actually implement a simple thing like read data from that function and we would be able to read that. And we also have full permission controls as well built into it. So if you wanted to provide, read or write access to the data scientist about a certain database or whatever, you could do that on the platform as well.  



I see. Okay. So what kind of service do you need to deploy this?  



Sorry?  



What kind of the service? You said you need a Kubernetes, right. How large is the cluster?  



So that really depends on the kind of loads that you want to deploy on the model. Like, we have a couple of infrastructure elements that we need to have deployed on the Kubernetes cluster. So, for example, we need a postgres database and one file system itself. So that's what we need. Like you're deploying on AWS. If you're deploying on an AWS cloud, then we need S three. If you're deploying on GCP platform, the minimum requirement is this one. So usually some of the Kubernetes customers that we have worked with are like nine CPU and 3GB of Ram in terms of sites, just to make sure that we have enough things to deploy the infrastructure that we need to run all these systems.  



You use the apex provided. Kubernetes.  



It usually depends on the type of usage. So I think without actually understanding the kind of usage of the platform, it's actually really hard to provide the cost because is it constrained on CPU? Is it constrained on GPU? Is it constrained on number of developers? How many models are you deploying?  



You charge by usage or you charged by user.  



So user is one metric, but we also have different levers on the number of applications that you are deploying on the model. It's not necessarily about how much CPU are you using, we charge based on that. But like, number of applications that you're deploying, number of models that you're deploying, the size of your set that you're dealing with. So there are a few factors that we need to consider. So it's really hard to give up charging.  



Okay. I need to have a rough idea about how much it costs. We need to report the budget.  



How big is the team.  



Which have copied 50 I think at least 50 people developer at least 50.  



Okay, got it. Let me just think once.  



So if we did not assume anything, if you had to really guess based on how many models are these developers deploying and stuff like that. If we really had to just index on that one number and give you an estimated price point, that would be roughly in the range of 200K for deploying for that many number of developers?  



Two hundred k per year. Okay, that's based on what? Based on users or usage.  



That's the only information that we have. So like if you had to guess a price based on that, we would say something in the range of 200 kwh. It can be more or less depending on the usage.  



Okay, so you need Kubernetes because our It teams, they don't like Kubernetes. We mostly use service tool. Can you not using communities instead of serverless?  



We can support serverless. So basically, Jeffrey, at this stage, as Nikon mentioned, there are some organizations we are working with Q and It's requirement is there. There are some organizations where people are asking us for new things. So if serverless is a requirement, we can definitely build on top of it. But we'll need a little bit more understanding whether of this are all models going to serverless?  



One question on the serverless, when you say serverless, what is it running like? Do you mean serverless? Like lambda serverless or running it on?  



We can use Lambda glue, job orchestration tool like Step function, Airflow, those are what people frequently use in our organization. And if you I don't think anybody use communities.  



We can do Far gate Fargate is something that is already getting used on our platform.  



Okay. Is there a way like we can have like trial version that people can test it?  



Absolutely, yes. We can also spin up an account for you that you can try the platform out. Get a feel of how the platform like deployments and all work on the platform. Absolutely, we can do that. Yeah.  



When you say open up account, it has to connect with your server.  



So if you wanted to just try out the platform, we generally recommend using not using any sensitive data for the trial itself. Right. You probably don't want to use any very sensitive patient data or something. Then you can directly do it on our cloud, like with some public data sets to get a feel of the platform. That's a very quick way of trying. We can open up like a user account for you. If you wanted to do more immersive testing of the platform, like trying out, then you could also experience deploying our platform on your cloud or something and testing it out there. But usually that process is a little bit longer and we recommend trying it out on the public platform.  



I see.  



In the long term, my understanding is that you would not want to use like public cloud for training your machinery. Models of the sensitive data that you deal with and everything would probably need to be by design within the ecosystem.  



Yeah. Okay, so what kind of information you need? You said you want to understand better of what yeah.  



I think you mentioned number one, that is like you are using AWS as a cloud. First of all, two things, that is how is your team structures? You mentioned you have 50 to 100 people. Do you have like a lot of data scientists who do a lot of experimentation but don't deal with like actual productionization of these models? Or do you have ML engineers who do like both kind of stuff like a platform team? We'd love to understand your team structure and then also would love to understand about what is production for you. Like when you deploy models, what does deployment mean? What kind of models are getting deployed? Where are they getting deployed? Would love to understand the deployment workflow as well. So yeah, if you can start with these two questions.  



So, as I mentioned, there are three products. So for the product project, so they have entire product team like data scientists, engineer, MLPs and they build a model by yourself and also deploy the model by themselves. They use mostly firm data like step function, like Airflow, good job, those things. And then model deployment. Sometimes they use Sage maker and then for the technical analytics, we sort of want to deploy products as much as possible. If your platform can help us deploy their work, probably it's meaningful for us. For that technical analytics, they just write the script in Jupiter notebook and conduct analysis and then the slides put into a slice and send to the business stakeholder. So you can help them to maintain their work and also connect with we have internal asset management tool.  



Can browse the model through internal website. I'm not sure the progress there I need to connect with them, but it seems like someone want to better manage those assets. It seems like your platform can connect with that asset management tool. Nice.  



Yeah. So actually I want to understand a little bit more. You mentioned about the technical analytics like where they write jupyter notebooks for creating a presentation for the leadership, right? Maybe help us deploy that. What do you mean by that? What kind of deployment there are you talking about?  



I mean probably just trigger the Jupiter notebook in a certain frequency. Sometimes there's a report that has to generate like monthly, weekly like that and that's probably one use cases.  



Understood.  



Okay, got it.  



And. The second thing is, like you mentioned about the asset management tools. Are you using like what are you using for your model and asset management right now?  



I'm not sure there's a team to do that. I will meet them next week to talk about how to collaborate, certain things for that. That's the internal engineer to sort of do this. I don't think they can connect with the model. It's just a place to put a link. There is a model name and it can link to the gift repository document like that. But it doesn't have a way to trigger the model.  



I see. Understood. Okay. So they don't have the full loop closing where the model itself is saved, but they can't actually trigger the model or get an API inference or something from the model.  



Yeah.  



Okay. Actually that's a good .1 of the other thing I wanted to ask is you mentioned that some teams use Stage Maker. Do you know which parts of Stage Maker are the teams using? Is it like more on the training deployment?  



Deployment. Usually they don't train on a Sage Maker, but they will deploy their training script using Sage Maker. So for the MBX, they will train the model, like once an update model and then use the back interest job to trigger the model.  



I see. Got it. Okay.  



For those products, I don't see any need for your platform. But for those ta work, they probably need some way to maintain their work and deploy put into the product phase. That's what suma want, I think.  



So just to make sure I understand this right, you're saying that for the training jobs that they run on Sage Maker, there is not value to be added from the platform, but basically managing your like, the model one place is something that we can add value.  



Yeah, managing those analytics work.  



Oh, I see. Okay.  



Basically I think some I want to build more like production life more work. Like those tape.  



Work is right now not getting productionized. You're saying Jeffrey?  



No, this ad hoc.  



And you want to get to a state where all of these are also productionized so that someone else access. Out of the 5200 members you mentioned, how much part of the team actually works on this technical analytics work?  



I think at least half of them.  



Half of them. Okay, fair enough. Understood. Okay. And curious to know about. I remember someone also referred to some projects like I think they had cryptic names, four of them.  



Which project? In WhatsApp group?  



I think the projects that you mentioned were like NBX CTAP.  



Yeah. Okay. If you want to pilot for this, I need to talk with C Tag and Pi team, see if they have the need. And also for MBX is very matured product. They have well maintained infrastructure. It's hard to break in. I think for C tub and Pi or bre. It's very new. But for BR e, it's a different story. It's not machine learning model. This is all business model, all the SQL. This is more focused on ETL part, but for the CPAP and Pie probably there is some opportunity there. I need to talk with some.  



Also out of time currently, but is it possible to have another call with you? We would love to understand a little bit more on the technical analytics part, like exactly what is happening, how many models probably. And then also if you are able to get some more information from CTAP and Pi and if need be, like we are also open to doing a more detailed presentation for this folks or folks in this team if that helps in some way. Like we kind of.  



Not in the merchant network, so I'm not sure, I don't think we can talk more of that part of the work. I can just tell you that they write into the notebook. Some of them are machine learning model. I think that's all. I don't even know the detail about what they are doing. The model won't be too crazy. It's not a deep learning model, it's just even simpler than machine learning model. In our work, the most heavy part is ETL part. The data is pretty noisy most of the time they spend based on the GP part, I can tell you, but I sort of understand your technology. But I haven't jumped into more detail about the infrastructure. But I need to talk with a different team and to see if they need this kind of tool.  



If you can you set up meeting, talk more about the infrastructure? Yeah, that would be more helpful.  



Yeah,"
11189924245,CVS Health,Yogesh Bhagat (yogesh.bhagat@gmail.com),,CVS Health,>1000,>1B,>50,AZure,Yes,Wellness and Fitness Services,"Woonsocket, RI",Nikunj Bajaj,,https://app.fireflies.ai/view/Yogesh-TrueFoundry::w9MTvnSm4j,"This will be just a chatting discussion.  



That's cool. If he decides to join, it's 09:00 p.m his time.  



I just want to bring some ideas.  



On how to build a model registry.  



Thank you.  



External to 1 hour and then we.  



Discussed a lot of different things, I.  



Think pending items back here, the actual.  



Platform price.  



As a clause if things.  



Go wrong, basically that is fine, that is very normal.  



It will not come for free, basically.  



Concept is very normal. It's not a big deal or legally handle. Legal can easily handle that. But once the main platform, they're not asking.  



The other thing is very easy for legal to handle.  



We have to cancel the meeting today.  



Can we check tomorrow morning?  



Is it a vendor form? What form? What is the title?  



I think it's a vendor form only.  



They are asking for. "
11189924245,CVS Health,Yogesh Bhagat (yogesh.bhagat@gmail.com),,CVS Health,>1000,>1B,>50,AZure,Yes,Wellness and Fitness Services,"Woonsocket, RI",Nikunj Bajaj,,https://app.fireflies.ai/view/Nikunj-Yogesh::BhS0z5KzABgsg0yP,"Hello?  



Hello, Yogesh? Hello?  



Yeah, who is this?  



Hi.  



Yogesh. This is Niquinh.  



I thought it's 930 and it's my bad, man. So sorry.  



Should I give you a call a little bit later or something?  



If you can wait for just ten minutes, I'm outside within ten minutes.  



After ten minutes, will you join the Google call or do you want me to call you on the phone?  



We can do over the phone, whatever.  



Makes okay, take care of it. Let's join the Google call in like maybe then. Ten minutes then.  



Okay.  



Okay, bye.  



Thank you.  



Hello?  



Yugisha on mute.  



Hi, Yogic.  



Yes, I can hear you. How are you doing?  



Doing good, man.  



I think two Yogesh are here, am I right?  



Yes.  



Did you join from a phone or something?  



Yeah, I jumped from a phone.  



I don't know.  



Should I remove the other?  



It's okay, ma'am.  



How does it make a difference?  



That's true, it does not.  



You are on the east coast, right? Yes, buddy, I'm on East Coast.  



Okay, nice.  



So Yogesh, Taranti might have given you some context already, right? A little bit about what I wanted.  



To chat about and stuff.  



Yeah, he gave up context and let's talk about it, because I'll tell you, I was also talking to one more lady. I'll tell you Bindu. I don't know whether you know Bindu or not.  



Do you know Binu?  



No.  



Give me more context.  



Okay, I will tell you. Her basically.  



Abakers AI.  



Yeah, I do.  



Yeah. They're in same business as you, am I right?  



Yes, similar.  



I was actually in touch with them and that's how I was saying I know the company's motorcycle. What's the purpose?  



Is it exactly same or are you.  



Guys doing little differently?  



No, so I think it's in the same space, but I think the operation is fairly different because I think the approach to Abacus AI is more like building AI in ML ops.  



Right?  



And we are building ML ops for AI. I guess that's the main difference.  



Got it?  



Yeah.  



All right.  



So we generally believe that so far, like, ML has not reached a stage where you need to apply AI itself to the ML ops processes. But I think with most companies that fairly early in their machine learning journey still it isn't a properly solved problem. So we actually start with much more fundamentals to meet the companies where they are, I guess.  



Got it. So, Nikhan, I'll tell you my read.  



On the situation of basically how I see the corporate America world. Then let's talk about.  



So I see.  



Three different type of companies here.  



Okay.  



The companies which are really using ML and Way, advanced Stage, Take, Facebook, all these companies which are way ahead.  



Okay?  



Yeah.  



Then there are companies which are small segment companies like where TD was working. They will say reflection bloom reach.  



They will use the word we are using AI ML but they actually never use that. They will build a rule engine behind the scene.  



The reason is their business operations is like that.  



Because the funding comes from merchandisers. If merchandisers are paying the money, then they need the voice in the game. So they don't let any ML engine ever survive.  



Right.  



Hot packed. Okay, then third is the corporate America section which is like company like CVS.  



CVS is a fortune for there could.  



Be 1400 also where we say a lot that we are doing ML and in reality we'll not be using ML anywhere. That's a hard fact.  



Okay?  



Now the reality why we are not.  



Using, there could be one small percentage less than 1% which are using. And if they are using, they want to do everything themselves.  



But the companies like us.  



It'S not.  



That we don't want to do.  



There is a lot of political maneuvering you need to do to overtake something.  



Okay, I'll give a simple example like chatbot. That should be the simplest thing.  



You will say hey, what's the big deal about chat box? Use any of the dialogflow, use any so many software out there today. Go in outside and just build it.  



What's the big deal about it?  



Build the fulfillment engine, build the intent.  



And response, you will get it. But reality is in my own area.  



I've been struggling to just build a.  



Chat boat in last nine months. Just political.  



Okay?  



And what you are trying to solve is let's say in a company like seniors, we really start using ML. Then comes the maturity of it.  



That's where you are actually going into that. Okay? When you build different models, maturity, that is where you guys will come into picture to operationalize it, whether it is somebody doing the study, with what data set you did it, how did you train everything those things. If I'm not wrong, that's what you're trying to automate. Am I right in first of all a summary of what you guys are.  



Doing or I'm way off.  



You're not way off for sure. There is just one thing that I would like to add here.  



Yogesh.  



So actually a lot of what we're trying to build is call it day zero type of things also. So what I'm going to say is initially when you build your first machine learning model, you don't care about all the maturity around it, all the debugging, monitoring, logging, all those things don't matter. All you want is I want to build a model, I want to get some business value out of it.  



Right?  



That's the only thing that you really want in the beginning. So we actually start there literally somebody working like on a jupyter notebook or something can deploy some models and stuff like that. And once you start doing that over and over again, that's when you care about maturity. And hopefully at that point you don't need to go find another solution altogether. Build something roundup the platform scales with your needs. So I think that's the only caveat. But generally I think by the way.  



My read of the industry is very.  



Similar to you that machine learning is the thing that everyone talks about. No one actually uses. Most people don't use it actually. That's in my read of the situation as well.  



Okay, see that is what I was thinking.  



You must be having a customer intend you would have done your research before opening up a startup. You got to feed money, so it must be somebody got impressed.  



Okay, how are you thinking about your customer base?  



Who are you targeting?  



Actually, that's the question I will ask you.  



So generally, actually we are targeting these large companies who actually have not built a lot of machine learning expertise but practically know that either now, four years later, three years later, if they don't five years later, they don't end up using machine learning. They're at risk basically. So for example, we already started working with a company called Synopsis, which.  



We.  



Started working with Reliance. So these are like all large companies, but they're not necessarily known for their expertise in machine learning. That's the general target segment that we are going after.  



Got it?  



Absolutely. Obviously not the Googles and Facebook and Uber of the world because the value add, value ad, you can't do any. Yeah, but with these large companies, we have realized that they want to do a lot of things, but they get stuck with a lot of things and they need that kind of customizability etcetera. Which is something that the cloud providers don't end up providing. Their documentation is really bad, so they pretty much need more hand holding. And hand holding is something that as a startup they can afford, but as an AWS, they can't afford too much of that.  



No, I agree with you and that is where I think I got the.  



Context now and now I know the thing. Now you can ask me the question how can I help or not?  



And how I think about it. Because I tell you, from my perspective.  



Sitting in corporate America, nothing works like.  



The way you think the world works. Totally different. It's about finding out a relationship and.  



Saying, hey, let's spend some money. You have money, let me spend some money for you. So that's what we need to do.  



It's not about a real need or not a need.  



It can become a need when we make some business value, really business case out of it.  



Okay, right.  



That's what I was telling was talking about something I said terran one basic thing, deliver by date, which Narva is doing.  



We can't even predict that.  



Forget about anything else. We don't build delivered by date engines and we talk about all kinds of MLS.  



These are the hard facts. So that's what I see as south.  



Of America, what we need to do is more of a finding where we.  



Can have connects and actually see the mindset that hey, do you need help.  



But you need to have relationship. That's how I look at overall industry. I really am telling you, if there is a company where there is a will to start up ML and.  



There is a real team, they will.  



Never ever go and say, hey, I want to automate something. They will not even go into that mode. They will build it.  



Whereas companies like CVS, they will spend.  



A million of dollars without even knowing why they are spending money.  



So target audience is your choice. I can help you reaching out to few people. I can help you in changing the view of your sales pitch.  



Right?  



Because I believe the sales pitch needs to change, right? That's a whole crux. What am I here for?  



Because nobody even understands or nobody needs, hey, I need some automation, or some everybody is like so much, hey, I'm getting a salary. I just care that this is a use case for this year. If this gets the ROI, either I will leave this company or I will get big bonus. I'm out of this area. I don't care about what happens after four years.  



So how many people are there, that's.  



Kind of a thing. So you tell me now. Again, this is my version of what.  



I think about this area and what I know about this area, right?  



So actually, Yogesh, before I get started with basically this kind of engagement in terms of customer intros, interactions, et cetera, actually I wanted to check a long time ago, I know that you had mentioned about engaging with startups in different capacities, like angels and stuff like that, right? Just like Garandee is engaging with us at this point. I wanted to first of all check like have you already done some of that since the last time we chatted or how is that coming that I'm still interested.  



I have really not done anything. Reason being is very simple. I have been through my personal challenges in the last one and a half years. I don't know whether they're untold or not. I'm going through some medical problems also and I lost my both my parents in one and a half years. So a lot of things going on my personal site.  



Okay.  



So I have not done anything and anything if I've been doing is there.  



Is a company called Branding Brand. I've been helping them a lot in their growth.  



Okay.  



Hello?  



Yeah, I can hear you, buddy.  



Yeah, okay.  



Yes. So you guys told me about your parents laws. I'm really sorry to hear that.  



And that's why I decided when I.  



Heard about that I was like, okay, I should not be reaching out at this point. Step back.  



At that point when I heard about.  



It, I did not know about your health challenges. By the way.  



Yeah.  



I'm hoping things get better quickly for you. I wanted to chat like if this is something that you are still interested in, is this something that you would want to pursue still or you feel like you don't have much bandwidth?  



No, I told her that I will be interested.  



I was very clear to him that I'm really interested in it. But I really want to see also that I can help the company see at the end of the day, if I want to be associated with someone, I want to help them grow. I want to see them growing. That's the whole purpose. Otherwise helping anybody is not worth it. If you are not going to grow, you're not going to like me. I'm not going to invest my time and waste it. So both ways it's important. So that's why I want to send more from you.  



Where do you stand?  



And then we can decide whether it will I'll be helpful to you also or not.  



Because it's a both way to season.  



Makes sense.  



Can I just take this very brief? Just give me 1 second.  



Yeah.  



Hi. Yeah.  



Okay, let me give you some background here. Yogesh. So the idea being that right now we are building out this platform relatively early in our journey. It's been like whatever, eleven months or so that we're building out the platform.  



Right.  



The goal for us is to work with about five companies, five large companies total.  



Okay.  



Very closely with them solving one or two of their initial use cases and proving value basically.  



Right.  



So I told you two that we are already working with. Third that we are getting like we already got connected with is Mark. So Mark is the third one that's in draining us for their MLS needs. Now we are looking for two more companies to work with practically. So that's the state that we are in. The reason being that we have built out a base platform. But we also understand that no matter what we build out, what we think that the company needs, we will almost always be wrong. And what are actual use cases that are needed? How do you integrate with existing systems? All those challenges will come basically when we start working with companies and we just can't handle more than five such companies.  



Just realistic, as a startup, we don't even need, if you work with five CVSS or five Mercks in the world, I think we are sorted for a decent amount of time basically as a startup. So that's the approach that we are taking so far.  



And we also understand that given our engineering backgrounds itself, there's practically no way.  



We can crack these customers.  



Basically, as you mentioned, we need the type of introductions and the championing that you need to basically get our foot in the door. So actually, all the three companies that you're currently working with, we have found out similar champions who have, like, internally helped us, drive us. And usually these champions are not just like you make an introduction and you're happy with it. You really need to drive this internally because nothing gets done at the right. So that's the general approach that we have. I would love to hear your thoughts on this.  



No, that's how it will work out.  



Because, anyway, I told you, it's all about I think other than internal pitch, nothing works out. And you already mentioned one good thing that you want to hand hold it these companies.  



The biggest thing is that you really.  



Really need to handle it. Otherwise, it will be always nothing will be done. And right.  



Third thing, which is way more important.  



Than this also, is how do you constantly be in touch with the executives.  



At the top level? Because when you will get engaged, you.  



Will be engaged at the top level, then you will work at a middle management level or a lower level.  



But if you lose, connect with the top level.  



And they do understand that top level is no more interested.  



You will be lost in entire auction.  



And nobody will care about you. So that's another piece which is way more important than getting put into it. And even you will be doing hand holding. But last stage is if you do something, how is your work getting propagated up and being shown as a value at a company who is basically that spokesperson for you? You need to always have that spokesperson. Who is doing that. So I think that's the third piece, which you need to be more cautious about. Because first one, everybody will do it for you. Second one, it's your money you are spending in hand holding. How does it matter to other people? Nobody cares about it. Third one is important for you because that is going to generate revenue for you. That is going to generate a long term business for you. So that's the pieces where I think.  



Is the trick line that makes sense.  



I'm going to take this advice and immediately apply for the few that we are working with already.  



Actually.  



I tell you one thing.  



This is how I want to start with.  



If I want to start with at.  



CVS, I'll start with very clear version.  



But hey, I'm kind of acting as.  



An advisory to this company. I have conflict of interest. I'll start with that because I don't have a business for you. Otherwise I can engage business directly with you and I don't need anybody's approval. The kind of money I need to give it to you is like a peanuts for me. So I don't need to worry up to 200 300 kwh. I can sign SW anytime.  



But if I do that, I need.  



To have a work for that. I don't have a work which is later to you. If you tell me hey, I can do your chatbot and everything, then I am more than happy to forget about our conflict of interest. First thing we can start with on a chatbot business itself.  



But I don't think so that's anywhere.  



Near to your product line.  



Okay?  



That's right. Yeah.  



Otherwise that was the easiest way for me because I have big work in that direction.  



But I have definitely lot of connect where I can take you, introduce you with the people and also keep yourself.  



Connected with the things. So, first thing I will start is.  



How do I get your foot into CVS?  



Because I believe one CVS is equivalent to actually not one mask. You will be actually equivalent to your five companies.  



Google.  



Exactly. Yeah.  



To be honest, we have 26,000 It staff. So it's a $2 billion plus expense every year.  



How many companies in the world will be spending $2 billion expense every year? So, data science itself, we have around 600 data scientists. 600 data scientists means there is something wrong with the people you have hired. Okay?  



Yes.  



So that's a hard fact. These are the things I will help you. One thing which will be very important, nikhunjes, the investment of the person who applies is like real doer because in this company like CVS, you will get.  



All kind of I call it as.  



In Hindi bull button, all kind of button.  



You will get it. But there will be no one who will take things to the end state and they need real people who can code fast.  



Right.  



Literally right now I am hiring a.  



Lot of individual contributor at a senior director level. Director level because I want that kind of talent.  



Right.  



I'm paying big bucks just because I know the truck flies. That how fast you can go. Not about how much you speak.  



Right.  



So that help I think is very important.  



When we place into CVS, I think that is one person whose favorite your superstar that needs to get a light.  



So that one can get okay, then what we can do together, I think as the next step is we can think about some use cases. I can come up with few use cases for us.  



Okay.  



Can this use case be built together?  



You have to assume that money wise.  



Once you get into it, whatever you will invest in a cringe, you will make five X of that. You'll never imagine what you will do. Any self putting is a problem.  



Okay.  



After that money is like a free flow here. I have never seen any other company in my entire life which can give this much money for you.  



Of course, basically you will invest nothing.  



And you'll get a lot of money.  



Okay.  



So that use cases we can talk about.  



I can give you multiple use cases even now if you have time.  



Like I can think of it, but like example one use case I can think of is this is a billion dollar plus product we can build.  



Okay.  



Serious is the number one healthcare company. You know that in terms of a pharmacy, we have maximum number of scripts percentage across the US, which is around 25% to 40% scripts.  



Okay.  



Now, what it also means we are dispensing maximum number of drugs.  



Every drug manufacturer is interested in knowing how did my drug performed.  



If a customer has an ICD code.  



Which is a disease code X, and he has five diseases. And when he was taking what he was taking three medicines X-Y-Z what were the symptoms versus if the customer took X y with the XY condition that I took, what was a symptom and.  



What was the improvement happened in that person?  



Now, if we can build a business around it, we can take review comments.  



Because people it's like a product reviews.  



Okay.  



You are taking only the stuff because it's a PH. I think you can't take a personality, but you can take the disease course and everything.  



So that's a big business case.  



No other company can get you this much better.  



And if we can build this much.  



Better than we imagine, how much we.  



Can give back to the customers and.  



Also to the manufacturing companies.  



Make sense? Yeah.  



Okay.  



That's the one big business case I see of that what could be built, right. So that's a business case number one in my mind, which is pretty useful.  



Okay.  



Okay, then one more business case, which I believe is good, is whole this.  



Concept which I was asking you, telling.  



You about a date, simple delivery date, how do you print? Because forget about Ups and USPS has.  



Fixed, sold everything once it leaves your facility.  



Okay, right.  



What has not been fixed or solved is pharmacy world is a big problem.  



Okay?  



Yeah.  



It's not like picking a shirt and.  



Packing it in the box. It has way more regulations and the processing times are way more complicated. How do you predict that?  



When will it be shipped? Rather than giving a wrong expectation that I'll ship you after ten days, but actually I'll ship you in three days.  



Or I say five days and I.  



Give you after eight days, both could be incremental. And specifically, if you're dispensing specialty drugs where somebody is going through arthritis or some kind of a real disease, terminal disease, you can't be delayed by even half an hour.  



Okay, so having that prediction model which.  



Helps customer take up right decision making.  



And constantly updating them without having a.  



Systematic basically thinking like not a process, but engine telling it that is another.  



Business, I think, which is very customer centric. These are two business cases I can think.  



Those business cases are more of a vision based, so I will not go on that.  



So these are the two cases I can think in my mind as of now.  



The first one is way more stronger.  



And a bigger use case comparatively to second. Although I think the customer impact is more with second.  



But people don't look at basically everybody looks at a short term benefit, not a long term benefit.  



The first use case is a real.  



Money making use case. Second use case is a customer satisfaction.  



So people will pay more for the.  



Building a dream rather than building a reality.  



Makes sense. Yes.  



So that's the two use cases I can think as of now, on serious side, I can think more.  



There are two which is on top of my mind. I definitely can think many more.  



It's not like possibilities are not there to think.  



Makes sense.  



Makes sense.  



So actually one quick question. I do have an investor call soon Yoga. Maybe we can set up a follow up call. But just one quick question. Are any of these two use cases that you mentioned about already getting worked on? Like somebody at least is trying to drive or nobody is trying to drive.  



Currently, I can tell you the first.  



One is pretty much at a very.  



Early stage discussions that do business thinking.  



Only person who's thinking like this is me. So I'll be pushing the business that.  



This is what we should build.  



I see.  



Okay, so I'm the one who's pushing guy here.  



Okay, I see.  



So I basically need to convince you.  



No, you don't need to convince me.  



We need to convince you. In this case, I will use you.  



As an accelerator or a person to say, hey, this company can help us, but let's listen to them how this can be useful.  



In fact, before that, I will introduce.  



Them to one more company which has done a similar work so that I will generate the interest in their mind that this is really a billion dollar idea.  



Because my personal thinking, this is more.  



Than a $5 billion business line for.  



CVS if it goes well.  



Right.  



So that's what my mindset says.  



Understood.  



Okay.  



So Yogesh, are you available on Friday around similar time to catch up again this Friday or something?  



Yeah, generally I'm available after 05:30 P.m. East.  



Very rarely I will have a call till 06:00. After that generally I'm free. 06:36 P.m.? Yest. You can always call me. Generally, no commitment.  



Okay, understood.  



Generally I'm based on PST, so actually that timing works very well for me. But right now I'm traveling to India actually to meet my team here. So like a little bit later on est would better for me if we can do like around this time basically. Would that be okay?  



Perfectly fine.  



Okay.  



What I'll do is I'll send out.  



An invite to you for Friday and then let's do a deeper dive into it. How can we define some of the use cases?  



Basically?  



How do we figure out the next step?  



I know it's going to be a.  



Long process, but let's get started now.  



Yeah, I have a lot of other ideas in my mind already going on, but all we can do so we'll.  



Talk more as we go.  



Okay?  



Awesome.  



Thank you so much, Yogesh.  



Yes. Bye"
11189911703,LEAD School,Sushantam M (mohansushantam@gmail.com),,LEAD School,>1000,10-50 Mn,<10,AWS,No,Education Administration Programs,"Mumbai, Maharashtra",Nikunj Bajaj,Lead School_Sushantam 28-11-2022,https://app.fireflies.ai/view/TrueFoundry-Sushantam-LEAD-School-::gK5X6gZUpMQkrGxN,"Meeting on 28th Nov

Attendees: Nikunj & Sushantam

Use Cases:

Batch infrence (no-real time)

Regression - prediction of student improvement areas on the basis of marks

OCR-based model

Retention and churn

Cloud: AWS

As-Is process

Have Sage maker not very actively used (skill gap)

APIs for OCR that AWS has like the table API, very expensive, but still there is a table API. There is a text API, quite a few of them text and form.

Data science team does is they build a model, they create a pickle file and pass it on to the engineering team. And then the engineering team is the one that will do containerization and deployment

Sagemaker notebook and pipelines

Passes JSON output file to the engineering team

End consumer is product (consumes using looker, tableu)

Team: DS pure:5 + Analytics:6-7 (2 senior managers in each)

Problems:

Package version control when pickle file is shared and engg team is deploying

Deployment timelines. Current timeline: 6-8 weeks

Approval / Procurement process

15-20L approval VP of product or any department

Budget of DS team: 5cr (all including - people + tools)

Current limitation

Budget constraints

Next steps

Share documents

Schedule demo (in case of interest)"
11189840489,Dozee,Amar Kumar (amar@dozee.io),,Dozee,100-500,,Oct-25,AWS,No,Hospitals and Health Care,"Bangalore, Karnataka",Nikunj Bajaj,Dozee_Amar 5-12-2022,https://app.fireflies.ai/view/TrueFoundry-Dozee::xLm29z9sbL,"Meeting on 5th Dec

Attendees: Nikunj, Vivek and Amar

Not using sagemaker

Problem:

Model version - a lot of manual effort

Currently doing on local machine

Main Client Goal:

Goal 1: Quick prototyping - dont need big servers. Save results. Secured as well.

One option that could work - run on local machine. Save on S3 or could. AWS can work

Goal 2: Tuning hyperparameter

Low frequency - 2-3 times a month. When they get new data

1 day job - mostly around code tuning.

As-Is: Uses Optunio, XG boast models

Long jobs running over night - not very frequency. 32 gig RAM. 2 TB (rented a server - inhouse)

Use googel colab for GPU heavy jobs - images processing

Final outcome

TrueFoundry is an overkill. Not the right time"
11185249632,Maveric Systems,Srivatsa Subbanna (srivatsas@maveric-systems.com),,Maveric Systems,>1000,500 - 1B,<10,AZure,No,"Chennai, Tamilnadu",IT Services and IT Consulting,Anuraag Gutgutia,Maveric System_Srivatsa 29-11-2022,https://app.fireflies.ai/view/Srivatsa-and-TrueFoundry-Introduction::idOadk7sp9pvL2V2,"29-11-2022

Brief background (if discussed)

Was earlier also at MU Sigma - Decision Sciences division. Abhishek - hands on Data Scientist kind of guy.

Maverik - working with MuSigma. Will come to what we were doing at other places.

Use Cases for ML - types of models (Is monitoring important etc?)

In other places, there was no platform - custom build models using python. Clean data, Univariate, BiVariate.

Here at Maverik - Partnership with AutoML Product Dataiku => getting people up to speed.

That adds a lot of value to businesses - EDA much better for businesses. We work with only banks.

Problems- 1) Predicting default for loans 2) Internally - Home Pricing prediction in the USA 3) Data gets updated, etc. 4) Cross sell, Upsell, Marketing analytics for Wealth Mgmt firm.

Current Stack for ML Deployments and pipeline

Python and R

Dataiku - lot of wrappers around Python and R

Add a trigger in terms of deployment

Israeli company: BigPI => Help build pipelines 5-6 times faster. Visually driven. Showed a demo

Do you serve the models to clients?

Problems being faced where looking for solutions

Banks have their own version of AutoML they use - H20, Dataiku, etc.

Pipeline based models? Data Engineers on the team - who builds pipelines.

Data Pipelines are also executed on Dataiku.

They have recipes - Azure teams // Teams who connect snowflake to Tableau.

Data Side - will be 350 people

If you want to do heavy computation, do it on our own database. In 1 linux server, there is Dataiku for example.
"
11185249388,Netomi,Bobby Gupta (bobby@netomi.com),,Netomi,100-500,50-100 Mn,>50,AWS,Yes,Internet Publishing,"San Mateo, California",Nikunj Bajaj,,https://app.fireflies.ai/view/Netomi-and-TrueFoundry-Introduction::Zn9s83sypxQoA80O,"Hello? Hey.  



Hi, Bobby. How are you?  



Can you give me okay, yeah, I can just hang on, let me turn on the line.  



Yeah.  



Okay, sorry.  



No problem. Bobby actually my co founder. I wish I had reached out to you, but he's not able to because he's not feeling very well, so I'm taking the call on his behalf. We are patchmates from undergrad and like our third co founder and Ruggage also, all three of us went to It for our undergrad.  



Great school.  



Yeah. Where are you based, Bobby?  



Yes, I'm based in New York City area. I was in New York City, but just moved out a little bit. Just a little bit north.  



Yeah, I see. Okay, so it's like 03:00 P.m now.  



Yeah. And you are india right now?  



I'm india. I'm generally based out of San Francisco. I'm traveling to India for a team of site, basically.  



Okay, I got it. What's your role then? If you can tell me.  



Like a quick background about myself. I come from a machine learning background, basically. After that I moved to the US for my masters. I went to UC Berkeley here, and since then I'm working in machine learning. Like, I worked at a startup called Reflection where we built out a lot of recommended systems. And then I worked at Facebook where I led one of their conversation AI teams. And after that I did one startup in machine learning as well in the HR tech space that got acquired. And this is like my second startup with the same set of co founders. And at co founder, I'm the CEO of the company.  



Okay, got it. Okay. What is the company that you sold, you said?  



Yes, the last order that we sold to Infoyage.  



Okay, what did that do?  



Do you know about Northwest.com? Yes, infoyage is the parent company of Nockley.com. Yeah. So were building an HR tech space. They were also building a similar product, but they did not have a lot of traction back then and we did not want to continue building in super operational space. Like you wanted to build more technical platform, basically.  



Got it.  



By any chance do you work closely with Abhi Abe?  



Yes, he reports to me.  



Okay, nice.  



How do you know him?  



Mutual connection.  



He's a smart guy, I like him a lot. He's brain behind lots of this stuff.  



You've got nice.  



Yeah.  



Awesome. So Bobby, basically like today's purpose, the reason I reached out to you was primarily in the context of Truefoundry. So the idea is that we are building this machine learning platform similar to what we saw at Facebook, where we improved developer productivity by simply building out tools that are used across different machine learning models for deployment, for monitoring, for tracking, for versioning, all those things that you end up repeating right across different models. So we're building out a similar platform and now working with a few large startups and enterprises where we are helping them solve their problems. Right. And in this journey, the idea was that I wanted to learn more from people who are solving similar problems. Just figuring out are people facing this problem, is this a legit problem or not? How many people care about it?  



That's what I'm trying to figure out right now.  



Yeah, so there's a way of a little bit of my background as well, right. I run all of engineering classically in lots of ways as well, product design as well, for not to meet. So I joined them from DND, where I was chief finance officer. DND was, I don't know if you remember, roughly four years ago it was taken private because they stopped going 150 year old company. So plan was to turn this data company into analysis company because you can charge more to analytics and then take it public. So I got a deal I couldn't refuse and challenge and we did this and two years later we took it public. So I catch my chips and join them.  



Let me write so clearly there it was all about DevOps because if you think about it and then my previous job was about that as well. So let's talk about this job. So DNB, obviously data company, they put large amounts of data coming in, they want to provide insights from it, they have customers who want to get insight from this data after combining with their own data. Right? So what we build was analytics sandbox, if you will, where old data was already curated. We already had template models for like a better word, already built. And then all they had to do was upload their data with certain information in a particular format, if you will, and then model would run and custom model would be built and it will tell you how much uplift it will provide.  



And then if they agree it gets deployed, then we should charge $500 per API call for scoring. Big part of all that, certainly data science was there. Building the model for risk was primary space, but sales and marketing, those are two primary spaces, right. Government was also there, but actually government was pretty big. So the entire Tart money that was given out, trillion dollars that came out of that sandbox, my team did all the answers for White House. I was on call because a US government can't figure out how many companies there are, the real or not, because you are not allowed to require to necessarily register, especially with the startup companies, right. So they could explain for example, how much impact certain things would have or COVID was having at an early stage, at least in Rustwell as an example. Right.  



So the point to be made here is it was all about big chunk of that offering with MLS, right? So we actually charging minimum $200,000 a year and then depending on how can you use it, how many times you run the models, how much data you take. We charge for the data also and then of course, once you deploy the model, we provided the service and we charge you for scoring $500 per scoring. Right. So they're paying a lot around that, like companies like American Express, other companies american Express, you can guess, is about risk, right? Somebody wants to charge $5,000 on a corporate card and they want to risk quoted before the charge is approved.  



Before that, before the charge is approved they would take the information on that business entity or the information on what is being charged, what location, blah, blah. And so they build their own models combining with data we had on various entities and then deploy it in our environment. They'll make a distributed call and they would just score it and send it back to them. And then that would in real time, well near real time reject or approve the car. Right? That's an obvious use case. I mean, if you could do that, you can make money, right? So there are companies that are coming in I recall two years ago that came out of Georgia Tech Atlanta that was providing a sandbox for risk trading and whatever else basically for those kinds of things in the cloud, right.  



You just upload your data, buy the data that is housed in the sandbox and you pay for the data to various providers and now you can within matter of minutes start running by building models.  



Right, right. Makes sense.  



There'S. That just like a long introduction. Last one I'll stop it. Something similar I did more constantly, which was in house because financial companies are lot more regulated and so they are not as open to using cloud where all the things are solicitly moving in their direction. Right? So we had 120 node cloud database for same thing, like 10,000 fees would come in on a nightly basis, all the data would be, you know, ETL curated, validated, combined together in a meaningful fashion and distributed them through distribution service, but also kept right there to be able to build models. To be able to build models.  



Because more recently the whole strategy was to grow wealth, which is still growing at the rate of 20% a year, even though the biggest wealth provider using machine learning so it would take 360 view of the customer, what you bought and sold, research on products, it could combine it and have various recommendations, actionable recommendations for customers.  



Right.  



So again, the biggest piece of that was really analogous. So the thing is that for ML Ops, you have a lot of data you need to be able to basically build models on it, have different versions of it, be able to activate old version and explain what was or compare version two with version one and explain what has changed from two to one.  



Right.  



Every six months you're required to explain why you think your model is still working and then generate alerts when it stops working, the genie and things like that and generate alerts. Okay, it was working. Then user behavior change, data change, now it's not working. Right. So generate an alert and then to say okay, now you have to retain the model.  



That makes sense. Yeah, for sure.  



What's your angle? Because now if I turn that around you do have a lot of competition in ML Ops, right. So you have to find at least something that differentiate yourself from somebody else. Otherwise it's a doggie dog race. Right?  



That is very true. What do you end up doing at Netomi for machine learning? Bobby Kennedy are you using a bunch of machine learning?  



We are a company before anything else, so we use a lot of deep learning. So we have a large number of models to do wide variety of things. Right. So from understanding what you just said for help, for customer help. Customer care, right. To basically have a conversation right out of the box using AI, just one thing to understand what somebody said is another to actually respond to it using it, which is much higher bar. So that to things like anticipate the problem person might have do the recommendations, things of that sort for upsell and cross selling all those classic customer care problem which you know, you can take it in the direction of productive care because now even IoT and things like that, if somebody bought something you got even.  



So you could take the events that are coming in and combine it with the data and practically make a guess out this customer may call in because their field just probably stopped working.  



Right. So you end up using AI for a lot of this customer experience, other things. Where are you using a lot of this AI? On the cloud itself.  



We use some services from Amazon sorry, google, like Google Translate and things like that. Got it.  



I see. And do you end up using like a Sage Maker or something for machine learning or like everything is on?  



Yeah, you have to realize that I use Sage Maker at Morgan because they are more process centric. Right. So they need to have different versions of the model. You need people to explain what changed from version A to version B and there is a separate group that actually reviews your model and approves it for deployment or not. Right. So it needs to be reviewed from the point of view that let's say is bias against women as an example or things of that sort. It doesn't increase company form by risk, things of that sort. Right. So it goes through all that. So we have to produce certain metrics before they would approve the model to be deployed. So you keep it, you use Stage Maker, you have different versions. Not only you have to snapshot the model that it's not good enough.  



But you have to snapshot the data conceptually, right?  



Right.  



So now you can see MLS becomes really complex and horrendous when you have to do that kind of stuff.  



Right, Netomi, yeah, it's just a startup.  



Right. I mean, you build a model, you run a model from a bucket, you make a copy of the data and you're not going to pay for it and anything big startup like yours. Right. So it's less about that. Right. And although we do deep learning and things of that sort, data definitely is not always as large as it was modern sunny right. Terabyte type of data. Right.  



So do you all end up using Kubernetes in that case?  



Yeah, we do use Kubernetes. We use ECS, certainly that I see.  



Even the machine learning models are deployed on Kubernetes.  



Yeah. So machine learning models are deployed a combination and they deployed on ECS. We do use GPD three for some of those. For some of them, Kubernetes just get in the picture. But I think it was more easier than GPD three combination than Kubernetes. But yeah, we are trying to use more and more Kubernetes.  



I see.  



Another problem is Kubernetes, to be honest, as compared to ECS, right. You need a residual staff of two, three people to manage a large info of Kubernetes. Right. So just because you have a few containers, it doesn't mean you can get away with quarter head count. But three head counts is a lot in a startup. Right. So that's other part, so you have to have critical size even to go for Kubernetes. And certainly migration from EC to ECS was fairly straightforward. So big chunk of our stuff is still in ECS.  



I see. Okay, understood. Yeah. A big chunk is using ECS, including machine learning models.  



And we do some catching in elasticsearch and things of that sort too. So other big part of machine learning is which nobody solves is real time scoring. Right. The building model is easy part. Right. So I can build a model easily. Okay. But I built it now. What that's? The time services I need to score in a performance way. And quite often the model you build, maybe that was a common problem at DMV. So my data is large. I built a model using Spark, but I'm not going to say deploy it on Spark for scoring. I mean, how do I do scoring on Spark? It's not that easy. Database always trying to solve that because now Database has almost like their own operating system that runs on the Spark. It allows shared sessions and things of that sort.  



So multiple things could be doing multiple things on this park. But until that came along, there was no way to really, in a regional fashion, put a service in a Spark that does real time scoring. So what would you do? You end up taking that model and take a position that help. But you know, scoring is much faster than building a model. So I'm going to take that model, but actually I'm going to deploy it in standard Linux hardware. I should say Linux hardware. And only challenge is really quickly accessing the slice of the data that I need. So I would use Elastic search for it, cache it in Elastic Search for long term caching, right? So that was classic approach we took. Right. So do the modeling on the spot deployed on the standard hardware and do the Caching and elastic search.  



Our luck here. If you have a better solution, there isn't for large data, then you have to do on the fly fast aggregation and then bring that into model. Right? So there are some challenges that one faces with large data, extremely large data. Of course those challenges are not as severe and we bypass them obviously in a rudimentary way because nobody is beating on our neck to say you have to be able to go back by a version or two.  



Understood. So Bobby, if you have to think about your current like in the startup Anderson, there are not a lot of governance challenges. What are some challenges with respect to your operationalization of your machine learning today, if any?  



Yeah, one of them is obviously the real time is coding almost fast. It is still more time consuming than let's say running a simple database query, picking a record and giving a response if you will. Right? And that's a challenge because if you think about it, for me I'm doing a voice, right? So I need help with luggage. Okay. So it goes through AI, whole thing goes through collection services. We turn into messages. So we are using Kafka to scale it. But at the end of the day then eventually the other science will say, okay, I got the message, this is for me, I need to score it and then it basically scores it. So I've solved some of this problem by making everything as synchronous so that my user will not hung up.  



Well, all things are going through this and AI going to score it and eventually send a response, right. User can say something else in the meantime or whatever else. But still you have a challenge of scoring it within 1 second or so realistically and sending a response. If it's a phone call, it's going to time out. If you didn't do that, most of the voices stuff times out after two or 3 seconds and so you face those challenges. So you have to really still scale it. The only way to scale it is really cache this data, right? And so you're not going to cache that is not durable. So you have to end up caching in redis elasticsearch. So that at runtime the service that's going to do the scoring can quickly get to the data, do the scoring and move on.  



Got it? So what's the server layer that you end up using for these models? Is it fast? API.  



Yeah, we use fast API. We use that. There are a couple of other things that are being used, but depending on models but that's been used.  



Do you know if the team has tried using any model servers like TF Server or Todd Serve?  



No, it's all in house, it's all open source.  



These are also open source frameworks that I'm referring to.  



We build a little extra stuff on top of those, right? We use all of those. Right. But it's not in that form. Right. You have to build more of assets and frameworks to be able to use all those. Retrieve the data quickly from a search and figure out which slice of data you need those kind of things and feed into them and go from there.  



Got it. I see. Okay.  



The case that any of these can be used out of the box entirely. So you either buy something from MLS product, I mean, HTO has got something and a bunch of other ones have other mechanisms for deploying it, or you built some glue to combine a bunch of things.  



Do you use h two internally?  



No.  



Okay, understood. I see.  



Payments. So HT was, to be honest, used a little bit at Morgan, but it was never deployed. It was just more like prototyping. They tried hard and really printed. So we build our own thing on top of Spark and hadoop infrastructure cloud era in this case. And again competition of elastic search for Caching the data. I forgot.  



How have you structured your machine learning team, Bobby? Is it separately data science team or separate engineering team or how is that structured overall?  



It's a small company, you really do that. But there is that structure, right? I mean standard structured, you've got DevOps does standard DevOps like network to setting up containers, Kubernetes, ECS and those kind of things. And then the data science team does I would say ML Ops to an extent with the help of DevOps. And then there are data science vertical teams that build models for various things. Like we are in voice of customers. There are teams that's the voice of customer. There are teams that build models for certain aspects we have around the studio, if you will. That one could be described as special purpose data robot the robot, if you will, that offers not only view into the data by doing initialing supervisors and unsupervised learning initially to figure out what should be done with the data.  



But then once it's deployed actually, there's an optimizer in the studio that suggests, based on again, AI models, where AI should be retrained so user can review it and accept it or continue with the refinement of the setup.  



I see. Okay, understood.  



The thing is that lots of these tools like if you look at S two O data robot and stuff. There are general purpose tools that do general analogs they really have no regard to the domain and things like that. It's the same issue at data bricks when I build a ML sandbox with databricks database has no idea about machine learning. So we have to think what are the subject spaces? But the whole end to end flow would work from the whole user experience point of view. Somebody who wants to build a risk model as an example, right? So the user experience was as follows, right? And you can't really build that custom user. My experience in an auditor robot so basically there was a portal somebody could say oh, I want to build my own model.  



Let's say it will ask you do you want to build this model, get sales and marketing model, whatever else is this model? We are five custom models, we support custom types of models. So you select one or within that what aspect of risk are you looking for? Right? So you select one. Okay, fine. So to be able to build this custom model, I need this sort of data from you give me the s three bucket where you have digital setting, right? So it sucks that in combines it with the data they have purchased the DNB data and business entities. I mean, American Express, if they want to build this, it would churn and say okay, come back after 6 hours or tomorrow, when it's run, you can look at it in the meantime, it's independent. So they are worth flow aspects, right?  



Independent state, it's running, they come back next day, it's run, it gives you then all data science metrics preceding the call doesn't make sense if no business user would understand precinct recall, right? Exactly. You give the upgrade this new model provides what's a traditional model that DNB has purely based on this on data set as compared to traditional model that DNB has it provides additional 30% uplift, right? And they can give you it'll tell you for what type of businesses, large and small, where it works, where it did not work, right? So that you could just use it for subset of customers or all of them you need to it now you're happy, right? So you would say okay, deploy it.  



So it would say okay, come back after 3 hours, it will be deployed when it's deployed automatically, more or less automatically, it's just a bad job that does it would take the data cache it in elasticsearch, it would do appropriate aggregations where it needs to do because there were well defined mechanisms for doing that. So that because you can't do 100 million records aggregates on the fly. So if your model requires that, well, then you have to do that beforehand. So it would do that automatically, right? And then it would put it in the cache and say okay, here's the end point makes sense, right? So to be able to do that you have to think about your business problem domains, various constructs you have and things like that, right?  



There is much to be made there because then you are counting on that's one angle so you're counting on business expertise, right? So I don't know if they ever build it. I left by the time I had this thought that there were three parties that had expertise in various areas. So the question became why should we build the models? Or a customer may not have live stream first of all now value to American Express is that they don't have to have in house this expensive livestream that's dealing with spark this and that they can upload their data within a matter of hours on their way, right? So they just could do it themselves, they don't even need it more or less, right?  



For deployment now they need ID but now they don't because it gets deployed on our side and it uses the security mechanisms that they want single sign on sample to whatever else they want to use it and they trust DMV so they don't have to deploy it. So all they have to do is have their service point to wait for scoring at real time when somebody applies for that, right? So then question really became that. So we got three things DMV builds the model that can do the scoring on DMV data a customer can build a model combining their own data and then it can be deployed in DMV site and we can charge on scoring and from building the model, right? And then the third part is, why should we do all this? And think about it?  



When building these templated models, the third parties may have insurance, the insurance companies, maybe they want to build their own model, like have Apple has Apple Store. Apple App store. Essentially, the third parties can build their own models finding created bases, using DMV data and deploying or maybe some of their own data and then basically hosting their models. And they can sell it to we can sell it to resell them or they can sell and we charge a small cut. So there was a plan for that last one ever happen?  



Understood. So Bobby, from your perspective, are you thinking of potentially working with any tools in this domain to kind of optimize for your dev workflows and stuff like that, like delivery in general?  



I mean to be honest that has not been a priority. We can talk about it certainly there would be no appetite to take too much money, right? That's the point, right? Because that's really not a priority. Nobody's complaining that I don't have versioning as an example, right? It's not a smart company that does become more and more important as we engage with large companies, enterprise size companies, which is where we are heading. So probably a year from now that next year, it might become important. At the moment, priority isn't that priority is to build a model, thinking about new ways of doing things right. So that's where most of the energy has been going.  



I see. Actually on this one, I think there is a little bit more to it than model versioning and tracking and the more bookkeeping stuff. What I would love to do Bobby, is kind of show you a little bit of what we have built out. Would love to at the very minimum get your feedback and sure I would love to probably have your team try it out, but at the very minimum we'd love to get your feedback here.  



Yeah, and I'm happy to talk and certainly we can compare and try it out depending on the cost. But certainly it wouldn't make sense to try it out at all if the cost was not reasonable. Trying to help me?  



I'm curious Bobby, you mentioned about the cost. So for a startup of like Netomi size, right, for a problem like operationalizing machine learning, like in an year, what's a reasonable cost? What would be exorbitant? What would be reasonable? What would be too cheap?  



It depends on use. Don't forget most of our team is india. All the sellers have gone up. It's really still not as expensive. Right, so most of the energy isn't really spent in ML Ops entirely, right? Because you already have DevOps that manages in physical container. And I would have to see that if you solve the things end to end because at the end of the problem is nobody provides entire MLS solution. If I define MLS as basically you would let me house the data, point to a particular slice of the data, build a model and then deploy the model in a container. But also you would also do the aggregations and cache it automatically in elasticsearch and figure out what needs to be cashed.  



Nobody provides it and then you would assure it to me to convince that you do all of them even. Right, so then what you end up doing is that you buy a product and then you spend shitload of energy putting extra good on it for what you might as well do a little more where you have full control. It works just your way for your specific frameworks that you build on top of it. That is a response. That is a fetchist response. One would respond to it question. One would respond to in terms of cost, you were asking how much cost maybe you made. There are two people in India were doing it. That's the cost because open source stuff becomes really good. You have made some simplifying assumptions because you're not building product, you're building product. But I am not right.  



I make simplifying assumptions based on my service framework, based on what particular technique I'm going to use Elastic Search for Caching as an example. How I'm. Going to cache it and think like that, I can make those assumptions. And then once you do that, there is not as much incremental work. Once you do it, same thing on the product. Then what you end up doing is you end up dealing with limiting well, I should say, let's say idiosyncrasies of the product. Right. So adoption of the product itself requires a team. Right.  



That makes sense.  



Yeah, it makes sense. At DNB, it made sense, actually. I was paying $5 million a year to data bricks for building that genetics center. That makes total sense. There's no way we could have done it ourselves at that level. Right. Because once you combine the data level, security aspects and things like that, it being client facing system, not that easy.  



Got it.  



Okay.  



So as I understand the cost for probably two people india, no more than $100,000 for you. So that should be the cost.  



Yeah, that's the point. That's the reality. Right. So I'm not saying that company, but probably you are not looking at right market. If you are looking at a smaller size company, they're just not going to have appetite to pay that much. If you're in a position sell it for $20,000 a year, probably you can. Right. But then you have to price it that way and then have a model that works in that direction for low end use. The real money is elsewhere. Real money is really, I'll tell you, is there at the peak of a database. I'll tell you, I was paying database $6 million a year by the time I left. And American Express itself alone was paying us roughly $3 million a year to use this inbox.  



Wow. It's crazy.  



But it was because it provided end to end stuff, because we had our own data, so they would pay for the data. So first video of it was not only they were paying $3 million for this sandbox, but it was like the best way of promoting your data. They would need the data they see, then they list, oh, I want to build a risk model. Okay, so you need these data sets from DND. Do you already have access to them? No. Okay. There is your credit card if you need it. Right.  



More money. More money.  



It would allow you to build the model. It would deploy it, and it'll allow you to score it'll, set up the security aspects of it, like which slice of data, who has access to, and things of that sort. So there were those aspects. So if you had not acquired DMV data, it's not going to give you access to that data where it was sitting there. Right. So it would apply the filtering when model is built based on what you paid for, things like that. Right. So that does take quite a lot of energy to build.  



Right, that makes sense. Yeah. No, I think I completely appreciate your suggestion on the market itself, bobby, I would love to take some time, like next week to show you a demo of the platform.  



Sure, yeah. So if you ever do that, if you could set up in the morning, I'll invite a couple of people from my data science team too.  



Yeah, that would be lovely. So what kind of timing would work out for you given that you are on the East Coast?  



Actually, to be honest, it doesn't have to be morning because one of my data scientists is in US hours. So let's see next week. This time is generally good. Like Thursday is good. I have to see if it works for my.  



Let me follow up with you over email.  



Yeah, we can do it by email. Tuesday maybe too short. Thursday probably would work.  



Okay, understood. Cool. Let me do that and I will have someone from the team follow up with you and set up a demo call. And it would be great to invite a couple of data scientists as well.  



Sure, yeah. Happy to spend some time. But I think probably the goal should be if you want a feedback, I'm happy to give you feedback and that's why I accepted meeting chances of it. Unless you're telling me that you sell your solution for ten, $20,000 a year probably is not going to happen. Right. It's just not a priority. Integrating any new technology like that and building process around it all the way down to zero to whatever else takes so much energy. It's a distraction when you are overloaded and you're trying to really build the functionality and get the deals that make sense, I'll be honest.  



As opposed to no, I really appreciate you being honest, Bobby. I will be open to the fact that it may not work out and at the very minimum, I'll get some good feedback from you.  



If it works out, I would love.  



To work with you and your team.  



Sure, yeah. We can chat, but I would put a problem of it being working. No more than 25%.  



Okay, awesome. Thank you so much, Bobby. Really appreciate it. Bye. "
10881905922,FreshWorks,sauravchakravorty sauravchakravorty@gmail.com (sauravchakravorty@gmail.com),,Freshworks,>1000,100 - 500 Mn,Oct-25,Multi Cloud,Yes,Software Development,"San Mateo, California",Nikunj Bajaj,Freshworks meeting 01,https://app.fireflies.ai/view/TrueFoundry-Saurav-Freshworks-::CmymfucTAK,"Meeting on: Nov 8th

Attendees:

Freshworks: Saurav

TrueFoundry: Nikunj & Vivek

Notes:

Saurav leads charter for ML in one of the BU in freshworks - Freshservice

ML models are mostly text-based, little bit of anomaly detection on time series

Everything in ML is extended to customers of FW - through the product

Main applications

Virtual agent application - support and help; Search, classification

Model for engineering automation - ticket-agent mapping, onboarding automation

Anomaly detection in IT service

Infra at FW

Application

Larger application engg team - monolithic code base

Dedicated team for infra (on AWS - not brought)

Machine Learning

Own infra: Each model application is an endpoint and the account name becomes like a parameter to the endpoint

Layer for the abstraction of the application from ML service - acts as an orchestrator for ML service. Every time a product needs a prediction, It tells us which account, which customer and what kind of prediction with some header information about what we are trying to predict.  Tt sends that request to us and one of our machine learning services picks that up and responds back.

This layer acts as record keeper

The ML services by themselves are spinning completely without any state. So they're just simple endpoints that you can hit one at a time. That's how they design it. 

ML services are specific for each account - light weight models

Serving infra on Kubernetes: Custom designed app with caching inbuilt which ensures that whenever there's a call come in for particular inference from a particular account, it is able to guarantee certain sets of SLA parameters and respond back to that. The main thing about the service is to ensure that service rate

Everything is online - model on S3

Number of models > 1000

There are few models - one model for all customer

Training: databricks

MLflow to track experiments - started a year back (POC was done for inference and model registry - decided not to go ahead)

Drift tracking -

Kubeflow was used but not finalized (complicated)

Fiddler also reached out but not bandwidth with the team

ad-hoc solutions: Internal dashboard and tools

Take on building internal tool

Expensive process

Won't do it again if give a choice

Next step:

Next meeting: show product and discuss on what can be built on top of our current offering to serve the SaaS need (ex- strong caching layer - can't keep thousands of models in memory and also controlling latency issue)"
10881905316,Yext,Michael Iannelli (miannelli@yext.com),,Yext,>1000,100 - 500 Mn,<10,Multi Cloud,No,"Technology, Information and Internet","New York, NY",Nikunj Bajaj,,https://app.fireflies.ai/view/TrueFoundry-Yext::xxT3Ml9av4,1. We can validate the pricing thing by offering discount
10881905316,Yext,Michael Iannelli (miannelli@yext.com),,Yext,100-500,10-50 Mn,25-50,Multi Cloud,No,Software Development,USA,Nikunj Bajaj,,https://app.fireflies.ai/view/TrueFoundry-Yext::CUiTUePEig,"Quickly check if Nikonjana Bishika joining. Just give me a minute.  



No problem.  



Hello. Hi.  



Hi Michael, nice to meet you.  



Hi Vivek. Hello. Sweet.  



I think Abhishek tal catch be able to join. But in the meantime I'll set the context up and probably we'll get going from there.  



Sounds good.  



Thanks Michael, for taking on time today I have with me Nickmanshu is the co founders sync CEO of Truefoundry Health IQ. Purpose of call today is mostly one to get to know each other in terms of where we are with respect to ML pipeline and what are the things that we are doing. And there are some things that we would definitely want to bounce it up with you with respect. CTO the problem that we are solving and take your opinion on the same. Right. So that's 1 second if time permits explore any possibility of some working together, possibility, stuff like that. But that's secondary, primary. We wanted to know your perspective on what are the problems that you and your team face while solving some ML deployment related problems. So that's the agenda. Simon Qovery to you.  



Nice to meet you Michael, I think. Thanks a lot. We wait for setting the agenda. Michael. Maybe a good start would be for us to understand, learn a little bit about each other and then take up the agenda of the conversation as weaker set up. So I can briefly introductions myself. I would love to learn more about you after this. So I personally come from a machine learning background Michael. I used to be at Facebook where I built and deployed a lot of conversational AI computer vision models for their virtual assistant product called Portal IBM. Not sure if you've heard of it.  



But it's like I Amar.  



Yeah, okay, you are great.  



Do you also use it by any chance?  



No, but I am aware of it.  



Understood. The proactive assistant, right. Like the virtual assistant is typically reactive. I was leading the proactive assistant part of a part of it at Facebook and prior to that I was at a startup called Deflection where I joined as an early engineer and helped the company grow team grow to roughly like 600 million monthly active users across hundreds of ecommerce websites. I built A Lot Of personalization And Recommended Systems at Reflection for the first couple of Years and later got a Chance to lead the Team to build out a Horizontal machine Learning Platform for the Company.  



Because by the time we had, like, five different teams building machine learning models and deploying that so it made sense to invest in a horizontal platform for the company, and that actually gave me a lot of experience and know how on what we're building basically at True Foundry. So that really helps where we are coming along. Between Facebook and True Foundry, my two friends Anuraag and Abhishek, together with them we built out another company called Entire in the HR tech space, which got acquired by the largest HR tech player india. It's called poetch. And there again we built out a lot of ML models, deployed them, et cetera. So Fairly has been working in the space, learning from different angles, like a startup angle, to a Facebook angle, to a mid company angle from reflection.  



So just trying to capture those learnings and deliver it to our end customers as part of Truefoundry. So that's where we are. As Vivek mentioned, the purpose of this call primarily please don't think of this as a sales conversation in any way whatsoever, Michael. The goal here is just to understand your current workflows. What type of models are you building, what type of solutions are you trying to use, where are you facing challenges just to learn? We are in a discovery stage right now as a startup and talking to practitioners like yourself is super helpful. So that's the purpose of the conversation. Okay, sure. I would love to learn a little bit about your background, what you do at yes and then I have a few specific questions that I would love to dive into.  



Sure, that sounds good. Hello Nick. It's nice to meet you. So I started off doing my graduate work in computer science. My background is largely distributed systems. From there I worked for the DoD for a little bit and got mostly into applied graph theory and machine learning over graphs. Where I got into the Ad tech space working with very large datta sets I was working about, the data set I would work with was about a petabyte a day that we generate. Yeah, and that's where we kind of it was very difficult to find like an MMoP solution that would work on that. We tried Vertex AI and we could never get it to scale properly.  



Then from there, about a year ago, in fact, about a year ago starting last week, I joined Next, which is smaller scale of the data is a much smaller scale, but a larger surface of machine learning models needed. So here our team is about ten or so people, but there are about 100 machine learning models introductions. So that's why we started looking at the ML Op space, trying to find something to better serve our needs here. Mostly at ten LP models. So embedding models, entailment models, ner models. Mostly within the NLP space. Each one kind of has its own requirements. So something like the GPB three models require much requires a much easier machine. Shane, say something running a simple Ner, right. Or a simple Embedding model. And we currently do not have a solution.  



Everything since we started a while back, nothing really served us and Sage Maker wasn't really a mature product at the time. So everything is kind of homegrown and it's not scaling particularly well, which is one of the issues that we're facing here.  



I see.  



So I'm curious by the way, this background that you gave is super helpful suddenly because of all these interesting area that you Shane worked into. Have so many questions popping in my head, but I'll try to go like a reverse chronological order, I guess, surge week. So you talked about like, yes, building out NLP models, Ner models, etc. Talk about using Sage Maker which did not like you mentioned that Shane Maker was very early when you all evaluated Sales Maker and now it was before.  



My time, but that is my understanding. I spoke with the team that originally evaluated it and it did not meet our needs at the time.  



Got it.  



By any chance, Michael, do you know what was it that the team was trying to solve for that Sage Maker did not offer?  



I don't know exactly. Another possibility is that the learning curve was just not they weren't unwilling to get through the learning curve, which is interesting because I think it's kind of a simpler learning curve. Shane, say Vertex AI, Google's product.  



To.  



Be honest, Michael, on that note, I actually talked to a lot of companies trying to use Safe Maker and Vertex AI and the way I have seen those is I have actually heard a lot of complaints about learning curve. And honestly, it's not just about the learning curve, it's about none of the machine learning platforms out there today, to be honest, including ours, are sufficient to serve as all the use cases of any company whatsoever, right? Like people will always have like use cases want to customize their solutions for. The problem with Sage Maker and Vertex both is really like their documentation is so bad that it's very hard to customize and people lose the flexibility of customizing things on top of that and then end up paying the premium that they pay for.  



So I think companies that like more sophisticated shops don't like to build on top of that. I think that's the general feedback that.  



I have received, right.  



That said, I want to understand a little bit. So you mentioned that you have a lot of homegrown solutions and they're not scaling very well. So if you could give us some context on what kind of solutions have you all built out for your homegrown solutions and which parts do you think the company is struggling with? That's something I would love to vivek deep into. Just one other question that for NFP models, are you typically using primarily using hugging face or are you using like all over the board hugging face by Torch, TensorFlow, et cetera? Are you using mostly pretrained models or are you doing a lot of fine tuning on top of that? How is that workflow happening?  



So we do mostly use hugging Face by Torch, Sentence, Transformers, and I personally use like Facebook Flare quite a bit, but that's not very popular within the company. So there is like a large surface of models that we use. So everything but TensorFlow essentially.  



Okay.  



But one issue is that right now there's only a deployment solution. Right. You create your model and you deploy it. And this can take anywhere between one week to a month to get out there because it's kind of this very manual process with lots of boilerplate code to write. So there's a lot to do whenever you want to deploy a model. And then if you'd like to do something like batch inference using this model, it's kind of like everyone develops their own ad hoc solution every time you'd like to do it. So there is no like, if you sage maker, there is like this whole concept of pipelines that you can go and build, dispatch, transformer, and operate over some data set.  



There isn't really a set of tools or standardized set of tools that we use for doing these various tasks, which one creates this kind of, like, a lot of unmattainable code. So it may be quick to get something out at the beginning, which isn't really true. It still takes a lot of time to get something out, but it isn't maintainable. So that's a couple of issues that we're running into. I could probably speak of a few more, but those are the two biggest ones.  



I see.  



Yeah.  



So diving deep into that, like you mentioned, the model deployment can take anywhere from a week to a month. What ends up taking time? What kind of boilerplate are you writing that ends up taking such long time to deploy a particular model?  



Each one is essentially we dockerize the whole thing. So we write the model, we write the docker container. We're responsible for determining how much memory you need, like updating all the versions, updating a whole bunch of little flags on it, stuff that isn't really the core expertise that you would find on a data science team. So, like, something like a simple hugging face model like this should be simple to deploy, right? Like, it should be a few buttons. It's not it actually takes a lot. If it was a custom model, I could understand, but then you expect most of the effort to go into the custom modeling. So because of that, a significant amount of time is spent on this boilerplate code, on actually just taking the model and dockerizing it.  



I see. Interesting.  



So, Michael, a lot of companies have built out these stocker templates and share that with their data science machine learning development teams to kind of like they can build on top of that, which basically allows them to move a little bit faster than that. Right. That's what we have seen quite frequently. Is that not happening? Are people kind of building out these scaler from scratch?  



No, there are templates you can copy and paste and do something not formally templates, but you can copy and paste from one model and plug in your code into it. It is also this very manual process with many hands off. So some of the issue is organizational. The data science team itself does not have everything it needs to deploy its own machine learning models. So something like a tool like Sage Maker for example, there's create you model, deploy it's out there's an end point. You can actually go and hit it. You can hand it off to other people. You can make it scale as horizontally if you want. We don't have that, right? Once we've finished it has to go to the engineers and then there's a whole bunch of other steps that have to be done.  



So a lot of it is just friction that exists between the teams.  



Understood. I see.  



And why is the team in that case resistant to adopting something like a Sage Maker? In that case, now that this process has become more cumbersome, like the internal tools, et cetera, do you know what's happening with that?  



It's general unawareness, which is why IBM investigating it. I don't think anyone is strictly opposed to it. I think we know we need something better. But no one is really familiar with Sage Maker. We don't really have machine learning engineers. We have very skilled software engineers, many of which came from Google. They like home prone solutions, obviously. And we have data scientists that don't necessarily have the systems backgrounds or engineering backgrounds.  



Got it. I see. Understood.  



Okay, that's very helpful background. The second thing that you mentioned was around batch inferencing, right? That you mentioned that different people have built out different solutions of their own for batch inferencing. Where are these batch jobs currently running? Like, are these running on some cloud hosted Jupiter notebooks or are these running on some Kubernetes jobs? How are these running today?  



It's not that sophisticated. They're running as. You start up an EC two instance and either just take the model and run it over like 30 gig it's not a huge data set, but run it over 30 gigs of data. Or deploy the model to some endpoint, given that process that we discussed earlier, and then make a whole bunch of asynchronous calls to it to run your let's say it's an Embedding model to create a bunch of embeddings and load them in some database somewhere. It's not like a real batch job, right? It's not like, here is the S three bucket where everything is here's where I want the output to go. Here's the function, I want to run over it. Go do your thing. No, it doesn't work. Like, it's nowhere near as robust as that, and I would like it to be.  



Okay, understood.  



And one other thing was around. You mentioned that a data center does not have everything that they need to do their job, right? Is it more did you mean more in terms of infrastructure provisioning or did you mean something else on that?  



It's not necessarily the infrastructure provisioning is fine. It's just the data scientist does not have everything he needs. CTO, say, deploy a model. Right. Like, it's not considered the data scientist's job. I wouldn't say this company, but at many locations, like, the data scientist doesn't necessarily own the model end to end.  



That sales sense. Yeah.  



So which parts of the entire workflow are missing, Michael?  



I would like a solution, like Sage Maker and everything it provides. So a very quick model deployment solution, a very robust batch and printing process. Those are like, the two big ones that I want tools to build out these pipelines and to test them, but also kind of a simpler learning curve, one that I can give to bring to the data scientist and impress them with because that's the only way it would make them want to adopt it.  



I see, okay, got it.  



And are you currently, like, organizationally speaking, I guess you mentioned that there are ten data scientists right. In the team currently?  



The team is roughly ten, about, yeah.  



Okay.  



The manager there's a couple of senior data scientists, including myself, and then the rest are like, data scientists and junior data scientists.  



Okay, understood. I see.  



And you are currently tasked with kind of like, figuring out what's the most optimal, like, either platform or how do you optimize this process of deploying and maintaining machine learning models.  



Yeah. So I am tasked with looking at the ML Op side of this. What can we do to improve our internal processes with this respect.  



Understood.  



And when are you like, what's the timeline of this entire process that you think you're like, are you looking to adopt something in the next month, in the next quarter, in the next year?  



This is open ended. There is no fire underneath us. I would prefer to have it quicker sooner rather than later, but that's not the same timeline as the rest of the organization.  



I see. Understood.  



And then in terms of the cloud, are you using AWS currently?  



We are multi cloud, primarily AWS and some Google at some Google and also on Prem servers.  



Oh, interesting.  



I see. And by any chance, do you know if not the machine learning team? Is the software team using Kubernetes, by any chance?  



No, were at one point, not anymore.  



Okay.  



So basically, like, the team oh, interesting. You were at one point and not anymore, so decided to move away from Covenant. Why was that?  



So do you know, from what I've heard through the grapevine is it was tough to get people to adopt it. I guess people didn't like the learning curve, which is all I really know. It was before my time as well.  



I see. Okay. Got it.  



So, like, the docker containers you mentioned that you all, like, end up creating docker containers out of your machine learning models and stuff. So do you end up storing that on an ECR or something and then just deploying the docker itself on like an EC two machine?  



Currently, yes. The docker containers are stored on an artifact repository and then the artifact is picked up by the engineers and then it is horizontally scalable so it can go in as many machines as we want, but that's the extent of the scalability.  



Understood. Okay, that makes sense.  



And you mentioned that there are 100 models in production. Are these hundred models like 100 different types of machine learning models? Or are these like three, four types of models across different customers? So you have one model per customer. What are these 100 models?  



Right?  



No, so not necessarily model per customer, but there are like classes of customer that will use different models. So for example, our customer bases all over the world. So there's a various different languages of the same kind of model and that obviously requires different model parameters and sometimes different models itself, but not usually. Then there are various sizes of models, so there's like various different sizes of language model. For example, there's Ner models and then many times you need them for different languages. So like, maybe like just the Ner model, but you need ten different ones for ten different languages.  



I see.  



Okay, that makes sense.  



Understood.  



Yes.  



And are you using so you mentioned some artifact repository that you're using for your models, right. What kind of are you using any tools like ML Flow or something to track these model versions or it's like plain vanilla, like S three or something where you sam you models.  



So that was the first outcome. With this evaluation, we finally started evaluating and selecting experiment tracking framework. So we do have MLS so running, and we do have Weights and Biases running, among a few others, but those are the two main ones that we're working with.  



Oh, so you have kind of signed a contract with weights and biases and then also deployed like an open source version of ML Flow internally.  



I don't know if you signed a contract yet, but we do have an open source version of ML Flow running.  



I see. Understood.  



Okay, so that is good CTO know.  



And then those are tracking solutions, right? They don't really solve the orchestration problem that we have, for sure.  



Yeah. And do you have you personally use both before Michael?  



Yes, I know, like weeks and biases, it's much better, but it also comes with a higher price tag.  



Zero.  



Got it.  



Can you help me understand Michael? This is one thing that I always love to learn about is like this decision making process within like different types of companies. Right? So like right now you are like tasked with figuring out which some of these ML Ops tooling, right? And now there's an ML Flow, which is obviously like less sophisticated than weights and Biases, but weights and biases which is like more expensive, they have like, per user pricing and like, how is that decision getting made? Who writes the check eventually in your.org, for example, something like weights and biases, did you have to go through and talk to a certain level of people to kind of talk about, like, should we go with ML flow or should we go with weights? And can you help me understand that decision making process a bito?  



So the decision making, this is made with lots of input from us and from my manager, the head of Data science. But then above him there is a Director of Engineering, right. Like someone who leads all the technology at Yak will have the final say. But they do value our input quite a bit.  



Okay.  



So the budget would come out. So the final decision maker in this case would be the Director of Engineering and they are the one who will sign off the budget.  



Correct.  



I see.  



And in terms of anything around, like, for example, weights and biases actually have different versions of its solutions.  



Right.  



One is that you could use the SaaS model. That is like, just get your API key and start sending datta to weights and biases and just deploy that. Right. The other could be that you use the enterprise model where the waste and biases can be deployed on some private cloud. It's not a multi terrain system and stuff like that. Right. Did you have any, like, discussions around that which model to choose that I'm unaware.  



So I was part of the ML GTM discussion. Raj waiting biases. But what I can tell you is that we would want overall for our research group, which IBM primarily involved with, there's no issue going with the sales solution for actually putting in customer data. We would need a VPC solution. But if you wanted to move some of the customer specific data that their end users have loaded into the system, then it would have to remain on our servers.  



I see. Understood. Okay.  



And in terms of the infrastructure provisioning for, let's say, hosting something like an ML Flow, did you have to work with someone from the It team who did this provisioning? Like, who started this process?  



I guess no, that I did up completely on my own. We do have the ability to provision our own infrastructure. That's not an issue.  



So you have access to the cloud account of the company?  



Yes.  



Oh, wow. Yeah.  



I know it's not very common with Data Science teams, but we are technically engineered. We are a proper engineering team here. Right. I have a systems background. We have a few others. We're engineers in the past. We don't have an issue with that.  



Wow, that's phenomenal. Okay, understood.  



So for you, I know when you mentioned that I come from a distributed systems, back then doing Data Science, I was like, good, I'll get both sales of the input from this call. So I about it, but I wasn't sure that the entire team itself has access to the cloud accounts. That's quite rare, to be honest.  



Yeah, no, I would say it's not very common with data science teams. It's common in the ones that I've worked in for obvious reasons. But it's part of the reason why I believe this frustration is there that we don't own the models and I rely on other engineers to do the work for me.  



Right.  



So I can understand it. If I didn't have the skill set, maybe it wouldn't be as big of an issue, but it's certainly as an issue for me. I would like the ability to own my own models I see. In my own services.  



That completely makes sense. Michael, I want to understand one more thing. So you mentioned a couple of times it's frustrating that you don't own the models. It's like it takes too gong, like a week to a month to deploy a model. It's frustrating that things are not standardized.  



Right.  



So there is clearly an engineering impact to it. There is clearly like a motivation impact to it. That part is very well understood. Right. My question is what's the business impact of it? Who would care if your model, instead of getting deployed in a couple of hours, is taking like 15 days to get deployed? What's a quantifiable business impact that maybe you can go and pitch to your director of engineering about?  



Very recently we've begun tracking how long it takes to deploy a model. That's why we know that it can be like up to a month. It can take a very long time. And largely, though, there is nobody on the table. Everything is sort of unless it's doing a hot fix CTO a model which that we can actually do fast. That's not a huge issue. The problem is this maintenance also is time consuming. Like many of the data scientists, rather than doing research, are kind of spent maintaining the models. And it can take the whole day, take the whole week. And their primary task is not spent. They are not focusing on their primary task of doing research or building new models or solving new analytical problems.  



Understood. I see.  



All right. I think this is very helpful. Thank you so much, Michael, for answering all these questions. I think this is super learning session for me. I feel like the last 20 minutes I've just learned so much about all these problems and tools that you're exploring. Let me maybe give you like a very brief overview of what we are trying to build. Michael, I'm sure nat you be curious CTO know that also a little bit. Just to start off with, have you seen any documentation, any material, any on our website? Do you have a sense of where we are going?  



No, I haven't looked at it yet.  



Okay, sounds good.  



So let me explain CTO you the background here. So as I mentioned, that come from a machine learning platform background in some way, right? Like I built out that platform at Reflection and then I noticed what kind of platforms does Facebook has in terms of expediting this entire workflow? And obviously Facebook, they had built out a platform where you could be deploying like models in minutes as opposed to days and weeks. Right? And also developer empowerment was a huge theme at Facebook that you let people, you build it, you own it. Essentially. That's like literally one of the Facebook mantras, right? And that's pretty much what we are taking out from as our learnings from Facebook and trying to encourage this in our platform. And you put it in a very clean way, something that helps.  



I might even steal that as part of our punchline if it's okay with you in our go ahead. Which is something that scales like Sage maker but it's much muthu simpler to use.  



Right?  



From a user experience perspective, I think that actually puts us very succinctly. In fact, the line that we used to describe ourselves is like start like heroku scale, like AWS basically.  



Yes, exactly.  



So we use that quite a bit to explain our product. And that's literally like, our goal. That, like, a data scientist can build and deploy and maintain their own models, like, truly be in control of what's happening, but not every time. They have to manage the entire infrastructure and all the intricacies of the entire infrastructure themselves if they want to, they have control over it, but they don't need to do it time and again, essentially. So that's the platform that we are trying to build and literally it spans from your batch inference. And that is how do you quickly deploy a batch inferencing model run phone conference, save the results somewhere and not pay for the machine costs throughout. So like a machine comes up, runs the batch and friends skills itself.  



Right?  



So that's where we start. Now, how do you quickly dockerize your model? Not worry about in fact, by the way, we also do some level of automatic creation of docker containers deployment on like in our cluster itself, like circulating an API endpoint that you can share with your engineering team. So we take you to that extent, right? And then later, once teams are more mature and their use cases evolve, for example, that how does this model then integrate with your CI CD pipelines? Because that's the other thing that becomes interesting when people start building and deploying more models. So we do integrate with your existing CI CD pipelines. How do you manage your model versions, model registry, stuff like that. So we have solved for those problems essentially as a platform.  



And the way we are building out this and our platform is we are working with a few like very limited set of enterprises where we really understand their existing use sales, what. Are the problems that they are specifically trying to solve and kind of customize the platform in the beginning for these early customers. But of course, even the high touch level of engagement that we have right now, we cannot actually work with many companies because we want to deliver really superior quality results to some of the companies that we're currently working with. So what we're doing is we are actually really evaluating the companies that we are working with to an extent where we understanding the space. This is exactly the use case that they have. This is exactly what our platform provides.  



So there's a really good fit at a fundamental level and the A team packaging level, we would work with you and deliver the best solution out there. So our POCs that currently we are working with now are like a new sudden superstars in their company because they're like, oh, you caught us a really good deal. You got people who are working with us for us and not charging free engineers worth of time to us, basically. So that's kind of how our current engagement is with our customers is looking like fairly early in our journey, but really enjoying this level of engagement with our existing design partners.  



So on that note, I can send you a document with a fairly detailed document of an exact prototypical problem that we work on here that uses all open source data sets and pretty much if you can solve that, it would be very useful to us.  



Oh, that would be very nice. IBM happy to check that document and see if our product kind of might be adding value in that document. I do want to call out Michael that this is something that we from our side will be taking very seriously on whether there is like a really good fit or not. So no commitments in this first call. And again, as I mentioned, not to do a sale, but just continue to learn. But let me look at the document and I'll come back to you with something. If it's something interesting, I will definitely get back to you. Even otherwise, I would love to just stay in touch with you, learn more about what problems you're trying to solve and also share from our experience of what we have built out and stuff like that.  



Basically what you described does sound like a good fit to where we are, but let me take a look at the soumen and we'll follow from there.  



Fantastic.  



Sounds good. Let me take a look. Please share the document. We will get back to you in a few days and let's maybe set up a follow up call after that. Actually, how about this? That whether or not there is a good fit or not, maybe we just set up a follow up call to sync up on my, on the document itself. Right. I am traveling towards the later part of this week, but next Monday is something that I would have an opening on. Would that work out for you?  



1 second. Let me just check. Monday should be fine.  



Okay, so and I will be in Singapore on Monday. So let me just quickly take a look at the time difference. Actually, like mornings PST would work out great because that would be like thing up overnight and I can actually take some calls there. So like Chris, like 09:00 a.m. PST or something work out for you on Monday?  



09:00 a.m. PST? It would be twelve eastern. Yes, that would work fine.  



All right, sounds good. So we'll send out an invite to you for nine PST. And then if you can send a document sooner than later, it will just allow me some time to read through the document before the trouble.  



Sure. So your email is on this meeting, right. So I'll just send it to you.  



I'll be the point of contact just in case if you need anything.  



Perfect.  



So I will send it to both of you and we can settle it there.  



Amazing.  



This is incredibly helpful conversation, Michael. Thank you so much. And by the way, again, whether or not there's a good fit to work together michael, if there is any way that I can return this favor to you, like I can be of any help whatsoever, please do reach out anyw"
10881904688,True Fit,Chitransh Mundra (chitransh.mundra@gmail.com),,True Fit,500-1000,,,Multi Cloud,Yes,"Technology, Information and Internet",USA,Nikunj Bajaj,,NA,
10881903880,IronSource,Ilya Gerlovin (ilya.gerlovin@is.com),,XANT Inc,>1000,100 - 500 Mn,,AWS,Yes,"Technology, Information and Internet",USA,Nikunj Bajaj,,https://app.fireflies.ai/view/Meeting-with-Nikunj::CzyRyOOgnZ,"Hi.  



Hi, Ilia. How are you?  



Fine, how are you?  



IBM doing very well. You're based in Israel?  



Yes.  



Whereabouts in Israel?  



I live in Jerusalem.  



Oh, jerusalem.  



Okay.  



Very nice. Yeah, I've been there once. It was amazing to visit.  



Okay. You have a nice office out there. It's just a picture.  



It's just a picture? Yeah. You can actually see like when I try to do this, my fan actually starts showing up.  



I see.  



Cool.  



She wouldn't know because soumen of Chris is possible.  



That is absolutely true. Awesome. Yeah. Shane, you thank you so much for taking time today. What time is it then as well? Now?  



It's actually morning, so IBM fine. I sometimes work in the evening as well, but at evening IBM at home and there are kids around, so it's harder to focus.  



I completely get it, yes. Cool. Awesome. I'm glad that I caught you in the morning and I'll just briefly describe you the agenda of the call. I think we discussed a little bit on the LinkedIn message as well, where what we're trying to do is kind of build out an infrastructure product specifically focused for machine learning to begin with, and then eventually we'll get to other software part as well. And I wanted to understand just get like a DevOps view on what we are building. What are you doing currently in terms of your infrastructure allocation? What works well for you? Where are you facing any pain points?  



I just wanted to spend some time discussing that and maybe if time permits, then I show you our platform and get feedback on that or at least they're going to learn from your experience a little bit. That's kind of what I wanted to achieve in this call and I really appreciate you taking this time and helping me out.  



Okay, my pleasure. So at all. Actually, we have Arsh independent data science team. Okay. The Wops is actually responsible to provide tools and infrastructure and assistance to the whole LNG, but in my experience, data Science is pretty much independent. Okay. Now it's also important to note that I used to manage the whole DevOps group, but now I move to another role, IBM architect. And just in case, IBM working very close with Data Science Team, okay. Because Data Science Team indeed needs some operational experience to understand better how to build the whole process better, to improve time to market, to improve operational efficiency and stuff like that. So in my previous development, I didn't have much interaction. I mean, I know a couple of servers, but mainly they managed it all on their own. They did have some developers working close with Data Science Team.  



They call it JS Infrared Team, like developers who are dedicated to datta science, data Science support. And, you know, most of our infrastructure is built on this TerraForm rank on spot instances based on Kubernetes or not. We have big clusters, lots of data, but the interaction with data science was pretty minimal. Now actually what IBM working right now is to make them a bit more independent. Okay. Maybe even move to some kind of managed service like Sage Maker or some open source base. IBM not sure what you are familiar with, but I related seldom there is a light chunk something application didn't get to it yet, but it's on my list. So IBM considering different options.  



Okay, so basically what you're saying is that you want to make your data scientists more and more independent and to achieve that you want to kind of like switch to like a managed ML Ops platform.  



Basically yeah, it's a kind of MLS platform that doesn't require good instant knowledge. Like basically I would GSM for team to focus on developing the tooling and not on the iterational part. Somebody wants to make changes to model, to make new features, to make another kind of training to enhance the model. I don't think they need to ask for Jason for help to do it for specific model. They need to discuss the way how we can make it possible to enhance our process so they can do it on their own.  



Yes, absolutely. I am such a big fan of like empowering people to do something on their own instead of always depending on other folks to help them out. IBM so glad that you're thinking that way. One question here is so far when you have been this entire process of evaluating Sage Maker or Selden or whatever platforms that you're validating, what's top of mind for you? One is that the end user should not need to know the knowledge of infra have the knowledge of infra. Any other factors that you are considering when you're evaluating these platforms?  



Well, like I said, I look at the following cast. I look how we can make this work efficiently and fast. This is very important because the automation on a big call today and it could be a very convenient platform. There's lots of features, but if we can't serve Chris specified amount of time doesn't matter. Okay, so this is a tough point and it's actually a very important point because you know living situations.  



Yes.  



Okay. When you start working about making it fast, start things that fall out of this automatic. Now second point is I do want CTO make it. I mean the cost is important. Okay. Of course it's not a government facility, it's a business. But when I look at the cost, I look at the whole issue. It's not just the cost of infrastructure, it's also the cost of making changes, operational efficiency, how much time it takes to fix issues. Okay, so all Chris is factor thing in my understanding. Of course.  



Yes.  



So if you can run on two servers, small servers, but it takes a team of deliverers to support this infrastructure because it's fragile, doesn't make sense.  



Costwise makes sense.  



Factor in the cost of the development system. So as far as I'm concerned, the convenience for developers and the data scientists is also part of the cost because it affects the time to market it affairs, actually the time, the overhead that we are paying for the deployment of the material items, focusing on the material development of engineering or whatever it is. So basically, these are two factors that it can fit the bill in terms of technology. It should work fast. Okay. And there are all kinds of steps actually factored into this and the cost.  



Got it. Okay. And maybe one other question is how large is your data science team?  



I'm not signed on NDA, so IBM not going to talk about numbers, specific numbers. Okay. Okay. We can proceed at some point later on. I do want to talk a little bit about what you are development developing, sir.  



Sure, absolutely. By the way, I don't worry about NDA. I will show you exactly what we are building and ask you for your feedback. Okay. On this, we are building out a platform that can you see my screen? No.  



Okay, now it's better.  



Nice. So we're basically building out a platform which allows you to deploy your machine learning models exactly what you mentioned without the knowledge of the infrastructure does not mean that everything is black box to you. The way we are building this out is you still have access to everything that you need to know that might be important to you, but you don't need to know those things. Right. So that's kind of how we're building it out. So what you're seeing here is, because you come from a DevOps background, maybe I'll show you those parts first so that you quickly understand where it is running and what. And then I'm going to show you maybe the data science structure of it. So as a DevOps person, like, the first thing that you are going to do.  



So there are two views of the platform. One is what the DevOps person sees, and one is what the data science person sees. Right. So first of all, I'm going to explain you the DevOps view of the platform, where if you have a Kubernetes cluster, you can just connect your Kubernetes cluster with us. Right? And here you can configure a lot of things around if you have any monitoring stack, if you want to have some specific type of machines that come in your Kubernetes clusters. And how do you want to do access control? So you do that and then you install some health charts in your Kubernetes cluster. So that kind of connects your Kubernetes cluster onto our platform. Basically as soon as you do that. By the way, I'm assuming that you're familiar with Kubernetes.  



Actually, I think for starters, I would rather create dedicated clusters with the data sets. Okay. And then we can worry about integration because sometimes I'm not familiar you probably use customer resources or whatever you do to do your magic. Okay. I'm not yet familiar how it works behind the scenes, so I'm not very comfortable to put it into our main production process.  



That makes sense. Yes. So you could also do that, by the way. You could have a sandbox environment kind of a thing, or a sandbox cluster, which is dedicated for a data science team. And you can just give them access to that cluster that they can start deploying models there. Yes.  



And you have some kind of manual or TerraForm or cloud formation or one click solution. CTO make it easy, right?  



Absolutely, yeah. Always. Everything is TerraForm driven. So no problem. You basically spin up a cluster, and if you're doing that, basically you create like, maybe a couple of workspaces, which is like a namespace equivalent of Kubernetes and just give it out to you developers. Right? And once you do that, like, what your developers are going to do is they're like, okay, I want to go ahead and create a new deployment where I want to deploy a model, and I want to deploy Chris on this particular namespace, let's say. Okay, then all they need to do is provide the name of their model, provide where the model is stored, like the model Uri. It could be stored on S three. It could be stored on any model registry. Okay? And if they want to do fancy stuff like configure the CPU, GPU, etc.  



Where is the model running those things they can configure, but it's an optional thing and they just hit submit. That's it. They do that and they see that the model already is deployed and available as an API endpoint, that they can test it out. Now, obviously, when they went ahead and did this entire thing on the UI, a few things that happened is we take the model, we containerize it, we deploy it on a Kubernetes quester, we write the fast API code around this. So all of that we do, and then we expose the URLs where you can actually go ahead and play Arun with the model. I'll take a pause here to see if you have any questions.  



Lots of questions. You don't have lots of time. CTO add it then. First of all, when you talk about deploying the model, you probably expect to get it in some kind of format you are familiar with. Maybe it's TensorFlow. Right. Maybe it's not sure what you support, actually. Now, the part you are doing behind the scenes, you create what TensorFlow sep service for this specific model.  



So you're right that if you wanted to just do it as simple and experience that I just showed you, where we literally provide as a path of the model and we will work, then it has to be in a few standard library formats. It's not our format. It's not truefoundry. You format it's any format that like TensorFlow Python. Check g boost psychiatlearn, like these libraries where you would output a model, that's the format in which it should be if you wanted to do something a lot more custom, where you don't want.  



To have but you didn't have to specify any format. Okay. Like TensorFlow, you didn't select it. You just provided uri right.  



That's right. Yes. We figured that out, by the way. We figured that out. So you could also provide us as a hint if you wanted to. But if you just upload the path, we are also able to figure that out as well.  



So the next point is model is fine, but I want to do some kind of preprocessing.  



Absolutely, yeah. So what you could do is your preprocessing could be any random Python code that you have written, right? Technically, you could do whatever you want. So one of the interesting things that you can do is actually we connect with your GitHub repository, where you can have whatever random pre processing code that you want. So you can specify the path to your script and, like, your requirements TXT. And then, like, if you wanted, like, you know, you can also define, like, other packages that you want to install. And that's it. You provide us the code that you're running, provide that to us, and we will deploy that. So obviously, right now I'm showing you to deploy it from GitHub, but you could also do it from your Jupyter notebooks.  



Like, if you have written any preprocessing code there, you could deploy from there as well.  



What are the API requirements for this code?  



Big question. So the way this works is, like, you have written, let's say. So the way we work is, like.  



We allow you to write in Chris past API. That's what you're saying.  



Pardon me?  



You're working this past API? Okay, so basically I need to implement some kind of handler function, which is then called on each Http request.  



Right, right, exactly. So you implement whatever function that you want, and we don't actually tell you how you should implement your function. You could write whatever arbitrary function that you want, and then once you're done with Chris and you want to deploy it without the knowledge of Zenfra, you basically get these five lines of code from us, and you run this code. And this is the one that kind of containerizes your fast API code that you wrote above and deploys that on the Kubernetes cluster.  



Okay, so the way it looks to me, that basically.  



You'Re right, that would work as well. Or if you wanted to package your preprocessing code with your model actual invocation, you could do that as well. Both would work, because to us, it's like any Python function is something that we can deploy. Now, that Python function could be like a basic preprocessing function or a cascaded preprocessing function with a model invocation.  



Well, actually, there is lots of sense to separate the model seven. Yes, okay.  



That is exactly right. I completely agree with that. So it depends, if that's your use case, then it makes sense to separate it out. If it's a simple like Scigitlearn model, maybe you want to run it everything on CPU, you can just keep it together.  



Yeah, but then you need to manage whole GPU allocation between different ports. It's little bito nightmare. Anyway, my point is think about big payload, few megabytes payload and if you do send a lot of data then there are all kinds of things I need to think about. Like do you need to what kind of format I can send? Whether it's rest API. Or maybe I can use Jrpc. Okay, I'm trying to think how you address this problem.  



Yeah, so we go the gRPC route basically for any large data set that you're transferring over, like we use gRPC.  



So you basically allow to communicate between the reprocessing script and the motel through gRPC service?  



That is exactly right, yes.  



Okay, what about service inbound like.  



The.  



Processing scripts receive the request in JPC?  



Yeah, so we also support that. I think we also have some documentation around that as well.  



Okay, sounds good. And you don't actually do any manipulation on the request, right? Like you provide the service which would run my Python script, I need to read the data from the socket, I need to know what kind of format I support and pass it and do the preferences. Right?  



That is exactly right. So we don't interfere with your code, we don't interfere with your data, we only help you manage the infrastructure. Like deployed on an infrastructure? Basically, yes.  



Very good. I mean, it's probably not most of the companies want, but I'm happy to hear that. I know that lots of companies develop lots of storing groundspeed making it easier to arsh the request to ben in there, but no, we can't make efficient for all the use cases.  



Yes, we generally believe that developers know their code the best and they should be empowered to do their stuff that they want to do and we can't interfere with it and do better job than the developer itself. That's our general belief by the way.  



Unless you can do it magically better then I'm fine. But if you nat you best way.  



Yeah, turns out we are software engineers, not magicians. We let other soft engineers be software engineers as well.  



Okay, what I see you have good packaging of open source tools. Okay. You provide you can create the whole infrastructure for sharing the processing and model. Do I do some kind of additional services like model optimization? Whatever you do, tell me what else do you do?  



Yeah, so basically being able to deploy any training jobs, phone conference jobs, and any models that's like the core strength of the platform. When you do that, a lot of good things happen where you basically let me just pull up something which actually has data.  



Okay. They don't have much time, so I would like to focus on and training is completely different issue. We can take it separately because the requirements are different. I don't want to talk about technical details. IBM a technical person, I like talking about technical details and that's actually what makes it interesting, right?  



Yeah, for sure.  



You can do training like in training, I also interested to know whether you can do it fast, how you can help, you know, make things paralyzed. What kind of infrastructure you support? Do you support for example, all kinds of technologies like what AWS has to offer, including EFA, the electric adapter which doesn't need to transfer data between big nodes. When you do train or do it not through GCP IP stack, do it faster. What are you saying?  



I think all the optimizations that you're talking about, it's not like all each of the optimizations that you mentioned are supported on the platform. We are an early stage startup and we have built out whatever we could build out in the last eight months. But a few things that we nicely support are like every service that you have is basically neatly version controlled. So you have all these different versions of the service that you can always see if you wanted to revert back to a previous version of a service. It's like very easy to do that. You actually just click on redeploy and you redeploy the previous version of the service.  



When we do this, we actually take care of a lot of good things like software engineering practices that when you revert, you can revert the version of the model along with the version of the code, along with the version of your secrets. So that actually helps everything in one unit, right? And then everything that we do, like we follow a lot of GitHub principles there. So even if you're doing something from the UI, we actually always generate like a YAML file that's always version controlled and saved on GitHub. So some of these things we do and we also allow you to do like your model monitoring so that you can check the performance of your model, you can check the performance of your data, you can compare the performance of your data with your training data set, see the featured drift that is happening.  



So you do a lot of these good things on the platform and you can also perform like visualize your raw data as well. So some of these things we have built out, we also do a bunch of experiment tracking and obviously are moving really fast in the interest of time, but just quickly showing you a lot of these functionalities of the platform. You can compare all of these different experiments that you have been conducting, compare the graphs and all of that basically.  



So it's actually Qovery cloud to the bottom okay. I don't want to test it. What do I need to make test? Is it possible to run it inside our VPC without expressing all the data or some of the data outside?  



Yeah, first of all, most of our enterprise customers use the platform like, that where the platform gets deployed within their VPC. None of their data ever comes out of their platform. So basically both the data plane and the control plane can be deployed within the client VPC. Right. That's how any long term engagement works out. Now, typically CTO try it out. I think it's like some people just don't want to install on their own cloud, like an external platform. So you could use our public cloud that we have where you can use your public data sets to just try out the platform and get the developer experience. And if you like that, then we can do the further work on how you install on your machine. It's very straightforward. It takes maybe an hour and a half to do the end to end thing.  



Well, I don't mind trying some kind of simple model and just to get self of it, but you need to understand that in order for me to run it on our specific use case, we have two options. I either need to run it in house, like making sure that no data leaves our infrastructure, like not metrics, not logs, not anything. Okay. Or I need CTO connect you with our security and by the time you complete the whole iteration process and I get clearance, you probably are the company.  



Yes. I never want to go the second route. I always choose the first route. Everything gets deployed on you cloud, and we built out a lot of infrastructure to make that process super efficient, by the way.  



So I'm fine with it. I have another company, okay. And due to some technical reasons, they couldn't guarantee that they don't export anything and this happens. Chris one is in July.  



Oh, wow.  



Okay, so young Simon is a teacher now. I said, Guys, if you can maybe cut down on functionality and get some kind of simple version the way I can deploy it, like I managed development. Not for nothing, right? I can make all the hard work by myself. I can do deploy, I can manage Kubernetes. I can do anything that's necessary. Okay, you don't need to do anything. But I have to leave everything inside our decision. Nothing needs our vision. Okay? If you can do that like that, we can move very fast, at least to the point of decision. I can do all the evaluation. If not, then you have to talk to our security, and this is out of my hands.  



Yes, I've heard that before. And like, we have chosen it out where we made it very easy to deploy on your cloud, on your VPC.  



Okay, very well. So I suggest you just send me maybe some kind of information, how you get that account or whatever. I will explain Chris little bit and if I see that it's good, I will try to deploy to now VPC without specific use case and then we can talk about performance. It's very important how CTO make things fast, make sure that everything works you properly. That's what you take all the networks from. You can measure the precision part and shutting part and outbound network connections and everything can be seen, right?  



Yes. So I have a question for you that I think our process is generally like we try and understand your use case a little bit better. What kind of models are you building, how frequently do you deploy, how large is your team, what's your tech stack looking like? All of that. Right, so I need to understand some of that before I can commit to engaging with you all as a company as well where you try because as a startup our bandwidth is limited and we only work with a few companies and if we see that we can really add a lot of value in your tech stack only then we start the engagement. So the platform actually is not open for self sign up. It's actually like in private beta so you can't actually go and try out the platform.  



Only those companies get entry to the platform where the use case is a great fit. Now you mentioned that you are not okay to share anything without signing an NDA. So I think a good next step instead of me sending out the access to the platform to you, I think a better next step would be that we sign an NDA, we get on a call where you explain me a little bit about your specifics in that NDA. Obviously like I state that I'm not going to share any of your data outside and then if there's a good fit then I open up the platform for you that you can test it out. That's what my proposal to you would be.  



Fine.  



So do you have a specific format of NDA that you would like to share with me?  



I will shoot out the mail to our legal and let them take care of it from here.  



If not, I have a specific format of my NDA and IBM happy to share with you, which is a mutual NDA.  



Need to go, it shouldn't be done. Probably a week and we are done.  



Okay, sounds good. Then let's do that and then let's set up a follow up call where we get through some of the specific questions that I have and then I'll open up the platform CTO you is that okay?  



You know that arrestors are actually in the process of bank merged this Unity, right?  



Pardon me?  



If you did your homework, you probably know that eversource merger with Unity.  



Okay.  



You're familiar with unity? Unity where is the chart? Unity. You are not familiar with it.  



No, IBM not.  



So they are not MercedesBenz Unity and it's big practical company in the game industry and they also have their own data science. So the time is important here.  



100%. Yes, time is absolutely important and that's exactly why I am always like I know that it's a large company like this, like mergers and acquisitions and all that good stuff is happening. But remember, like for me as a founder of a young startup, the most important thing is my time. And I want to make sure that my time is only getting spent where I can really add value to the end user of the product. I will not commit to any customer without understanding the use sales. Like if I know that I cannot add value, I don't want to work with whichever customer it is. That's like the bottom line that I draw.  



I think it's a fair thing to do it's Qovery. Good thinking. So I'm personally low maintenance. Like you give me access, I can get things around, especially if you have some kind of documentation. Okay. I normally can like the sales done, I look at the package, something didn't work for me. I vivek PR for them just to close the gaps. I know my way around. Okay, but I don't understand your point. It's a very good point actually. I understand that this is very critical for you. I am fine with it if you want to do it this way. I'm perfectly fine with it if you want to give me some kind of answer. CTO poke around because I know online source is very hard casting, okay? Especially for startup. They are very focused on efficiency both in terms of operation and cost. Okay.  



And if you have access to some companies which have less mature infrastructure, not so technical keys, you probably can get faster deals and maybe with higher margins. Okay. The answer I have to tell you, it's not going to happen. Okay. It's not up to me. Okay. You can have a great technology, but we manage most of the open source projects on our own. Like we don't have managed car clusters. We are mainly using open source technologies and we know how to manage them on our own. Okay. So I realized it's very hard sales, it's probably not most rewarding. It's a great exposure because they're a technical company and at least in Israel is kind of validation of what you have is great.  



Okay.  



But if you have some other deals changing, you better focus on them because like I said, we can get faster deals and better mins.  



I see. Understood. Yeah. So honestly, I will be optimizing for my learning again as a startup, early Shane Startup Founder my product is not fully ready. I work with companies as a design partner, like deliver solutions and solve their problems. So I do optimize for learning. But again, as I mentioned that even. For learning. If I know that there is no value, that if a tech stack is such that our platform adds no value or very little value, then I would not even try to start an engagement right now because I would rather come back to you one year, two years later when I feel like we can truly add value. That's the mindset in which I generally operate.  



Absolutely agree with at least two companies which actually helped grow. Like one is Spot, which was later achieved by Neto. We are one of the first customers and they create a telephone provider for sports just because of us. Okay. Because we said if we can't manage these terraforms, we don't need it. Okay. And another is our Spikey ball as well. It's very interesting technology. What is it? Also they started working with them when it was very small company with promising technology. Now it's much more mature company. Okay. But they are one of their instructors. So IBM always looking for new technologies. I have also another example, but they are not yet so known.  



Okay.  



I very much like working in startups. I think it's a different kind of speed. You get to talk to people, direct access to people who make decisions. You can make things fast. Normally you work with people who are patient about what they do. Okay. It's a lot of fun working with static.  



Yes. Nice. Awesome. I am so glad that we connected India. And I think like the way you're thinking about the platform and like working with companies. This could be a nice collaboration if you decide to work together. If not, no problem. IBM. Anyway, glad that I connected with you. What I'm going to do as a next step is ping you on LinkedIn to get the NDA started from your site. Let's take a week, let's get the NDA sign and let's get to a proper call where we discuss more technical details. If that's okay with you.  



Okay. Else here and the conversation. Okay. Okay. By the way, if you want to talk about technologies, pinpoint anytime pink thing, you are LinkedIn.  



Anytime.  



Anytime. If you want to talk about technologies, if you want to discuss soumen technical problem you encounter, probably maybe I can help somehow.  



I would really appreciate that. So what we are trying to solve is not just from a machine learning perspective, but even from a DevOps perspective is extremely complicated, I feel like. And like anytime someone like you who is experienced in the space offers to help, I will shamelessly grab the help that I'm getting from you.  



Okay. Maybe I will not be able to help. But IBM familiar with the all the time what data scientists. Shane tracking me. IBM very hands on.  



I love it. Cool. IBM going your offer and like gong to come back to you with some specific problems. But I could get your help. And besides that, let's get this ball rolling.  



Okay. Very well. Thank you very much.  



Thanks a lot. Have a good one. "
10881903880,IronSource,Ilya Gerlovin (ilya.gerlovin@is.com),,XANT Inc,100-500,10-50 Mn,,AWS,Yes,"Technology, Information and Internet","Santa Clara, California",Nikunj Bajaj,,NA,
10881877574,BuzzFeed,Peter (Yuchun) Wang (py.wang@gmail.com),,buzzfeed,<50,,,AWS,Yes,Software Development,"Palo Alto, CA",Nikunj Bajaj,,https://app.fireflies.ai/view/Nikunj-TrueFoundry-Peter-BuzzFeed-::93UeMNuZTJ,"Hello. Hello.  



Hey guys. Sorry, running two minutes late. How are you doing?  



Hi Peter. I'm good, how are you doing?  



IBM good. Putting baby's sleep?  



Yes. He started crying. I just came to the meeting and then he's was sleeping and he started trying, so I just got him. Yeah.  



How old is he?  



Four months.  



Four months? Wow.  



Do you have kids?  



I have three.  



Oh nice.  



I have three kids. They are the youngest one is five. Okay. How CTO baby.  



Make sense?  



Your first one?  



First one? Yeah.  



Oh cool.  



Yeah. That suddenly keeps us busy.  



Well, Qovery, nice to meet you. I feel like today I was actually going over the truce on to the doc you sent over. I think today could be a good introduction call. The next time I'll bring my head of machine learning on the call. His name is Archie. I think that'll be good because I just want to compare notes with him on his take. We definitely have challenges like every company that is investing in ML, but I wanted to hear from you guys directly.  



Absolutely. Yeah. Happy to share some context. And thanks a lot Peter, for opening up the floor for conversation. Maybe I can give you a very brief introduction about myself and true Hungary to begin with. I would learn more about you and like you mentioned that you're having some challenges and I would love to get your perspective on that. I know you might have more details that you would probably share about Chris. Let's kick this off. So speaking a little bit about myself, Peter, I come from a machine learning background myself. Have you heard of the Facebook product called Portal? Yeah. Okay. So I actually was leading one of the conversation AI efforts at Poland. By any chance do you use the product?  



I have not. I thought about buying one and I've seen people on the receiving end when calls tracking. What do you think of the product? Do you think it's worth buying? Are you using you're not using that right now?  



I'm not using it right now. I have it in the other room, one of the photos. I like the video calling feature quite a bit. I think it's really nice even in kitchen when you're calling family and stuff, that is very helpful. I don't think we did a good job at the virtual assistant bit of it. I think it's just too shallow a product in the virtual assistant domain. Honestly, the part of the team that I was working on, I feel like that's the part that has not done a good job overall. But the team that I was not a supervisor at all, that is like video calling. I think that's quite nice.  



I was driving one of the conversation I heard there at Facebook and prior to that I worked in recommendation system space where I built out personalization algorithms for the ecommerce industry as part of a startup called Reflection. And at Reflection I spent first two years building out these models and the last year and half building out a horizontal platform for the company, like machine learning platform for the company because we got like multiple teams that were building machine learning models and we realized that we're duplicating a lot of efforts so centralizing that through a platform made sense. And that's part of the story of what we are building at Proof only as well.  



Like, you know, my learnings from Reflection, what worked, what did not work, and then what stark difference that I saw when I joined Facebook in terms of the platform, that's part of what we are trying to capture in our efforts at what we are building at Truefoundry. You I see.  



When did you start truf?  



Andre it's been over a year now that we're running through Foundry. We are fairly early stage, like maybe 15 people in the team and seed funded, working with few enterprises currently as design partners. So that's where we are in terms of the journey and realistically one of the goals of what we're trying to build peter at Truefoundry is we believe that it's really hard for any external machine learning platform to just take away the ML platform work within any company whatsoever. Companies will always have custom use sales gong, tail of use cases of machine learning that's really hard to just kind of automate with a bump size, fit all type of product.  



So our Gmail is that we build out a platform which makes it really easy for internal platform teams, internal intra teams to build on top of essentially so a lot of common things that everyone tries to build out. We support that out of the box, but we provide enough flexibility that people could build whatever they want on top of it and kind of scale up their platform. So like, you know, if you're thinking in terms of like the optimization of cost within the company, if let's say we are two infrared developers are able to serve a team of like six to ten people in machine learning. Ideally those same two developers should be able to scale to whatever like 25, 30, 40 people in your machine learning team. That's kind of the goal that we're looking for.  



You can scale up your machine learning team, but you don't necessarily have to put in enough resources to continue building internal dev tooling and scale with them essentially.  



That makes sense. That makes a lot of sense. We have a relatively small, fairly small machine learning team, like data scientists, handful engineers, and we've also found that and we just started a team, I would say less than two years ago.  



Yeah.  



So it's not a long running team. And our use cases are I would say there's definitely a cluster of use cases around recommendation for sure and recommendation could be on the lines of stories, shopping products and also specific types like quizzes as example, because mostly quizzes on the key offerings that we have and it feels like a long journey for us, for sure. So, for example, even it wasn't until this year we started offline testing, there's so many things that happened even to get CTO this point, even the speed, which the number of models I think I remember speeding Chris number doc chris is very common challenges, right? The speed of integration, the monitoring piece, the observability piece itself. Some of it is more people, soumen definitely but some of it definitely is tooling as well.  



Which product service is ready for a test, whether it's the app home fees, for example, whether it's a module for shopping. Anyway, there's quite a few actually just even talking to you remind me of a slew of challenges.  



Yeah.  



And there's some other initiatives that we're working on that are not just about model specifically, but more leveraging, but more building audience segments for advertising. That is one of the newer initiatives that we have. But they're not new use cases in the space, right? Look like audiences based on what? Coercion data, optimizing performance during campaign and so on.  



I see. So I guess like one question here is like, you talked about a few challenges in the space, which I think you and I both understand that these are fairly common challenges for teams trying to build their machine learning applications, build and scale right now. And of course you have done a bunch of work, your team has done a bunch of work to get to where you are already solved a few of these challenges along the way. What are some of the new set of improvements that you think that the team is going to be investing on for the next six months to one year? What are some of the top of my challenges currently?  



I think definitely speed of testing models. Okay, one. So how many models can we test? I feel like the rate of iteration on the models can be faster. We have a set of them. I'm actually looking at a list. We have a list here. Different versions of them start out with a very near use case and when expanding the use cases, now we have X number of use cases. And then there's iterations v zero v, right? And so on so forth. How quickly can we get to that duration without adding people? Linearly one, challenge second, there's a few IBM just thinking about one of the key ones. There's some issues not too related that IBM thinking about right now, but IBM sentilink them anyway. Like in the space, a lot of failures are expected, right?  



And us getting comfortable with the failures, understand why moving on and also explaining to our own teams and external teams why we Gmail what's next and not feeling like we go from iteration CTO duration, the anticipation of like we should continue seeing positive results is just not realistic in the space.  



Yes.  



And actually that's hard for sometimes for outside of tech to understanding the space. Oh, you run as many models, you spend as much money, but net from zero to b two, you only got 43. Modeling doesn't work in this space. And IBM shameless because also we're budgeting for next year. So I'm actually figure out the best way to maximize iteration speed and quality while minimizing the cost across product service areas. Because we have six different brands, even the brands are now less relevant in the space as they use case themselves. But even the underlying data, we do have some work to transform, say, complex data. We're still transforming into a common datta model that Buzzy has because we just acquired complex a year ago. So there's an underlying work needs to happen before we can even provide service to them.  



Right?  



Yeah. I think you can relate to a lot of these Qovery similar problems that we have. Another one will be, I think, the amount of tests we can do, which is both human as well as technical. I'll give you example. We're testing different audience segments both on scale. How many audience segments can we create relevant to business? Which ones can run? Because it depends on the campaigns that are available to us, how many feedback loops. So if you like right now feel kind of slow, get them like, hey, this on a segment and get tested compared to our current baseline, get results back after couple of weeks, what's the next one? So it's kind of very not as if we have just a massive model here. All the tests, we could just run.  



Them.  



And then compare and then say, what's the next sep? It's not production. No, it's not productized yet. So it's very slow adoption problem right now to get it, this is more along the audience segmentation initiative.  



Right?  



What else? Is it helpful so far? Absolutely.  



Yeah, totally helpful. And as you mentioned, there are a couple of I think number one and number three are primarily tooling challenges. And number two, as you mentioned, is not necessarily a tooling challenge, but in a way, Peter, it is. And by the way, it's not a problem that we necessarily solve. But I feel like I have heard similar problem from some other leaders. And a lot of it, Peter, boils down to visibility. Right? Like, for example, the team is conducting multiple different experiments. Some are bound to fail. Right? But if there was a good way of creating visibility into what kind of experiments got conducted, why did we take certain design decisions and what was the output of that?  



Even if it's not like positive output of every experiment, I think that itself helps people narrate teams, narrate a story that, hey, these are the things that we did. This worked and this did not work. And this is the learning that we have. But with machine learning, a lot of times what happens is that the experiments that we conduct are very siloed. Like, let's say a data scientist or an ML developer is kind of training a few models. Learnings don't get transferred from one person or one team to the other and stuff like that. I think that's the thing that kind of creates this feeling of like, we did a lot of work, but nothing came out of it kind of thing. So that's one of the things that I've heard some companies trying to adopt to solve this problem.  



And to be honest, I think there is some level of truth to it as well. That I think we can be a little bit more efficient with our ML experimentation if were able to capture learnings and kind of apply from one project to the other, which does get dropped because of like lack of tracking. Tracking. I don't know. By the way, I might be overstepping it here. I don't know, maybe you all have built out a lot of visibility related tools or maybe you haven't. I don't know what's the current state, but that's just like a common problem that I've heard.  



I would say we have some, but I think the way we I mean, I think within I would say a small subset of us within the team understands where we are, has high visibility and understanding of where and why, then the majority second like two degrees away. They don't understand it. And it's actually how you tell the story needs to be humanized in a way that isn't just about zero. I mean, we have great documentation on things, which I think is very helpful. But even as I portrayed, I say if I'm trying to finance it's just an example, right? What they care about is time to impact and cost to impact. I chad to explain from that point of view, say, okay, so if you've done all this, when do you think we can improve the conversion rate for this in a meaningful fashion?  



That's really the army for. And what's interesting is we're not quite there yet, but as you know, it could happen. It's not like we can plan for it. But you've got CTO keep going until it happens, right? Is that kind of consistency. Of course, part of what we're trying to do is break shorten the cycle.  



Right?  



And I think that's a very true thing. How do we shorten the cycle? There's a few things actually even design is one piece of it, which I did mention before. Design meaning actually because we have many recommendation services and we are introducing algorithm into it that's different versus curated. We shane to change them when they coexist. What's the pattern that we feel we can coexist and then right across multiple. So it's not like we're rethinking this again every time it adds more time CTO it. So there's almost like a predesigned process of saying we're going to do recommendation for say, content. We have X number of modules that all do very similar things but in different contexts. Apply the test to this specific set subset. Okay, let's make sure the design patterns are similar enough we can just deploy to them.  



Right.  



So I get a plan ahead, way ahead. And engineers and designers talk about designing a picture is good. Then you how do we integrate, agree on that too. How do we monitor, how do we value the results, how we can be exposed to and so on and so forth. It just takes a lot of coordination. Today I feel we have a timeline cycle. We feel it's way too long. We have different phases from thinking about the problem itself, about the ML development itself, the integration itself. Together it's I would say maybe four months per site per space for problem space.  



I see.  



And integration takes the most right now and integration piece. If it's a new space, I understand first time new integration, but really hoping iterations are exponentially shorter. Initial ramp up is so long and sometimes the roadmaps don't match. Right. Because they're working across different teams. The amount team is working with apps team, the app team is not ready yet. So there's misalignment issue that's more people than tools. Anyway, I'm just sharing being transparent here because I do think that we can do a lot better. Yeah, for sure.  



Thanks a lot for sharing this Peter. Honestly, I would love CTO do a deep dive on this with you and also kind of showcase a little bit of what we are working on and just exploring that. If there is any way that we can potentially help to kind of reduce this cost of experimentation and hopefully increase the speed, that's something that we are very actively focused on. So maybe that's something that we can do a deeper dive into in our next call or something when we meet together with Archie.  



Yeah, let me leave Archie on this and share the doc that you have. This Shane a discovery call together with him, right?  



Yeah. And meanwhile, Peter, in today's call I wanted touch base one more thing with you, which is I noticed from your LinkedIn that you have been like mentors, mentor, advisor, investor in a lot of companies as well. And I was doing it based out of New York area. Right.  



Actually I moved recently to Southern California, but I do go there very often, every couple of weeks. I'm in New York three weeks ago.  



Interesting. By the way, around there. Do you know Mike Boufford? Mike Buffer, michael Buffett? Like the CTO of Greenhouse.  



I don't. I've seen him. IBM aware of who he is, but I don't know each other.  



I understood in this journey we are getting a few key leaders in the space on board with us as angel investors and all different capacities basically. Right. So of course at this point you don't have enough visibility into what we are doing, who we are, what our team is, et cetera. Happy to dive into that a little bit, but at a high level. I wanted to check if this is something you're still doing, like working with startups on parallel personal sense.  



Yeah, I'm still doing that.  



Okay.  



I think it's actually very helpful. Personally I find that pretty important because my world is a bubble and the bubble has a lot of blind spots and I have constraints within the bubble. So I think that's why the ventures, the startups advisory, it's actually my way of understanding a what a new approach is to existing problems, which I see a lot too. Are there new problems? Actually different we're looking problems because that's actually a lot of times where innovation happens refrain and that's when if I'm not out there, it's not going to happen to me. I have to go out there and talk to people and work with them. Then I see what's going on. Like first person on a first person basis. So I really do appreciate those. And you're in an interesting spot for sure. This is the space.  



I've seen so many challenges.  



Right? Yeah, I like to believe that. Right. So far we are on the right side of the history. I think there's going to be a big change that's going to come in the space in terms of machine learning, adoption. And I feel like adoption really is contingent on how easy you can make it. And clearly right now it's not easy. It is actually meant for very savvy teams who are able to build and scale ML platforms and we need to make it like hundred x easier, not ten x easier to really take you to mass adoption. And it's gong to happen. Like that is for sure. Question is, are we playing an important part in that journey or not? That's the main question.  



Yeah, exactly. And when you were at Facebook, what did you see that you felt like, okay, that was a good approach. The way they approached it is something that other people can't actually much broader, applicable to a much broader community versus just within Facebook. What are some of the lessons? Curious for sure.  



Yeah. So at Facebook, basically what we had was they were almost like if you think about the overall ML development lifecycle, there were different tools that are built out for each part of the lifecycle. Right. I think that's more understandable that you have a feature store, you have a distributed model training component and then you have a deployment component. Right. You had all of those three. And as you would imagine now some of the interesting things that I noticed that Facebook was that once a model is deployed right. Once the model is trained sorry, not deployed, trained. Right. After that deploying that model literally used to be like one click. Like how you get that model trained model to be deployed as an API endpoint, right? And then deployment does not necessarily just mean you have a fast API around the model itself.  



There's so many things, interesting things that happen when you go deeper in it. For example, is your model getting deployed, that is, getting deployed, scaling automatically in different regions that you might be serving traffic with that particular model, right? When you deploy that, are you logging every prediction that the model is making and then are you able to connect it with the connected dots later with the actuals that you might receive? So for example, in recommendations, maybe you want to track your clicks that people are doing, right? Can you do that automatically? Can you track the performance of the model right after that? Right. And then whatever things that you received, can you quickly feed that in a retraining loop?  



So these things were sorted out so seamlessly at Facebook that as a developer, I could actually build and deploy probably five models in a day and like five really high quality models, right? When I talk to other people frequently the timelines that I hear are in the range of like weeks to months for a single model that people are doing. And even then I feel like that the level of maturity that we saw is not available. And Peter, one of the learning that I feel like sucks the most after I tal to people is most people actually don't realize what they are missing. And that actually is by the way, is a risk to our startup as well, to be quite honest.  



But when I talk to people and they are like, okay, we take like three weeks to deploy a model and they seem to be comfortable with it, they think that's the status quo and that can be done. But realistically they're not able to expand to other use cases because their developers are busy training and retraining and maintaining the existing models, as opposed to being able to expand to new use sales or run new experiments, essentially. So that's one of the other things that I had, and by the way, including myself when I built out the platform at Reflection, I thought, oh, we have achieved a lot in terms of from where were, but from where we could be was the visibility that I was completely missing. So that's one part of it.  



The second part of it was around the training component of itself, where I felt like Facebook itself did not do a very good job at. And that's the other learning that I took. For example, frequently Facebook would train models and like the resource allocation was extremely sequential. So I have myself waited for my models to come back, train like for 8 hours, 24 hours, 48 hours, which I feel like for a company like Facebook should be, you should be able to solve that problem much faster. Ideally, like put parallel computation and stuff like that, which I don't get a very good job at. And that's something that I'm taking a lesson learning that okay. When IBM building the platform, you're going to solve for that as well. One from each angle.  



I guess one of the other things was that no dependency of machine learning developers on the India folks until a certain limit, and they did it with context and not controls. I'll explain what I mean to say frequently, like, companies like machine learning is compute intensive, right? So you want to get more and more machines to be able to train and deploy your models. Now, machine learning developers are frequently broke on intra folks to either provision the clusters for them or install certain new libraries or do a lot of develop work around that. They automated all of that. Now, once you automate all of that and you empower the developer to do whatever they want to do, they can actually increase a lot of compute cost.  



So they did a very simple tweak to this automation where every time you train a model or deploy a model, it shows your cost number, like a dollar value, that this training run that you are going to spin up will roughly cost us $4,500. That's it. They're not stopping you from doing it. They're just giving that context to you. And that itself made us as developers, like a lot more sensitive that maybe I can run with a smaller amount of dataset when I'm doing the experiments, because just to realize the typo in my code, I don't want to run a $4,500 job. Basically few key learnings that we got from there that we're trying to incorporate one by one in our platform. And I feel like the people that we are working with really appreciate some of those features that we.  



Are building in that's key, actually. Yeah, those are like little fine. Exactly that's kind of developer experience.  



Yeah.  



That's awesome. Thank you. Let me bring in terms of scheduling. Did you have a calendar link? Is that how we got connected?  



Yes, I can share my calendar link with you. Pardon me?  



I will make that easy because oftentimes Chris is when things break down, no different than Chris integration point. Calendar is one of the worst integration points between people.  



Totally. I pinged it to you on Chat here, but I'm going to also ping you on LinkedIn or email or wherever we are connected.  



Just email me. Yeah.  



Okay. I'll just email to you and then maybe we can set up a call with RC together.  



Yes. Okay, sounds good.  



Awesome. Thank you so much. Have a good one, Peter. Thanks. "
10881876910,SupportLogic,Krishna Raj Raja (krishna@supportlogic.io);Shuo Chen (shuo.chen@supportlogic.io);varma.bhupatiraju varma.bhupatiraju@supportlogic.io (varma.bhupatiraju@supportlogic.io);peter.sparks peter.sparks@supportlogic.io (peter.sparks@supportlogic.io),,SupportLogic,,,<10,GCP,Yes,,,Nikunj Bajaj,,NA,
10784174762,You.com,bryan bryan@you.com (bryan@you.com),,You,<50,<10 Mn,,AWS,No,Software Development,"Warszawa, Mazowieckie",Anuraag Gutgutia,,https://app.fireflies.ai/view/TrueFoundry-You-com::NCABRX06Hb,"Hello. Will you leave in a moment? I can try. Okay, yeah that's fine. If you can just ensure okay. I don't miss major things. I think that should be good. Yeah, they are in sep. Okay. And they use database quite a lot. You have to keep on answering questions and tell me if they are missing major things like as well like a vivek is stuck in bus. This is very unfortunate you're not able to postpone this because this meeting already got moved once and soumen people are not available who are joining from their side. Is there some background list you.com CTO is there? It's a big company, they are kind of trying to build a private search engine and then Infra and ML head people are also joining. The best will be kind of moved this, but I'm not sure.  



The thing is like if they want an overview of the infra IBM just trying to figure out if there's anything they'll be able to show. Is there anything we generally show? Is anyone aware?  



So generally we saw the overall walk through of the platform and that vishal keeps on explaining only majorly in office. We have sold like a few things as well. Do you have anything like I can think you may be on slack one or two things from ETV synopsys like if just open those letters.  



Let's see.  



I really want this. Hello? Hey. Hi Brian.  



I got Sam on here.  



Hey, Sam. How are you? Hey. Hi Nat.  



Hey, sorry for me. How's it going?  



It's gong good. Great to see you. So Brian, myself and a vivek, we had chatted earlier. So first of all thanks again for taking time for the call and I know this meeting got postponed a little bit so we wanted to kind of definitely do it sooner with me I have Srihari and soham leads for us. He was a part of the Cojack platform team and Sierra he was at postman. A vivek would also join in but Abhishek is the CTO. He was at Facebook. He's just in the midst of travel and it got delayed. So somehow internet connectivity is the problem. But we should be able to kind of take you through the platform and give you a walkthrough. So Brian and I spoke take a note of you.  



Like I sent out some materials sam, that did you all get a chance CTO take a look at it at all?  



Yes, so I did get to take a little bit look at the platform. It seems like it's a managed Kubernetes based system for orchestrating machine learning experiments and pipelines and such and organizing machine learning teams.  



Okay, that's great. Like that's a really nice description already. Sam. NAD, what about you? We're not able to hear you. At least I'm not sorry.  



Hard. Yeah, no, I mean it seemed interesting. As Same said, I think we're currently using ML flow and I would ben curious on what you see is kind of, I guess, key differentiators there and why you've seen people change from the other tooling that exists. And then also maybe some very low level questions, but for different types of cloud support and stuff, there is Azure and ML Flow integration or say, the docs, like what that kind of flow looks like in terms of setting up.  



Sure. Cool. Sounds good. Thanks. Brian. Did you have any specific agenda in mind in terms of already based on the discussions internally or do you want to kind of set the stage and then I can go ahead and we can kind of show a high level walk through of the platform and then answer questions that Sam and that have as well and see if there is a potential skill that we could try for you.com? Yeah.  



I think to echo what now is saying. The biggest question I still had after we had our initial talk was like how it would fit in to our flow and or like which parts of things it would replace and what exactly are the trade offs on doing that replacement. Which parts of our pipelines would it actually be a substitute for and what are the trade offs between what we have now and making that substitution? That's pretty much it, yeah.  



Got it. A few questions for madam. Sam is like, currently, like the India management in terms of not only for ML but just outside of ML, since everything is based on Kubernetes. How does the India team kind of manage this in terms of the view of the services, the view of different models being deployed? We love to kind of get a two minute overview from either of you.  



So it's not all Kubernetes based. We largely use Databrex as an MLAP platform, so that runs a lot of our pipelines. We use the Databricks manage ML flow for our model registry. We do all of our evaluation through data bricks. We have kind of like automated pipelines that for retraining. We don't really shane anything currently for module drift yet. So that is basically like from data to training to registering a model. All of that is done in Databricks. And then on the other side of the registry, we have our actual microservices which interact with the ML Flow model registry, pull models, set them up in an API, and then those are hosted within Kubernetes. So basically one end of the registry we have Databricks. On the other end of the registry we have Kubernetes.  



All the inference time, like the production serving, but it downloads from the registry. That is, whatever.  



And I september Brian mentioned that you do the training on the GPUs. Like, what about the inference? Like, is that also on GPUs or is it on CPUs?  



All CPU at this point.  



Okay, there's one exception.  



There's one exception, but that's our little special stuff, like, which is our large, like, deca Bellia and parameter models. Those are arun on a big GPU.  



Got it. Cool. One other question which I also asked Brian last time was in terms of the current state of the employee. In terms of ML or in general. Cuban Dismanagement. Is there anything that you feel is a bottleneck or you would want to probably be in a better state or where you think of employing. Say. Resources from the team. Which you rather not. And want them do something else? So we'd love to hear that once.  



So you mean in terms of, like, shipping models or what? Which part in particular?  



In terms of maybe the managing of the enterprise? Well, as in terms of shipping models. Both.  



Yeah, that's pretty easy right now.  



It's been mostly okay, honestly. I mean, I think the pain points I think we've seen are having total confidence that we can have a sense of what production level quality will be online versus offline. And then maybe one we haven't hit so much. But I think that's kind of intermittently benefiting some sense of, like, performance or latency. I think, like in a production setting versus offline that people kind of assume, oh, they run it on their, whatever local machine that is, thousands of dollars, nice hardware machine they use, give it all their CPUs and Chris, and that's not really what we do in production. We have much better weight VMs and stuff, and we have in the past, little less maybe with machine learning models per se.  



But related things seem like some people be surprised, like, oh, I shipped this code and it took 15 milliseconds on my local machine. I was using eight cores. And then when you ship it to Prod and has fewer cores, it's slower. People don't remember. Right. Like, oh, I have to actually I should actually benchmarking on the resource constraints we have introductions, combined with the fact that I think this is where there can be some there's a little pain, I think, in the managing of services that not everyone on the team is super familiar or comfortable with. Kubernetes in theory, it's all clearly documented in the code.  



Right.  



People don't always know, like, where to look or what the implications are of that. So I don't think people get like, oh, we're specifying different. Here's how you find the setup and resources for this service versus this other service in real life, and here's how you might validate you food against that. So if there's one problem, it's been not a huge one to Sam's point, but it's when we had in the past, and I could see popping back up if and when we hit things where we're really resource constrained in terms of the latency of a model or we're CPU constrained, I guess, in particular.  



Okay.  



The only other thing that I would say that I would add to that I think that's pretty accurate is one thing that we just decided not to do that would be interesting to see if your platform supports would be like shadow mode for models. So very easily being able to say we've got three different versions of the same model, all our candidates to be the next production model. I want to run inference with a production model and then also these three candidate models in shadow mode and not have any impact on our latency, but also be able to get online metrics for a week or something for shadow models and then be able to be like this one actually did the best online promote that one.  



Got it. Cool. I think that helps quite a lot. So what we can do is if it's okay, Sam, and that I can give you a two minutes overview of what we are building and then maybe the best thing would be to walk over the platform at a high level in terms of what are the things and pose that we can answer two or three questions that you pose. One is with regards to the shadow mode. Second is with regards to the performance and latency and how it can potentially be optimized in the future in terms of the way we are thinking and other questions in terms of how it can indicate with the existing structure as well. Does that sound good? Sure. Okay, cool. I wish I could see anything to add.  



I guess you can go at it after the demo. There are some things I think we can definitely help you guys.  



Okay, cool. So just to kind of set up the stage, we are building a ML platform and the goal is really to kind of make it very easy for developers to deploy models into prod or into any environment that they are managing internally. And these models could be anything from the smallest models to the most complicated graphs and so on. We have tried to make it very simple to learn, like enabling even data scientists to do a lot of their steps independently. In a lot of cases, data scientists or other engineers might not necessarily have the knowledge of humanity. So those are things that we try to start out from them. The main goal is to enable entire thing to be moved in a very fast manner and following all the best DevOps principles.  



And finally, one of the things we try to do is ensure that the entire monitoring is set up right from default. Like from day zero. You don't have to worry about it. I'll skip this thing. And then as a part of, as I think NDA sam, you pointed out, we are also built on top of Kubernetes. It's kind of a platform on top of Kubernetes. We make the entire management of that easier and then expose it so that it can be used for DML pipelines.  



So in terms of the capabilities, like a few of the things that we'll kind of see already as I kind of walk you through is first of all the infrastructure management part and being able to connect to you infrastructure so how you can do that in an easy manner, how you can allocate and manage sources among the teams and how it integrates with your existing system. So that's the part that will kind of showcase as one part. The second part is in the terms of deployment we support like three modes of deployment whether it's Python, YAML or UI way and both realtime as well as Jobs. And then we are adding models and pipelines here.  



It kind of works with your CI CD, it auto scales on CPU, GP or other machines and then as I think Sam you were saying with regards to Canary testing those are things that will be supported. It's already a part of the roadmap, it's not there yet but yeah, we are waiting for customers to kind of have that requirement and we can build it out and sumit can give you a perspective there and finally we can quickly walk you through what is there in monitoring and what will further come as well. I'll not dive into the details of the PPT given the constraint of the time I directly walked into the platform and tried to explain the kind of things that are already supported. Are you able to see my screen of the platform?  



Yes, I can see it.  



The first tab that I wanted to walk you through is this part where you come and connect. This is where your DevOps team or anyone who actually admin and managing the infrastructure comes into play. So you can see like right New York connected to clusters GCP AWS you can connect as many clusters of your own here. So adding a new cluster is as simple as going to here and adding like you name of the cluster, the cloud provider you can select a reason and configure them. You have full access control easily in terms of user member admin view you can configure your ingress base domain. URLs and so on and then you can set up the links for your monitoring like the plus like the link for the Grafana URL.  



URL or whatever and we can also integrate with that and then you also have an option to configure your instance and is the kind of machines etcetera you want to be provision like including CPU. GPUs. The kind of machines and so on and then there are advanced fields that will come in here as well. So once you do that your cluster gets added. So even if your things are on multicloud like right now you are on the sales but basically it allows and then in order to connect all you need to do is kind of click disconnect and then you kind of add this two foundry helmet and then run this and install it at your end and then the cluster will show as connected. So just like as you see here.  



So similarly, you can kind of connect as many clusters and all your workspaces that are existing on the cluster will already migrate here. And I'll show you where that happens. Before going into kind of the workspaces, you can also connect with your docker registries. You can connect with Git Bitbucket and GitLab is coming soon. You can connect with the different secret store. And then there is a secret manager that we have on top of it to kind of make it easier to manage your cloud accounts. And then alerts channel is something that will come. So any questions so far already in terms of the cluster connections and no. Okay, good. Let's then go ahead to the workspaces. So once you connect you clusters, your workspaces kind of start showing up here. You can have like different ways of managing them.  



Like some companies would configure them as fraud, dev, test. Some companies will have all these three in a separate cluster and they will have different teams assigned to it. So you can accordingly manage that each workspace. Like you can put resources in. So for example, if I go to this and I edit it, you can see that this cluster has again admin editor and there are resource limits constrained. You can kind of change, configure, put your own custom resource limits here. And even at a workspace level, you can actually configure your instance families, like basically which kind of machines are supported. You can whitelist them basically. And you can even connect your service accounts here. So which makes it easier to manage all your service accounts. And then you have access control at this level as well.  



So all your workspaces, you can then accordingly allocate between teams, etc. So this is the place where the workspaces comes in. You have full views and so on. Just give me 1 second, please. Give me one moment. I'll be back. In the meanwhile, if there are questions, I just need to connect my laptop through a charger. Just one moment. Hello. Any questions so far? No? Okay, cool. So once you have the workspaces, then the next thing is the part that is accessed by the developers. So I'll come to this deployment staff. This is where you as a developer kind of see all the services that are deployed by the jobs. So here you can see that these are services and jobs. There are two tabs. You can see that services and Jobs. You can see the two tabs here.  



Creating a new deployment is as easy as just clicking on a new deployment here. And then you can select like if you want to kind of deploy a service or a job, you as a developer select the workspace in which you want to deploy it. And then you kind of go to this step wherein you have functionalities to kind of deploy either directly from the source code. Or you can also deploy docker image. You can give the URL of your GitHub, repo the branch, etc. If you have a docker file, you can directly give the docker file link. Otherwise you can just give the Python build back and give the links of the corresponding same files. You kind of can enter the ports that are there.  



As for the file, you can configure your environment variables here and then you can configure the resources and so on. Here again, you can configure your instance family. So instance family support is there at the cluster level, at the workspace level and even while you are deploying a service or a job, you have the support for the same. I'll just kind of showcase an existed existing deployment. So I'll already kind of just for the sake of the demo, I'll just showcase something that's already deployed here. So you can see that Chris service is there which is already deployed to a workspace with the service. You see the end points. It's a fast API endpoint. So you can go and even test it here. Then if you wanted CTO, see the logs and the metrics, you can see them here.  



What other kinds of APIs do you support? For instance, we leverage hugging face pipelines APIs pretty heavily. Do you support hugging France APIs in that manner? Do you support like OnX APIs? What do you support outside the fast API?  



Yeah, this is pure Python code. You can pretty much write. You can just provide us a docker file and the system will automatically build. That's pretty much no limitation on the kind of API that we can deploy.  



Do we take our existing inference code and just deploy it in the settings.  



It should ben as of New York? Pretty much. I think you're writing docker sales, right? And then you are building the docker images and then deploying it.  



Yes.  



So it's going to be pretty much the same. So you can just give us a link to our docker file and it automatically deploy that. It's just an abstraction layer on top of Kubernetes. You can think of it like that. We don't put any limitations on what things can be deployed or cannot be deployed. Also there is one more feature that they're adding that you can directly deploy models. So if you have a housing case model and I mean it's a quick way in some cases that New York. In many cases you can just find a charming based model and then you can just deploy it out on the Google. So that is something that is coming.  



Okay, yeah. So a few more things here just in the deployment, like right now you are seeing you service job. We are adding a task for the model and then we'll also be supporting pipelines so those will come in soon as well. So you'll be able to directly deploy a model from here in terms of jobs like just take an example job in the jobs of it similar here you have a functionality to kind of also kind of specify if you want to trigger it manually or you want to schedule the job. So you can pretty much enter your Cron Tal schedules and so on and you can manage it. So here you can also run your training jobs as well. And if you want to trigger it manually then you basically do it from here.  



Would it be reasonable to think of Chris as like a managed alternative to Kubeflow?  



You can think of it like that. We have tried to make the interface much more simpler and easier to manage. Also doesn't allow you CTO deploy any service. It's more for pipelines. They do have the scaler component on top of it, right?  



You would use scaler for that. Yeah, but like Kubeflow pipelines plus Kserve equals truefoundry.  



I would say there is more to it if you go CTO the final details but roughly you can say that yes, like both model deployment and training, we handle both of those in the process.  



So once you go into a deployment you can see the runs the ports under deployments here you also have an option to redeploy and here we do some magic here like in terms of if you want to redeploy an older version of the model, the secrets along with that will also be kind of moved to the older version. So that if you secrets have changed in the meanwhile, you don't have to worry about that and also case the secrets part as to how we do that. So this way like the entire versioning of secrets can happen and it can be taken care of here. So you can see your environment variables or the spec file that you can actually directly go and use on the ML spec file.  



So a few things that are coming here in the deployment right now we sold like a deployment as a fast API. Here we are adding the functionalities of model servers. So depending on the type of the model you will already get an option of okay, this model server is probably the right method of deployment and there you'll be able to deploy and even load test the model against it. So this kind of I think Sam answers some of the questions with regards to the performance and latency part that you are asking about. Right now we are not planning CTO Shane optimization from our site but later on we will kind of try to kind of suggest what might be the best option and try CTO optimize as well.  



So for example if you have a set of machines or set of instances like which might be a better instance depending on the load etcetera. We might be able to give you suggestions. But right now it's not there but in the plan that is one thing that we have in the roadmap. So here, once again, I'll just showcase for the models. What we also do is you can actually take a model and you can store it here as a model registry. So here you can see like the three versions of the model are stored. The schema, etcetera. Can also be stored for this model. It's not there. And from here, like once you have the FQM, you can actually directly deploy the model from here. So the model registry part is also something that is automatically present to this.  



Like there is a component of the experiment tracking that is there, but that is like base there just to complete the workflow of the data centers. And it's an enhancement on top of like whatever ML flow provides. That's not the core differentiator of the platform. The code differentiator is still in terms of the overall management of the resources and then the entire ease of deployment. I'll also showcase you one more thing. Any questions so far here before going into maybe further details, talking about the secret? So here you can actually store your secrets and the secrets can be directly used at the time of deployment. So you don't have to remember the actual things and you don't have to expose this credentials to the developers. And we've seen the secrets as well.  



So when you roll back to an older version, the secrets themselves are also kind of moved to an older version if you want. So this is kind of just on top of whatever secrets manager you are using. It's kind of a place where you can manage that. Anything here to add a V shane or something?  



Just an abstraction on top of the existing secret manager. It's all actually saved on your secret manager.  



Then I just wanted to kind of quickly showcase you the monitoring. So what we also do is we try to make it very similar to get monitoring out of the box. So I'll just showcase you this part. So, suppose you have your model code, which is the faded code in that you pretty much log this log prediction line and that's where the model which also comes in because your model is logged with us, so we have access to all these things regarding the model. Once you're able to do this, we automatically generate the monitoring metrics for you. So Chris gives you an example. So, starting with the raw data, wherein you can kind of see the raw data for different versions of the model, you can filter, et cetera across different conditions and so on.  



Then you can actually see the distribution of the data across like the training, the final prediction, like the final inference and so on. And you can actually compare the distributions for the different features and see the trend of the features. Here we'll also add data drip so that automatically you are able to capture drip, especially for say, structured data here and you'll be able to see and send alerts as well if need be. You can add an alert and you can send it over Stack or Gmail, the feature health app is coming soon and then the summary of the model in terms of the performance of the model and so on is also kind of captured. So all of these are automatically generated right away without having to do a lot of work at your end.  



Where are these actuals coming from? Do we ingest that data from like a click stream or how do you get the actuals today? We would probably like get our predictions from last week. We would get what users click on and what they actually did and we would see if the labels from the users are the same, right? Is it similar to how this is ingesting the data?  



It's similar to this. Like there's an API that we expose log actual. So you can just log whenever it's coming to you. So you have to wire that up and the prediction ID, it has to be stored on your back and basically the exact prediction ID we need the prediction ID and basically login.  



But in both scenarios we're just taking the logs and matching them up with the introductions. Except using this API versus doing that in like a notebook, I suppose.  



It'S pretty much the same. It's just kind of helps you like in notebook, like you'll have to do it every single time manually. You have to match it up and then plot the graph. Chris kind of does it automatically whenever we want to visit or if you go to the other.  



So one more thing. I think you asked about traffic shipping and canary testing. So that was something that is in the roadmap. We should want to kind of quickly kind of walk through that.  



So we do want to make it very easy to add. Like let's say you deploy one service which has a model version and then you want to quickly shadow test or you want to either shadow the traffic or you want to spread the traffic. So basically you can just add a V two model and then you can decide the traffic percentage to us want to make. The experience as this is that it's still not there yet. So I cannot show you a demo of it but it's something that is going to come in the projectpro pretty much like you want to make it like two to three clicks and you are able to quickly chat with the traffic or driver the traffic like 5%, 10%, 20% as you want.  



And what will happen is if you just add that log prediction line to both of these models, both of them will start clocking the data and then you can use the monitoring dashboard to quickly compare the data between the two models and see which one is doing better or not.  



So.  



That's how these two things will actually tie up together.  



For stuff like this, can you have region aware or cluster amar splitting or how does that.  



So as of now we don't do like monthly I guess you mean that if the same service is deployed on two regions and something along those lines, right?  



For example, yeah, let's say we have whatever Europe cluster and North America cluster, they're both serving soumen version of the same models, but we want the model to be roughly colocated with the rest of the services for latency reasons.  



Oh yes. So when you actually deploy the service, you're actually housing the cluster and the workspace. So every time I don't know drug if you show the UI once the deployment you actually choose the workspace and the cluster while you're deploying, you can have like multiple clusters attached to this platform. Like one can be Europe, one can be in the US. One can be Asif, doesn't matter that's where you choose and you can actually leave it to like we can show warning that we're actually deploying something that each other is not colocated but it's up to the user to locate the service. So when you go to new deployments Tal, this is the name of the workspace. Usually we try to put the region name in the website. We recommend doing that. So I think most people put that.  



So we'll probably find the region there and we can probably show the region so we can decide.  



I think one more question that was there with regards to connecting to your existing pieces, which was like data breaks at one side before the modules, that like the major part that is on Kubernetes. So can you touch on that a little bit in terms of how easy is it for say the team to kind of connect and integrate with their existing workflows.  



Actually rely more on the side of things and database so you can continue using the model registry, you can write services like no matter what language you're using, that's how I think you're pretty much doing it today. Also pretty much the same thing is that it allows you to manage the clusters and workspaces much more easily and also divide the namespaces and Kubernetes across teams in a much more isolated and.  



So I think just one more thing is this. I think we already tested on in some things like okay, model testing is something we are adding. One of the other things that wanted to kind of do in the platform is being able to deploy across event cloud serverless. Google cloud run. Et cetera. Depending on the type of the traffic pattern rollbacks I already talked about along with version secrets optimizing the cost for the model deployment in terms of the model servers as well as the kind of machines in which the model should ben deployed. And finally it kind of serves as one place for the entire catalog of services and model set while I showcase this for a model and that's where we are primarily focusing. There's no reason you can't use the same thing for managing your other services.  



So yeah, at the end goal is really to be able to kind of see like an entire catalog of one place where everything is there and make it really easy for the entire team and the ML team to really manage that.  



What is the normal sizes of the teams that you see as far as the infra team and the ML team go? What's the standard size or the average size you'd say you're servicing right now.  



In terms of the clients you're servicing?  



Is it?  



Yeah. So for instance, like you say it's going to be easier for the infra team. I am the infra team. IBM one person. So what is it going to do for if we're discussing like if you have an infra team of ten people, it's gong to be really easy to navigate. That's not really super helpful for us. I'm curious, like what the actual for me, the actual infra team, what will it do for me?  



That's a nice question.  



Maybe I can perspective being sort of like the few info guys that we have in the organization. Basically the way we have tried to decide it is that it should be easy for an infrared guy to look at the platform and operate on top of it. When I say infrared, I like mean an operations person and it should be easy for a developer to do his job without actually raising a ticket every time that he needs to say create a new service or get a new endpoint, things like that. So from an infra perspective, what the platform provides is workspaces and access management. These are the two major features from that perspective.  



In workspaces you are basically defining the amount that you want to assign to a particular person or a particular team when I say amount of resources and then you are also taking care of the kind of things that you want to allow to run on that workspace. Once you have taken that decision, what actually gets deployed on that workspace is something that you can actually leave to the actual developers who will have to basically deploy and then benchmark on top of that to actually figure out what is the sweet spot for their services. So this is how I would typically see this platform getting used from an info perspective, but there can be other use cases as well.  



Okay, that makes sense.  



Sam, one thing I wanted to add is right now we are still very early. Like it's just been eight months since we have like nine months since we have been building this platform and the goal is to kind of evolve it based on the use cases. We hear from teams of different sizes, say a few member infra team, like a single member infra team, multi large infra team. So right now we are w"
10784174762,You.com,bryan bryan@you.com (bryan@you.com),,You,,,,AWS,No,,,Anuraag Gutgutia,,https://app.fireflies.ai/view/Bryan-TrueFoundry::IyHLGGxpww,"CTO at the same time, so we can take investment from them. Right? Because hey. Hi, Bryan. 



You are you hey. 



Are you busy to hear us? 



I can hear you. 



Awesome. Where are you based? Are you in San Francisco? 



I'm a little South Palo Alto. 



Okay, got it. Cool. Awesome. Cool. First of all, thanks a lot for taking time for the call. It's really nice to kind of connect. I think we have been following back and forth, but somehow both of us miss this other, so it's good to finally connect. So thanks for that. What we can do in this call was I'd love to know a little bit about how your assistant with the Builders Fund is, and then we can talk a little bit about our backgrounds and what we are building. Would love to hear a bit more about you.com and how you have been listing in ML if were talking about it. So I wanted to kind of get that as out as well in the call. 



And if you have anything else that you would like to cover in this call, I'm more than happy to do that. 



Yeah. Awesome. I just love to learn everything I can about you both and Truefoundry and Akhil, everything. I can just start out with a little bit of a background on me. I'll give you the total overview. I'm the CTO and cofounder@you.com right now. Started with my Cofounder, Richard, a couple of years ago. Been around for about two years now. We were at salesforce together. He was a chief scientist there. I was on his team doing research and AI for about four years or more. Transfer learning, multaraining, large language models. Went through chainer PyTorch alpha testing, all the different forms of GPUs and GPUs. And it was an early kind of salesforce software of GPUs and got them hooked on those. Yeah. And with you.com, we're about 30, 40 people now, probably 20 to 25 engineers. 



A lot of machine learning work happens, but also a lot of product engineering, stuff like that. Now, my connection to Builders Fund is well, I met our Pete when I was a salesforce and had worked with him for all those years while were there and worked with them on a bunch of different projects because I was a lot of time working with different devices or kind of some of the experimental stuff or really large models at the time. We're working on contextualized word vectors when production was just using word vectors and stuff like that. We talked a lot about incoming work streams for the platform and infrastructure and everything. That's where he was. 



So were talking a lot about kind of the recent advances and how we could build things, but also how we could also support research infrastructure and bring things from research to production more quickly. So, yeah, we had a really great relationship there. And then he's an advisor@you.com now. He's been kind of advisor on our infrastructure side as well. So pretty much since the beginning I relied on him a lot for how we be setting up even the basics arvind with Kubernetes and stuff like that. And he's built a lot of experience over the last couple of years as well since then. But we still meet with him every week. And so, yeah, we've been working together on things in various capacities and in various ways. 



Now Builders Fund is kind of like a more formal way of us trying to also continually do that with more teams and bring what we've learned over the last six, seven years and kind of answer questions, but also guide people through the inevitable process of continually evolving services and technology. And then, yeah, with you.com we have an ML team, we have a big platform team, so we're also potential customers in that way. And if that makes sense too, then I'll loop in some of our like, ML Ops people and platform people and then maybe some of our like, engineers that work on the modeling site too. But yeah, so lots of different avenues. 



Awesome. 



Hopefully that connects the dots a little bit. 



Yeah, it does, like, had some background already from them, but I think this kind of completes it. So thanks for that, Brent. Maybe we can go ahead and share a little bit of our background. So just starting like two foundry we are right now at Gmail 15 folks, as Arthur mentioned in the Gmail, we are building in the machine learning deployment space, trying to make it very simple to deploy models to production and scale it without going through the hassles of learning new things each time. So we'll dive into that a little bit background, like we Karen, three of us, as founders. Mia. We shake. And Anikunjunj is based in the Bay Area. We shaken myself, we are based in the Indian time zone right now in Bangalore. Just quick background about myself. I graduated from Indian Institute of Technology, Pin, India. 



Did my bachelor's in electrical engineering. After that, spent like around six years working with a hedge fund called World Fund, where I was first doing development of quantitative strategies using a lot of data, and then later on went on to be a portfolio manager, building and maintaining a portfolio of around 600 million at the same time. I was also a member of the CEO of this with Knock, various growth initiatives there. So a part of this tent was based in US and Singapore for almost three and a half years. Nikhunj, his background has been on the ML site primarily after India Kharagpur, he went to Berkeley for his masters, and then he went to join Reflection, which is a status in the recommendation space. 



He was the first ML engineer there, so he built the ML platform there from scratch along with the rest of the team. And then later on he joined Facebook as a lead ML engineer in the team called Portal which is like a competitor to Alexa. So he spent his time there. So had experience with the Apple on our platform or the internal platform of Facebook and brings the rest of his experience on the ML side. And Abhishek will probably tell about his background as well. We all left our jobs in 2020. We built our first company in the talent space and then we sold it Pin Forge which is one of the biggest players in talent space india. 



The reason was we are facing some challenges in operationally scaling it and we felt that it was not involving a lot of tech like selling to the HR folks. We were not able to kind of get them acquainted with the technology and let them adopt it. So at that point as a team we thought that we are more from engineering background, we want to build something in the tech space and that is where we kind of thought about this idea of making machine learning much more simpler. 



So the journey connected in the way that when were at building that talent platform which we used to sagar enthire we had also built like model for matching up the job recommendations with candidates and in there like while the local building was easy, like the productionization was really tough and were discussing like it was so easy to productionize models in Facebook. And with that like we started thinking why is this so tough externally? And we thought that if we can bring something that Facebook has to everyone right from smaller startups to big companies, they don't have to do the heavy lifting of building a platform themselves, but they can actually take and use something from scratch and build on top of it. That was the idea that led to found it. 



But yeah I will go into more details there maybe I wish you can give a quick background. 



Hi Brian. I also graduated in 2013 Pin Computer science and then worked in Facebook for around five and a half years. Worked on different teams there. Like the Distributed Caching system team was leading the mobile performance team and was eventually leading the leaders organization there. And then early 20 to quit my job. And then the journey is same as Pin broadcast like we did the start up on the talent space and currently working on Profoundry. So that's roughly about myself. 



One connection is there like Richard, we have met Richard once, not me and Amish but Nikonje has met. So one of our investors Anthony Goldman is actually the founder of Cagele. He and Richard apparently run a fund right like together. Yeah. So I think connected lot of connections. I just remembered like now as you. 



Karen talking about it's funny, I feel like the more time goes by I feel like the more people are just kind of invested in each other and status are just customers of each other. I think in many ways it's good. It makes the whole network in some sense more robust, I guess. But yeah, it's funny, I feel like it's everywhere I look now. Do you do like a normal kind of intro to Truefoundry itself and kind of what you're doing and maybe you can just treat me like a you.com customer and then I'll see what my normal questions are and stuff that come up and we can see if there's any overlap there. 



And yeah, in the process I'll have the secondary in the back of my mind of builders fund and everything and what that means for you all and our pee and that group as well. Sure. 



So generally when it helps to understand a little bit more about the stack a bit because the overall introduction generally is qualified, but it'll help if you can give like a two minute overview of the you.com current state and then we can talk about Co Truefoundry, if that will be fine. 



Sure. Yeah. Well, which part of the stack do you care about? I guess the most. 



Starting from the data science, like the model building to the training side and where are you storing the models, how are you serving the models? 



Sure. So we're built on azure. We do everything. Pin azure. We use data bricks. So a lot of stuff gets done in data bricks. All the analysis, plotting and dashboarding, things like that. We have ML flow. Flow a lot. We use Kubernetes and pull images into machines and stuff like that for deploying ML services or like downloading models from wherever. I think right now, maybe the model files are in block storage, but she's basically Amazon s three. But I shouldn't say that because I'm entirely sure. I think we have some blob storage. I know we had some on like independent taxable volumes and stuff like that. Whatever we need to set up. We don't have a huge problem with starting new services up. Usually we're not like starting a lot of machines and tearing them down and stuff like that. 



We just roll over once a day or something like that. But the startup cost isn't too bad there. Let's see. Yeah, we mostly use PyTorch. I think we're in the process of trying to figure out nvidia triton stuff for model serving and things like that. But there's been some hiccups. Translate all our models into onix for serving. Between GPUs people use a combination of notebooks and pipelines. 



For pipelines you're using data breaks for pipelines? Yeah. Okay. You don't have anything like Airflow or anything like that? 



We don't use airflow. Yeah, we don't use airflow. 



Okay. 



We TerraForm all of our pipelines too. 



One quick question. Who does the deployment of the models? Like, basically do data scientists do the deployment themselves out there? ML engine to do the deployment? 



We have like one data scientist and they don't at this point work on like, modeling as well. They're mostly like just in the data and analytics and stuff like that and engineers. 



Okay. And the platform thing that is there Bryan you mentioned about. So they kind of build the platform for the general software engineering label and the same platform, like the ML platform also sits on top of it? Or are they currently separate? Like how is it. 



The platform is developing a lot of the patterns for the services and how they're deployed, but then like, an ML engineer would, like, actually do that. So yeah, like the set up for the Kubernetes clusters, how keys are handled, how models are handled, those patterns are all mapped out and set up by our ML Ops and DevOps people and some back end engineers. Everybody pretty much has some ML experience so they know what to look out for. Those people have also in their previous jobs just been ML engineers or specifically ML Ops people. And then an ML engineer will come through and have another model. And if they've learned the pattern, then they'll just do it themselves. They'll be like more self served, obviously with some reviews and stuff like that. But yeah, I mean, some people are new. 



Every once in a while they'll sit down with those, like, platform people and be like, how do I deploy a model here? It's mostly selfserved. 



Okay, understood. I think get at least a high level sense of the system. Maybe we can give a brief about two foundry and take it from there. And we'd like to kind of take your help also branding, figuring out like where there could be a mixed good fit in terms of you so close, that's fine. So I'll start and wayve I wish you can add so truefoundry are actually a path on top of Kubernetes specifically designed to support the ML developers in helping them deploy their models to production. So you can think of it in a way that as a developer, if you Karen writing your models, after that you just have to like add a few lines of code and post that. Everything from the deployment to the basic monitoring is taken care of by the platform. 



So it supports like, real time as well as batch case deployment of models. If you deploy any of the models, like if you deploy real time model, you'll be able to get like an end point. You get like service monitoring out of the box, locks out of the box, as well as you get basic monitoring for your models, which is like ML monitoring and basic drift characteristics out of the box. If you're deploying batch models, you get the scheduling and other things out of the box. The platform sits on top of either our public cluster or it can also connect with your own cluster. For the DevOps team. It allows them to easily provision and manage the resources across different teams. 



Suppose your teams are working on different types of problems, then you can have different instances set up and then within those you can have different workspaces created which can be allocated to specific teams. And for each workspace you'll be able to track how much resource are being utilized and how much cost is maximum. Team can enter and you can allocate it to a team. And then within that the teams can then start deploying services and so on. As you deploy service we allow like a fast API mode of deployment and then we are adding like a model server out wherein you can actually choose the right model server to kind of deploy. And within the workspace we also locate the kind of machines you want to deploy so that you can easily optimize your deployment for better, for a more optimal cost, et cetera. 



So the goal is really to kind of make it very easy for deployment. And if this deployment is supported both either from a UI or from a CLI or from like a Python wayve, so all the functionalities are supported. So even if say there are data scientists on the team who do not know engineering, they will be able to deploy it directly through their notebooks. In a simple way. It kind of enforces the best samira principles. It kind of automatically ensures that CI CD is done and it kind of enforces those kind of things. And if they are ML engine then they can use like the CLI or any other way of deployment. 



And then on top of this, what kind of the goal is sense the more complicated things that you might need as you scale for example traffic splitting or saddle trafficking, saturday traffic, those things are supported by default. So your learning curve for any of this is not very high. You can get started on the platform at a high level. That is the way we are building it. And which do you want to add? 



No, I think we can show you. I think it will become just by talking it will just become too much. Maybe we can show some of the experiences that we Karen trying to give to our developers for deploying. That will better. If you want I can show it to confess on my screen right now. 



It will not do. A license. 



Is Airflow, like you mentioned Airflow. What was the nearest kind of thing in the space that you feel people would be already using or you'd want to be switching from? Is that kind of where you imagine yourself plugging in? Like where people are using things like Airflow? I haven't used. 



Not airflow. Brian, what your platform team did now, right, bryan, you said that they're laying out the best practices and to make it selfserved it must have taken them one to two months, like a couple to three engineers. And these are like the best of engineers, probably you have brand@you.com. Many of the companies don't have the luxury of having engineers plan out and making itself. So it takes them like years to get to a platform stage. So we are trying to give this to companies from day one that they don't have to build this. Again, every company is getting it to a stage where they make the covenant stand up and try to make itself serve for ML engineers and data scientists. And still the experience is not that great because they constantly have to maintain it, something gets upgraded and things like that. 



So we kind of try to take over those things. And I can show you what I mean by that. 



Just like then we'll not do a live demo because that will go a lot into code and also we'll do like a high level walk through and maybe that will give you a sense of what the platform is. And based on that, you can also think of some of the use cases and then we can have another discussion and maybe get some of your other members of the team as well. Take a look at it if you feel that it could be worth. 



Yes. So basically this is our platform. And you can connect like your existing Kubernetes clusters to it. So you can just bring any cluster and just add the cluster here. And once you have the cluster, then what you can do is basically the India team or the platform team or whoever that can just allocate different parts of the cluster, we call them workspaces. So different parts of the cluster, Karen, different teams. And you can also limit like, okay, this workspace. Karen limited to four CP and Hgbm. So there is no way that somebody who has allotted this workspace too can spend more than like $50 a month. So even if the code screws up, even if they mess up, there's no way they can screw up the rest of the infrastructure or spend a ship ton of money, basically. 



So basically this basically allows you to provision the cluster and divide it among multiple teams or people just to play around with. And this is your kind of the admins view of who is using what. We also plan to show a cost estimate right here that this workspace isn't quite this many cost later. We also want to give cost insights and things like that. That probably the service you can do this week to make the cost go down by 20% to 30%. So that is on a more admin level. And for our developer, the view is going to be as simple as if they want to deploy something. First of all, you can get a service catalog of all the micro services that are currently deployed in your company, like all models, everything. 



So as of now, you can see services, jobs, and there's one more thing which is coming, which is the models. So jobs can be like your training. Jobs can be you bash conference link. You can trigger them right away from the Uri right here so you can trigger the job. It is currently suspended. You can also make a crown job that runs every Sunday or whatever frequency you want to run them at. You can also see the logs and metrics of the services right here. So some of the services will have logs, some of the services will not have logs. 



So you can see the logs and metrics of the services right here and you can go inside also to see how many versions of the service have been deployed, what are the environment variables, the secrets that the service is using and then basically to create a new service or a job. It's as simple as you go here as a developer you'll see which workspaces you have access to and you'll just click here and then you can just give the name of the service like OSBC and then you can just choose the data repository from where you want to deploy and this will automatically figure out okay. The repository already has a docker file so everything is set and all you do is just click on submit and it will automatically go ahead and deploy the service. 



So this is initializing, it will actually build the docker image for you and then it will automatically deploy the service. This is kind of a self service. 



Platform that and then it supports basically like stored by a docker but if you don't have a docker find then it will automatically docketize and create it for you and so on. It supports all the frameworks like Psychedel and Python and all of those things automatically just like GitHub, it has support for Bitbucket and then we can add GitLab depending on if there's a new customer who has the requirement. Right now the pipeline thing is not there in the platform but that is something that will come in where you can deploy like a pipeline as well and run a pipeline as well. 



And then a few other things that we want to be able to do is like the model server thing is right or not there but that will come and you'll be able to choose like you want to deploy via model server so you can actually optimize your deployment for a certain case. Then GPU will be something that will be supported and then machines, like the choice of machines and all that you want to have in a workspace or while deploying will be coming as well. 



So one more thing we are working on right now Brian is to add a model thing here and you can just select the model and you can decide you want to deploy by trite and you want to deploy OnX like two or three different strategies and then you can just run a load test on them quickly to see which one is performing the best. And you can also choose the machine types here. So for example usually in case of if you're doing performance optimizations and all the type of CPU and the type of memory, the process makes a huge difference. So we also allow you to quickly choose like whether you want to deploy it on intel processor, on AMD processor or things like that. So you can just choose like I only want to go for a Zone processor. 



At the end of the day it's the same self sense platform kind of thing that you're building internally. 



Yeah. 



And then few other things is like we have our model registry on model registry but you can also bring you own model registries and connect to it or the Doctor registries etcetera. And then the authentication and secrets management is also there as a part of the platform. So you can either use your own secret store or you can use our own secret store. So those are some of the things that are there. Brand so at this stage we karen still very early working with a few companies and we wanted to work with slightly more advanced tech companies to be able to kind of mature the platform further and be able to support their needs and in the meanwhile like add value to them and maybe actually improve the platform further. 



Yeah, I mean that was the UI way of deploying brand that I showed you can do the same thing by Python code. So this is the Python code that data scientists have to write and then they can just judge to job deploy and automatically deploy the job and they'll get to see on the Uri like it's healthy and all. And this was the service that I just deployed right now. So hello SPC and this is already live and there is an end point that is automatically generated where you can go and play around with the thing. 



I think it'd be great. Yeah, I feel like it'd be great for you to chat with kind of like the person who leads our main engineering and infrastructure development and basically like our lead ML ups DevOps person. Do you mind if I set something up to follow up with them about this time? 



Yeah, that will be great. 



That will be great man. 



I think we wayve something we have. 



Something. 



On Tuesday already set. 



We did have something on Tuesday but I received a mail from Freder that she just canceled it given it was set for today but I can just request her to maybe reset that meeting. 



Do you remember what time it was? 



It was ten ups. What do you think Brian? Like something like this and obviously the platform is in a certain way, but it can evolve depending on the requirements and needs. So do you think there could be a way for a need case or a use case for you.com to try this out. That will be great if you love to hear that. 



Yeah. At least initially here. It's definitely the kind of things that we've used and it makes sense to me. I think that big. The initial questions that come to my mind that I want to see from my team is office how big of a pain point is this solving right now? How much effort and or time will it take for them to switch to it if they think it is a good thing worth trying out? And maybe three is like is this the right thing to is this the right time to try something out and experiment with something new versus using what we have right now? We try to focus more on down our product market fit or like our revenue models and stuff like that. 



So I think it's a matter of kind of understanding the resource, the resourcing, time commitment included pin resourcing and kind of the timing on that. Like if I was just wearing my CTO hat that's probably the next question I would be thinking about. I'll ask them do they think this would make their life better and if so, where do they think it fits in the queue on making our product better versus making ups more productive? Yeah, that's how I kind of evaluate it while looking at it. 



Makes sense. I think we'd also love to learn a little bit more a few more questions to understand if there is some more top of the mind pain points that might be there for the ML team so that will help us even present this to them in a better way. 



That would be great. I'm going to add you to the meeting now. Let me grab the other one. Do you want all three? 



Yeah, I will just send out the emails. 



Cool. Well, this would be great for me, the rest of the team and you can see if this is a good fit for you right now. Like it awesome. 



Thanks a lot. Van helpful and thanks for being so helpful as well. 



Sense you next week. 



Bye. Bye. Let's see prompt and you sense the difference in experience that you have internally versus this. Actually. 



I know when they use the product maybe they will ask the moment he takes the Credential. They have very high bar of reliability and everything. Any good person will understand really fast. The team goes some point but at some point they will reject because of stability and too early and all those reasons. But his company is not the right customer. Bye. "
10777412370,HeyDay,sauvik sauvik@heyday.co (sauvik@heyday.co);ben.goswami ben.goswami@heyday.co (ben.goswami@heyday.co),,Heyday,,,,AWS,Yes,,,Nikunj Bajaj,HeyDay_Sauvik 01-11-2022,https://app.fireflies.ai/view/TrueFoundry::EKV6sLqkLM,"01-11-2022

Formal call with the team

Airflow Infrastructure for data engineering

Some ML applications, we are training on Sagemaker. Notebook running on Sagemaker- takes long time to execute and finish. Log into AWS, get servers and run it there. On the inference part as well, we are trying to see if we can right from the desktop interact with ML Models and test it out and roll it out. Thats where we were wondering if I see if your thing can help.

We want to run locally fast.

Some combination of local + cloud would be good.

Ours is all batch - no realtime.

Ideal thing would be that we write code in VS Code and then fire off training- say 90 CPU and 10 GPU of this model. Input is say from S3. Create the artefacts and push it to S3. For productioning this- how do you productionise this type of job. I cant even run very simple model on my local machine. If we dont have to go in and type in from Sagemaker. If we can manage all of that from our local machine, that would be great.

We currently use Sagemaker just as notebook.

Productioning to us means Airflow - this comes secondary. Initially we want to productions our inferences. What we are planning is to take our model.

We havent yet automated the batch inference part.

Eventually we would like to monitor the performance of our models.

Started off with LSTM Keras types and now doing more classical ML. We do multiple of them to see how each one is performing.

Informal call with Ben

We have a tech org and a business group. Business group also does ML Data science work.

Some temporary work on Customer service and all is also happening.

Business group DS- they dont have resources. They know R, Sql and we dont have bandwidth to support them.

Ours is more tech org and we have resources and tools and budget.

We dont have any DevOps - we are not a software company. We dont want to use Sagemaker based on our experience because it doesnt have the bells and whistle."
10754642439,RadCode,Rad (rad@radcode.co),,RadCode,,,,GCP,No,,,Anuraag Gutgutia,RadCode_Rad 21-10-2022,,"21-10-2022

I am in Versa / RadCode.co

We are a venture Builder - Lot of companies that we developed

We can use TF for the clients

Utilising TF in our Start-up to get fast GTM => We dont want to see how to deploy the model.

We have 1 start-up we are looking at. Psychotherapy => Matching using Neural Nets, If they are asking questions, how to classify them etc. GCP - We run it on regular VMs. That is hidden behind web servers. Do you use Kubernetes? (15 members in SDev). Have somewhat monitoring cooked ==> Grafana. Built in Google Analytics.

DEPLOYMENT: Realtime // We will use both // Size? Traffic? ==> Few thousand clients a day => 20K Requests a day (NEPTUNE ML - We use that for Experimentation, Switch to W&B)

QUESTIONS: * We deploy our models in your platform? Docker or Raw models?

WHAT ABOUT PERFORMANCE?

Are you agnostic ? Yes, we are. We dont want to be limited to some versions

3 DS and S Engineers => who will deploy the models. How are they performing and not spend too much time on deployment?"
10611816710,Jumio,Apurv Bhargava (apurvb@gmail.com),,Jumio,100-500,100 - 500 Mn,25-50,Multi Cloud,Yes,Software Development,USA,Nikunj Bajaj,,https://app.fireflies.ai/view/Apurv-Jumio-Nikunj-TrueFoundry-::MefcRErKTf,"Abu. How are you? I'm doing great. How are you?  


Speaker 2 
· 
00:04 
Doing well. IBM doing well. Where are you based now?  


Speaker 1 
· 
00:08 
So I'm from Hydrawat. I'm staying in Hydrawad.  


Speaker 2 
· 
00:13 
Okay.  


Speaker 1 
· 
00:14 
What about you guys? Where are you joining from?  


Speaker 2 
· 
00:17 
I am in San Francisco.  


Speaker 1 
· 
00:18 
Cool. Right? And IBM reserving crot. Cool. There's some note taker.  


Speaker 2 
· 
00:32 
Yes. Recently we got introduced to this fireplace note taker that I really like because often I'm giving demos on a screen share and I can't take notes. And this really helps. It has some AI driven notes that I don't think is very helpful, but at least the transcripts are very helpful.  


Speaker 1 
· 
00:51 
Awesome.  


Speaker 2 
· 
00:55 
Thanks a lot for taking time to speak with me. I was actually quite excited about this call because given all these different places that you have worked at and how I rarely get to speak with people who have worked on such relevant stuff, basically. So I was like, I was quite excited. CTO Keep, I learned from your experience at Lyft, Google and then Fiddler, and now basically driving ML of itself.  


Speaker 1 
· 
01:25 
Right.  


Speaker 2 
· 
01:26 
So all these were very interesting. So I was looking forward CTO this call. Primarily, I'll just give you a little bit of a background about myself and set some context here. So I come from a machine learning background myself, have been working in Recommended Systems Conversational AI at Reflection and Facebook. And right now, the company Truefoundry All Hands, we are building. The inspiration comes from a couple of different experiences in life. Like at Reflection, I had built out a horizontal AI platform for the company. And then at Facebook, I realized that how much more could we have built that we did not end up building a reflection. So I think those two things really helped in shaping what we're building now. And as a team, like, my co founders, Sync, Bishop and Rag are also from It, correct.  


Speaker 2 
· 
02:19 
So we are batchmates from there, and we built out one other company before starting Truefoundry All Hands got acquired by Info H. And in this particular call, my primary goal is just to learn that, like, what's your point of view on the ML of Space? What are some of the interesting problems that you're trying to solve at Fiddler? What are you trying to solve here at Juo? Is that how you pronounce it? Sumit.  


Speaker 1 
· 
02:42 
Yeah, junior is right.  


Speaker 2 
· 
02:44 
Okay, so what are you trying to solve at Sumit? And that's pretty much it. Just literally just learning throughout. And by the way, Anohau is a member of recently joined maybe three months, and he's a member of Founder's Office Parth.  


Speaker 1 
· 
02:58 
Truefoundry. Awesome. Yeah. Thanks for the introduction. Like, you're right. So I've spent some time at Google. My background is more in product infra and tech infra, like, building back end systems which can really scaler or reliable and performant. So I wouldn't like to think of myself as being an expert in ML Op space. In fact, when I got out of Google, I was like, kind of overwhelmed with all that's out there in the open source world, right? Like the public cloud. So the thing about Google is, like, they have their own internal stack on which folks work. So you are pretty much live in that bubble, right? So when I came out, it felt like there was a lot to learn.  


Speaker 1 
· 
03:52 
And I think last few years I've mostly been consulting with these different organizations, primarily focused on setting up their data platforms and machine learning platforms and helping them get started, right? So that's where I think I'm really passionate about understanding why they want to build this platform and then just sort of advising them and helping them build it up and yeah, there were like, different things to learn and all these different companies that you mentioned. Right. So I think Fiddler is very relevant to what you're building at True Foundry, right? So they started with model explainability. Right? And I think they were doing pretty well, but I think they pivoted to doing model monitoring because I think that's what most of our customers were asking. So for me, I think the biggest learning there was working with the founder, like Krishna Kade.  


Speaker 1 
· 
05:00 
He's also a Facebook guy, by the way. He worked on the Feed team. He was the manager on the newsfeed team, and he's very charismatic. Right. And I've been on sales call with him and he just has this deepak understanding of customers and their problems and he can understand the stuff really well and sell his products really well. So I think that's what got me hooked onto Fiddler, right. And yes, there was kind of a steep learning curve. I think Model Explainability is real cool, right? Like, what the stuff what they were doing with shapely values. Right? So shapely values, if you know much about them, it's like computationally very expensive. Right? So there was a lot of work going on how to make Shapy values, like, more performance, right?  


Speaker 1 
· 
05:58 
So Fiddler actually did some research and something called Fiddler Shop, which was like much more performance. They even have papers out there if you're interested in reading about this stuff. Then they also did a lot of work on integrated gradients. Right? So I think for all the deep learning models, right, those integrated gradients is what gives you sort of a better signal of the model state. And like so they use those to sort of explain deep learning models. And they did a lot of work in that area, right? Especially some of the models are very challenging, especially models, which sort of takes when it's like normal tabular data, it's all good, right. You have standard tools to sort of deal with stuff, but when you have more complex models, like text models or image models, things start to get more challenging.  


Speaker 1 
· 
07:07 
It's more art than science. It's like how you really model. So there's no clear ways on how to sort of explain those models or how to even monitor those models, right? And a lot of models are getting that way now because very hardly do you see customers which have simple sort of tabular inputs going into models, right? Most customers will have some sort of complex inputs, right? Like trying to condense the inputs into some dense vectors. And how do you make sense of those inputs and explain those in terms of the original human readable sort of inputs, right? So I think there's some interesting sort of challenges in that area, which is what Frederick did really well, right. I think where they sort of like pivoted, right, at some point was they thought that this was not really selling for them.  


Speaker 1 
· 
08:15 
There was not enough market for Explainability, right. Because most customers who are buying the products were not like those tech savvy companies, which you might think of, right? These were like big banks and insurance companies which were buying the products, right? And for them, things like monitoring was like paramount, right? This is what they wanted, right? They wanted monitoring. They wanted ability to detect bias and fairness and models. So that's what they wanted. Right. So fiddler really pivoted from explainability. Now I think they are the leader in model monitoring space. I think they kind of nailed it. Like most of the deals that happened after Series A, right, were like, mostly in model monitoring space. And I think one thing which really helped them was I feel that they built some really nice sort of intuitive UIs around the product, right?  


Speaker 1 
· 
09:26 
UI, which, like, you know, which, like, you know, a regular sort of, like, person could like, just use, right. It wasn't like complex and like, you know, you didn't have to do a lot of digging into. It was very intuitive and very nicely done. And I felt like it was responsive and I don't know. Have you seen Friddlers Demo?  


Speaker 2 
· 
09:50 
I have, yes. I closely follow them.  


Speaker 1 
· 
09:54 
So I think that really worked for them. And I was involved with that part of the product, right. I helped them sort of build their sort of monitoring tools and added support for that and the product. And I worked with a lot of different customers trying to understand their monitoring needs. And really so I think one of the key things which customers were interested in knowing, of course, whether their models were drifting, right? Like how their models were performing in production. Right. And some of the big challenges there were, like, unlike test data, which is like neatly labeled, right? And you can clearly tell the performance, right? Like the model is breaking or it's drifting. It's hard to tell for models, like, in production because labels are not available right, at production time, right.  


Speaker 1 
· 
10:54 
The labels are only known much later when you get humans in the chain. So I think being able to detect drift, right, in real time without knowing the labels was a very big challenge were trying to solve. Right, and this would really help customers if there was some way to sort of figure that out, right, so they don't want to know like three months down the line how their model was doing.  


Speaker 2 
· 
11:26 
Yeah, exactly, that's not useful.  


Speaker 1 
· 
11:27 
Just want to know now, Kiara. And then the second thing they want to know out of this whole thing is like when it's time to retrain. Right? So training models is expensive, right, and it costs expensive GPUs and there are customers who have hundreds of models, right. Imagine training hundreds of models on a daily basis just eats up into your budget, right? So if you had like a neat monitoring solution which would somehow help you decide when it's time to sort of retrain and not only decide, it could tell you subsequent of the data which are underrepresented in training and help you build sort of new training sets on which you could train the models. That is like just the icing on the cake, right?  


Speaker 2 
· 
12:21 
Absolutely. I have a couple of follow up questions on that. This particular topic that you brought up is actually very interesting to me also. So I guess here data drift, right? So data drift itself can be computed based on some simple metrics like, okay, divergence, etcetera.  


Speaker 1 
· 
12:41 
Right.  


Speaker 2 
· 
12:44 
By the way, is that what people mostly end up using today? Like which is at the feature level KL divergence and all? Or are there other techniques that people are starting to come up with? Number one, the second question is that is data drift truly computed in real time when the query comes or the common pattern is that you would actually compute the drift at certain cadence, let's say once every hour, once every day or whatever, and store the number with respect to certain reference sets. How does this thing work realistically?  


Speaker 1 
· 
13:18 
Right, so first of all, you're right, like you can compute like data drift like on the features, right, feature drift. Because like the features are known. Right. But that really doesn't give you a true sort of indication of your model performance, right, for finding what you want to find is drift in the model output. Right. That is why I was saying you need to know the predictions and the labels, like the correct labels for those predictions right. In order to compute the model drifts correctly. Right. So you are right, there is a feature drift but there is also a drift how accurate your models are. Right. So I think that is one of the signals which is more important to know how well your model is performing data drift. It's a good sort of proxy, right?  


Speaker 1 
· 
14:15 
Like if your data is drifting, it's likely that your model might be like the performance might be going down. Right. But I think one of the key challenges there is to understand is the model output drifting or not, right? So say you did some tests, right? And you have some performance numbers. Like this is how well your model performed, because in those tests, the sort of labels are already known, right? And you can calculate the model performance based on those model output and this is the label. So you can compute the outputs, but in production time the labels are not known, right? So you have to figure out ways to compute the performance without the labels being known. So I think that's the challenge I was talking about. How do you go about doing that?  


Speaker 1 
· 
15:06 
And thing which you mentioned about the cadence, you are right. Most customers actually don't look at real time signal, right? In fact, they are mostly good with daily or hourly sort of numbers. But there exists like models in which people want more real time. So, for example, there are like in the publisher world, right? So what happens is in publisher world, you have these trends which go up and down very fast. So we need to be able to detect those trends. So they have like very sort of detective things that are going up or down in a much more real time fashion. So they even look at signals at smaller buckets, like 1 minute, five minutes, how they want to be able to, like I think you were talking about recommendations, right?  


Speaker 1 
· 
16:08 
So I think a lot of publishers use recommendations and they would want to put their highest bids on articles which are most popular, right? Right. So I think that's where these sort of more models which can compute sort of ROI in real time help. So there are customers, even ecommerce customers, right, because they also have all these trends and all. Sure there were customers who asked for this, but I think not all of them have trendy data, right? So most of the banks and insurance companies are completely fine with one day off.  


Speaker 2 
· 
16:49 
So I guess one question that I have in this context is from a product perspective, right now, you can compute data drift. For computing data drift, you need basically one reference data set, one current data set, right? Now, if your reference data set itself is a training data set and then your current data set is your inference data set in certain time windows, basically, I can imagine that you compute the drift with respect to your training data set for all these different time windows that okay. Reference training data set compared to day one of prediction, training data set compared to day two of prediction, and so on so forth. I can imagine that setting up a process for that.  


Speaker 1 
· 
17:26 
Okay?  


Speaker 2 
· 
17:27 
But also sometimes people are interested in seeing the drift for your itself compared to three weeks ago to now.  


Speaker 1 
· 
17:38 
Right?  


Speaker 2 
· 
17:38 
How is my system doing from three weeks ago versus now? Right? And now, Chris, time frame itself is usually configurable in products that you can select that reference data set is June 1 to June 7 and then my current data set is June 20. CTO June 27. Right, but you can tweak that.  


Speaker 1 
· 
17:55 
Okay.  


Speaker 2 
· 
17:55 
My reference is June 2 to June 8 and my current datta set remains. So basically the number of possible combinations of this like two data sets is actually infinite. You can have however many and that kind of a setting. You can't actually pre compute the drift and store it, right? Correct. So do people put like product constraints that users can't do stuff like this? Or do people actually compute this in real time once they know exactly what are the two data sets? How do people solve this product related, right?  


Speaker 1 
· 
18:30 
So at the time, Fiddler did not have this and there were customers asking for this, right. They wanted to be able to compare production data sets against each other. Right, exactly. Like how you're saying how is this week's data comparing to last week? Right. So you are right. It's very hard to sort of precompute distribution numbers and like, you know, run them against each other. And this would require it's computationally very expensive, right. So I think that's where you need CTO invest in tools which could sort of help you sort of overcome this. Some of the sort of analysis which we did, we saw that instead of really using the whole data set right, and like computing distribution numbers on the entire data set, we could actually take a sample of it and still have pretty accurate numbers come up, right.  


Speaker 1 
· 
19:30 
So I think sampling was something which worked really well for us throughout the product. So there was like I remember there was these algorithms for computing feature importance, right? Like random feature ablation, which is also something which is very computationally expensive. Right. So we found that we could tweak the sample size and have significant less sort of CPU going at it and roughly have the same quality of numbers. So I think you need to be able to run experiments to figure out how much data you need to use and what's the kind of quality of answers you can produce with that sample. So if you have something so what happens is if your data is pretty much stable, right?  


Speaker 1 
· 
20:22 
So you can actually sample very aggressively, but if it is like it's not very stable and it's very spiky going up and down kind of time series, I think that makes sampling very hard. Right, and you won't get good quality results also. So I think it depends on the nature of the data or the time series for which you are trying to compute these numbers. But I think sampling is something which really worked for us. And by the way, this is something which customers asked for, right. So I think disabling this in the product is not a good idea, right? I mean, maybe making it like having customers so that they don't consume all of your CPU resources, right? Maybe giving them some sort of credits or something, like you have these many like five free credits or something.  


Speaker 1 
· 
21:21 
To do this kind of thing and to do more deep analysis, you have to take the enterprise version or something. So I think that works. I think that technique worked for us in Google Analytics too, right? Right. So about four years ago in Google Analytics, we pivoted to google analytics has been a free product since forever, but I think four years ago we decided to monetize it and how we did that was it for free customers. The queries which they make are on the sample data sep, right? And the quality really suffers, right? So if you're a big customer with a billion rows, right, running analysis on 10 million doesn't help. So to run analysis on 1 billion, you need to pay, right? Sales sense. So you always have to monetize on your sort of compute, right? They come up CTO service for free.  


Speaker 2 
· 
22:24 
Makes sense. And then it fits into that completely makes sense. You pretty much don't have any other way at this point. If you expose that function to the end customer, basically arbitrary, basically in that.  


Speaker 1 
· 
22:45 
You can actually have limitations. You don't make it like they can select per minute buckets, right? You make it so that it makes more sense. Currently a computer, you can cash it, right? Then you can serve it from the cache.  


Speaker 2 
· 
23:06 
Actually, I think I have a question. Yahap is not necessarily about per minute or per hour, right? I think it's about the range. Even if it's like per, whatever, like per hour, the combinations are many, right? Even per hour. Actually, if you think about June 1, June 8 compared to June 20 june 27, right? Now you can restart and end dates and kind of get almost many permutations, right? June 2, June 9 comparison like that in that data changes, right? And you can't potentially pre compute it. Maybe you can even cache it if the user wants to use the same thing. But you can't potentially precompute it at that point, right?  


Speaker 1 
· 
23:52 
Correct. Like some sense. I think it makes sense. As you see, we used to compute only at hourly buckets, always gong to be from twelve to one, right? Never from time series. So you can cash it, right? So you only need to compute it for one CTO two and then I further aggregate it with twelve to one, right?  


Speaker 2 
· 
24:41 
But I get other distributions computer like what you're saying would make sense, captain twelve to one, the distributions computer but other drift computer actually, let me show this to you, maybe this might become clear. What is it that I'm referring to? If I selected Is and if I go here, data distribution ticket. So we allow something like for example, if I go and select this time frame, let's say, and then I say add comparison, right? So imagine current data timeframe. The issue is the user can technically select otherwise sell the product, expose the user can technically select whatever time ranges that they prefer.  


Speaker 1 
· 
25:57 
Right.  


Speaker 2 
· 
25:58 
Prediction distribution in this time frame, when I'm comparing against this time frame, then there will be a drift number that I can compute, right? Key distribution by the sales divergence is zero nine or whatever, right. Does that make sense?  


Speaker 1 
· 
26:11 
Yes.  


Speaker 2 
· 
26:12 
And the same thing for your actual same thing for all your features and all like the map feature, distribution of balance the carnot, for example.  


Speaker 1 
· 
26:18 
Right.  


Speaker 2 
· 
26:21 
If I want to introduce like a drift column here, let's say, which will just take you like this feature in between these two ranges is so much drifted that itself I cannot possibly pre compute that. Okay, like I'm missing something here.  


Speaker 1 
· 
26:36 
Correct? Yeah, I think drift you're right. I think you're right to skill it, you need the entire sort of data set to compute. Right. So you can't write. I think at the time we didn't run into this problem in Fiddler because we did not expose like lot of free form sort of selection updates. But I think this is something which people were asking for. Right, okay. So how expensive do you think it is? Right? It really depends on your data set, right? That's what's important, right?  


Speaker 2 
· 
27:30 
Correct. Yeah. If you sample and try to do it in real time, I can imagine a solution working in real time. And real time does not necessarily in this case have to be like 2030 milliseconds. It can go to a few seconds right. At the end of the day, it's analytics tool, basically.  


Speaker 1 
· 
27:46 
Correct. Like some of the customers which you've shown it like they are okay, giving you their data running on your cloud.  


Speaker 2 
· 
28:02 
We get both basically. Like some customers are like we will use the sales version where we just get an API key and the thing runs on our cloud and we will do this thing. Some people are like, our data can't leave our cloud. So we actually have a deployable version of the entire product that runs on their cloud. No data ever comes out of their cloud, basically. So we do both actually.  


Speaker 1 
· 
28:22 
Right, so if it is on their cloud, then it's because you would just need the compute power, right? Because then they will be paying for those resources. Right. So you could actually use some sort of like fancy sort of distribution and compute it really fast if needed. So you are trying to optimize for.  


Speaker 2 
· 
28:46 
The CPU cost, the CPU cost here and also just the infrared itself at some point if the product we give some flexibility and that reduces the experience, makes experience of the user bad. Like it becomes really slow. If you're trying to compute it in real time, then that's not a good thing.  


Speaker 1 
· 
29:08 
Right.  


Speaker 2 
· 
29:08 
Like from a product perspective. So I'm trying to understand that where to draw the boundary of giving flexibility to the user and ensuring good performance, I guess.  


Speaker 1 
· 
29:16 
Got it. At least two feature may drift. Those could be computed one at a time, right? Like that. You don't have to compute all in one shot. Right. I think features have to be expanded. How does that work? When you make a request.  


Speaker 2 
· 
29:50 
The way we are doing this is we will actually have these parallel computations that we spin up a pool, a gig feature. We are able to compute the distributions of each feature and store that and then a gig feature, we just send out computation in parallel and aggregate the data and send it on the front end, basically. But we haven't tested with really large data sets right now. So just sampling by the concept that we have not built in yet and we know that it's going to be a problem. So it's like an understanding problem hoga. But I guess we're trying to prepare. Once we start doing this with large customers, how do we solve for this problem?  


Speaker 1 
· 
30:29 
We're trying to figure out that's what I was right. Sampling was something which we did. We actually use sampling all across the product. Not just like for monitoring, but even for like explainability maybe like I was telling you about future importance and all. I think sampling was key to. I think also it boils down to how you sample, right? You just like do random sampling. What happens is if there is something known about the data, right? So, for example, a lot of customers will have user data, right? So instead of doing some purely sort of random sampling, you try to include few users journey all the way, right. So that you can have more sort of richer insights on the user journeys clearly captured in your sales. It depends on the kind of insights you're trying to show. Right. Makes sense. Yeah.  


Speaker 2 
· 
31:36 
Understood. Yeah. I think that overall suggestion that you have I think is very helpful. I think I'll try that out, actually, to see how nicely we can do this in real time with a proper sampling technique. So let's let me actually try that out. This is very helpful, actually. I had a few more things that I wanted to discuss with you on this topic itself. But before we dive deeper into that, two questions. Number one, that we are kind of on time, like, not least the blocked time that we had, is it okay if you go 1015 minutes over?  


Speaker 1 
· 
32:09 
Yeah, sure. So I was done for the day, so let's do that.  


Speaker 2 
· 
32:12 
Nice. The second thing is, I'm also curious to understand what's the charter for Junior? What are the type of problems that you're trying to solve at Jumu as well?  


Speaker 1 
· 
32:22 
Right? So Junior is actually a much bigger company, right? I think they are like 200 million arr and they are like 300 engineers worldwide. So it's a much bigger company. So they are into ID verification and identity verification, right. So US banks and all these people who want to sort of do KYC on their users, right? I know that it's a real user. They use junior services, so their users will upload like the documents, right? This is my passport, or this is my selfie. I am the person in this. And then Sumit will say yes. So traditionally in Kakia PUDA back office, Mi Otata, they had a back office in Jaipur, right. So all these requests would go to Jaipur and some human person would say yes, no, or Sich Amar. But now the company has grown, right.  


Speaker 1 
· 
33:23 
They get more than like a million sort of verifications per day, right? And it's not possible to scale these sort of back office. So they are investing in AI ML solutions to automate these pipelines, right? Whether it's usable, right? Mostly image based models is matching together. So all this can be automated, right? And they have like 100 different models for doing all of this, right? Many. Everybody simplify, but believe it or not, there are like 100 different models doing 100 different checks. They have like, I think, around 40 data scientists building these models for them. So I think there's a couple of challenges at Sumit, right? So all across the model lifecycle, right? So one of the key challenges hard for me to tell you all the problems, but my charter is EndToEnd infrastructure support, ML AI team support, end to end.  


Speaker 1 
· 
35:12 
So I'll be the tech lead building that support. So I actually have more than 20 engineers on my team based out of Bangalore and also based out of Vienna. So half the engineers are in Vienna. So the idea is there are different phases, right? So, for example, the developer experience is one big thing, right? So what happens is they are very uptight about privacy. Developers should not be able to see production data, right? Logunki photo logan phone number. So they want to expose smaller subsets of data to these data scientists and make it easy for them to run experiments on the bigger data set without really exposing that data to them. Right. So, for example, Python code TensorFlow model and then the same thing, you can run your evaluation cycle on the large data set, right? Right.  


Speaker 1 
· 
36:29 
One of the challenges is how to build a developer experience key from whatever notebook stuff they are writing. And so it can be very easily be translated into sage maker pipelines, right? And Arun on larger data sets. And we can tell Karee this was the evaluation results and developers can iterate faster. So this is one set of challenges, right. How do we go from a developer environment into a production environment and be able to iterate fast throughout because they have different access levels, right. Like I was saying, developer environment will have some test data, right, which might be anonymized or which might be a. Smaller subject and the production will be the entire thing. So we want to enable this CTO, develop hydrate quickly. So that is one of the problems we are trying to solve, how to improve their experience.  


Speaker 1 
· 
37:52 
So that's a broken experience, right? We want like that's minute may, a past minute May joking experiment. So that is one of the big problems we are solving. The second problem we want to solve is again, it's related to monitoring. It's very hard to predict when is the time. So first of all, it's all image models, right? So there is no off the shelf monitoring which works for them. It's very hard to say how is your image drifting? Fine. Metadata you can tell, but image itself cameras become like sharper or cameras become different.  


Speaker 2 
· 
38:52 
Very hard to detect.  


Speaker 1 
· 
38:55 
There is some research efforts along these parts so I'm not involved with the research, but IBM kind of leading the effort to automate this work. How can we incorporate such signals and automate our sort of retraining pipelines and help customers? How can this select training sets more effectively? So I think automating that like post deploy monitoring and selecting training sets is another sort of set of problems which we'll solve but we are not solving. Now what happens is like Junior May, it's a very old company so there are like six different teams like data scientists and all of them have their own sort of orgs almost right processes or they've all built their own feature stores, they've all built their own sort of training sort of pipeline.  


Speaker 1 
· 
40:20 
So I think the biggest challenge would be to get everyone on the same sort of platform, right? To convince them you can add value. I wouldn't call it a technical challenge, but I think it's more of organizational challenge. So I think that might be one of the first challenges I might have to solve how to convince people that will platformize all of this for you. Because what happens is when people get used to some way of doing it, right? So there are some people who are using Airflow to build these pipelines or there are some people who are using Sage makers now say if you come up with the platform which has some way of doing it, they'll find it extremely hard platform, I think even when you go and sell your sort of tools, right, the same resistance, right?  


Speaker 1 
· 
41:31 
So to go into an established company logo, infrastructure built Kiawai palace, it's always gong to be hard, right? It's much easier trying to onboard like new sort of teams who are looking for infrastructure. But if they already have infrared you need to show how well your infra is compared to theirs, right? Yeah, I think those are at least some of the challenges. I think again, one of the other big challenges is the price, right? So they have like more than 100 models"
10611765880,ProdigalTech,Amit Shinde (amit.shinde@prodigaltech.com);Akshat Vaidya (akshat.vaidya@prodigaltech.com);Atul Kumar (atul.kumar@prodigaltech.com),,Prodigal Technologies,50-100,10-50 Mn,<10,AWS,Yes,Software Development,USA,Anuraag Gutgutia,,,
10611765880,ProdigalTech,Amit Shinde (amit.shinde@prodigaltech.com);Akshat Vaidya (akshat.vaidya@prodigaltech.com);Atul Kumar (atul.kumar@prodigaltech.com),,Prodigal Technologies,50-100,10-50 Mn,<10,AWS,Yes,Software Development,USA,Anuraag Gutgutia,ProdigalTech_Amit 10-10-2022,https://app.fireflies.ai/view/Prodigal-TrueFoundry-Demo-Call::Ud5eAAbgp3,"10-10-2022

Brief background (if discussed)

To see what we have built is useful for them or not. AWS+ Kubernetes Stack. Amit - leads the DS Team (NLP focus). 7-8 people who work full time with us. Biggest KPI is push models to Production.

Use Cases for ML - types of models (Is monitoring important etc?)

Atul - ML Engineer for 3 years. ML Pipelines. DS Part + Latency of deployments (2018) // Chintan - 5 years- lot of work on NLP and Semantic journey // Praful - 2022 grad => Intern - joined Full time and have been exploring full time.

Different Product lines: Major focus is on collections. Beyond collections, also open to other parts. Real time as well as Non real time - how agents performed in the call, etc. Recent additions: Real time capabilities to guide the agents in the call. How to navigate the conversations in the most optimal way. Transcribing it in real time. If deviations, we prompt agents on how to say.

Summarization model - that is purely powered by ML. Auto-submitted in the CRM system. There could be model to identify 1 marker or 10 markers at the same time.

Current Stack for ML Deployments and pipeline

Most stack is inhouse. AWS Stack. PyTorch and TensorFlow. Everything is dockerized. Kubernetes is the only thing we attach the models to. All models are GPU Instances or Sagemaker notebooks.

We push the image to ECR and then DevOps team comes into play. We use those data points to pull to a EKS cluster. Most services in Production have logging enabled - that's where we use CloudWatch services.

We usually set a SLA for making the endpoint available in production from the time image is ready. Instance selection logic => does the DevOps team decide?

Problems being faced where looking for solutions

Everty model - test it against 6-7 machines using SageMaker notebook. The numbers are published and it gives info to the consumers of model.

For Non real time, we don;t care about The latency as much.

Do you use FastAPI? Or use Model servers? We have the backlog items for time. You work with things until that break.

Gold Standard is to train a model, containerize it and deploy it. Staging and Production environment? Does DevOps come in every piece ?

Model Side: Data Drift, Model Drift and Concept Drift. Functional aspects - we realize mostly on users.

Questions asked wrt Product

We had been using EC2 instances for training =>started to adopt SageMaker as well. It saves us cost by charging us only for time when training the models. Sagemaker deployments - we tried, but it is higher than EKS.

We have used MLFlow and WandB as well for tracking experiments -> they are not a big part of our platform.

SageMaker doens't do well on the Data capturing for NLP tasks. We want to have a tool that is integrated with the entire system. Annotation - label studio. We need to label it, process it and then go to training.

Until now, we had public endpoints - Enterprise customers care about data privacy.

Feedback wrt Product - what would make them possibly adopt it

Answers on bottleneck -

1) SageMaker - have tried out different kind of deployments it offfers => What better we offer than SageMaker.

Their Annotation tool and entire ML Pipeline - lot less offering on NLP side.

For NLP models - do we offer something where annotation is done on the same machine etc.

Are you more Cost effective than SageMaker? Batch, Real-time, Synchronous

AMIT:

1) Integration of Systems or Cohesive environment to work in => Collaborative way of working in a Problem without having to share Notebooks explicitly.

2) Maintenance of models and improvement over period of time => Doing pretty well in terms of getting them on production.

CHINTAN: 1) Optimization of the Model Inference. ONNX or Trition or any route we are goi

Concrete Next Steps

Scale: Half a million for Non teal time, 100K for real time. Per call - 128 or 264 requests in parallel"
10611764602,UpStox,indranil.chandra indranil.chandra@rksv.in (indranil.chandra@rksv.in);Gaurav Chauhan1 Gaurav Chauhan1 (gaurav.chauhan1@rksv.in),,Upstox,100-500,10-50 Mn,<10,AWS,Yes,Financial Services,India,Anuraag Gutgutia,,https://app.fireflies.ai/view/TrueFoundry-UpStox-Follow-up-call::NbzEp3QBZM,
10611764602,UpStox,indranil.chandra indranil.chandra@rksv.in (indranil.chandra@rksv.in);Gaurav Chauhan1 Gaurav Chauhan1 (gaurav.chauhan1@rksv.in),,Upstox,100-500,10-50 Mn,<10,AWS,Yes,Financial Services,India,Anuraag Gutgutia,UpStox_Indranil 12-10-2022,https://app.fireflies.ai/view/Anuraag-TrueFoundry-Indranil-Upstox-::q2wvt38J8Y,"12-10-2022

Brief background (if discussed)

Would love to know about our offering and where we specialise in. Set up the entire ML thing from scratch. Transitioned from Principal Engineer to an architect role.

Recently also report to Chief Architect. Looking at other aspects of Platform engineering like Chaos Engineering etc.

Set-up practices, leave it upto the Individual teams to take it forward.

Use Cases for ML - types of models (Is monitoring important etc?)

Started from Ground 0 => No way to interact with Company's data. How many trade orders were placed in the last week? Took 3-4 days to get access to the data. Very slow process was there.

The gap was produced because of the data exposure events. We ended up setting a Lakehouse from Scratch. AWS Cloud - Native AWS Services and frameworks we have built ourselves. From ML Side - actively from April, May and June of this year.

TIER 1 -

1) Associate Partner Incentivisation

2) Trading Nudges program - Push the user in doing things (eg: Not understand the F&O Market)

3) Loyalty Programs

4) RFM Cohorts - How much active a particular user is? User classification and MArketing campaigns

5) Churn Detection

6) Re-activation campaigns 7) CLTV Prediction

Search is painstaking. Abhishek is someone who is trading the Equities. Don't show him ETFs.

User Base of 11 Mn users - not possible for us to priortize.

Current Stack for ML Deployments and pipeline

Search is not Typo Tolerant. Lot of personalization related objectives.

We are heavy on K8s- its the Green Field Project. Migrating all of our workloads to Kubernetes ==> Application part: Tier 1 services, more than 50% have migrated.

Data Science Team is Serverless - SageMaker Notebooks / BI is working out of Lambda Notebooks (EARLIER they were using R Studio) . Other application services - one of the top priorties for migreation to Kubernetes.

All of the models we have hosted are Batch Models and all the workloads are hosted on SAgeMaker. Why using SAgeMaker and not general Kubernetes Infra. Only have 1 MLOps engineer and rest all the team is Data Scientists. We want to see quick time to value - doens't matter if the infra is complicated or well managed. For setting up a new tribe, you need to show the ROI.

USER Flow and MLOps Pipeline look like?

WorkLoads are batch processes that run for 5-10 minutes in the day.

Everything is on SageMaker itself.

Request is Product and Data Strategy team - we sit together and deliberate on it. Exploration phase - figure out the model or data => Feature Engineering is needed or not? Once done --> Deployment strategy is pretty standard. Feature generation job that runs based on trigger. We don't have ML Offering exposed to API EndPoint. ML Offering is not integrated with Frontend. We can support easily using SAgeMaker Endpoints. There are also challenges in serving those kind of responses

Problems being faced where looking for solutions

One of the features that Data Engineering team made live - 45 Ms for the latency for Data Science team.

As your portfolio value grows, your net worth also keeps on fluctuating.

ML Engineer is the one that does it. There is no custom deployment for each of the models. You just configure the final script tht the DS team has given and then there is a CI/CD pipeline.

SageMaker notebooks => convert to script => commit to BitBucket. Jenkins automation jobs for CI/CD.

We have not reached the stage - at a stage where we are proving the ROI. These kind of problems can be solved using ML. Have gone through the same journey in Data Engineering.

Cost incurred in infra? 1st part of the journey.

Questions asked wrt Product

NIMBLEBOX.AI => They have been in touch with us for the past year. They have also built something similar.

Understand the shortcomings of SageMaker - they are however not even relevant to us. Data Platform also we set-up => work with the AWS Service teams => we have a say in priortization of features as well.

We have a leverage working with AWS.

Why would I like to use TrueFoundry? Buy v/s Build => Its always because there is a time to market.

HOW are we different from other guys as well? What's our X factor?

Concrete Next Steps

NEXT STEPS:

1) Would love to see the live demo - Take an example

2) Data Residing at a certain place. How do you build features? Evaluate which is the right model? Exposing the endpoint?

SIMPLE batch jobs to show features --> Show a live demo

3) SHOWCASE a live demo"
10611764294,Amagi,Srivatsa S (srivatsa.srinath@amagi.com),,Amagi,100-500,50-100 Mn,<10,Multi Cloud,Yes,Broadcast Media Production and Distribution,USA,Anuraag Gutgutia,Amagi_Srivatsa 21-10-2022,https://app.fireflies.ai/view/Amagi-TrueFoundry::9Dp6MMdvRW,"21-10-2022

Use Cases for ML - types of models (Is monitoring important etc?)

""Major Questions going into the call: 1) Role of the platform team that maintains the central Infrastructure

2) Monitoring and Debugging Capabilities - how does the re-training pipeline look like

3) MLFlow, DVC for Dataset Mgmt. Something on top of MLFlow for Distributed Training

Overall: 500-600 Team members ""

Current Stack for ML Deployments and pipeline

""1) Maintaining of Secrets in an easy way - who can launch machines, who can create more clones. Broadly as it stands now, we have unfederated access to GCP Resources. Platform team comes into picture in scaling. EVERYTHING related to Scaling is taken care of by the Infra team.

Containerisation: Itself is not a big task - whatever dev environment we have, we use. We don't have to use new containers.

2) Fully on Kubernetes? - Evaluating DataBricks right now. BROADER Data Lake - common umbrella.

3) We are not too much on Jupyter notebooks. Most of the work is on CLI

Problems being faced where looking for solutions

""Videos is stored in cloud env. Most of it is CLI. Dev - mostly Remote. OnPrem GPU ==> Its only when work overflows the OnPrem GPU, we look to train on Cloud Env. Mostly use Spot instances for training.

How do you spin training? We go ahead and get started.

Mostly using state of Art Models. We use SageMaker and their Model API. And that works. Where we are at: If we get a 24 CPU Ram Machine - the moment it starts overflowing, we will look to have a separate server.

Internal infra to launch these runs. Video files are put on S3? You mount them and download them over the network.

There is a Job Queue => processing the jobs ==> In between capturing metrics that are stored in MLFlow.

OverFlow of the Models Memory during Inferenencing. Suppose there are several aspects we can get out of the Video. Currently the Overflow doesn't happen. 16GB Instance is good enough. ""

Questions asked wrt Product

""Where does DBricks fall short?

9902264196

Concrete Next Steps

""Major Questions we want to know from them:

1) Quequing system for Videos

2) Batch Inference - How are they handling Scaling system?

3) How are they handling Multi-Cloud?

4) Secrets Mgmt: How are they handling Cross Cloud secrets? That is hard.

Ideal thing would be to talk to someone from the Infra Team - We can try to win this guy over and then get to the platform team.

VIDEOS: We don't have support for Videos related Use cases - we don't have support for Queuing based system of videos. """
10611764294,Amagi,Srivatsa S (srivatsa.srinath@amagi.com),,Amagi,,,,Multi Cloud,Yes,,,Anuraag Gutgutia,Amagi_Srivatsa 12-10-2022,https://app.fireflies.ai/view/Srivatsa-S-and-Abhishek-Choudhary::HTr0kGhBQC,"12-10-2022

Brief background (if discussed)

Have been in work for 20+ years. First was in process engineering. Then did technical mkting for Kodex. Came to know about the audio-video space.

Then realised that happy to know what happens in the marketing space. Wanted to be more technical.

Started with Network company, then with Stylumia - Vision and segmentation.

Then with Financial mgmt space - Document extraction etc.

Use Cases for ML - types of models (Is monitoring important etc?)

Video ML - Building ML Graphs, add descriptors for the videos.

Bread and butter is cloud based content delivery solutions. There is a lot of videos viewing that has been happening. A lot of people wanted to move to cloud on content side.

Over 500 channels that we deliver.

We don't deliver or control the final UI - Roku sons and tv ==> content owner comes that I want to monetize.

Data Science: Long Tail content => there is very less metadata. Looking at the video, what could you infer about the video. It could talk about a historical documentary etc

1) Where do you want to insert advertisements given a video? ==> Digital players - don't want to spend time

2) Recommendation aspect - Nature of ad, Recommendation

(Current size is 5 members + offers for 3 more people)

Current Stack for ML Deployments and pipeline

We use AWS, GCP, SageMaker, VertexAI - There is a separate platform team that helps in scaling.

Delivering containers to the Infra team. They will build deployment containers on top.

Current use cases is batch - unless business need, not looking to move to real time. Video files -> do annotations -> add to knowledge graphs. Live ue cases - looking to start.

I don't hire people who come in with mindset that I only work on Jupyter notebooks. MONITORING PIPELINE, RETRAINING Etc. Hire people with CS skillset or people who are okay with taking these things up

Rarely work on Jupyter Notebooks. Mostly CLI using dockers extensively. Automated training - lot of parameter search. We are cloud agnostic - we haven't done distributed training. We use MLFlow, use DVC for dataset mgmt. For training, we use something wrapped around MLFlow - launch instances.
"
10540896567,Greendot,Kirit Kottam (kiritreddy@gmail.com),,Green Dot Corporation,>1000,500 - 1B,<10,Multi Cloud,Yes,Financial Services,USA,Nikunj Bajaj,Greendot_Kirit 07-10-2022,https://app.fireflies.ai/view/Nikunj-Kirit-Green-Dot-::diRAv1oBNJ,"GreenDot is much more advanced compared to 

Goal is to scale the product

Descriptive prescriptive  

Moved into predictive space for fraud detection  team is advanced here

Also marketing analytics  team is advanced here

NLP related to customer satisfaction 

If you want to build next best action models, there is a data piece, personalisation piece. 

We are doing predictive modelling on AWS. Deployment does need some IT support. We do have some production loads running. 

Across the enterprise we have a handful key model. I am building a model for every other week for lead scoring but its not lead scoring. 

We are a small team- enterprise-wide handful of data scientists. I have 1 analytics and will get 1 Data scientist. 

Couple of XGBoost models are mission critical. 

At the end of the day I want to scale the product. Anything related to next best action. There is a lot we can do on unstructured data. Personalisation, next best action, and stuff. Predictive model is mission critical. 

We dont have decent structure. Data here is great. From an ML standpoint, feature stores. Deployment of models, MLOPs. 

The week after next would be great for a call. Stopped playing cricket and now playing soccer. Visited Kgp for interIIT. "
10455870769,UpWork,Ashim Datta (datta.ashim2@gmail.com),,Upwork Global,500-1000,500 - 1B,>50,AWS,Yes,"Technology, Information and Internet",USA,Nikunj Bajaj,,https://drive.google.com/file/d/1ggvKQL9DVZhvTeaAqYwG5x4UAiBdNgJ7/view?usp=sharing,
10455870769,UpWork,Ashim Datta (datta.ashim2@gmail.com),,Upwork Global,500-1000,500 - 1B,>50,AWS,Yes,"Technology, Information and Internet",USA,Nikunj Bajaj,UpWork_Ashim 11-10-2022,https://app.fireflies.ai/view/TrueFoundry-Upwork::M3vNaPvvIV,"Another compute platform will be a headache so Private cloud is better.

We use databricks for the last 3 years.

He has discussed some architecture on how the process will look like. For model training- when we move from Databricks to AWS on how the model training will look like.

How can we deploy a model as an internal Agora Service. He currently does not have a great plan for it.

He wants to make sure this migration is seamless.

He expects people can deploy Sagemake endpoints to Agora service.

Agora is an internal tool for managing any services. Swagger is an open source API management tool and Agora is an internal name for it.

FrontEnd team cant talk to Sagemaker endpoint directly.

Ashim says deploying a Sagemaker endpoint to Agora service would be helpful. If TF can also deploy Sagemaker endpoint that can be helpful. His team feels like AWS endpoint is good.

ML team is model building heavy and not engineering heavy. Iniitlaly weused to have two separate team. ML Engineering team was formed by merging these two teams to become e2e responsible.

Historically, DS team will build something.

80% core modelers and 20% backend engineers now.

We are trying to expand the scope of traditional DS. We are also expanding some scope of backend engineers.

ML Monitoring matters as well. Infra team is helping us build some tools.

I am leading discovery aspect of ML Engineering team. Product facing ML teams- search & discovery. Search is doing ML to get the best results. Discovery is when you dont type in a keyword how do we recommend the jobs.

We have 5 ML teams- 3 verticals search, discovery and a small trust+safety team. We have two horozonyal teams- ML Infra which provides platform on how to deploy, compute power etc. 5th team is ML workgraph who build embedding layer of entities. They create signals for us to use.

Overall ML team including 5 people is 70 people. Did not answer the split of teams. ML Infra for 10-12 people. Discovery has 8-9 people. Search has 10-12 people. My title is director for search and discovery. Other is also search and discovery. But we divided search for her and discovery for me.

They have some data drift monitorong. Its an airflow which sends an alert on slack. Today, data is loaded and they say some features look good. If there is an anomaly on data. Everything is a slack alert. These alerts are hard coded which runs simple query.

MLOps is a big focus for us. We have this as part of our next year plan, CI/CD and CM (continuous monitoring). The service should be integrated on UI. Nice UI on monitoring would also be great.

What the ML infra team was suggesting was using MLFlow because Sagemaker integrates very well. Log the metrics in MLFlow itself and then build an alerting system itself. If you have a better solution than that.

Pickle file would be turned into a function and a json would be accepted. We have about 22 such models. Models like availability, based on freelancer activity, gives a propensity score for how available they will be in the next few months. This score is used as a signal on how likely they are to accept. Similarly we have autosuggestion and similar query. On your search bar as you type, this API gets triggered, what are the other keywords that you recommend. A ranking model for recommending models towards job post. When a client creates a job, we feed that job into a job simmarizer service. It gives us 20 keywords, using that keyword, we can fetch freelancers which are close to them- 1000s. From there we get 300 odd people based on ML models. Then on propensity model we sort it out.

We have some mega services. FOe example, ranker model which encapsulates some scoring and ranking models.

Similarly, we have another service for job recommendations. We recommend jobs for freelancers. So 22 models are aggregated into 10-12 services. The goal is to make all of them as a separate microservice.

We also wanted to make the search results very dynamic depending on your response to our search results.

Embedding team itself is very new.

Fasttext- word2vec embedding for job post. What is missing is how is this job post related to other elements. This job post might be connected.

I am saying our biggest hurdle is CI/CD integration easy way to deploy the model. We believe not only will we have product improvement, high quality models etc. but also speed of development will incease. We are going to evaluate the metric and all. We will evaluate based on how many experiments can be done? Right now, we just run 1 experiment per team per quarter. Problems will be of the type- how do I visualise results and all. The problem of dark launch and all will come only later.

Nikunj asked about what is CI/CD to you. He read it out- modernising of ML Infra.

According to recent audits of JIRA devs spend 40% o time managing tech deby which reduces speed and aulity. We want to improve throughout by standardising ML pipeline, model retrain so devs can work on advanced modeling. Drift over time, setting up alerts, help capture model performance etc.

We want to make retraining automated so ML Engineers can work. Deprecating old signal into new signal itself is a complicated process.

Experiment resulted in 5 weeks.

For next steps, we will have converstaion with ML Infra team. If you can give 1 engineer to do the setup- 2-3 week project. Show us how to do dpeloyment in our infrastructure in a qyick way. Give us 1 developer license. We try it out. If it feels like its adding value, we make a pitch for developer account.

Gave Pinterest example- Retool. Suggested you can restrain the number of deployments. You can create the urgency to pay you.

Nikunj talked about an inititation fee- Ashim pushed for below $1000. We have learning budget from where we can utilise. Nikunj needs to make sure that infra director is impressed.

Do a kickoff meeting. Meet him and get to know him. First meeting its better to not pitch. Show him that you would be committed to this account. Ashwin is the Infra director. It can be a quick call. This can be a purely relationship building. You can say you will come back with a proper demo.

His team also needs ot give one engineer

https://www.linkedin.com/in/ashwin-prakash-3494192/ - Director of ML Infra

https://www.linkedin.com/in/ericaleeai/ VP_ she mentioned we cant have separate teams - one that builds models and one that deploys. She integrated them.

The way Upwork used to work is freelancer based. Upwork would have really good product managers and freelancers would build the silution. Our CTO was hired last year. He just completed a year this August. Upwork started thinking about engineering at Scale. Your platform and tool could play a good role.

In the All hands, new hires were announced. 16 people hired in the last quarter. Out of these 70!

Erica integrated 20 backend and 20 DS. Used to be a 50-50 split. Now we are hiring model specialists. Pure backend we are hiring less. We want to hire full stack ML Engineers. Backend engineers that existsed still exists and we want to expand their scope a bit. We want to hire a few more.

Ashim mentioned right now whatever you do you will evolve. You would benefit a lot from working with Upwork."
10455870769,UpWork,Ashim Datta (datta.ashim2@gmail.com),,Upwork Global,500-1000,500 - 1B,>50,AWS,Yes,"Technology, Information and Internet",USA,Nikunj Bajaj,UpWork_Ashim 05-10-2022,https://app.fireflies.ai/view/Ashim-TrueFoundry::1ElHrwwHj2,"Defined a job using sagemaker, and then using your platform scheduling or running it, on a daily basis without any hassle, containerize it. 

Come up with some dummy data, how you deploy using your own cloud. If you are willing to do a small POC to try out deployments- give us one account access. It can be a pilot project, we can expand from there. 

Will talk to ML Infra director. Talking to him this Friday itself. Ill bring up. Ill also talk to my VP. His push back will be - be prepared that it wont be funded. 

What we have to keep in mind is that- in case, we do this POC. You should always have a plan for derisking. What is our plan B? 

How Sagemaker endpoints or different? 

Solr cloud connects to ML store.

Model files as pickle files 

Solr cloud should just call an API endpoint We want to use AWS endpoint. 

Model is connected with feature store for realtime prediction 

Grafana monitoring manually set up 

Training happens on data bricks. 

Tecton is bloated up 

Quarter to half a year if its a fresh model update. 

3 weeks to 1 day is a great feature to try out. 

Comfortable with a server less kind of environment and kind of wanted K8s details abstracted away. 

Developer productivity is a goal and we want to go from number of experiments run to be increased by 20%. We want to be able to run 12-13 experiments a year (A/B testing kind of ). One way we evaluate developer productivity is A/B tests. 

Listening to the Upwork call, they want to increase number of A/ B tests being conducted by 20%. They are a in a low traffic business which means being able to perform shadow testing can really help them achieve their goal. We should highlight traffic shaping and shadow testing in the context of A/B test to them.

Models converge with the principles of Ci/CD right. They want to do continuous training but they are also married to Sagemaker endpoint. 

Need to know when to retrain them. Are the features correct or not? This is more on the monitoring part. 
"
10455870769,UpWork,Ashim Datta (datta.ashim2@gmail.com),,Upwork Global,500-1000,500 - 1B,>50,AWS,Yes,"Technology, Information and Internet",USA,Nikunj Bajaj,Notes not updated,https://app.fireflies.ai/view/TrueFoundry-Ashim::1aLWmvSKUw,
10443270777,Neurobit,Amiya Patanaik (amiya@neurobit.com),,Neurobit,<50,<10 Mn,<10,AWS,Yes,Home Health Care Services,USA,Anuraag Gutgutia,,https://app.fireflies.ai/view/NeuroBit-TrueFoundry-PoC-Walkthrough::47Ab5G6QkqtAd4Zj,
10443270777,Neurobit,Amiya Patanaik (amiya@neurobit.com),,Neurobit,<50,<10 Mn,<10,AWS,Yes,Home Health Care Services,USA,Anuraag Gutgutia,Neurobit_Amiya 05-10-2022,https://app.fireflies.ai/view/Amiya-NeuroBit-TrueFoundry::LVgGjAyXXa,"05-10-2022

Current Stack for ML Deployments and pipeline

We have the GOOGLE bucket, we have all the config files in the same bucket. All models exposed through GRPC in the same box.

Right now, its just a box with Kubernetes connected.

CPU Inference time: Depends on the type of Model. The most similar one will be similar to inception network. Most complex ones have Million weights. Inference time is roughly how much? 1000 Batch requests - half a second to deliver the output in 1000 classifications. Not really real-time as the 1000 is available offline.

There are use cases where we do need real time as well.

Roughly - what is the memory consumption of these kind of models - None of the models would take more than 2GB. GPU is T4. It is a pretty normal standard machine with 16GB of RAM.

END USAGE: 1) Web Portal => Upload the data => Get a report (Support) 2) Wearables and Sensors - which you wear with our app => pushed to the cloud.

Problems being faced where looking for solutions

* What is the Problem you are facing? 1) 100 Simultaneous requests if I do, it crashes immediately. Docker file has to re-start. We don't know how much it can handle?

Load testing and optimization has not been there. But its critical. If a clinical trial is running, ML Service fails, no report generated => 1000$ for someone doing the trial.

2) Authentication: I did a hack using Private and Public key. How do you ensure that GRPC end point is not open to anyone outside the Dev team.

APP Team is deploying on Kubernetes. 2 Questions here - Why not deploy on Kubernetes?

Is the ideal state? One Service for Models? Or want different endpoints for each model?

One AUTHENTICATION is fine. Optimize Money and Reliability - Clubbing some of the bigger ones together and then the smaller ones in another.

The Real time one could be a separate Box itself. There inference time matters. But in batch, it doesn't matter - takes upto 5 minutes.

Traffic pattern: Some models will have morning traffic.

Why not in Kubernetes? Within the company, very few people have access to the models. Only few people in development know deployment.

Questions asked wrt Product

Cost: Paying a $1000 bucks for a few customers.

For me, it doesn't make sense.

100 Customers - We are paying 1000$ right now.

RPS: Very Few requests ==> Clinical folks will identify a lot of files together.

EVERY 6-7 months, it crashes a lot.

Feedback wrt Product - what would make them possibly adopt it

I have tried Cortex - its only on AWS. It is like wherever the credits go, we follow. We got new credits on AWS, so we can move to AWS.

Was a year and half back - it wasn't as easy as the website says.
"
10443270777,Neurobit,Amiya Patanaik (amiya@neurobit.com),,Neurobit,<50,<10 Mn,<10,AWS,Yes,Home Health Care Services,USA,Anuraag Gutgutia,Neurobit_Amiya 29-09-2022,https://app.fireflies.ai/view/TrueFoundry-Upwork::M3vNaPvvIV,"Highly Qualified - Next call set-up for diving deep into the Problem Statement (this is on Wednesday tomorrow)

Amiya Patanaik - NeuroBit    

IITKGP Person; EF // +6590623184

The problem we are solving is going to be universal for any company. As a company, we dont want to spend time worrying about Ops thing. 

2009 - Graduated, around financial crisis time 

Worked at the juncture of AI and Neuroscience 

Sleep is not a state of being and very strongly connected to longevity, sleep condition, mental health 

Measuring Sleep is not a trivial thing. Non-actionable - measuring sleep 

Spend a lot of time developing AI = Take low grade info but can extract high quality sleep, vitals etc. In process of getting the FDA 

AI Perspective: TensorFlow for developing, Notebooks, Protobuff of Tensor Flow Serving. We update the bucket where models are. 

Challenge Issue: 1) 20 Different models running - Cost skyrockets. Dont want to spend money randomly ==> How do we efficiently deploy them? 2) How do we have a pipeline so that DE follow a SOP and give a model? 3) Canary and Shadow testing 4) Right now, what has happened - 90% models built by me. Model is the IP of the company. MOST MODELS DEPLOYED BY AMIYA . They access it like a GRPC API. Authentication is another area where we are struggling. Own Private key and using it. 

Very Strong use case ==> We work closely with Academics. 140+ University on our platform. 

We used to deploy all our apps on KuberNetes "
8632654345,CourseHero,Reza Jamei (rezajamei@gmail.com),,"Course Hero, Inc.",100-500,100 - 500 Mn,25-50,AWS,Yes,"Technology, Information and Internet",USA,Nikunj Bajaj,,https://app.fireflies.ai/view/Reza-Nikunj::4sK7t4V8ln,"Hello? Hi, Nicole. How are you? Hi, Liza.  



Okay.  



Hello?  



Yes, I can. I'm actually out for family function, so you would notice me with these weird things on my forehead.  



What are you doing?  



I'm actually out for a really major family function where they have some ceremonies and all that they have to go through. So that's what I'm doing right now.  



Nice. Cool. I sent you a text message this morning. A bit late. I don't know if you saw it.  



Actually, I'm not receiving my text messages anymore because I'm india and that number is not active, actually.  



I see. Yeah. So the only thing I was trying to say was we are still waiting for Lena's assessment. I just noticed I hadn't seen my notes. But we are waiting for Lena to make a decision. Right?  



Actually, I'll give you an update on what happened with Lena. So she had a few questions during the meeting. I think I answered all the questions that she had. In the end, I casually asked her that. What are some of her concerns? So she brought up only two points of concern that I will share with you. Number one, which is like, when she has to get through Founder deployed within 40 cloud, people have to spin up like a small dedicated Kubernetes cluster for this engagement, which is like couple hundred dollars, maybe a month cost or something. Right? So that's one thing. And the second concern she had was if your team will be able to justify the ROI, the return on investment. Because I had told her that it's like $400 per month per developer. So that's the part that I told her.  



So she's like, we will discuss these two things internally and get back to you. I think these were the only two things. So it felt like mostly it is about the cost. It could be a little bit about a little bit of an increase, the customer management, but that is a very small change. So I think most of it falls down to cost reserve. And that's where the part that I mentioned to you that if you wanted to bring up to the team, to the core Zero team, that given that this is an early engagement, we can actually offer, like, a 30% discount or something. Maybe that's something that you can pull off internally that I negotiated with Nickunge and I got this discount. So we'll try to not make this a blocker, basically.  



So that's one thing that maybe I just wanted to share with you and not lean on directly.  



Yeah, the cost and all the discounts and so on are very helpful. But the main thing in my mind is the thing that I am trying to get to happen, and I don't know what is the blocker for that is we have a really large number of really useful functions that are being developed in Python at the company, within the team. And many of these functions, they are really useful. But the people who need to use them, it's not that they don't know Python, but they are not on a day to day activity they are doing. They don't need to get into that environment. So it's not convenient for them for calling such and such service to get into a Python environment.  



So my main interest has been to make these functions just very quickly make them available as a service so that other people can use them. And what I'm hearing from the team is that this is easily doable both with Truefoundry but also they are saying this is easily doable with tools like what is it called? AWS sage maker can make these really easily available. But what I don't understand is then if it is easily available, why is it that we are not able to create this? In other words, the process of so usually it's not worth it for me to ask the team to create one of these services. If it's going to be a Sprint long activity, I sort of envisioning that if it's available in Python then it shouldn't take any time to make it a service.  



But somehow I'm still hearing that it takes time. That is the biggest thing for me. Is it going to be compared to, let's say stage maker service creation? Is this going to make things much faster for us or not? That is unfortunately something I'm not able to understand for some reason. I need to take a class in DevOps or something like that.  



Actually, I can give you some perspective on that result. I think the team is not wrong in saying that this service creation can be done with Sage Maker as well. They can do it with Sage maker. There are a few things that are different though in Sage Maker. Number one, that the APIs are not as simple as what we have built out. Like, the developer experience is not as seamless. And this is something that it will not be too hard for you to validate for yourself, which is actually you can go and check out the documentation of Sage Maker and, like, the experience that you and I got, like, in five minutes. You were able to deploy a function using Truefoundry.  



I can almost bet for anyone new to Sage Maker that they will not be able to do that in, let alone five, I would say even in an hour or two by the way. Right. So that's something that I can bet on in terms of the developer exist. Now that said, once people have really learned Sage Maker and they've become an expert on Sage Maker in some way, I think they can do it fairly quickly. Maybe like an hour or 2 hours or something like that in Sage Maker. So it's doable, right? But in that case, you will also not have like the arbitrary function deployments that we have. Like you would have model deployments very easily, but arbitrary function deployments would generally not work out. So that's one thing that we differentiate with sage makers.  



And the last thing, to be honest praiseA is the level of customizability and support that comes just with the merit of truefoundry being early in a journey. Right. That as a founder, I have a huge control on the products roadmap. And if one of our early customers needs to customize something, needs our help in figuring out something, needs our help in building an integration, they would actually go ahead and do that, which would generally from an end customer's perspective, make it much stronger value proposition compared to going with like a vanilla service like Seed Maker. You can actually get at any point in time in life, right. Seat maker will always exist. But true boundary type of deals only come when the company is early enough.  



Because even for us, to be honest, we cannot continue doing this after a certain scale, once we have hit like a certain number of customers, we will not be able to offer that level of customization. So this kind of deal only comes like, when like there's a really good match, basically. So from that perspective, I think that's the other thing that works in favor of our early customers, which not most people will get. So I think that's truly speaking and trying to offer as unbiased perspective as I can, I think those are the only two things. Like our developer APIs are much more data scientist friendly. And the second thing is the level of customisability right now is generally unparalleled.  



Sounds good. It sounds right to me. The similar experience I had once was early days of actually AWS. AWS was never actually very small scale, but when they wanted to grow really fast, they were offering consulting services from which I told you, I learned Spark myself from the representatives. Like I would get on the call with them and they were available twenty four seven. I would call them and say, I need to do this. And in one of the meetings they told me, oh, you can do this with Spark. And here is how fast you can learn about it. Like the very basics of it. They tried to math and reduce and then they said everything else is ridiculous. So I am very much interested in that.  



If we can have your time to be explicit about it as an advisor, almost not necessarily very directly being even about two Foundry service, but let's say I ask you. So I'm very interested in that. I think I can push for it.  



Yeah. Actually when I talked to Lena, she said practically the ball is in your court, actually, because she was like, so long as Reza feels confident that he can justify the ROI, I feel okay with it. I think that was Lena's final message to me during the call. Basically.  



Yeah. I feel that I have no idea how price sensitive we are. I even don't know exactly who is the gatekeeper on that. I don't think Lena makes those decisions. Maybe Daniel makes the decisions. I can talk to them. I assume that you are okay with a month of trial, like free.  



So usually we don't do a month. Usually we do a week of trial because honestly, it's actually more than sufficient to try out the entire platform. But if it's something that matters to you, I can make it whatever, like a few extra days, like maybe two weeks or so. But I think a month is generally anyway, you will not end up needing that much amount of time to try the platform.  



No, I'm saying it more because one of the two things will happen. I suspect it doesn't cost you anything to provide the services for a short time. The only thing that matters is are we going to be to onboard on the platform? And if we do that, it probably will be for a long time or it doesn't work for us, which means it doesn't matter if it's a week or two weeks or how long.  



It doesn't matter. You're right. Really? It does not matter to me if you want to take an extra week or so. The only thing that ends up happening, and this is just I'm experienced, speaking from past experience, that if you put a headline with your team that, hey, we have whatever this thing for a week or two, whatever, like a week or so, you actually get your response much faster. But when you say that, okay, we have this for a month, people forget this, and even last week they would not actually try it out properly.  



I see. So one quick question. All the Kubernetes set up and permissions and things like that are completely clear between you and Lena. Basically, my team doesn't have to do anything about it?  



Yes, your team does not have to do anything. I've answered all the questions for Lena.  



Okay, sounds great. Now I'm suddenly very curious what it is on your forehead. I'm seeing something red, but yeah, it's called a Tillock. It's not the same as the usual Indian dot. Is it the same thing?  



No, it's not the same as usual. Like the usual Indian dot is usually for females and usually it's called bindi. But this is like any time you do a worship, it's kind of an auspicious thing to put on your forehead. That's what they say.  



Yeah. Very nice, very cool. Let me see. I don't want to promise something, so let me just think out loud so that you know, also where we are, it's the last three weeks of the year because end of December, usually a lot of people don't show up. And we have a few other kinds of Deliverables, which is not. But you haven't met Dylan, have you?  



I've not met Dylan.  



Dylan dylan is the most engineer, ring type person on my team who knows a lot more about engineering aspects of things. And he has a little lighter schedule or Deliverables. I wonder if I can ask him to test this. So what's the right thing to do? To tell Lena that we want to move forward with a week of free trial?  



Yeah, I think we can do that.  



Okay. And put Dylan on it and ask Dylan to test that versus AWS and give us a final answer? Sure. For sure.  



I think that would be a great answer. That would be good.  



Okay, sounds great. I'll message the three of us, or four of us. Okay. To see what people think. Okay, awesome. Have a great time.  



Probably sometime in January.  



January. Okay. Sounds great. But I assume nothing about location effects. Bye. Good day. Bye. "
8632654345,CourseHero,Reza Jamei (rezajamei@gmail.com),,"Course Hero, Inc.",100-500,100 - 500 Mn,25-50,AWS,Yes,"Technology, Information and Internet",USA,Nikunj Bajaj,,https://app.fireflies.ai/view/TrueFoundry-Demo::DDFoKGvsfv,
8632654345,CourseHero,Reza Jamei (rezajamei@gmail.com),,"Course Hero, Inc.",100-500,100 - 500 Mn,25-50,AWS,Yes,"Technology, Information and Internet",USA,Nikunj Bajaj,,https://app.fireflies.ai/view/Reza-Nikunj::OWvV5WtL96,"Hello? Hello?  



Yeah.  



Okay. I think my ipod got connected. Give me 1 second, let me just get my phone and message reject.  



Okay.  



Personal ID is very easy to get.  



Hello? Hello. Hi, Rizal.  



How are you? Good morning.  



Good morning, how are you doing? Very well, awesome, thanks. How are you Sri?  



I'm good, thanks for asking.  



Would you say three or three?  



Hurry, everyone got a three.  



I had an advisor named Sriram and they had a close colleague, a lot of threes around, basically.  



By any chance? Sriram? Your adviser is Sriram at Linden.  



No. Sridram Sharla is a physicist at UC datta Cruz. Or at least at the time was at UC Center Cruz.  



Oh, I see.  



Okay.  



So you mean like in terms of the former.  



Okay.  



I see.  



Understood.  



Yeah.  



There was a physics poster. Yeah. Nice.  



Yeah. The reason I asked is there's this person called Sriram at LinkedIn, a fairly senior person. I have probably met seven to eight people independently in the Bay Area who have called. Sriram is a mentor. Oh, Sriram is an advisor who is the Sriram advisor? There's so many people in the Bay Area.  



Interesting. Sridram. Yeah, I don't know this LinkedIn. What's his last name?  



I'm forgetting actually, everybody is like Sriram freeram IBM checking.  



Yeah, I just remembered actually my last boss was three, but I don't remember his last name either. It's weird. Is it a memory problem I'm having? I think we never called his last name, so this is why. Okay, yeah, I remember Chester's last name because there were papers and things like that. Let me see, three subramanian was my last boss at oh, he's not at Amazon. Cool.  



Awesome. Basically for today, our main thing was to kind of try out like a model that you might have built. By any chance, do you have access to a model that you wanted to try today?  



I have a model on my work computer. I have to bring it to my personal computer with which I'm talking to you guys. But you know, actually looking at the example that went through, I feel there isn't any point in trying a different function. Basically any function can be deployed, right. And anything that can ben installed on a local machine, we basically put the same imports and all of that will be put on the virtual machine that you create. Right, that is true. So I would say that is not I mean just changing the function without any introduction of hair into it. There is no point in it. Like anything that we get from, let's say hugging face is just going to work, right? There is no point. So one thing I wanted to try was one of these text summarization models.  



But because, you know, it's very interesting to me, a lot of hugging face models, people are building businesses around it without adding much else to it. I don't know how successful those businesses are. But if somebody can quickly go through adding a little bit of nicer user interface around them, you can try a lot of businesses quickly to see what sticks anyway. I would say no need to try anything along those lines. But if we can go through some hairy situations that have a potential of not working, for example, when several computers have to talk to each other, I assume that may be a new thing to consider or even training of a job, a machine learning job where the data is sitting in a distributed place, not on the machine that trains it, anything like this.  



Is this maybe a more reasonable thing to try?  



So the idea being that the datta resides somewhere else and the machine like from the machine where the job is getting run. That's what you're saying?  



Basically, yes. I understand that most of the time people actually bring the data to the machine, train something, and then if there is more data that can sit in the memory, they delete it and then they bring a new batch of data or new set of batches. But are there facilities to do this behind the scenes so that at least the user doesn't see explicitly what is happening here? Let's say the data is sitting on S Three, so it's not in memory or anything like that. Is it possible CTO tell the model. Here is my data, go train on it and anything that is needed to happen in the middle, that is chop it up, chopped the data up, bring pieces of it in, train on those batches, get rid of that, bring the new data and that kind of a thing.  



Are there facilities from Giri truefoundry or do we need to write it ourselves?  



Yeah, so I think we might already have an example where we read data from S Three.  



Right.  



Free actually, I was trying to make it work, so I actually went through your use cases. So it seems like you're fedging data from S Three and doing some sort of transformation. IBM initially writing some embeddings to Pine Cone.  



Right.  



Is there a specific use case?  



I think that is like the overall use case that Reza described, I think. All right, now Reza is asking is basically, correct me if I'm wrong in understanding this. Wherever the code is running, which is, let's say a Kubernetes machine, a Kubernetes pod, the data sep somewhere else. So let's say S Three and basically that function is able CTO read the data from S Three and do whatever processing that it needs to do. Right. And then there are different times when the data itself can be read in full or it can be read in batches from S Three. Right. So all of those things, can that happen? I think we already have some examples. I think I might be able to pull up something yet to show how that is working. But honestly, from a programming model perspective. It works.  



There is no reason that it does not work because all that is happening is that you need to provide the access to that S Three to this function where this thing is running, to this pod where this thing is running, so long as you have access. After that it's running a Python code. You can either read the entire S Three bucket or you can read it in loop, or you can read it like if you have like a data reader from TensorFlow or something and you wanted to read it in batches like later out of it. All of those things are basically vanilla Python code. And the way we have built out the system is any vanilla Python code can be deployed, basically. So theoretically, this will work. There is no reason that it will not work.  



And we even try to pull up an example.  



Actually, no, if you're saying that there is no reason to think there may be anything that doesn't work here, we can think about the more useful do.  



You want to add something?  



No, I was asking what is the size of the data we are looking at here?  



So let me see what example we can think of. So if you want to think about course heroes use cases. We, for example, fine tune existing NLP models that we import from hugging faced and we fine tune it sometimes on our own documents. We have something like 100 million documents. And I don't remember what's the typical memory size of these documents, but let's say on average, let's say three pages of text. I think it's rather small. So let me see how much is do you guys know? Sorry, go ahead.  



That's fine. And each of these documents is a single file on S three?  



Yes.  



Okay, go ahead.  



So I just wanted to estimate, let's say 1000 characters. Each character is how many?  



And we're talking about 100 million documents. So it's gong to be 300 GB.  



Basically it can be brought to one machine because 300gb can sit one machine, right? Not in memory, but it can sit on the hard drive by machine. So there is no point in not bringing it to one machine, right?  



It could be expensive, I think generally, I think with 300gb data, you probably already want to get to either read it in batches or something. Maybe that might be the reason. It depends on the type of model that is getting trained. Like if the model itself is such that there are some models which are like more well, you're talking about text data. For you, it does not apply. You can actually always train in batches. There's some data that require the entire data to be accessed at the same time. Those models, I don't think you will want to claim one machine. Just one quick thing that I wanted to show you here. So, for example, here's a real model that we had deployed. And this is like you can see that the code is written in a very jupyter notebook style, to be honest.  



Right?  



It's like there's no functions, nothing. It's actually a code that's coming from the jupyter notebook. Like somebody wrote like these multiple cells of code and we just put it together in a Python script and deployed. So notice that in this case, the user is reading some data from S Three, basically. And you can see that this is like an S Three bucket from where the data is coming from. And then you read the data, you do whatever processing that you want to train the model and the entire thing that we chad discussed the other day and after that it's like the vanilla model training code, basically.  



So the deployment of this is basically the same as like what we had discussed the other day, where you write like this one function where you're defining a job and then you hit a job deploy and this function basically gets deployed. The only thing that is happening is that you end up providing we have a Secrets manager, we will show you that you just give access to that S Three bucket. That's the only thing that happens.  



Not a demo. Actually, I can just show the UI for this as well. I actually made a script that is sort of running some scheduling a file from S Three and sort of blues and processing on top of that rate. So I can show that. I just started working on it a while back, so I'm still not finished with it. But I think pretty much 90% of that is there. What it's doing is I also did the Pinecone thing as well because we figured you guys use Pinecone for slowly embedded. So basically what we have here is a CSV file, input CSV. And let me see by the data frame on some notebook. So it's actually a bunch of text. So there's a title and there's a context.  



And what I'm trying to do is generate embrace for each of these contexts and stored that with point contact. And this file is actually existing in an S Three bucket. So what I'm doing here is I'm using the S Three library, the Model Three library to catch it here and it becomes available in the environment as input CSV. And then I'm just reading that generating embryos using the sentence Transformers library.  



And then finally 1 second, just as a quick reminder. So what trees are describing is when we met in person in Redwood City, we had discussed a use case that you mentioned about that this should be a product manager who can point to a different data in S Three and then use one of the hugging face Transformers and then index it in Pinecone and then play around with the data set, right? So were just trying to see how would that entire experience look like we have a platform and she was just trying to build that. So that's what he's demoing. We are reading some data from S Three. Instead of hugging page, we are using some transformer like just to generate Embeddings and then indexing it in fine cone.  



Got it. Can I interrupt you for just 1 second? Is it useful if I bring an engineer who typically does this kind of thing so that they can ask you better questions about? So nico gender have discussed this. The thing that the main interest here was from our engineers I am hearing that we have technology to create these services and deploy them. But the time it takes typically to do that kind of thing is a lot longer than what True Foundry shows. So my main interest is, which I kind of don't understand myself, is what is the difference? Why is it so much faster to do? Like you and I, we saw that last time we just sat in front of each other and did this in five minutes.  



And then when I asked my team, they say it takes a month to do some of these things but I don't know the right questions to see what's the difference should I bring somebody? Are you guys okay with bringing another person to this meeting? Yeah, of course. I want better questions to be asked. Just give me 1 second. I don't know if people are it's a bit early in the morning for but give me 1 second to see if I can sure. I have CTO ask somebody to come here unqualified. Sorry about this guys. Looks like we won't be able to get them in the next ten minutes. Let's continue here with ourselves. So this is the deployment thing, right?  



Yeah, exactly.  



What I was able to do is just take the script and basically make a deployment out of that. And so couple of things I had to configure. One is like I'm asking history here and I'm accessing Pinecone here. So I had to sort of have a way CTO authenticate this. So if you look at the configuration that I provided, what I said is okay, IBM going to call this my Embedding Generation job EMV Generation. I have it on GitHub. This code that I just wrote, I just pushed it to GitHub and I connected my repo from that sort of this takes a very long time here and finally how to run this right? I mean it's a Python script so I basically have to just call Python Job PY and the bill context is Job here.  



This is my report just to show you where it decides in Job folder. So I just tell the program that tell the application form that it's inside the Job folder and then I had to configure the credentials here. So I need two things. One is a way to connect with S Three and the other one is a way to connect with finecorn. So that is being done using the pineconnect key here, which is provided as a secret. So what I do is like a secret manager is something that is available with us. So you can just store it API cash with us and then use it wherever you want.  



Right.  



So I already stored the point code, API key and the access key and secure key with us. And then I just made it available to this particular program. And then I hit submit, right?  



Basically.  



And that creates a job for me.  



Just one quick second here. If you go back to that. Any chance we can go back to the UI?  



Yeah.  



So, Riza, just again, reminding one thing that I think you asked one question around why does It Take So Long in the internal Engineering Team? And why did it seem so fast? What we did? Well, there are two parts to that question. One thing is script itself that the engineers are writing. Right? This entire logic of reading something from S three, putting the hugging face transformation and all the code that is needed for that logic itself. Right. That itself can take long. Maybe they're doing a lot of experimentations that will take time, et cetera. Right. And to be honest, Roofon, it does not optimize for that time itself. Like, how much time?  



Let me give you an example.  



Yeah.  



Our team has an internal service which is already written in python where you can give to the python function. You Give the Name of Athena Table and the Name and Two list of Labels that you are interested in predicting some subset of the Columns, and another List, which Is The list of features and When You Give It to It, when You Are in a Hurry to analyze the data Set to See, let's Say, for predicting such and such column, what Features are Important. You Want to Get a Quick report on it, but You Don't Want to have the model, you Don't Want to scale it or anything like that. So we have a function. You give it a table name, it goes sales the rows. Doesn't even use the whole data set.  



Okay.  



And Uses the Features to predict a predictor random Forest model and just Reports the feature importances. Now, this is the thing we have. I can see. We can run it in Python and it works. When I Was Asking, can we make this a Service on our site so that anybody, let's say from analytics Quickly can Go enter the name of the Table and this Thing returns on the Web Page. No Python or anything Like that tells you for predicting this feature, the important features are these Features. And this from after having the python function for it to making it a service. The estimate I got was two weeks to create a service out of this.  



Okay, I see.  



I wanted to understand. So I'm thinking the right kind of meeting maybe for us is one of my engineers who typically does this and maybe because I have heard that some parts of the time is communication with DevOps, what kind of permissions and things like that should be allowed. I don't know if we want to do this async like through chatting or through video sharing to see what is the difference between these. Because I am seeing that you are showing like last time you showed that this actually goes through in a few minutes. Right. I don't know what to ask now to see the difference between I don't understand the middle steps well enough myself. CTO okay, understood.  



We can totally do that. We can actually have one of the engineers come in. I think usually it's best with new tools that we do it synchronously that is over a call as opposed to doing it async because usually what happens is anytime people get stuck in any one part of the tool, it ends up taking a lot of time to figure that out. Basically. I think it's just better that we do it together so that we also build a better understanding of the Use case and your team also understands that. How is it happening? There are fundamental reasons why. So the thing is that this platform that we have built out itself took us maybe like four engineering years, maybe even more to get just dedicated like building this thing itself. Right.  



So there are a lot of things that we have built out internally that allows us to deploy things faster. So ideally, once we scale this thing, it's going to take like we are actually hiring more people. It's going to take even longer. There's a lot of fundamental reasons why this is like happening fast here. We'd love to showcase this to an engineer, kind of like even potentially. I think the best type of showcase is that you already have that one function that you're talking about that you wanted to create a service out of. We still have a meeting and in one working meeting we tried to deploy that. Maybe if you're not able to do it in one meeting, we take another meeting.  



But hopefully in one or two meetings we should be able to deploy that essentially like one meeting where we have a better understanding of the Use case and maybe in the second meeting we just go ahead and deploy it essentially. So that's good next step.  



Yeah, sounds good. So I am guessing that our DevOps is going to need NDA's and these kinds of things. Are you okay with that?  



Absolutely. We frequently go through like NDA process and all.  



One other question, sort of question about your business, not about this specific thing. What if are you even thinking or is it a possibility to instead of sort of buying the service from you, the platform? Possibility. We include the engineering time to do even these parts that you are showing. Like we buy the whole service from you, like asking you to create a service that does these things if there is any other wrapping function and things like that around it. So our contract becomes Create a Service for US that gets the name of Chris Service, name of this table, runs this function on it and puts the output here and make that possible. Instead of us even doing the wrapper, instead of using the platform, basically you become the Consultant on Instrument.  



The short answer is no. Our focus as a Team is to empower the Teams to be able to do their stuff fast. Right. And we want to maintain our Focus to do that. That said, in the beginning, if there's one use case that you thought potentially we could help with, we build out a small thing to showcase the value of the platform and all we are Happy. And that's one of the advantage that companies get of working with us as initial design Partners that we are flexible to do a little bit of that, call it the consultancy work. But honestly, in terms of any kind of long term contracts, I want to make sure that the expectation is set right. That's not something that we want to take up long term, essentially.  



Got It. Makes Sense. Should I send you? I want to create an email thread between anybody from your site, myself, Lena from our DevOps and one of Engineers on my team. So that if there is any question that needs to be asked before we all get on a call, they can ask it. Is there any free information you can send? Including this, so that everybody knows. I don't know exactly, but anything that you are Guessing that they need to know.  



Even before we Talk, I think we should also have some questions. I think we know A few Questions that generally comes up so we can try to send it out. You have some things that you want to ask.  



Yeah. Before we involve the DevOps, we want to see what is the exact use case that we are trying to solve. Like you mentioned that you need to create a service for a function. Like how often do you get this requirement? Is it quite often. Like let's say we're a consultant. So how often will customer requirements come?  



Yeah. So? Good question. So Almost all of the services we currently have in our company are Go Services. So a very different type of engineer that I don't interact with. Typically, they go build our services. As a result, we don't do it Very often because all the Engineers that are more familiar with Machine Learning, they all work in Python. And So Our process of creating services typically becomes the engineers who know Machine Learning, write the Python functions and so on and Then We completely hand it to a different team. And that team has a million other things to do. They don't just do machine learning, they deploy all sorts of things on our site. So that becomes a very slow process. So we have very few of these services.  



What I am hoping to do is I have so many of these small services that I like CTO have, and I think it increases the productivity of our company quite a bit. Like having a Python if our engineers who create these really nice tools in Python, if they can quickly make it available to everybody in the company, like the one I mentioned quite often, a lot of the time of our analysts is taken for doing something exactly the same kind of manual thing on a data set. Like somebody asks them from executive team some question about, let's say, our subscription stream. And the exact same set of things have to be done up to a week of time each time from analyst to go see. The first thing they have to see is what are the columns. What types they are.  



How many numbers they have. What kind of correlations they have. Do some kind of imputation on it. And then build a simple model. Like a random forest or something like that. See what are the important features. Get rid of unimportant features. Build a more sophisticated model. All of these things. And I'm thinking that we should have a service on a page that anybody can go there, enter a table name and all of that information shows up for that table. And similar things like for joining tables, which columns are join up, all that kind of thing. So after all of this discussion, I want to say we currently don't build a lot of these services. We only build services that are needed for millions of customers as soon only when millions of customers will benefit from it. We build it currently.  



But I want to change that to building these things a lot more often. Does it make sense?  



Yeah, that totally makes sense. Riza so, I mean, we can definitely work as that team to whom basically we can work with your ML engineering team, reserve, and we can help them build all the services initially. It can require us hand housing your current engineers, like whatever they're doing. They're currently handing over the services to another team, this Go team that is doing the services. And it takes a lot of time because they have a million things to do, as you mentioned, right? So we can do that and we can work with your engineering team closely to get all their services deployed. And over time, after they do two or three times, they themselves will want to it's so easy to use, like once you use it two, three times, rather like they will develop themselves, learn that lot.  



So the only thing I want CTO add to this is we also have built a template service at the company for Python deployment. But that template, I hear that even that template takes like a couple of weeks to deploy the simplest function. After all the functions are created and everything is available and so on, just to turn it into a service that anybody can look at, it takes some time every time. So better to say I haven't seen any of the services I have requested. For some reason. I'm trying to understand what that reason is. That is why I want to bring an actual engineer to see.  



Because one thing by the way, that was interesting to me from the demo yesterday, a couple of days ago was when they create some of these services like another team at the company, the supply side team, their interface kind of looks very similar to what you were showing, even the colors of those. So I think they are using a similar tool in the background. But the deployments take a lot longer than just you were showing in a demo. So that is why I want to see is it an issue of platform? Is it an issue of DevOps or what is it that causes this to be a lot slower than this? So this is why I wanted to bring one engineer and one DevOps person to see if we can actually do this.  



One thing I am worried about is the part that street was just showing credentials have to be entered here and so on. Is our DevOps going to have a problem with that kind of thing? I'm not familiar enough myself to know. So that is why I want them to be in contact with you guys. I think that makes more sense.  



One question that I have for you and this is so by the way, this entire DevOps thing that we are talking about is always the thing that you want to make sure that all the stakeholders involved are on the same page and are involved basically, right?  



Yeah.  



But generally what we have seen work the best is first having the team that is benefiting from a certain service actually vouch that okay, what you are talking about is meaningfully changing my workflow and adding value. So getting to a point where the team itself realizes the value because if not there's a point to get more people involved. Because at that point, like the user of the platform itself is not on board. So there's no point getting other stakeholders involved, right? So generally like doing this in stages, that is like first getting the team on board and then once that is sorted, then we get the DevOps team involved.  



Between the three of us, we have worked with enough number of companies at this point that we have a fair sense of understanding of what are the things that DevOps generally want to make sure that are sorted and we can answer those questions. My recommendation. Would be to do it in stages. But if you suggest that you want to do everything together, I'm on board to do that. I'm just telling from my I wasn't.  



Suggesting jumping into action yet for a project on course Hero. I wanted them to ask the questions. I am a little bit unsure what the questions should be. I'm not sure what the cause of the delays are, but that I can ask the same question from you. So for example, one thing, I don't know if AWS provides that kind of thing or not, but I remember BigQuery, the Google sort of presto like engine has a lot of public data. So for example, if we want to build a service that I bet this is actually kind of publishable, if you create this as an example for your company where somebody can put the name of a BigQuery public data available to anybody, they can put the name there.  



This goes there, picks a small number of rows from that table and builds a random forest model for the provided label from other features.  



Right.  



And gives you that. So imagine if we create this service, if somebody shows the tutorial on this. That is exactly the thing that I wanted to I was asking our team to create. Makes sense.  



Yeah. This is BigQuery ML, I think that you're talking about here.  



BigQuery BigQuery ML.  



Yeah, it's called BigQuery ML.  



I haven't used the Google cloud in a few years. Maybe the names have changed. I september at the time BigQuery was planning to add a lot of ML tools. Today BigQuery, I don't know if they have added it. So maybe that after adding those, it has become BigQuery ML. At the time, the only model they were running you could run in BigQuery on the data was linear model. But I remember they were talking about adding anyway. Okay, I'll send an introduction Gmail to a couple of people. Should I include nickel? Only you. And then you can ask it to anybody you want.  



Yeah, that's fine. If you include me, I'll include the relevant team members from Giri. Truefoundry. So Riza, I want to make sure that I understand in this email what are you going to propose and what do you think shall be basically what's the next sep that you are expecting? What's the best case scenario for you?  



So let's say if you can send me maybe a link to a demo you have a video of a demo? I will talk to one of our engineers in a few minutes and I will tell him what I have heard. Basically that if we have a function available, if I don't function available, it takes you guys ten minutes to turn it into a service. As soon as that is available, that can become a service. I want to ask both of my engineer and DevOps, first of all, given that this takes ten minutes, are we going to be able to do exactly the same thing within Course Hero, given all the constraints and so on.  



Basically my first question, not for you, but for you to answer if they have any questions, is if we attempt to do exactly the same thing in Course, your environment instead of in an open network, is that ten minutes going to become at most half an hour, or is the two weeks that my team tells me these things take exactly a result of those other constraints? Is the efficiency coming from your work on the platform or is it coming from something else? Does it make sense? I didn't describe it well.  



It makes sense. It makes sense, yeah.  



And so basically I want to put them in touch with you so that jordan is my engineer who has worked on a lot of interesting projects like these. And he creates services, like some of the services we have, he has created. But none of these services are sandbox services, sort of sandbox in the sense of any service that our company's customers, the company has resources to do those things. But if some service, the customers are the employees of the company, we usually don't prioritize them because it's not worth two weeks of our time to do that. But if each of these services can be done in half an hour, then it's definitely worth it to us. And that is what I want to say.  



So one of them is Jordan Wong and one of them is Lena with a very difficult last name from India. So you guys may have an easier time. Anand so does it make sense?  



CTO be honest.  



Yeah. So I'm going to send an email to you guys and let them ask questions they have to find out. And if you can send me the best short video, especially with the emphasis on the kind of thing that DevOps may need to know and engineers need.  



To know, I think actually we don't necessarily have already a good video where I can both the DevOps and the engineer angle together. And I think what might be reasonable riza is we shoot the email, we tell the value proposition. This is what we're trying to understand, if there's any value add or not. And then maybe we show them a demo of the platform. Vivek basically understand their use. Case also, one of the things whether that hasn't happened so far that is very important for anything meaningful to come out of this discussion is us understanding the actual technical stack. Because, for example, there are certain constraints in which our platform also works the best, right? And if the stack is completely different, then maybe that's already a nonstartal. So we also want to make sure that there's like a good understanding of the stack.  



So we come together in a call, we show them a demo of the platform. We give them an overview of what's happening here we understand the technical details a little bit from the code zero side as well, and from th"
8632654345,CourseHero,Reza Jamei (rezajamei@gmail.com),,"Course Hero, Inc.",100-500,100 - 500 Mn,25-50,AWS,Yes,"Technology, Information and Internet",USA,Nikunj Bajaj,,https://app.fireflies.ai/view/Reza-Nikunj::k5W52J02i5,"Hi, Nickren.  



How are you doing?  



Good, how are you?  



I am doing well.  



Invite sent. I just noticed it. Sorry about it.  



I think it was sent last week only, but I think it's possible because I think were communicating on your personal email, right? As opposed to, like, a company email or something. I think that's why the invite probably got missed from your end.  



Can we postpone it to Friday afternoon?  



So this Friday afternoon, I am not able to take a call because I'm actually traveling for this weekend, but I actually have one more call set up tomorrow at the same time, if that works out for you.  



Tomorrow? Unfortunately, it's our first day back to the office for the whole company.  



I see. Okay. So Brazil and I are just talking about potentially finding another time for the call. So just give us maybe a minute or two, and maybe we'll have to reschedule it. But let's see. Yeah. So, Reza, tomorrow is your first day at office. You're saying do you think we can do Friday morning instead? Like 09:00 a.m.  



Yes. Nine? Or can we do 930 to 1039?  



30 to 1030. All right, we can do that. Yeah.  



So let's do that. Sorry about this. I just saw the message.  



Yeah, understood. No problem. So I think we can do that. Then we chat 930 on Friday. PST. Maybe given that we're talking, maybe we can take five minutes here. So when we met, we had discussed a goal that we will have you deploy one of the models that you'll bring yourself. Now, obviously, we ourselves have some examples and demos that you can try out. So to get started, just the toy examples to kind of set up some context. So if you want, we can do that first on Friday. So that should not take more than 25, 30 minutes. Right. Tree the examples that we have should not take more than 25, 30 minutes.  



Right.  



Between 20 minutes. Okay. So that part we can get through fairly quickly. And then after that, I have today.  



Actually, if 20 minutes works, I can do 20 minutes. Now, I have a 1030 meeting today, so if we can finish it before 1030 oh, yeah.  



So I think we can do that. At least we'll get you started with our own examples that we have set up, and then on Friday, you can probably bring one of your own models, and we can do that on Friday. How is that?  



Sounds good. Yeah.  



All right, let's do that. I guess the way we'll do this is, Reza just give you a quick five minute overview of the platform once, and as soon as we're done with that, we will have you maybe I'll ask you a question. That is, do you generally like to program in Jupiter notebook environment or in, like, a Python script environment? Because we have demos for both. Basically.  



Both, actually. So let's start with notebook, I would say, because I always start there.  



Sounds good. Okay, so she will do that. But let me just give a quick 1 minute overview to Raza about the concepts to begin with, and then we will go ahead and have Reza try out the notebook. Is that okay?  



Thank you. Cool.  



So I'm going CTO, share my screen now. Do you see my screen?  



Yes.  



So the primary thing that we wanted to discuss was that you have from our call, basically, right, that let's say somebody has like, an example, a model and a function that they have put together in their Jubilee notebook. Right. And how do they quickly deploy that? That was the primary thing that were talking about in our call. There were a couple of other items, but this was the overall theme. Is that right?  



Yes.  



Now, when we think about deployments, there are primarily two types of deployments that you would do in machine learning. One is that sorry about this.  



Can I switch to a different audio? 1 second?  



Yeah, no problem. Yes, we can. Can you hear us?  



Yes. Fantastic. Thank you. So you can continue. Yeah.  



So basically there are like a couple of different things that we would, like, run some quick experiment and deploy something. Right. Now, when we think about deployments, there are two primary types of deployment that you could potentially do. One is from a model point of view, one could be like a batch inferencing which runs as a one time job, right? Like it runs stores somewhere and gets done with it. The second could be real time inferencing which is more like a service that it's always up, always running. You can anytime send a rest API request and you're good to go, essentially, right. So those are the two primary types of deployment here.  



Can I ask you a question here? The batch imprints type, I assume that we deploy it as if it's a service, except that the service, let's say the function that I provide, instead of taking one row example, gets the address of the whole datta set and goes and does something and comes back or not this way you could do that.  



Generally when you think about a service, Reza, it's like an API endpoint, right? And when something is hosted as an API endpoint and you send like a lot of data to it, that is basically sending that data over the network, and it becomes expensive and slow. So when you have, like, when you're dealing with, let's say instead of one, you're dealing with ten points, maybe you could do that. If you're dealing with 100 points, maybe you could do that. But if you're dealing with ten housing 100,000 points, you probably don't want to send it over an API request. What you want to do it is like kind of run it as a back end offline job or something at that point. And usually like our machine learning, like, batch inferencing, we want CTO do it on the full data set.  



So the way you typically want to do this is that your machine runs that job once stores the results somewhere and in a database or in an S three file or something. And then just the skills itself.  



I guess those are the brings up a question. I want to give the question, but we don't have to talk about it since we want to finish by 1030, the main demo. But one question suddenly your conversation brought up for me is do your back end support things like these gigantic natural language models that, let's say even the whole model doesn't fit? Usually one machine, they have to break it up, put it in different places. Do you have any facility to make that process easier? Or your client netmeds to take care of all those aspects?  



This is basically like the Distributed model processing, right? There are two types of distributed processing. One will be datta processing. Distributed model processing. So in terms of Distributed Model processing, are you running inferencing on the Distributed model processing razor currently? Like, what's the use case of this one? Maybe if you want, we can chat about this in our next call.  



Okay, sounds good. Let's talk about it. Sounds good.  



All right. So moving forward, let me just quickly show you. But yes, by the way, like what you mentioned is right. Like technically you can have an API endpoint that instead of taking one row, it can take like a batch of rows, like ten hundred thousand, whatever you want. And if it's like a really small data that you're transferring, you could even do 100,000. It doesn't matter.  



I meant not even getting the batch of rows, but getting the address of where the data is. I see the bucket that it is living.  



Yes, you could totally do that. Yeah. So that's like more of a lambda approach. And that's totally possible. All right, so I'm going to show you this thing very briefly, Reza, that in our deployments, like, you have these two types of deployment, right? Services and jobs that we talked about. Job is like a one time thing. So, for example, model training would fit into a job type, right, that you run. You train the model once. It takes 15 minutes, let's say, to train the model. And then the machine kills itself so that you're not paying for the computer, essentially. And the service is always running kind of a thing. So basically you have a way where you can see all the services together in one place that you have started. Okay. Now, each service itself by design is actually versioned.  



So you can see that currently which version of the service is actually deployed. Okay. It is linked with the code, so you understand exactly what's the code running behind the service. And it basically gives you a hosted API endpoint already. Okay. So I'm going to open a couple of these and just show you a demo here. So for example, I guess this one I'll show first where this is an API endpoint. Imagine this is your predict function and here you provide the inputs that your model accepts and you will send these inputs. And when you hit the execute button, it's going to give you a response here, like the modal response. So it's actually internally invoking your service. And this link that you see is actually a hosted link. So you can actually share this with other folks on your team.  



Like, let's say ML engineer can share with a back Pendo engineer if they want them to try it out or integrate in the product, et cetera. And of course this comes with your typical code request and all that people can try out. Any questions yet?  



No, it's good.  



Okay. And we also have some simple stuff like we chad talked about streamlit, right? Remember that? How does streamlit works? Actually, you can have these streamlit demos.  



I got disconnected, sorry, go ahead.  



Okay, sorry, disconnected for a second. Understood. So you can actually build out these streamlit demos and basically host these streamlit demos as well and send it out to folks like somebody even like less technical user. In this case, there's probably an error in the code itself. But the point being that the stream rate code can be hosted and this link can also be shared with folks in your team. So essentially, like from notebook you have now put it on a cluster, on a remote cluster that can be shared with other folks in the team. That's the most important point here, right? And of course when you go to details of this thing, you can actually see what are the different versions of the deployment, what are the associated logs with it. So like debugging. So here you're seeing that one, two, three.  



These are the three rows of deployment and you can see the logs for each of these deployments. So those things are available. IBM skipping the details for now because we will anyway sep this live in action, right? And I'm just showing you a quick thing for the job itself. Now a job, once you have put a job here, you can actually just go ahead and click a button and it will trigger the job right now. So for example, if I just do this, what will do this job is triggered successfully. And right now you can see that this is already running. So whatever this counter job is, you've already started it will run this job once and then the machine itself will be sales. Does that make sense? Riza yes. Okay, cool.  



And now we will show you I will just quickly show you the documentation and this is what I'm going to ping you next. So there's like an installation and setup stage for our deployment. So actually you probably won't even need to do any of this because we are starting off with like a Jubilee notebook experience, right? So I think it's actually everything is contained there. But let me just walk you through the code a little bit here that the way we have built this out is almost a unified API actually for both your job and service deployment. And we do this such that you don't have to learn like a new way of programming your training and interesting jobs.  



So for example, here you have your training pi and this code is like what your data scientists would have written if there exists no truefoundry essentially, right? Like there is zero truefoundry code in this Snippet here. And then what we ask you is if you want to deploy that using true quantary, all you do is copy this like seven, eight lines of Python code, put that in your Python script or Jupiter notebook, August, a couple of parameters and you get going. You hit a job, deploy and you basically see the job already getting deployed as a line item on this dashboard, basically where you can come monitor all the deployments that you have done. That's pretty much it, honestly. You install one library, people install one library and you invoke this seven, eight lines of function and your code is deployed.  



That's kind of how we are building this out.  



Quick question here. If I want to deploy this on my own account instead of your servers, what information about my account do I need? Sure.  



So the way this works is you would basically have imagine that you already have your own clusters. And what you can do is you can actually link your existing clusters with our system. So you can see that we have these clusters and you can actually link your clusters. Like for example, if you use AWS or Google Cloud platform, you can do basically either of those and connect with your account. Essentially.  



Can I just create a cluster there? So basically what is involved is basically you need to create object here on our platform. Like in this case, you come here and you create a cluster. So you give them a cluster, choose where you want to deploy that, maybe under the region as well. Then you can grant permissions to people on your team on the platform. Then we allow you to sort of configure some of the parameters. Like these for example, if you bring your own based on URL, basically if you have your own RDNS, then you can configure that. If you bring your own, for example, Grafana logkey from EPS, we sort of visit your running yourself, we can bring that, but these are not necessary basically. But you basically add a cluster.  



So once you create a cluster, you basically get a command that you need. Arun basically CTO help Install Command what you can do is you can actually create a cluster on your account.  



Actually reserve question is a little bit different. Clarify your question a little bit more.  



Yes. IBM mostly trying to guess what DevOps is going to be concerned about. I want to see if we want to minimize the amount of data becoming basically observable by your servers. If you want everything to be completely separate, we only use the technology aspects of what you have built, not let the data flow to your servers. I know that you don't care about it, but our site probably is going to make sense.  



We actually get this a lot and in fact, let me just show this to you. There's no point, I guess, showing how it is done. Technically, it's actually very easy to do this, but we actually already have a proper documentation around this. That is you can deploy through foundry on your own cloud and literally the entire platform of True Foundry, including this UI, can be completely deployed on your own cloud. So nothing of Truefoundry at all is what you're using. Everything is installed on your own cloud and you're basically using us only as a tool and not as a computer platform. Basically.  



Okay, sounds good.  



A lot of our current customers actually use us that way only for the exact same concern that you mentioned.  



Yeah, I mean, if I know that at the end of the day that is possible, then I'll probably start with trying to for testing purposes and things like that. Like my own server, let's say my own model. We will use your server, exactly.  



Yeah, I think this is the most common way. Like people try use our cloud for the pilot and then when they're ready to move to production, they will get the DevOps team involved, do like a quick install of our platform and then move it. That's a common process.  



Sounds great.  



Awesome. So now we have like ten minutes. Let us ping you the link. Like, can you ping the collab link? And that has to make a copy kind of a thing, right.  



I think it's better if you create a copy of this club notebook and run it yourself.  



And maybe now you can screen share. So we'll try to do this in ten minutes. I don't know if we'll be able to get up and running in ten minutes itself, but let's see.  



I think we do a couple of the basic examples.  



Will also need to sign up, right? Should we have him sign up on App Develop?  



Yes.  



Okay. So if you can start by sharing your screen.  



Okay.  



IBM, maybe I can begin with let's actually go ahead and create an account on our platform. So I think sheepingdollank app Developt, refund.  



Sign up to Google.  



Sure, yeah, sure.  



Okay.  



I think that's probably good enough for now. Like you can go back to the notebook.  



Okay.  



I think you can even run this code and sorry.  



You can just run this code. And I think you'll be able to. I think there's a line before that, right? I mean, this is probably not the first block.  



Yeah.  



So right now, the first thing that we're doing is installing a library in Chris notebook. And then after this, we are just doing like a couple of setup lines. What's the expected demo in this one?  



So there are actually two parts to it. The first one is simple jobs and services like Hello World programs. But if you skip CTO, the second section, we are training our Iris model there. We are using two companies to store the train model and use that as a service later on.  



Sounds good. Okay.  



Can we instead to, let's say, write a function that given a number, multiplies it by two?  



The first bit of the example is that I think these are very simple programs. So I think we can move that instead.  



Yeah, that's what we'll ben doing. So, yeah, let's go ahead and run the code here. You will basically in this link in this second cell where you're importing service Truefoundry. You will notice basically what's happening, Reza, is you're connecting this notebook to your account of Giri Truefoundry in some way. That's the way to think about it. You need an API key, right, to authorize this thing. That's what is happening in that part of the notebook. Essentially.  



That is fine. That should be fine. I think it's.  



Just a logger setup. In this case, it's just logger sep, obviously, like logger and all. We just do this for good programming. But technically you can skip it as well if you wanted to. Then if you run this code, this is the line where you will notice that Truefoundry is getting linked to this notebook. Essentially.  



Can I take a quick look at what I'm running? So we are setting this and Japan get passed from the place. So it will connect to the account we just set up, right. To get some kind of a password. Right.  



This is the one that is going to ask you. It will give you a prompt that now you enter your workspace. And after that, it's just setting up the service boundary to kind of know that, okay, here's the host that I'm talking to. And here's the listing up. So if you go back, like, if you click on that link that just got generated here this one. Yeah. Here, this is where you'll be able to generate the API key.  



Okay, copy that and just paste it back in the notebook.  



Close this.  



Yeah. Hit Enter. Okay. And now it's saying that, okay, go to a workspace and get a workspace, basically. And by the way, workspace is our concept of like permission management. So, like, you as a developer, get access to this workspace. And this is kind of where you can play around with essentially that's how it works. So if you see that actually, if you close this one, you notice that you already have a new workspace created. And that has an FQN column, right? Third column. And there's a copy of the next FQN column. So that's the thing that you need to copy. It's almost like an ID of the workspace. It's almost think of it as a playground in some way.  



So currently, you do not have permissions to create workspace. So I just added you to one of the workspaces that I created before. Now your configuration is set in the next couple of months. You can see how we can sort of run a job on the platform.  



Yeah, I think she let's skip this part, maybe actually, if you can scroll down and we'll go CTO the multiplication part that you were talking about. Right. So let's go to the service part and actually deploy that in the interest of time. So let's go here.  



This is making something on my local machine, right?  



No, it's on the virtual machine, Google Cloud.  



This is already running. Even the notebook is running on your machine now.  



It's not on our machine. It's Google's VMs.  



Right?  



Google Cloud.  



Google.  



Haven'T purchased it yet. Cool.  



There are a couple of functions there. In case you want to add a multiplying function. You can add that as well. There in case you want to test one more function. Anyway, we want to deploy these two functions as endpoints, but in case you.  



Want to add one more, write something. Does it have any it only takes the real, let's say, text.  



Can you order type two X as well? Can you order type two X.  



Or something? Whatever.  



Is this necessary or is it just nice?  



I think it might be necessary for numbers.  



If I don't say int, it can be a string.  



It can be a string, exactly. Because the thing is on API, across API's, it's always datta transfer as strings. Right. So I think that's why by default, it becomes string and you need to tell it that. Okay, it's an int. So that we are able CTO like that. The platform is able to integrate that.  



Something like this. Okay.  



Okay.  



Now I have this can I understand this really quickly? Functions pi. It's going to write these functions into this file, right?  



Yes. That's a feature of Jupyter notebook, right? File. Just create that file in the local environment, in the environment that you're running this one.  



And this is actually on the whole cell.  



All of this is running on collab?  



Yes.  



Whatever thing that you have written in this cell will get copied to that file called Functions.  



Here you have to add one more line because you added this function. Can you add a line service register function? The name of the function that you just created. And you have to import that as well from functions.  



Just let me make sure I pendo you understand everything here some kind of logging.  



Can you add the type? IBM not 100% sure if type is inputting these off string as well.  



It will be nice to see even when the error happens, what happens, right?  



Yup. So you can follow that link to the UI that is sent CTO there. So this is currently building an image. Basically it will package the code and sort of send it to the cluster. So that's what we're having right now. It should take around 1 minute.  



If you go back to the code. Just I'll explain you one thing.  



Yeah.  



So basically what's happening here is like when you run this code like service deploy, what it is doing is it's taking all these functions that you have written along with the Python dependencies that we put together in a requirements TXT, creating automatically creating a docker container out of this. Okay. Implementing the server code. So like the fast API server code that you would typically write, it's implementing that server code and deploying that on a Kubernetes cluster. So that's what is happening under the.  



Hood, by the way, is the under the hood viewable. Can I see inside of this?  



Yeah. So you'll not be able to see this function right here. If you do it in your Python script, you will be able to go inside the function and see what's happening. Because it's a library that you're installing.  



IBM saying Chris library is open. Right. The code of it is available.  



Yeah. You're installing it on your local machine. So you'll be able to see that.  



Exactly.  



You can browse through the library. Yes. And if you go back to the UI, I'm going to show you one more thing. The previous thing here. Here if you look at the application spec right?  



Application spec. Tell me where I look.  



Tab like in the middle of pods, deployments environment variables and application spec. Yeah, this one in the middle. Yeah. So you notice Chris is basically generating this entire automatically generating this entire docker image with which the deployment is happening. So you're able to see all those here and here we are following basically you're devastating knowing are we following GitHub principles and all. And this kind of leads into that. But actually we are kind of over time. We're just going to show you just one thing.  



If you scroll up give me 1 second. Sorry. Just 1 second.  



Yeah. It will take only one more minute.  



Okay. So let's see 1 minute.  



Okay. And click on the deployments. Now on the top there is an absolute top left. No, absolute top is the deployment. And here you notice that you have a link right under the metadata. So if you open that link now, your function was my first. Right. Can you open the my first function? On the top there is a green like over second. My first. Click on try it out. And here.  



So instead of string, I put whatever I want.  



Yeah, exactly. You see, response hello is my input. Basically, that was the function that you implemented.  



I see. It's fantastic. Awesome. So we will do the same sorts of thing with something that let's say on Friday, I'll try to bring something that maybe has some kind of possible hair in it, like, I don't know, needs data connection or something like that to see what happens.  



Sure. Yeah, absolutely. So you already have access to this notebook. You can actually just go through the code to get a better understanding and then bring your own code, and we'll help you deploy that on a fighter meeting.  



Okay. Okay. Fantastic. Awesome. Thank you, guys. Really nice stuff.  



Awesome. Thanks a lot, Risa. Have a good one.  



Thank you. You too. Bye bye.  



So Friday meeting, I think we'll have to let you know. "
8632654345,CourseHero,Reza Jamei (rezajamei@gmail.com),,"Course Hero, Inc.",100-500,100 - 500 Mn,25-50,AWS,Yes,"Technology, Information and Internet",USA,Nikunj Bajaj,CourseHero_Reza 10-04-2022,https://drive.google.com/drive/u/0/folders/1RQdB_HLHysOYvq0zWqoMKMV5Za2oFH46,"10-04-2022

I wish I had seen your video ahead of time. I would have recommended to talk more to supply side.

I worked in recommendations and search- for recommendations we cache as much output as possible, precalculate things. We do have need for in-session recommendation.

We have a large library of learning materials for udnergrad students

Supply side folks, who do NLP on our documents. To tag them to questions and answers.

Demand side folks, discover- search and recommendaton. They deploy their model through APIs. My team usually precalculates about caching.

AMazong couldnt clearly state what ercentage of cloud markup they will change

Training has never been a pronlem for us. Our engineers are MLEs and they are never having problem with training but they have issue with deployment

Imagine I want to have a python function and I am imagining

This package sounds like really good to me. I do a lot of activities for non-professional activities. I will really like to put it on a webpage that people can see and upload output. If you can do this it seems really useful to me.

Is there a way in a very small scale you can have this work. Can I independent of my team.

I am advocating for our company internal services for a non-engineer to use them. They just need to run the model on a corpus embedding and save it on a S3 bucket. Every time an engineer has to run it but I want to have a PM actualy pick the model pick a corpus and ask it to run on that one. We keep having an engineer supporting just running a package that feels like this is automatable.

I want a user to be able to go to the webapp but when you click the predict essentially, the job is to be done over a week. I want to trigger a whole script which brings up a separate cluster, runs things etc.

Definitely want to use it for my personal projects and susect that company level might be usefyl to. Really interested in deployment and webapp.

As soon as your product is ready, give me - I will try it out and also loop in an engineer.
"
10442803370,Nomad Health,Liming Zhao (limingzhao@gmail.com),,,500-1000,50-100 Mn,<10,Multi Cloud,Yes,Hospitals and Health Care,USA,Nikunj Bajaj,Nomad Health_Liming 29-09-2022,,"29-09-2022

Building technology for two related hospital temp staffing needs for contingent hires/ They take a 13 week assignment in a hospital. 13 weeks for maternity leave for the nurses. In healthcare provider side, clinicians are always at shortage. No hospital can hire permanent staff to fill their need and every hospital fills it with elastic needs for their contingent hires. NomadHealth we onboard credentials and do recommendations and help them get their job. If you just do a match the percentage is low. When clinician is on assignment we do onboarding, checking, payments, etc. 

As any other labor marketplace there is so much inefficiency and companies build with their recruiter model. Here we believe that with their data, we supply for their shortage. Machine Learning is the key here- likelihood of interest, and outcome

One of the barrier in industry is because it is in Healthcare it has a lot of compliance. Even if we dont know the language we can learn it on the spot.For healthcare it is a no- unless you are actively licenses there is no way you can be hired. How do you understand these requirements. Is it 2 years of experience as a primary practice or secondary practice. 

Built a robust process to standardise the process. Do the match recomendation. 

On the engineering team we have an SRE team which is SRE and Devops. Containerised and deployments on K8s.

3 disciplines- DS which is working on data modelling, they have to vaidate their models. Then we have ML Engineers and they have sufficient knowledge on both. They make sure ran executed qualified models in production.

ML Infrastructure team builds deployments, feature stores etc. 

Altogether 10 people and each team hs a few people. Everything is built on VertexAI. 

Just like any hosted service its not cheap. So far we build our workflows. Future development training testing and endpoint. 

ML Infra team has ownership of vertex AI platform. They manage feature store, machines etc. They build workflows so MLE can do the training and validation. So far we have permission to everyone to spin up or spin down services.

When its ready for production, Infrastructure team has a process for ceating end points that is used by MLE team. 

The entire MLOPs team is 3 people. One person working 1.5 months. The manager of the team is great and he drives very good actions. We chose VertexAI together. I have used Sagemaker before. 

We can do all of this again in 1 person 1 month

Our biggest challenge is a data story. Data may not be transformed in the right way. It may work for this model and not work for other model. The entire process of coming and testing a model and validating it is a receptive work. 

Any extraction of higher layer understanding logic is our Infra teams goal for the year. Such feature transformation can happen and persist features. Its your choice of what you put in there. 

Not only does it take a lot of time but also our own self-inflicted challenges. How we extract the features for testing vs how we handle live streaming data for processing. There could be discrepancies and we waster a lot of time to make sure they are perfectly aligned. 

We have moving definition of data models from a raw data perspective. 

Sounds interesting- If you can send me some additional material, I am happy to see and explore if this could be useful. Was interested in monitoring quite a bit. "
10439869350,SenseLoaf,Rutvik Trivedi (rutvik@senseloaf.com),,SenseLoaf,<50,,<10,AWS,,Software Development,USA,Anuraag Gutgutia,,https://app.fireflies.ai/view/SenseLoaf-TrueFoundry-2nd-Discussion::CX2x3oauTX,
10439869350,SenseLoaf,Rutvik Trivedi (rutvik@senseloaf.com),,SenseLoaf,<50,,<10,AWS,,Software Development,USA,Anuraag Gutgutia,SenseLoaf_Rutvik 30-09-2022,https://app.fireflies.ai/view/Rutvik-Trivedi-and-Anuraag-from-TrueFoundry-Team::71a3dFwykd,"Inbound lead Qualification Call

Unqualified for Deployment, but trying to see if can push for POC irrespective. Will just get a trial user.

Change name to SenseLoaf

Gautam and Rutvik from Senseloaf  : 30-09-2022 

In office - AkshyaNagar. 6 people in tech team. 

SenseLoaf: AI models for HR Tech industry. We provide the pre-trained models and pipelines for training the models. Resuming parsing, automatically matching documents, Jobs to resumes etc. Hiring specific model bias. 

We are just building at the tech level - Interview as a service, Recruitment marketing platforms etc. 

Currently exposing it as APIs - 2018 Dec => tech part, Austin - Bhagat. 

Mid Size HRMs etc are the target 

We are domain focused and then help customers to build end to end 

WHY INTERESTED? Dont know much about that.Similar frameworks - Hugging Face, Tensor Flows, AWS. 

No Challenges on the Deployment side much. There is challenge in Tracking the experiments. We load the data from MongoDB into a training machine. Saving 1 model and then evaluating the performance. We need more tracking and visibility 

We have looked at MLFlow and Weights and Biases. We are on SageMaker completely. We can do the training very seamlessly. 

Lambda and EKS - If individual tasks that takes a lot of resources, we decide based on that. 

Training is triggered manually

We are building the Quality Pipeline // EndPoint Monitoring: We have a user layer 

NEXT STEP: Experiment Tracking they want to do. "
10441289250,Wayve,Richard Shen (richard@wayve.ai),,Wayve,50-100,10-50 Mn,Oct-25,,,Software Development,UK,Anuraag Gutgutia,,https://app.fireflies.ai/view/Richard-Shen-and-Anuraag-from-TrueFoundry-Team::SBU7jncESJ,"Hello. Hello.  



Hey, Richard. How are you?  



Yeah, doing well. How are you? Sorry, I'm getting the pronunciation wrong. Anurag. Anurag is great.  



Yeah. You can also call me ANU, by the way. That's that's fine.  



All right. Nice to meet you, Anna.  



Yeah. Where are you based, Richard?  



Based in London.  



Okay, great. That's great to know. It should be pretty early or time around 10:00 or so.  



Is it 11:00 a.m now. I'm in the workday.  



Okay, awesome.  



Cool.  



So, Richard, first of all, thanks a lot for taking time to set up this call. A few things would love to kind of get through in this call. Karen, the following one would love to understand as to how you got to learn about Pro Foundry and what was the motivation behind that. We'd love to kind of do an intro as to what part of the product you are taking care of at Wayve. And then we can do a brief intro from our side like she has joined his co founder and the CTO and we love to dive a little bit into some of the use cases at Wayve in terms of the model development, the model improved and see as to how it aligned with what we are building and if it kind of suits well.  



Or we can go into a place where we can also showcase you a demo of what we are building and take it forward from there. Does that sound good? Okay, perfect. Do you want to start? I would love to hear a little bit. I know you've been working on the ML side of things for four years at Wayve. Would love to hear the journey and what you're looking forward to, what you're looking at in Wayve currently.  



Yeah, for sure. So maybe I'll start with, I guess, like Nikhil Truefoundry and my current responsibilities. So, yeah, kind in in D and Truefoundry through just launching investment news. So I think congrats on the latest founding funding. But yeah, just kind of heard you all through email newsletter alert. And I am generally responsible at Wayve for all our tooling around, I guess, all of ML ops. So my responsibility in products is just trying to figure out how we develop better workflows for our model developers so that they can consistently ship quality models with certain performance guarantees to improve with the actual driving quality on the vehicles.  



So that means that I kind of run the full gambit of tooling that we use away, both externally bought and internally developed for how we do anything pin the model lifecycle so responsible for our integration, like weights and biases and tooling for robot visualization, for debugging models, some tooling we have internally for ball testing and artifacts versioning and deployments and so forth. So anything that runs the full gambit them all. Lifecycle was interesting to your foundry because it looks like they'll have some interesting designs to how you do a lot of these components of the workflow, but I don't have enough clarity on exactly the implementation that you all done. So that's why I don't have exact requirements just yet.  



Okay, no, I totally understand, and thanks a lot. A few things like Richard would love to hear and know. If I understand correctly, you all built AI for the autonomous vehicle industry, right? And what sort of things do you help them kind of work with? Is it like a full suit that plugs into an autonomous vehicle and then kind of does everything around it? Or is it more like something on top of whatever the autonomous vehicles are doing and then some analytics on top of that and predictions on top of that?  



So we're building the full AB stack for autonomous vehicles. So our approach is focused on more entering machine learning for this problem area and developing off of internal fleets where we basically bought vehicles from various suppliers. So like Jaguar Ipace, for example, and then retrofitting it with our technology.  



Okay, understood. How many people are there in terms of the ML engineering team? A lot of work I'm guessing would be then very much AI first focused, like what percentage of the employee base or currently how many people are there who are like data scientists, ML engineers and so on?  



ML engineers. So we have a couple of different roles around ML developments. ML engineers, ML scientists, data scientists have a bunch of different definitions. The number of roles is probably about, let's say around a third of the company. So maybe around 60, 70 people. For a company of about 200.  



What is the workflow? Richard and you mentioned you are setting up this entire stack for ML. Are you doing the entire work of the setting of the platform in house? Are you using take a note of open resource or how are you thinking of this stack overall?  



Sorry, when you say stack, what part?  



So maybe starting from experimentation to development to deployment, to monitoring, how does that kind of damage look like for wayve?  



Yeah. So for wayve, most of that is internally developed because for the autonomous driving industry, there's not really many off the shelf products that work. For components of this workflow, the major ones that we would use externally would be probably waiting biases for experiment management and tracking experiments through training, but nothing really beyond that step in the life cycle. And then also Azure and Azure ML for training and almost just for training. So anything that's kind of the standard parts of experimentation workflows. In particular, we have external tooling to provide it. But then I think beyond that to actually bridge machine learning into more of the production setting and to deal with more mohan one engine workflow, then that's the scientist workflow that's mostly internally developed. So I think around deployments, around monitoring, around testing, even, that's also internal solutions.  



Okay, got it.  



For your deployment, do you end up using Kubernetes at all, or do you directly use as your full deployment?  



We have custom systems because we deployed to the vehicles, so it's not really kind of the null deployment setting. Right. Experience.  



There is nothing like you don't have, like, end points hosted or anything like that for these models.  



Entrypoints for running entrance to me?  



Yes.  



No, because it's directly on the vehicle itself. So we don't really want to expose entrypoints in any form beyond anything that would be talking with the model within the vehicle. Otherwise, then, yeah, it just comes to safety risk.  



Yeah. And do you export the data out, like, these vehicles, when they export, do you get the data back or do you get the data back from these vehicles for further retraining or analysis?  



Offline ingest, so we don't cut over the air data ingest.  



I see. Okay. And Azure you mostly use for training, so the most of the work is actually on the training side because deployment, like, you pretty much just package it with the software that goes on the car, on the vehicles, right?  



Yeah, pretty much. There's definitely some things we want to improve on the deployment and monitoring sites. So ultimately, yes, there comes what we need to do over the deployments and be able to handle that. But it's probably not like the similar problems and most employment problems you'll have. Pin traditional ML settings. Monitoring is still something that we're figuring out, to be honest. So, like, what that looks like in the setting. Real time monitoring is probably not necessarily something that we're interested in at this stage, but at least be able to have some better concepts of how you do performance monitoring and triaging from there. It's something that we're actively interested.  



Okay.  



And these models, could you give a sense of some of the models? And do you also care about the data drifts, etc. For pin these models? Richard.  



So data drift in the model is something that we do care about, but I think it's not necessarily in the setting of we need to actively monitor for tests. Okay. All the Mergens of the values in real time at this stage, we still have safe drivers monitoring the vehicles themselves. Like, that degree of detention of distributional drift is not really relevant to us. It's more so, like, major distribution shifts on anything that tells us that we're out of the operating domain that we're looking at. So it's something that we're actually looking to develop internally or buy office job solutions, if possible, to sense how we can best accelerate our development process when we can identify the major shifts in the data that we're seeing on Road and see how we can actually develop towards this new deployment domain.  



But, yeah, it's not like really real time trip monitoring that is probably we're.  



Working with images, right? Like images or video data? Both, yeah. I mean, not much text data or structured data. You don't have many use cases of that, right?  



No, not really.  



Yeah, cool. And what is the biggest, Richard, like time sync or something like that you're planning to anyways, that is one of the biggest things that you really want to improve in your pipeline today.  



So that we're planning to build or just in general planning to build.  



It generally either plan to build or buy or the major time sink. What is the thing that is missing pin the pipeline today that would drastically improve your speed?  



Basically, yeah. So I think anything around really tight pin the feedback loop for us of what exactly are the bugs we're experiencing at deployments and how do we actually identify what those bugs are and develop solutions to improve our learning performance on top of them. So anything around. I guess. Identifying domain ships at the moment to be able to identify what's maybe some data we need to collect further to be able to target the situations that we're seeing and so forth. Or being able to have more robust testing of our models to identify benchmark performance that you want to make sure we don't regress on or be able to better debug into the model and understand the bugs are occurring when we actually see performance issues where we can monitor on the road today.  



So, yeah, I guess I think that really makes model development much more of an engineering practice and a scientific one. So actually, as long as we do deployments onto the vehicle itself, what are the steps you need to do action and the visibility that you need as a mobile developer to actually consistently fix the issues that we see pin on road deployment right now? Because a lot of this tooling doesn't exist and we're trying to reactively Karen, trying to figure out what that might look like. The feedback loop from on road performance is mostly just based off of scientific intuition.  



I see it's a slightly different use case. In traditional systems, you can just upload the logs, like when it happens or the bugs or if it crashes happen and all you can upload it. But in this case, I guess the data is stored on the device and then once in a while you upload all the datta to the servers right. For this analysis and figuring it out. You don't get live updates from the model?  



Not yet, but I don't think that's necessarily the biggest issue for Ups. So the question of when we get the logs back isn't really the bottleneck for us. It's more so, like, what do you do with the logs? So we can have a lot of data labeled by safety drivers, like what were the bugs that we saw or like the performance you saw on road. But that doesn't necessarily tell me how do I fix that intervention or disengagement. So just because I know that the model maybe potentially didn't stop for this red traffic light and this weather setting in Jury London winter. That doesn't necessarily tell me, okay, how do I actually fix that? That's what we need to figure out how to do.  



Okay.  



Any other questions? So we should go on this.  



I was trying to think, Richard, what would be an ideal solution to the problem that you have? I mean, like, let's say all the logs come and then we have a system that will visualize all the logs for you, will show you the distribution of things. But since this images, data, what would you want to see in that case? I guess that's the question that we Karen, trying to see. We don't solve the problem exactly in the form that you have, but I feel like it can be addressed to some extent. So that's why I was trying to understand, like, is it fine if we show you where the model error and the images and the state of the world at that point of time? Will that help?  



Or do you want an overall distribution of these errors, Karen, happening during this time, like some sort of clustering or something like that?  



When you say, like, where the model error, what do you mean by that?  



So, for example, as I mentioned, right, let's say the vehicle is running, right? And the model is also running doing its predictions. But you said you kind of know when the driver said that the model made an error. Like, you get that input right there, right? The model did the opposite of that. So that means the model made an error or something, and you'll probably have this data saved on the device. And then once in a while you get all these logs back on the server and then you'd want to see all the cases, the state of the world when the model made an error. Right, and then do retraining on that.  



Yeah, something along those lines.  



Yeah. I mean, we have a way where you can give us a bunch of logs, will show the visualization, will show the cases where it is, but it won't work in the current system that we have. It will need to be modified a little bit according to your use cases, which we can do. But I just wanted to get an idea about that part. We actually deal with mostly deal with real time in France, to be honest, it does help in real time. We can show you the demo of the platform, like what we do, and in scheduling training jobs and making it very easy for data scientists to run jobs and services. So that's where we are main value proposition. And we also help in monitoring of the models. But currently the monitoring is mostly done for structured data.  



But we do have a system for visualizing a bunch of images, like where it headed out and things like that. It's still in the work but two. As of now, the mode of engagement resource that we have is we work with a few companies closely. We don't work with a lot of customers because we are very early. So we work with like five to six companies closely and have them get to a state where we are solving a few of the problems and along the way we kind of build our product also. So that's the kind of mode that we operate in right now.  



So since your use cases, like we can show you the platform, but it sounds like something that we can also help you build since you'll be building it internally anyways, like we can work with you to build it out. We'd love to understand the use case a bit more and things like that. If that sounds interesting to you, like Richard, we wayve can collaborate along those lines.  



Yeah, it's potentially interesting. I think for us, the thing is there's always a complexity around for our specific use case. We've already thought pretty deeply about what exactly this might look like and if not actually implementing the projects themselves and designing the potential systems around it, and particularly think about how they integrate with all of our more fairly complex internal ecosystem of systems and tooling that Wayve already experienced, it's pretty hard to integrate with a lot off the shelf systems. So, Canada, I think we've done similar projects in the past of code development. A lot of these tooling with other suppliers for some of the tooling we make sure we didn't want to have to build ourselves.  



But if it's a situation where this is not clear what additional clarity on what the product might look like is provided by the other team, then there's not really a compelling reason for us to invest in these code development projects. In many cases we find that's more of a distraction and actually helpful.  



Yeah, of course. I think Richard, if it is interesting from both perspective, we can definitely try to provide the understanding of what could the product look like based on your use case. And then if it feels aligned, then we can then take it forward. But before that, I think it might be useful for you to see as to what we are currently building. And then we might have a few questions around your use case for this. And based on that, I think we can then think about the codevelopment project. Does that make sense?  



Okay, yeah, that seems pretty fair.  



So just I think I'll give a two minutes overview and then Abhishek can directly instead of going through sites. I think you are well aware of this space, so there's no point trying to dive into that. I'll just give an overview. The goal really Richard, for a set proof of this, to make it very easy for data scientists and ML developers in general to be able to test out and deploy their models into production. So right now whenever people are building models, ultimately they have to depend on ML engineers or DevOps team to be able to take their models into production and even hosting like testing entrypoints or testing use cases which they can expose for their members pin the team.  



So the reason for this is that data scientists and ML engineers like ML developers, a lot of times they do not have that core engineering skill set and therefore working with infra becomes challenging for them. So what we try to do is we abstract away the enthire infrastructure for them and expose the ways to kind of do and run this using simple lines of code, boilerplate codes which they can easily understand. And we do this in both Python way, YAML way as well as through the UI. And we enable you to then take your models to Arnaud. And these models could be of different types, it could be a real time model or it could be a batch inferencing model because it's batch you can also do a training job run into it or you can do like a production run into it.  



And once we deploy, our goals is to kind of be able to kind of provide you with insights that allow you to track as to how the deployment is going, which is basically we set up the monitoring for you out of the box. The monitoring includes system level monitoring as well as includes ML level monitoring. Things like what you mentioned, like the performance criteria, the benchmarks with respect to different types of department and so on. So that's what pretty much we are building. Initially when we started were thinking of building the whole platform, but we realized that this part of the platform is where a lot of challenges are pin and therefore we decided to focus on this cost. Does that give you an overview of at a high level what we are trying to do as a product?  



Yeah, I think at a high label for sure. I think there are a number of similar products in the market. So I'm always just curious to get into the actual nitty gritty of the ten code details.  



Yeah, a few things before we go into that demo we have natively chosen Kubernetes as our place where we deploy and because we wayve chosen Kubernetes by designers cloud native. So you can deploy on AWS or GCP or any other cloud for that matter. And we try to make the system in a way that the entire access, control and authentication system works properly across the use cases and then it integrates with your workflows. Like for example if you are doing a CI CD, it will integrate with the CICD. If you are doing things like traffic shipping, there is a functionality to do traffic shipping or if you want to do, there is a functionality traffic shipping as well. And similarly other use cases, for example, use of GPUs, use of model servers versus normal models, et cetera. So that's at a very high level.  



Happy to answer questions. Otherwise, will you kind of go a walkthrough of the platform under the other things you want to do?  



Yeah, I can do that. Any questions you have, Richard, before?  



I not one.  



Okay, I'll just share my screen.  



So.  



Basically I'll show you first the way to deploy model. Like either you can deploy training job or you can deploy a model as a service. And there are multiple ways to do it. You can do it by Python code, you can do it by ML, and you can do it by UI. It's as simple as if you want to deploy something, you can just choose where you want to deploy. And then it's as simple as you just give us your GitHub link wherever your service is hosted, you give us the branch which you want to deploy. And if you haven't written a docker file, then you can give us a docker file. If you don't have a docker file, we can automatically generate one docker file for you and you just give us the version that you want to deploy.  



How many replicas supports and the environment variables pin the CPM memory usage and the moment you click submit. Basically this will create a service which is shown here. Like for example, it will generate an end pin for you. So the service you can open and you can test it out. So for example, the service has one inference entrypointtype, video batch inference endpoint. This is regarding a service. You can also deploy a training job if you want to deploy a job. Same process goes here. Again, you choose your GitHub repository and deploy. And for the job, you can either have a job that you can trigger manually using the UI or Python code, or you can make it run on a schedule. For example, you can choose the schedule so it runs every 30 minutes or something like that.  



And a job is pretty much just a Python script. You can give us any Python script, any piece of Python code and just put it here and the job will continue to run. So for example, some of these jobs, like teachers job, that is training xg two models and it's running every 12 hours. So we kind of make it very easy for data scientists. I can show you the Python code also that runs this thing, which is our this is basically the Python code I'll show you. So this is our Train PY, the standard training script that you have. And to deploy this thing using Python, what I showed you was the UI way. And if you want to deploy it using Python, you pretty much just need to write this much code to make the deployment.  



And you call your job to deploy and your training job will start running on the cloud. And at this point, any questions? Richard, is it clear this part?  



So for the actual image building and interacting with Truefoundry, all I need to do is specify a command to run and then some just requirements for dependencies.  



Yes, exactly. We'll automatically build the docker image and we'll automatically deploy it for you.  



Got it.  



And you can come and integrate your own docker register. It's not like our docker register. So you can integrate your own docker. It can be clarification question.  



So that means that theoretically, AI transcription can be any arbitrary logic inside it. It's just that needs to be like no yes or no.  



Yes, it can be arbitrary. This is where you can integrate your GitHub bit bucket or get lab repositories. If you have your own Kubernetes clusters, you can also integrate your own clusters here. So basically, it will kind of work with everything in your cloud, and you can just use it that way.  



Sorry, you go back to the last page, this one. So I get like, why they should be integrated in some ways. I'm just trying to understand from this view, what is that design for, how to like, how does it interact with this integration? It's just like a record of what integrations are available. Like, if I'm going to the git page, what else does it do there?  



Yeah, so once you integrate your GitHub repositories, that's when you can whatever is integrated here is what options you will get to choose when you deploy. So when I made the deployment and the list of repositories was coming, this is coming from the list of integrations, the repositories that have already been integrated.  



Okay, got it.  



And regarding clusters, it's more like in a cluster, what we do is we kind of try to give every data scientist or team a safe space to play around with, where they can do their own deployment, the rest of the system. So I can basically what I can do is I can create a workspace. I can give it out to a team, like, let's say hello, world space. And I can add, like, editor, viewer, admin, like different team members. And I can limit the size of this workspace. Like, I give out, let's say, four CPU and a GB to certain teams, and they can pair up with this. They can do all their prototyping development, even production rate stuff in this, but I know that they can never screw up the rest of the system or exceed beyond that.  



So the cost is you kind of curtain the cost, and you give autonomy to all data scientists and engineers to do to carry out their own things. And this is where they can do all the deployments. Once you run a job and all, we also have a component where just very similar to I think you're already using weights and biases. So very similar to, like, you can track all the runs, the metrics and everything and the system metrics and the artifacts that you have logged. So this probably already have weights and biases for it. And then if you want to do data monitoring, like so let's say you deployed your model and is making inferences. So this is where you can see the model, how many predictions it made, what was the actual value, the loss that has been logged.  



You can also see the data distribution and these things work well. As of now they work for structured data. We karen trying to extend support for nonstructured like images and text through some proxy metrics that you have for each image. So this is where you can just see the distribution. You can compare it like let's say model version three is running this month and I want to compare it with the same time last month. I want to compare this day to so this is where you can see the model, the different distribution of different features that happened between yesterday and day before yesterday, and the actors and the predictions. And you can also browse through the raw datta of the model, all the features that the model was getting and what was the actual value that got on the predicted value.  



Good. So sorry guys, I got to actually run to my next meeting. So this is always interesting. It might be worth continuing the conversation later. It looks to me like two founders were a nice way of integrating a lot different tooling and components for both the ML scientists and on your workflows and nice abstraction layer on top of the deployment problems you would have with Kubernetes deployment. That sounds pretty interesting, but to be honest, I'm not quite sure what will fit into our systems right now. But maybe it's just a conversation because I really have to run.  



Yeah, sure. We can continue the conversation over email, which thank you so much for putting out the time to speak.  



Cool. Alright, thanks. Bye. "
10439124882,6Sense,"Samira Golsefid, PhD;samira.golsefid samira.golsefid@6sense.com (samira.golsefid@6sense.com)",,6sense,>1000,10-50 Mn,Oct-25,AWS,Yes,Software Development,USA,Anuraag Gutgutia,6Sense_Samira 28-09-2022,https://app.fireflies.ai/view/Samira-6sense-TrueFoundry-Deepak-Referral::Rz3ZEXIFzS,"Call on 28th September

Anuraag, Abhishek took the call

Next steps: Don't think there is anything. She is more on model building side

Brief background (if discussed)

Connect via Deepak, who is an angel. 18 years ago, there was only Data mining.

Based in Iran and then worked in US for a couple of companies.

Use Cases for ML - types of models (Is monitoring important etc?)

""6Sense: MidSize. In 6Sense, WB - Internal tool that we use to deploy models.

DS Leader - 1) DS Team has a good pipeline for feature engineering. Good platform - do feature engineering, do testing. When you have this capability => installation would be 50ms, in 6sense- it could be for a day.

Need to have understanding about data - Descriptive analytics. Then you start to build a model. What if there is a tool - looks at the data and tells you what model is best to use. 2) Some sort of metrics in market eg ADC etc - I prefer to lose at the percentile analysis of the model. 3) Its important to monitor the model => Auto-training I don't like. Only if distribution of data is changing, then re-train the model.

Current Stack for ML Deployments and pipeline

Data Engineering team does the Productionization in PayPal. In 6Sense, data scientists do it.

Industries where SLA is more important: That industry needs complicated platform.

My team writes SQL on a Hive. The Vinci is the internal system for deployment. In PayPal, we have something called TypeRest.

Imbalance classification is the hardest problem to solve in Fraud detection.

CanvasAI: PayPal start-up

Industry which is tech oriented is hard to target. Industry where core is not tech. Don't spend too much on complicated the model.

Problems being faced where looking for solutions

Questions asked wrt Product

Feedback wrt Product - what would make them possibly adopt it

Concrete Next Steps

No next step. She clearly mentioned it won't work.

One learning is: Even Head of Data Science is not the best persona for reach-outs."
10439123494,SeatGeek,Brian London (brian@seatgeek.com),,SeatGeek,100-500,100 - 500 Mn,Oct-25,,No,"Technology, Information and Internet",USA,Anuraag Gutgutia,SeatGeek_Brian 04-10-2022,https://drive.google.com/file/d/1kbCHZP63ZdI51xz_skcickoopGoAUxVr/view?usp=sharing,"Call on 4th October

Lead came in via an inbound

Next steps: Unqualified as they use nomad as a Stack. Still trying to talk or set-up another call to see if we could get feedback on Monitoring or ship monitoring to them. Although can consider it UNQUALIFIED.

Brief background (if discussed)

We provide all the Production loads and DS. Tooling to help with offline experiments.

Use Cases for ML - types of models (Is monitoring important etc?)

Data Science team is about 30 folks + 11 on engineering. Analytics is doing insight driven research. Domain for Insights - Marketing, product. Engineering - a lot of forecasting. Some personalization - delivery time predictor.

Popularity prediction etc.

Current Stack for ML Deployments and pipeline

We are on Nomad - not on Kubernetes OR managed AWS version of something. A lot of what we are doing is fairly bespoke. Highly coupled into the Production or application code.

Actual application is written in GO.

Full Spectrum - AWS Compatible. Do you have Jupyter hub? Or SageMaker? Use Sagemaker only for compute supplements. We don't run things online in Jupyter hub. Increasingly, people are moving things to Hex. Hex connects to the data sources etc? We are very conscious of what we are giving HEX access to. Hex Data WareHouse and other data warehouse.

All of the application loads are running on Nomad.

There is no specific need - We have adopted tools as we found them :) Realised that they solve the problems.

Problems being faced where looking for solutions

""Productionisation depends on Online or Offline - If Offline, we have a framework that the DS can themselves deploy something.

Read from Data WareHouse, write back to Data WareHouse.

Online one - DS Engineering on the outset. We give them the title ML Engineers.

Infra team gives a server. Use anything for tracking ? We have an internal A/B testing platform - we don't use that to re-train models.

Questions asked wrt Product

""How does the system integrate will all the Software Sysems?

We are on top of Nomad.

Monitoring could be one thing interesting - all is structured data.

Current metrics: Drift, Outlandish predictions => Would want to track what?

Volume of predictions you get: It differs significantly => highest volume of traffic ==> Avg - 300 predictions per event page field.

No of features: 10s -100s for each model.
"
10356677034,Pricelabs.co,Pedro Borges (pedro@pricelabs.co),,PriceLabs,50-100,<10 Mn,<10,,No,"Technology, Information and Internet",USA,Nikunj Bajaj,Pricelabs_Pedro 29-09-2022,https://app.fireflies.ai/view/Meeting-with-Nikunj::06JqUHVCnV,"Meeting on 29th: Intro call

Next steps: Set up 2nd call

Chemical engineer turned Data Scientist

Did my undergrad in Minnsesota for math and chem engineering

Worked for Chicago public schools. DHL doing some machine vision. Then Pricelabs. Did some freelance work as well.

Anuraag was one of the founders on the team when I started and now we are 70+ people.

Moved to Brazil for family. 7 month 2 days. Arthur is the name.

Dynamic pricing, scrape data from Airbnb, VRBO and Booking.com. We power model that does price recommendation. Look at past data to detect broad trends. Also do future trending models. Couple other products like revenue estimator. How much you should expect to earn. We also have a BI tool that we get the same data and put in a format that users can use.

We are growing company and got funding this year. The goal of this funding was to hire more. We are a significant people in this market. We have been hiring. Year ago we were 20 people and the goal is to get to a 100 people.

With hiring comes some challenges, and its becoming more important for us to manage DS at scale. We are still a small company and dont want to change radically but want to keep an eye on the future.

There is a separate engineering team but our DS team is also engineering heavy. Digital Ocean as a cloud provider.

Sometimes we want to try different databases, what are our workloads, partner engineering teams. Cut off point between us and engineering team is the APIs. Our team puts the APIs in production. Different companies have different models.

What makes it interesting - there isnt an immediate need to implement something brand new but just keeping an eye out in the future.

DS team around 8 people (mostly DS but some python engineers). We also have 1 developer between DS and engineering who helps with reliability. Engineering team.

We don't do docker containers, deployments are manual. We keep our stack conservative with our stack. We want to know well the few things.

We use Flask and deploy behind gunicorn.

Historically, our models were based on heuristics. One of our founders Anuraag comes from revenue management where he did pricing model from United Airlines. It was tuned manually and it became a fixed thing. We are reinventing pricing models to incorporate more data. There will still be some heuristics involved. Some models are run more offline.

There are two types of models- mostly on historical data which uses years worth of data, 2.) Future looking models are run online and updating those models is kind of important. Not sure how often we will retrain it.

Regular traffic is kind of uniform- there is some time zone thing that happens. We did run into some load spikeness issue when we were doing a webinar where 400 people joined. He was leading to people to use the functionality at the same time. We are working on some load balancing.

As the team grows, we want to make the things more uniform and more stable. Goes with onboarding as well. We dont want to spend time on onboarding the team. Want to make data easier to find, making our systems easier to work on.

Like you were saying, scalability is interesting. A lot of our jobs are batch jobs and that scales horizontally well and for the stuff that is online we dont do a good job at it.

We don't monitor the model much (but seemed to resonate well)- but because our models are based on heuristics its very hard to monitor. We are developing new models that have more well defined objective functions and are planning on adding some monitoring. That is a good point. We are monitoring data sources.

Now we are starting to use some scikit learn and there is some bespoke time series models that we built ourselves.

I find it hard and its my whole career for doing engineering heavy thing. Isnt it a little scary sometime to oursource your load balancer to TrueFoundry.

Was very sold on future evolving."
10346094649,HealthIQ,Abhishek Prakash,,Health IQ,>1000,500 - 1B,<10,AWS,No,Insurance,USA,Nikunj Bajaj,HealthIQ_Abhishek 29-09-2022,https://app.fireflies.ai/view/TrueFoundry-Health-IQ::KtNTvfmwXF,"Based in LA. Have some kids.

Astronomy is at heart Data Science.

Just know that most of the stuff we use is in house and we will use AEP for the next 3 months. Annual enrolment Period - we have a code freeze. We cant break anything.

All the insurance companies do their 70-80% business during AEP. Once AEP ends you have to have exceptions.

Overall I have built and deployed several models and we use AWS ecosystem.

How the businsess flows- we negotiate discounted rate for people who qualify for a certain criteria. The way customer would qualify is based on healthy lifestyle.

When you live a healthy lifestyle- eat healthy, dont smoke, dont drink, etc. you use your insurance less. They are willing to provide discounted rates. We sell them our policies. If the agent makes the Sale, we have to do paperwork together. Once carries is approved people start upaying premium.

How to generate better leads? Identify important features

Then we do a lead scoring. The faster we want to dial and engage with them. Dialing with the leads. Typically we have more leads than the dials we make.

We also create features of those leads.

Additional features are also included. When we start diaing people, not everyone pick up the call and talk to us. Our SLA is two times a day. Once we have number of

Plenty of vendor that can augment the data. This lead might have a lot of demographic information. If somebody is a mover or willing to relocate then medical supplment plans

Strategic shift of entire company towards precision medicare. We are pulling all the possible health data of the person. Based on thier current and future needs we recommend them a plan. What drugs, what prescriptions, what doctors etc.

Our data science and predictive modeling work together.

The challenge is not about deployments. You have 3000 counties and there could be 70k total plans. Depending on the county you wll have 7k active plans.

We have 3rd party vendor that provide information. This is not a modeling problem.

Biggest challenge: We have 200 diseases and have multiple conditions. Diabetic patient can have dental erosion. Multi label multi class problem.

Sagemaker & Sagemaker Studio thats where models are deployed. Initially worked on my laptop and worked in CDR phase.

Use sagemaker container. ECS.

Engineering and data engineers take care of infrastructure.

Data scientists and MLEs are the same.

Data engineering part is raw input data to me. Redshyft and DBT.

From there my team does everything.

When we need to retrain the model. Inference.

The volume of work is a lot. 200 diseases multi label multi class models. Neural networks is the way to go but they are not light. Good model is important but agility is also important.

We generally try to build 2-3 layer neural network.

I know everybody can do delpoyments. For example, deplyment part is now moving towards engineering and they will do that. This also saves some time. Model also needs to be integrated with the mobile and the app.

We give preprocessing script, model in pickle format, and postprocessing script.

We do everything on code reviews.

Dont know about model monitoring- right now I am at a higher level of leadership. Dont know the details of the processes.

Preprocessing is done by python script which gives you and output.

n+1th feature will do testing and training and push the code to Github.

@Akshay TF Can you get me a call with someone else at HealthIQ before my follow up call with Abhishek? I would love to understand some more of their systems because he himself is not opening up about problems too much. Additionally, they use Sagemaker and no mobitoring solutions so there might be a good fit there as well. Need to prepare this before our next call."
10323043405,Phablecare,Abhinay Vyas (abhinay.vyas@phablecare.com);Bhavesh Jardosh (bhavesh.jardosh@phablecare.com),,Phablecare,500-1000,<10 Mn,<10,,No,Wellness and Fitness Services,India,Anuraag Gutgutia,Phablecare_Abhinay 23-09-2022,Phablecare_Abhinay 23-09-2022,"Meeting on 23.09.2022

Attendees: Bhavesh, Abhinay

Brief background (if discussed)

4 folks from Data Science team including leaders. We have in-house MLOps Platform. Set-up context a bit on TrueFoundry => 2-3 mins overview.

Use Cases for ML - types of models (Is monitoring important etc?)

Lambda - ETL written (AirFlow is trigerring Lambda) => Time is not a constraint.

There is a data on S3 => ETLs.

Context

Work on AI problems in healthcare

They do not want to share the information about their infra.

They work on Computer vision and NLP

They have a roadmap in which they want to do testing etc before deployment

They are focusing on model building right now and not the deployment part

They want demo with HuggingFace Model Deploymen

Current

They think their deployment was fairly convenient for the first model

All the pieces of the puzzle are there

Team

15 member team

In Data Science team has 6 Member team

Most people have come out of IIT etc

Some people come out of Computer vision

DevOps team

Scale

1 Model in production, Not client facing

They have a batch processing model

Soon would have a real time inference model

They do not need or in near future need heavy infra

Stack

Immature Stack ? No use of Kubernetes

Must Have

Aspiration

Want to get to user facing models

Want to volume and scale But they would not be huge

Want to get their infrastructure to the production grade

Looking to have a service model deployed soon

They need

Model testing

They want to evaluate different deployment settings and machines

Output"
10323043378,BlackBuck,Pappala Sekhar (pappala.sekhar@blackbuck.com);Darpan Kumar (darpan.kumar@blackbuck.com);Thejasvi Bhat (thejasvi.bhat@blackbuck.com);Deepak Warrier (deepak.warrier@blackbuck.com),,BlackBuck,>1000,50-100 Mn,<10,AWS,Yes,"Transportation, Logistics, Supply Chain and Storage",USA,Anuraag Gutgutia,BlackBuck_Pappala 23-09-2022,https://app.fireflies.ai/view/Blackbuck-prep::dlTmLB7bX3,"Meeting Notes - 23rd Sept, 2022

Brief background (if discussed)

4 folks from Data Science team including leaders.

We have in-house MLOps Platform.

Set-up context a bit on TrueFoundry => 2-3 mins overview.

Use Cases for ML - types of models (Is monitoring important etc?)

Lambda - ETL written (AirFlow is trigerring Lambda) => Time is not a constraint.

There is a data on S3 => ETLs.
"
10277539714,Branch,Nishant Kumar (nishantkumar1292@gmail.com),,Branch,50-100,100 - 500 Mn,<10,AWS,Yes,Financial Services,USA,Nikunj Bajaj,Branch_Nishant 27-09-2022,https://app.fireflies.ai/view/NIshant-TrueFoundry::BNiouzsAez,"New Feature launch is a frequent problem but not such a huge point. I would rate it at 5-6. This problem is a slave to our system and not a problem for DS in general. If you are doing evaluation, you dont want to tinker the production system. How you pull feature is using the production system. We can have another system spun up which deploys code using a PR branch. The code is still in development, and data is still in production.

Mentioned in this context spinning up and closing pods is a problem.

Once you have the feature how do you know if this is adding value.

Maybe you don't want to run entire hyper-parameter tuning job to test out one feature.

Another technique that we have tried is you have a full suite of features and you introduce a random feature. If your feature falls above the random feature you are good.

Model observability is a 9/10 problem. Tracking mean median around the scores. Then the score drops. Baseline shifts. Then we built certain indicators around why that happens. If there is a tool to debug that, I would classify as 9 out 10. We also have feature monitoring. We use shap values a lot. When asked about external tools, mentioned have not evaluated any. Might benefit from our distribution and debugging dashboards. This feature distribution job is set up as an Airflow DAG. It runs weekly. DAG populates the snowflake table which we use Periscope for BI tool.

Also some problem around data collection. We have a lot of data and we train our problem on a subset of data. What is the best training sample that would help product module for future inference. Ambiguous question for us. Should it be a 50-50 split, should be a 80-20 split. Train sample selection space is too many to test.

Remove the sample storm- when RBI restricted the lending.

What is the process for your infra?

3 systems- you receive requests (run some heuristics), feature store, model endpoint. Spinning up a pod would mean the feature part. Spinning in prodiction is not a pain. But spinning during development around connections, config files and environment variables can be a pain.

This is not necessarily a problem on a day-to-day basis because DevOps have set up for our needs.

Taktile provides endpoints.

Can certainly try out the platform on a personal front and provide feedback. If you can give me an account, that would be very nice.

We are doing our Q4 planning at Branch. We have figured out certain projects for Q4 and I can talk to a couple of ML folks and figure out a POC that we can do on your platform. I will try to do that. It involves a lot of stakeholders. Generally involves DevOps or CTO taking a final call. From an ML use case standpoint, we can define amongst our team."
10277539714,Branch,Nishant Kumar (nishantkumar1292@gmail.com),,Branch,50-100,100 - 500 Mn,<10,AWS,Yes,Financial Services,USA,Nikunj Bajaj,Branch_Nishnat 22-09-2022,https://app.fireflies.ai/view/Meeting-with-Nikunj::cHFby7aj7G,"Us based company with operations in India and Africa.

We primaily do app-based micro-lending.

In African countries we have other features like wallets. Have a bank in Kenya and getting a license in Nigeria.

We receive application for loans and use ML models for loan applications. We generate credit score.

I am an ML Engineer in lending team. 4 ML Engineers.

If you just wanted to analyse models, it can be given to analytics or policy making team.

We have built out a lot of dashboards including shap values and all.

I joined the company about 1.5 years back. In India it was pretty small.

Back then we were deploying models on Sagemaker. Then we moved on to this German startup - Tactile. It was even before me. I can connect you with folks if you want to understand this.

Advantage with tactile is you can use the functionalities of git to enable deployments.

The gist of the problem is there were manual efforts and it was not automated at all. We wanted more automated deployment. Our system is pretty sophisticated and we dont want to spend effort where it is not worth it.

We dont want to be a servant of infrastructure. I dont know what other vendors were evaluated. Our deployments are sophisticated now.

Models vary by product and country. Major operations in 4 countries. We have models for different borrower type to bifurcate different customer segments.

At any point in time we roughly use 10-12 models in production.

We also have POCs running for some internal use-case. They are in addition to it.

Some are not ML models but they are heuristics. They are not deployed on Taktile.

Our infra is optimised for Taktle so we deploy everything on Taktile. Sagemaker would be easy if you deploy.

Completely AWS, Deploy on Taktile K8s cluster.

Training happens on our clusters. Deployment is an model endpoint.

Really should do a deep dive into this in the next call.

ss

All model training happens on Sagemaker. Data resides in S3 so its easier.

A model is a repository and each branch is a model. The main branch is the one currently deployed in production.

Once you start model training, Github actions does it. We have a folder called analysis which is displayed on taktile platform. Its a good UI to use. You could also use Github to render the markdown files. Inside the company everyone has access.

From our side it is only a commit hash that we do. On the backend, Taktile, spins up a pod for that branch. All endpoints, memory configurations that we have defined in git config.

Pod maintenance is not our headache now. We do have access to the pods- you can look at the errors.

We have assets for the model which is all the files, for example, preprocssig code, test and eval data. They also reside on the branch.

All the preprocessing logic is packaged as pickled objects.

Lot of challenges still remaining- we are solving a lot of challenges. Not on model deployment front but on features front. How do you decide when to create a new version of a feature. Trying to solve it using feature distribution for training and test data.

Your data is stored in data and you do joins to get the feature. Now you have trained a model and put in production. You know somehow that the feature is misbehaving. You create a new version of the feature because you cant just edit the current code and train a new version of the model for that feature. For example, I am a food delivery service. I want to create some features on driver GPS data. From the shop, how much time that the driver will take. Say I am creating a mean and that is not performing so well. If you change to median you will have to create a separate version of the feature.

Our A/B testing system is outside Taktile. We send it to different endpoints to Taktile. Splitting happens internally. We have built relatively sophisticated stuff. Experiment is not just a model deployment thing because its not just the model but other artifacts into the pipeline. You could have android screen or backend APIs.

The problem is once you get a new feature you have to go through an entire model deployment process. Training itself can take time. Fast experiments dont work there. Its all about statistic.

Next Step: Ask more questions. This problem he mentioned is super interesting and the person is nice."
10245511681,DocSumo,Bikram Dahal,,Docsumo,<50,<10 Mn,<10,GCP,No,Software Development,USA,Nikunj Bajaj,,https://app.fireflies.ai/view/TrueFoundry-Docsumo::FCQwYMkxcB,
10245510764,Elijah,Elijah Roussos (elijah@cerebrium.ai),,Cerebrium,<50,<10 Mn,<10,,#N/A,Software Development,USA,Anuraag Gutgutia,,https://app.fireflies.ai/view/Elijah-Roussos-and-Abhishek-Choudhary::hi4Q8nDIt6,
10245510764,Elijah,Elijah Roussos (elijah@cerebrium.ai),,Cerebrium,<50,<10 Mn,<10,,#N/A,Software Development,USA,Anuraag Gutgutia,,https://app.fireflies.ai/view/Meeting-with-Anuraag::QlcUGtVgZ2,
10238582319,MavQ,Hridul Gupta;Arsh Sidana (arsh.sidana@mavq.com),,mavq.com,50-100,<10 Mn,Oct-25,,Yes,Software Development,USA,Nikunj Bajaj,MavQ_Hridul 19-09-2022,https://app.fireflies.ai/view/Meeting-with-Nikunj::XMrffNFQQA,"Hridul- VP Innovation, AI implementations, productising AI for customers, very big in public sector.

Predominant sector is state agencies- data privacy, securiy, model resilience, scale and replication. Uptime budgets.

Forked with fortune 500 companies- nike, home depot etc.

One of our AI products is for clinical travels- predict lifecyle of clinical trials. XGBoost, Random forest- here model is a bit simpler.

Intelligent document processing- e2e life cycle managemnt for public sector. We host those models so we can extract information. 1-10M documents per agency per year. We provide them a product so they can provide the data and annotation required and the system is

Lot of ensemble models packaged together as containers which can be dpeloyed themselves as a service.

We need a productisation of horizontal scaling so someone can control it. That shapes their cost. Generally cost is a big factor.

Because cost is a big factor and this is used 8 hours a day and we want to scale down to 0. If there is a new request that comes in, it should be up again in a couple ms.

This racking cost at infra level is very important.

Our USP is that they can generate custom models. Entire system should be able to handle the HPA. If we have 10k models running all the time it will be a huge bill. Something that we are asked for- disaster recovery and resiliency. What is your RTO or RPO. How do you guarantee with the entire region going down.

There might be agencies who want a throughput of 10 documents /sec or 1 document / sec. That will drive their cost. This will enforce their HPA.

We use multi cloud. All our ML models and everything we do is cloud agnostic. Replicable fashion and bind them as containers. They ask us to host on our own cloud.

Entire infrastructure is based on K8s. We sometimes allow development on colabs and AI servers and deployments are exclusively on K8s.

For CI/CD we have our own CI/CD tool which we have configured for ourselves.

We have an ML Engineering team which is my team. Our outputs are containers and the Devops takes over.

Our document processing is human in the loop.

We have our own AI servers which all engineers have access to. This is own machine that are in our offices. They are big enough servers.

Dev, testing, performance or UAT or release candidate and production. Testing happens manually. We put scrupts which put a lot of load on the system. There are test cases for each module which talks about the code coverage.

For internal monitoring we use Grafana. Catches all the logs of K8s pods and gives us a nice view of memory utilisation and all. For exposing metrics to customers we have not done that yet. We give them a dashboard of AI services used.

For different model versions that we create we basically capture metrics across each experimental run. For the product part of it, all stored in database. We have metrics logged but no tooling around it.

We use MLFLow/

Right now we control how scaling and all happens. I want to get to: Once we click the dpeloy, we dont need to get involved with infrastructure and all. One time configuration on K8s we can do but how do we get to empowering customer for that.

Scaling down to 0 and coming back up in 1 second is important.

We have a lot of frameworks that we work on. Document processing is ensemble of 3 deep neural networks. They are completely custom workloads. Custom runtime is a problem we are trying to solve as well. If we had only 1 runtime we can scale it down to 0. With Multiple runtimes open source solutions fail. Especially with GPU

We were exploring TFServing. So long as you use these runtimes, you just load the object and we start serving. This is fine. Then if you have two models which belong to different runtimes, then you cannot use that toolset unless you use your custom runtime server. We are moving towards being model agnostic. The framework should not worry about runtime and should be able to scale down to up.

Next Step:

Product demo, platform demo, if we wish to go ahead we will need NDA. Akshay and Arsh to set it up. Add Akshay, Chirag, and maybe Shubham to the call. Will need preparation from our side.

Cover: scaling down to 0, bringing back up, multiple runtime serving."
10128442512,Credibly,Sachin Goel (sgoel@credibly.com),,Credibly,50-100,50-100 Mn,Oct-25,,No,Financial Services,USA,Nikunj Bajaj,Credibly_Sachin 13-09-2022,https://app.fireflies.ai/view/Meeting-with-Nikunj::nDXmf376Fc,"Most important model is probability of default model or credit risk model.

Credibly has two different models- credit risk which is prequalification or lighter model.

Another type of model is the fraud side. Since 2020 - there is an increase in digital fraud.

Fraud model - this person is more risky in nature. He / she wants to run away.

PD models are supervised models and fraud models are unsupervised.

Data crunching data gathering is done by the data science team. After that it is a joint effort between data science and tech.

IT or tech who own the systems, products and pipelines.

One of the major signal came from credit bureau and data is lagged. You want more realtime signals.

We started relying more on manual process. We track a PSI metric- probability stability index.

Non-tech component you have to get it approved by compliance team, managers & investors. It took about a month including tech and non-tech. We never end up updating frequently but maybe with DataRobot we will do it.

On the model building side, sometimes you have too many features. One challenge we had was having the right number of features. Second issue that we saw on model build side was reject inferencing. You have the target and for whom you declined or you approved but they did not fund with you. We had to spend a lot of time on reject inferencing.

Why does it take so long?

One thing is model and other 4 things are running in parallel. We have offer calculator or pricing. We have a model to price our deals. That is directly identified with the model. How you impact other things?

If something is not meeting this guideline. Reinventing the wheel.

If you can focus on two numbers for lenders- increase in approvals and reduction for losses - that will get all lenders attention."
10128442512,Credibly,Sachin Goel (sgoel@credibly.com),,Credibly,50-100,50-100 Mn,Oct-25,,No,Financial Services,USA,Nikunj Bajaj,Credibly_Sachin 09-09-2022,https://app.fireflies.ai/view/Meeting-with-Nikunj::c6IAHLxfyC,"- Fintech company in commercial lending space.

- Loans and MCs to Small medium owners.

- Typical lending business and more digital

- Something called broker or agent through which Credibly acquires customers.

- Has Internal sales team

- Prime lenders good credit history. Another segment, near-prime or sub-prime. They take more risk and interest rate is higher. Highly profitable business.

- Usually takes 4-6 weeks to take approval and highly rigorous process and underwriting cost is huge. They have to do big ticket loan to justify costs. Credibly can generate smaller size loans.

- Now coming to customer acquisition- subprime customers who dont have good web presence. Lenders like credibly.

- Once you receive application from these brokers, you do an underwriting process, you want to automate as much possible. They dont have very god credit history. Different people might have different views. People who run Credibly believe in hybrid underwriting.

- Need to parse data from a lot of different sources. Can we automate this?

- When you look at the cashflow data, you want to check how consistently you are generating revenue. Check credit history for this merchant. Good social media presence.

- Credit bureau data is standardised and some vendors parse data automatically from different sources. Data extraction from web is very costly and inaccurate. Google, FB APIs etc. We recently started using this data for our underwriting process. Facebook has a different tiering system. You can go to another level which gives you more data and you can go to premium levels. How to find right customer for Social media. This matching of right customer is already hard.

- Subscribed to a vendor called DataRobot. Trigger point was hiring Chief Data Scientist and he was working with DataRobot earlier. He came in and proposed DS and we did POC.

- Asked about problems of DataRobot- whole process of data building and acquiring DataRobot. They have some AutoML - and they have a lot of monitoring. Two drivers were- you can build faster, you can try different algorithms, you get transparency with DataRobot. We did a 1-year contract with them. Dont have any limitation side and some limitation. CXO was involved in decision making process. Needed finance and legal approvals. Once you prove it is adding value. POC took about a month."
10097842831,TurtleMint,Ashish Gawali (ashishg@turtlemint.com),,Turtlemint,,500 - 1B,<10,AWS,Yes,Financial Services,India,Anuraag Gutgutia,,https://drive.google.com/file/d/1d3_nzkQkWR5MtsBDOj7KJDZWgA_7Lb8P/view?usp=sharing,
10097842831,TurtleMint,Ashish Gawali (ashishg@turtlemint.com),,Turtlemint,,500 - 1B,<10,AWS,Yes,Financial Services,India,Anuraag Gutgutia,TurtleMint_Ashish 05-09-2022,https://app.fireflies.ai/view/Ashish-Gawali-and-Abhishek-Choudhary::CATuttpErL,"Brief background (if discussed)

""Connect via Linkedin - MLOps is pretty hot space. Look at the entire ML Journey of Turtlemint.

Ashish - Late last year through acquisition joined Turtlemint. Been in Software product for more than 20+ years. Earlier product was in Cloud Analytics space.

Large enterprise experience. Joined to start off Data Science and Data Engineering - was there basic level.""

Use Cases for ML - types of models (Is monitoring important etc?)

""Data science: CV for recommendation and NLP based problems - Performance improvements.

Open problems around Finance - fintech (Insuretech). Open problems as well. Business Centric and highly aligned to business.

Statistical things, Core ML Algo and Deep Learning also in picture. Reinforcement - nothing yet.

Fintech with major focus on Investment through MFunds - but small part. Turtlemint pioneered B2B insurance through brokers. ""

Current Stack for ML Deployments and pipeline

""Primarily based out of AWS Stack - we use Kubernetes, everything is managed. Good team in DS and ML.

Primarily applied - lot of things in production.

Intent: Can't answer or share all data. Benefitting - Team wise: < 10 Members in DS. More focus on Quality rather than Quantity. 2000+ Employees, 200 Engineers. 15+ Data Team (Small team)

Models in Production: Everything is applied. Can write papers on XIV - Emphasis has been on putting models in Production. K8S for deployment, EKS => SAme thing. Deploy on Pods and can scale them. Through APIs gets into Production. Backend Data pipelines.

Thing could be - at juncture of company, you might not worry about scaling using K8S. Some use pHP and remain on PHP for many years. Data Scientists will come from non-CS background. Data Engineering team is well positioned to handle it.

Both real time and batch inferencing. Not using SageMaker - was tried and maybe we could use it later. MLFlow, EKS could be used. Would like to see TrueFoundry. ""

Problems being faced where looking for solutions

""You cannot imagine DS to be writing YAML.

Looking at ZenML, SageMaker.

Was not giving concrete answers - seemed to be fearful.

K8S is not seeing adoption but based on the company, it could be different.

Checkins on the KubeFlow side have stopped.

Egress and Data Security both are important ""

Questions asked wrt Product

Good Questions Overall- Couldn't record this though

Concrete Next Steps

""Right persona that we have targeted in this case.

Little reluctant to share details, but later on, went into asking a lot of good questions.

Set-up another call for 14th Sep - 1) UI Overview 2) Product Overview 3) Technical Architecture """
10097873824,Tangent.AI,Mayank Kumar (mayank@tangent.ai);Nitesh Garg (nitesh@tangent.ai),,Tangent.ai,<50,<10 Mn,<10,AZure,Yes,"Technology, Information and Internet",USA,Anuraag Gutgutia,TangentAI_Mayank 02-09-2022,https://app.fireflies.ai/view/Tangent-AI-TrueFoundry::jQflyWvIpm,"Brief background (if discussed)

""Worked with Mayank in the 1st org earlier. Services start-up, end to end apps. ECommerce and personalization data space - acquired by Flipkart.

Have worked on Engineering side of technologies. ""

Use Cases for ML - types of models (Is monitoring important etc?)

""B2B SaaS company - Ecommerce store owners. Beauty is 90%. AI Selfie solution - Zero party data personalization. 150 data parameters => personalization layer on top.

AI has been a strong selling segment for us. Makes us stand out and apart from competition.

AI Models - SKin condition detection, some for hair segmentation, etc. InHouse Kubernetes Cluster where we deploy. AI will be a part of our product as we go forward. It will be the enabler. AI will have some share of it. 10 People here - AI Researchers: Couple of them. DevOps and Backend - I take care. ""

Current Stack for ML Deployments and pipeline

""AI Researchers put models in production themselves - how and where we should deploy? Have shared access to them and they do it themselves.

What made you choose Kubernetes? Ideally we will keep on adding these models - some will have more load depending on requirements and we will need small microservices. Docker Swarm and Kubernetes - Kubernetes is easier to manage. We are currently on Azure and not on AWS. Helm Charts are not updated that frequently - Any new model to be trained and deployed takes months => to take it to Production Quality.

We have few test machines where we used to do any kind of scratchpad work . After we have tested things on separate machines, then do it. Push to Docker hub and then deploy on Kubernetes.

We recently made dev space and prod space. Jupyter notebooks live on the same machine? Right now, we use the normal IDEs instead of Jupyter Notebooks. Not using it for scripting. USE More of command line. GPU requirments are there and they are not fulfilled on local machines . ""

Problems being faced where looking for solutions

""Models in Production are a combination of CPU and GPUs . Real-time or Batch? Mostly RealTime. Do you use Inference servers or use FastAPI etc.

2 cases: Models and services scale differently OR where they scale together.

We haven't found a need where we have to do it separately. Haven't found the need to scale it separately.

Pros for scaling it separately: Say you have GPU Model and then transformation happens on CPU. If you get a request, you have to make sure GPU is at full capacity. People will take the part that runs in GPU and extract in a separate service. (LISTEN RECORDING: 21 minutes to 26 minutes)

Which framework? Django, Flask, Fast etc. Mostly Flask or a Falcon server that is pretty light for model inference. Cloud Charge as of now - is it very small? 1500-2000$ per month. Don't expect Cloud costs going up a lot from here. Product not expanding on AI side as much.

You know you have to do it eventually - migration becomes a problem later on. We have talked to customers that you will reduce cost by 30% if you migrate. But too big to migrate. DO you also have any inference monitoring? Do you label, sample etc to improve the model in anyway? For us, the data is majorly selfies. When re-training, how do you do model evaluation? ""

Questions asked wrt Product

""2-3 Problems -

While training, we keep on tweaking these hyperparameters => if go to something older, try on that.

This is something that can be tried out at our end. Other things are not a problem for us. ""

Concrete Next Steps

""Very happy with the Kubernetes based set-up they have for deployments.

Next steps:

1) Try out the Experimentation Piece of the Platform

2) Do a follow-up for the deployment demo and feedback. Won't be a customer. """
10097841502,Simpplr,Achintya Shankhdhar (achintya.shankhdhar@simpplr.com),,Simpplr,100-500,10-50 Mn,<10,AWS,Yes,Software Development,USA,Anuraag Gutgutia,Simpplr_Achintya 12-09-2022,https://app.fireflies.ai/view/Achintya-Anuraag::DQrLG3hVAC,"Brief background (if discussed)

""- [x] AI Industry for 4 years => NSIT 2018. During that time, involved in research in IBM etc. After that, personally interested in start-ups.

- [ ] Had joined SkitAI earlier -> Exciting company, working on VoiceAI. Solved a number of ML Problems, wanted to solve ML Problems and hence joined Simpplr. Solutions at Vernacular were client specific. Hence wanted scalable solutions. ""

Use Cases for ML - types of models (Is monitoring important etc?)

""- [ ] Started ML journey 1 year back - 7 people ML Team. Trying to solve a range of Problems. Employee experience company. Intranet where company can put organisation level announcements. We were working on Problems like - Sentiment Analysis to analyse overall sentiment of Employees, trying to find topics they are talking about, What people want with the help of ML. Earlier just facilitating conversations. (JIRA, Confluence, Atlasssian, Drive, Teams, Slack) , Attrition rates - the biggest problem when it comes to scaling ML Models

- [ ] Clients: Companies like Zoom, Nutanix. The kind of conversations for both these companies would be different.

- [ ] 400 Customers => Individual models per client. Train, Deploy and Monitor a model is a challenge. ""

Current Stack for ML Deployments and pipeline

""- [ ] Till last year, ML trained on data from all orgs, but fine tuning the data. Data sits on SnowFlake Infra (Platform is based on SalesForce and data aggregated to SnowFlake). 7 DS, D Engineering team is different, SE Team - works on Deployment side of things. We are designing the infra. Which AWS Service would be best to deploy the model? SE Team also does integration of models onto the main platform. Need to get a ML Engineer to do MLOps work.

- [ ] Integration: Sentiment Analysis model - they will deploy and do CI/CD Pipeline - integrate with backend service.

- [ ] Right now - Experimentation part and building the end Point. GPUs for training - AWS Instances like EC2 Instances. Started with hosted notebooks, but better pipelines. We are using dumps - dumping something on S3. Scripts that picks the data, train and dump the model ==> DVC for versioning the model and data.

- [ ] Phase of exploring MLFlow, NeptuneAI, etc. Targets in quarter to finalise the MLOps platform (Model Experimentation) // (Deployment) // (Monitoring). Was exploring Triton but havent adopted. APIs are deployed on EKS - Kubernetes but we are not exploiting benefits of GPU based infra. Have a person who understand Kubernetes - Engineering team part. ""

Problems being faced where looking for solutions

""- [ ] Combination of Scalability in Deployment and Monitoring. Generally, general model and then fine-tuning the model. How do we ensure we have 400 different models and how to monitor the quality of these models. ==> Alerts mechanism of the same, Improving the results of the services.

- [ ] Monitoring is Ad-hoc. We are doing offline evaluation of batches. That is something we are trying to roll out. Data out of the system, get the metrics etc. Small team and dont plan to expand size of team. ""

Questions asked wrt Product

""- [ ] We have started experimenting with Airflow but not sure if we are going forward with it. Using it for creating these pipelines - Mostly doing NLP. Standard transformations that are happening with data. We host a different service and then things flow. Looking to do parallelisation to reduce latency of the system.

- [x] TOOLS EVALUATING: Want to be able to use it for next 5 years and will it be relevant. Makes the adoption process a bit slow. Most of the Evaluation has been for Single tools. Evaluated - MLFlow and NeptuneAI ==> Model Mgmt tools - how easy is to do versioning, experiment tracking. How is the UI? How is it to do resource monitoring? Follow trends a lot. Product has a continuous support - most tools are Open SOURCE tools that are community supported and how actively versions are being released. THINKING of trying out SageMaker and havent tried it out so far - no flexibility.

Concrete Next Steps

""High Relevance Customer - Super Good Fit.

Need to connect with Kaushik on September End.

Try to also find another connect here.

Rest - everything maps well + looking for ML Tools and platforms.

Set-up a demo call with the ML Engineer coming friday on 9th """
10095655759,OneCode,Manish Shara (manish@onecode.in),,OneCode,50-100,<10 Mn,<10,GCP,Yes,Financial Services,USA,Nikunj Bajaj,OneCode_Manish 12-09-2022,No Recording,"- Industry standard is 10% conversion rate- but we want to get to preapproved experience. We have more than 10% already for some brands. Right now we think of each brand as one model.

- Ankit would look at approvals and rejections and fine tune the models.

- We have multiple brands- credit cards and loans we have 6-7 brands.

- Earlier we didnt check any bureau or customer detail. Now, we are gathering customer information like bureau details. Apart from this, bank cheques like incomes.

- There are other 3rd party services that they have. What we are trying to do is we restrict the customer that this customer can be only sold for these 3 cards. We build decision tree based models.

- In a bank called IDFC, we have seen that they dont give card to age < 23. Then we also noticed credit limit of 15000. This is what we have built right now.

- Going forward the aim is to build an ML model that can show a person, what are the chances of approvals.

- Challenges --- Dataset is very small. For certain bank, we have <100 approvals. 100 positives and 2000 negatives. Hope is to build models when we have more data.

- Almost 200k users for bureau details and now adding 1.5 lac customers per month.

- Main part was to discuss the problem and potentially brainstorm some solutions. Small dataset problem is one."
10038142068,OneConcern,Shabaz Patel (shabaz@oneconcern.com),,One Concern,50-100,<10 Mn,Oct-25,,Yes,Software Development,USA,Nikunj Bajaj,,https://app.fireflies.ai/view/Shabaz-Nikunj::W6G3zPhvdV,"Undergrad IITH- Graudate school Stanford

Started a company in ML Experimentation- fundraising time we got acquired by OneConern

Team of 30 DS

Natural catastrophe model. AI ML models and physics based models to understand the impact of climate change, from flood, seismic, typhoons etc. How can we provide that information and data to various customers, emergency managers.

We are basically data providers.

We also have our GUI to help them understand and visualise the data. We also provision using Snowflake.

Few pipleines which are realtime. Domino is offline predictions and

Ready makes online predictions- forecasting of floods or earthquakes.

Our problems range from monitoring of datasets to now going towards building a self-serve data science platform. We want DS to be able to deploy quickly. We are trying to figure out.

We have Argo pipelines, building our own model registry and data registry which can be used for a more data-centric approach. The config of model can change and updating of model configuration.

We think of horizontal scaling currently. At some point we are thinking of vertical scaling as well.

Productivity and repeatability of pipelines is my main concern. We need to be able to accept new requests from customers. The models are shard with engineering team. Sometimes they are hard coded stuff. Have configuration to pipelines.

DS expertise has their own niche area of expertise and they are not engineers. Huge burden for them to learn new set of skills.

I am letting the team come up with their opinions. Not pushing down some tools.

Was focusing on experiments and deployment at scale. I am curious how the market has responded to your product.

Silicon valley is not the right place. Started getting revenue from regions like Minnesota and all. When your product is so good.

We have solutions team where we explore POC projects.

I am writing a book with my ex-cofounder

Moving existing pipelines is hard. Its a difficult justification process. To me and my manager.

QA team is doing

Argo, greatExpectations, K8s,

We have used Streamlit a bit. Gradio given its within"
10038142068,OneConcern,Shabaz Patel (shabaz@oneconcern.com),,One Concern,50-100,<10 Mn,Oct-25,,Yes,Software Development,USA,Nikunj Bajaj,OneConcern_Shabaz 02-09-2022,https://app.fireflies.ai/view/Abhishek-and-Shabaz-Patel::hIk6Ox4b73,"Brief background (if discussed)

Was earlier Senior DS at Uber - it seems they build the platform for SC.

Nexus is banking as a service platform - They partner with e-commerce companies. Bukuwarang eg. Using Nexus, other companies. 3 Members in the DS Team.

150 Members in the Nexus - Was started within SC Ventures. Central team is in Singapore.

Use Cases for ML - types of models (Is monitoring important etc?)

In terms of growth, just launched Product in the public domain: Getting users to sign-up. With data collected, can build data products.

* Churn Prediction: Given transactions of users, how will they churn.

* Built a number of dashboard to monitor metrics for the business.

* AirFlow for scheduling, Super

* Customer Lifecycle Prediction

* How will the user transition from a high value to a low value customer.

Current Stack for ML Deployments and pipeline

Models are tested in local environments. These models will be real-time. Built as a micro-service.

Stack: Rope in the data engineers but in a nutshell - our Data Lake is on S3. We use kubernetes platform to deploy our micro-services.

Interested in our solution and how it can be used. HAVE you worked in a bank? They are actually quite dfferent and what you can do in banks is not as easy.

We do micro-service in a local environment and pass it to DevOps to deploy it to Cloud.

Mostly doing the training locally - its done locally.

Framework - ScikitLearn => currently not using GPUs.

Problems being faced where looking for solutions

Looking at for example - for DS ==> Modelling aspect. Data Engineers and DevOps => DEngineers and DevOps will be tied up with different other tasks.

Questions asked wrt Product

We have a real time model and we update the model in 1 week? We test and deploy the new model to Production => what kind of workflow do you have for this kind of scenario. I am fine with doing re-training every few weeks. We have both. 1st one => schedule a training job and can be done easily in our platform.

Building a complete approval loop is something we are working on. UI for the platform and then do a real-demo with your team.

* Suppose I say I see the result is good and I want to deploy the new one? The old one is already running. Standard procedure - you will want to do it using CI/CD. We have examples in our documentation as to how we are using GitHub actions. Once you committ your changes, it will automatically commit your changes. Go to UI - click a model and click deploy.

* When models are running in production, do you have a tracker which models are running in Production => How many customers are on new models and how many on new models. All your instances will get updated quickly. You need to make sure a gracious exit of all models. OLD Models might still be serving some models - we don't want connection to be stopped. 2 Ways to handle it ==> 1) Adding a hook in the code meaning we will not force-kill your system. It will not be a forcekill - it will be a gracious kill.

Feedback wrt Product - what would make them possibly adopt it

Do you support multi-Kubernetes system? It could be private cloud but different locations.

You could have different Kubernetes clusters as you like and have Kubernetes Clusters from different platforms.

Maybe you can also send the slides => if interest from them, we can get them into the call. There are various competing tools. Bank has been using DataRobot, DataIku, H20 - AutoML Platform.

Few parameters where we are different: Solely focussed on Deployment and making it very deep and different. We have HC companies as clients.

Concrete Next Steps

""Very Suitable Customer

Qualified

* Set up next call with product demo -> Will get the Data Engineers.

Looking at a ML Platform - improve our development better.

Next Steps

* Send out a PPT

* Set up time for a Demo next week

* They need WorkFlows and all ==> We are a bit late there """
9984644130,Nexus,Yibin NG (yibin.ng@sc.com);Julian Tan (julian.tan1@sc.com),,,>1000,10-50 Mn,<10,AWS,Yes,Banking,England,Anuraag Gutgutia,Nexus_Yibin 26-08-222,https://app.fireflies.ai/view/SC-Ventures-and-TrueFoundry-Introduction::HnQHiEriGw,
9984747349,CommerceIQ,Mohit Dhawan (mohit.dhawan2510@gmail.com),,CommerceIQ,50-100,10-50 Mn,Oct-25,AWS,No,Software Development,India,Anuraag Gutgutia,CommerceIQ_Mohit 24-08-2022,https://app.fireflies.ai/view/CommerceIQ-Nikunj-Mohit::0es0WBVCOF,"Use Cases for ML - types of models (Is monitoring important etc?)

""DS Sales team - Manage MarketShare, Other is DS Advertising. Both use same tech stack - 2nd team's tech stack is mature.

Data Scientist: 3 Data scientists => CLassification part I am the full owner. Delivery is managed by another DS. 2 Intenrs and few product analysts""

Current Stack for ML Deployments and pipeline

""Most solutions are based on SageMaker. Created our own CLI and environments. Setting up automated pipelines - lot of things happening here and there. Models depend on a lot of other models. SageMaker instances are doing pretty well for us.

Tech Stack: Python, Queries: SnowFlake, Everything is set-up on Sagemaker

Amazon is so dynamic - Dataset is too dynamic. Majority going for unsupervised models - mostly on Sagemaker notebook instance. Given Dynamic data, we will have issues on scalability

Didn't have an idea on Kubernetes. EndPoint is hosted on the UI managed by UI Team.

Store the models on S3 => The communication of SageMaker to S3 is pretty fast. ""

Problems being faced where looking for solutions

What happens is the model output is not given. Sales estimate, classification => then combine the models.

For each client, the model changes - Do you host separate models for different clients?

RESOURCE ALLOCATION: We were struglling.

Set of scripts running manually.

Questions asked wrt Product

The issues of adopting new platform is people are from engineering backgrounds and people are comfortable with SAgeMaker, unless models expnd and it becomes complicated."
9934183436,Synopsys,Manish Sharma (manish.sharma3@synopsys.com),,Synopsys,>1000,<10 Mn,>50,AZure,Yes,Software Development,USA,Nikunj Bajaj,Check the drive for all the mateirals,https://drive.google.com/drive/u/1/folders/12xM04vYA05k9SZvapjY1MxYeUSUqGEaF,
9913843896,AlgoAnalytics,Aniket Gambhire (agambhire@algoanalytics.com);Abhishek Pawar (apawar@algoanalytics.com),,AlgoAnalytics Pvt. Ltd.,50-100,10-50 Mn,25-50,Multi Cloud,Yes,IT Services and IT Consulting,India,Anuraag Gutgutia,,https://drive.google.com/file/d/1aqMRLzKmG545rET_st4fbBvlCi-4JFQu/view?usp=sharing,
9913843716,WayCool,Sydney Lewis (sydches@gmail.com),,WayCool,>1000,50-100 Mn,Oct-25,AZure,No,Food and Beverage Services,India,Nikunj Bajaj,,WayCool_Sydney 02-09-2022,
9905698334,Narvar,Rashmi Margani (rashmi.margani@narvar.com),,Narvar,100-500,50-100 Mn,<10,Multi Cloud,Yes,"Technology, Information and Internet",USA,Nikunj Bajaj,,,
9905698123,Jungle Scout,Seqian Wang (swang@junglescout.com),,Jungle Scout,100-500,50-100 Mn,Oct-25,,#N/A,"Technology, Information and Internet",USA,Nikunj Bajaj,,https://app.fireflies.ai/view/Nikunj-Bajaj-and-Seqian-Wang::UoFSnup4Gu,"Hello. Hey.  



How do you pronounce the name?  



How about yourself?  



Nikhil.  



Nikunj, okay, nice to meet you.  



How are you doing?  



I'm okay, I'm just pulling the different resources you share with me. Yeah. Fun stuff. I briefly went through you LinkedIn, you worked at Meta for a while and then you sold your previous company and now you're starting a new one.  



Yes.  



That's ambitious, I admire that.  



Thank you, I appreciate that. The most fun part of this entire startup journey is working with my undergrad batch sales. Turns out my co founders, IBM, Ragging, Abhishek, went to the same school, we're in the same hostel and everything. So we shane spent a lot of time together.  



That's awesome.  



Where are you based?  



I'm based in Toronto. Calendar.  



Okay, understood.  



They're almost days about to end for you.  



Yeah, absolutely.  



Awesome. Were you able to discuss, I think we discussed on the email on the LinkedIn that you'll probably talk to some of the machine learning engineers in a company and see if they are interested in the call business, other folks.  



Yeah.  



So in order to projectpro that time, I did not further that I was considering it, but if you don't mind, let's just have that conversation and I can direct you to certain people to reach out on LinkedIn. You can let them know that I've referred you to them, but I will let you do the outreach if you don't mind.  



I see. Okay, understood. No problem. Cool.  



So I think I've already seen the background and everything. Just a brief a little bit about myself in terms of the work. I have been personally working in machine learning for the last few years.  



At.  



Meta I was doing more conversational AI stuff so working with a lot of natural language datta. And prior to that at Reflection, I led the machine learning team and built out a lot of recommended systems for the ecommerce industry. Sure, just give me 1 second, okay?  



Yeah, of course.  



Actually just give me 1 second.  



Hello?  



Hi, Shelley. Hop on a different call. Can I call you in 30 minutes?  



Okay.  



Is we are having soumen fundraising conversations.  



We're just talking about that.  



Important ones. Yes, makes sense.  



Cool.  



So basically I was just saying that I built out a bunch of recommended systems, personalization algorithms and scaled the Reflection machine learning systems to about 600 million monthly active users. So yes, I work both on the modeling side and the engineering side of machine learning. And now what we're building at True Foundry is very similar to the work that we did at Reflection. We built out a horizontal AI platform for the company and now we're trying to build it out for other companies in the world with some inspiration from our time at Facebook as well.  



Makes all sense.  



That's where I am. I would love to learn a little bit about you. So from your linguist, seems like you are product manager, a bunch of Datta science projects. So we'd love to hear about you current work as well.  



Yeah, that makes sense. And I try to specific addresses a piece that might be of interest to you. So as a product manager on the Data Science team, my goal is to bring data Science projects to the customer. Right. And one of the challenges we're facing is that our data pipeline, whether it's the data lake, the data quality check, or the MLR platform that we use, is not very mature. So the team, the Data Science team end up spending a lot of time doing manual model decision and promotion on whether we promote recent models as well as bring a R and D project to deployment. Right. So my team specific at this time is working on assessing which MLS platform that we want to use in the future. And I do have a few slide decks that I can show you.  



For instance, when you're asking the problem description right, this is what we're currently looking at. We have a lot of issues when it comes to dependencies, right. Like, a lot of data scientists run stuff on the locals, but then once you want to kind of like they don't have a similar same derivative notebooks that they work on. So there's a lot of differences in terms of like the code base. And right now we are at the phase where we kind of deciding whether we go with Sage Maker or Kubernetes, not cuber flow. My bad.  



Okay, I see.  



These are the problems that you're facing and right now you're in the middle of deciding between Sage Maker and Kubeflow.  



Understood? Okay.  



Yeah, that's right.  



Got it.  



And what are some of your decision making criteria here?  



The one that I'll describe here are the main ones. I think cost is definitely one set up time, how managed it is versus how flexible it is. That's always like kind of preference base. Let me see if I can find other things. Slides. Yeah, I think that's the main one. The other piece which does not play in your favor is that generally speaking, teams are looking for a platform where they can find a lot of features. Right. So that in the future, if they want version control, if they want, I don't know, like monitoring system, they just want more and more features. Right. We may not use it right away, but just because of its maturity and existence, we prefer to go with something that we believe is going to be more sustainable. Gong term.  



Right, I see.  



So this is an example, like currently comparing the two and trying to see the difference between the two.  



Got it. Okay.  



And in terms of this decision making, for example, how far are you guys from actually housing a platform and starting to use it? And how large is the team who will be kind of like building on top of it internally.  



Right. So the POC itself and that assessment has been running for about two months and were scheduled CTO complete within a month. So the POC and the assessment itself was three months. Once it is deployed, it would be supporting three data science team, which would be around four, five, six, let's say seven to eight data sciencemaengineers combined.  



I sep. Okay.  



And then just want to call out. One thing is I'm a product manager, but I also have from an organization structure perspective, I have an engineering manager who will end up making the decision. I'm involved in the criteria that we kind of want. Right. What are the benefits that we want to bring? But he's the decision when it comes to costs and feasibility and kind of implementation complexity.  



Got it. I see.  



One other question is, whenever you onboard a platform like Sage Maker or Q Floor or any of this internally, you need some work. Like somebody in the team has to build on top of that. You can't, like, generally directly expose like Sage Maker and all to a data scientist because it becomes too configured frequently. So is there like a team who is handling that as well?  



So right now we have one data scientist that has a POC project, and then we hand it off to the two ML engineers who are doing that assessment.  



Got it, okay.  



So basically the same team who will be kind of evaluating these platform will ben the ones using the platform? Essentially, that's what it sounds like.  



Yes. To a certain extent, the ML engineers are the one evaluating the data scientists are providing their feedback and opinion. But ultimately, the implementation would be owned by ML engineers and the use of it would be by data scientists and engineers.  



Got it, okay.  



And you mentioned seven to eight data scientists and melee combined. Out of that, how many are me and how many are data scientists?  



Three ML engineers, five data scientists.  



Got it. Okay. I see.  



From your standpoint, like, as a PM, what is it that you're really looking for? There are a lot of engineering considerations. I'd love to hear your thoughts.  



Absolutely. Ultimately for me is to accelerate deployment from POCs to being able to demonstrate that to the customer.  



Right.  



And looking nat you platform, it's pretty fast where you can bring that to the UI. That's essentially my intention.  



Right.  



It's acceleration of experimentation to be able to demonstrate it to the user.  



I see. Okay.  



So do you frequently end up doing this where you build out new models that you have to demo to the user and once you see some positive feedback, that's when you end up doing with the full integration and engineering and deployment around it or you don't have CTO demo too much to external world. What's you typical use case like?  



Yeah, let me think about this. It depends on the project. So that's why I'm thinking out loud because there are projects that are directly to the end user, and there's project that is more internal.  



Right.  



Like a data drop. And then the user is the UI team or the application layer team will take it and they will use it for their purposes. Ideas in IO will be we demonstrate that directly to the customer because then we remove that layer and that risk of, like, having the UI application team pick it up as well and then deploying it and then end up being as useful to the customer.  



I see. Okay. Got it. Okay.  



So I really would like to get feedback from the customer, but right now you're not able to do that because there's like a UI layer in the middle, which ends up taking a lot of time and effort.  



Assume yes, that's correct. The second piece then. That being said is that we have customers who are more data oriented. So as long as you can give them a flat file, they're okay with using that and providing input and feedback. So the UI component is not necessary. It's necessary in certain situations only.  



Okay, I see. Understood.  



So even for the exposing the data to them, what tools do you use? Because even that needs to be hosted somewhere that you would expose that they can play around with. Right there. What do you currently end up using?  



We are just on AWS. So S three and then exposing s three buckets.  



Okay. Got it.  



And currently, in terms of the models, do you already have a bunch of machine learning models in production?  



Yes.  



Like, what kind of models do you typically end up dealing with?  



It's very simple regression models, I would say. Those are the key ones. It's basically given a list of features, what is your self estimate? Like, how many this product has been sold per week or per day? That's the main one we have. We also have certain kind of given this set of input features, what is the keyword, the volume and so on. So those are the two main use cases. I can come up with more, but that's on top of my head.  



Got it.  



Okay.  



Do you envision that with the platform that you guys are onboarding? Would you rather want to serve more machine learning use cases or kind of like the major purpose just to optimize the existing use cases?  



We want CTO serve new use cases. We are challenged where we have a lot of R and D projects that are just sitting there because we're not like the ML Ops team or the ML engineers is over utilized or under resourced. Right. So as you can see, we mentioned five data scientists and three ML engineers. You can appreciate that this is creating deployment and introductions, frictions that make sense.  



Yeah, I understand what you mean.  



Okay.  



And are you guys completely on AWS of the cloud?  



Yes.  



Okay. Got it. No on premise, nothing on prem.  



Okay, got it. Are you integrated with any existing monitoring systems at all? Like, I don't know, like data dog nearer, like something about AWS.  



You know, we use Datta dog. We use MF flow. I believe that's the term. Yes, MF flow. We were looking to Great Expectation as a test unit case. I don't know, that's less of monitoring, but eventually, like, unit test, data dog. That's for the data engineering piece. What else? Yeah, that's pretty much it.  



Got it. Okay.  



When you say that you're looking to create unit test, do you mean like, unit test for machine learning models?  



It means yes. Given this pipeline, the output, is it within a reasonable rate or like it's like there's a certain percentage that is outside the France.  



Right?  



Because, like, usually the unit test is a yes or no kind of a problem statement. Right. Whereas Great Expectation is able to work with not ambiguity, but percentage or kind of right. Distribution. Yeah, sorry. Yes, we're actively using it, actually.  



Oh, nice. Okay. Got it.  



You're going to say no, just asking that. You're basically saying that the team internally ends up using that heavily. Great Expectations.  



We want to use it more. We have started using it, but we want to introduce more of kind of testing around it.  



I see. Okay, understood.  



Okay, that's good to know. And then the other question is, in terms of the libraries frameworks for machine learning, do you know what kind of libraries the team ends up using more frequently?  



I mean, Pandas data framework, usually one we use let me check the documentation. Different people have different use cases, but if you're specific, talking ML heavy, like when it comes to neural network. Yeah, it's definitely TensorFlow. Chris is TensorFlow. I was just going to try. Is there anything else? Yeah, otherwise it's the usual ones, pandas and Sci-fi and so on.  



I see. You got it. Okay.  



And do you know if the other, like, the non machine learning part of your software, is that getting deployed using Kubernetes or how is it typically deployed?  



Do you know?  



Right now we use a program called Cadre I don't believe is the equivalent of Kubernetes. This is where I'm a bit out of reach or out of depth. There.  



IBM familiar with Ketchup.  



But Keto is not equivalent with Kubernetes when it comes to container and stuff like that. That's why you're asking.  



Yeah, sure. It is more like they allow you to manage your pipelines a little bit better.  



Yeah.  



They have a concept of this entire projects and everything where you can set up, like, your training and like yeah. So it is more for your pipeline management. So think of it as a layer on top of airflow for data scientists.  



Right. I don't believe we actually use Docker or Kubernetes, but then that's a good question. I'm trying to find out what alternative we use then given that we don't use those.  



Yeah, sure.  



Sorry, I'm trying to get to my GitHub repo, trying to figure out yeah, technically sufficient offense to end stuff.  



No problem. Cool. Google.  



So this is already helpful. The other thing is, you mentioned that you saw the demo. You liked a few things about the demo itself. What caught your attention? What did you think? That we might be able to add some value on top of the platform that you're already evaluating?  



That's where the challenges? I think for an organization, a simple organization, it's really about reputation and the risk of having to change a different platform in the future. Right. The one thing I really like, based on the example I've seen, is the speed and the velocity. You can do this, right, because you have the basic building blocks, CTO, able to quickly kind of deploy something in a container and then being able to monitor it. That being said, I'm wondering whether the use case is more for data scientists to kind of experiment and then kind of push the POC a bit, an extra layer, or whether ML engineers will also want to use a platform like yours. That's why I'm wondering.  



But the one thing that definitely spoke out was the simplicity and the ease of use and getting into it and being able to deploy something.  



Got it. Okay.  



And by the way, that demo exactly demonstrates basically showcases the ease of use part of the platform. So you got the message from that, right. And you're right. It would appear that it's more for demo use cases as opposed to the more heavy production use cases, basically. So I'll actually explain you. The way we think about our platform is that people who are starting to optimize, companies who are starting to optimize their machine learning development workflows. So imagine that companies who are at a sweet spot of about, let's say one ML engineer, full time, focused on just managing the info, building out the tools. Essentially, that's kind of where our sweet spot is. And at that scaler of the company, not everything in the machine learning operations platform is generally needed as features for the company because they are actually getting started with something.  



They're solving more basic problems at that stage. Generally.  



Yes.  



The way our platform works is that we expose the basic features that companies might need in the beginning and as their needs evolve, the platform is there to skip. And I'll explain why it goes from a data scientist prototyping to a complete production maintenance is because we are completely built on top of Kubernetes, just like so, except we have exposed a lot of simplistic APIs for the data scientist, but at the same time we expose the full power of Kubernetes to our users. Essentially, technically, if true found it did not exist at all for whatever reason, and somebody was using proof on the platform, they can actually continue to use it because we shane a layer on top of Kubernetes and everything. Kubernetes is exposed to the user, if that makes sense. So we actually don't take away any flexibility from the user.  



They can actually do every detailed thing that they want, but in most cases, people don't need all that. So like, the abstraction layers on top of it helps gain the speed and usually what would end up happening, by the way, I'll tell you this from my personal experience, is take a note of companies that we speak with, try to build out layers on top of this, these platforms. Nobody builds it from scratch. So everybody will be building on top of Sage Maker, on top of Kubeflow and stuff like that. But even building out this layer and getting to a maturity, that okay, people are frequently using this platform. Coming up with the right set of abstractions typically ends up taking about twelve months when about three to four engineers are working on the problem.  



And after that, like, there's about a 30% ratio of maintenance. That is like for Qovery, three data science is working. There's like a one full time person who's just maintaining that platform.  



Essentially.  



That's kind of how the platform scales up from our conversations. And our goal is to reduce that number. Basically, that your time to production of the platform itself. You don't need to take like twelve months to do that. You can actually get ready in a week or two, basically.  



Right.  



That's one of the value. And in terms of the maintainability, you don't need like one is to three ratio. You can probably maintain like one is to ten ratio. Right. You will always need soumen internal, we don't believe that you take a platform and that's it solves all your problems, but can you have more people work on the product facing features as opposed to the platform facing things? That's what we are trying to solve for.  



That makes a lot of sense. I think then the follow up on that is which is harder to establish, but like the trust and the reputation around your platform, right? Because what you're saying makes a lot of sense to me. And there is a need within my organization to kind of have that, so that we're not spending that much time just QC and then starting to use it. And our team is also spending a lot of time on Toyota work. Right. You said a ratio of one to ten. We have a ratio of one to three as well, at least on the deploy models.  



Right.  



So it's the same kind of things that we're trying to reduce toy work on. That's a huge value add for a business like ours, or any kind of cost optimization or efficiency. Right. So I think then the question is, if you present your platform to an engineer, how much they would trust it to be able to meet a need. Long time, long term, right?  



Sure.  



So there are two main things that I want to call. I would not be like at the end of the day, decisions like these are internal, like how much people are willing to trust. There are a few things that I want to call out in this context. Like number one, that our backgrounds, we have actually built similar platforms. We have used platforms at Facebook and we have like 15 member engineering team whose fulltime job is this? It's unlikely that somebody internal, like three to four member engineering team will be able to move faster than, let's say 15 member engineering team who come from similar, like ML operating accounts. Right.  



So basically in stuff like this, you kind of bet on the team that you're working with as opposed to the current state you're looking for, how can I get to the future as fast as possible? That's number one. Number two, as an insurance to a new platform, we understand, we appreciate this concern from any company who's trying to build out an internal MLS platform. And as an insurance for that, as I mentioned that everything Kubernetes is exposed. Like tomorrow is Qovery. Literally. No CTO. True foundry. Switching from True Foundry to a Kubeflow is like probably one day of effort.  



Right.  



Because everything is Kubernetes and all the things we do go infrastructure as a code, right. So nothing is like platform specific. Right. That's number two. And number three, the trust that we have received from the community, like for example, founder at Kaggle, the person who created Facebook's internal MLS platform is one of our advisers being sequoiacap. So the trust that we have built out from the community and the support system that we have generally will give confidence to people that, okay, these guys are backed by good enough people that they will have that support system that you need as an early Shane startup to continue to grow. Right. And at that point, internal leaders have to make that decision that do I want to take this? You are taking a chance.  



Any time you work with a startup, I will call out that you are taking a chance. But do I take that little bit of a chance and have a shot at building something ten X better and faster? Or do I want to take the safe route? Like the old saying goes, right? Nobody got fired to take an IBM, right? But nobody got rewarded ten x to take an IBM. Right? So that's a bet that somebody internal, like there has to be internal. Champion would take that bet and yes, you get the ten X reward for the little bit of the chance that you take.  



Yes, that makes a lot of sense. Yeah. That's amazing. We use doctor instead of Kubernetes? I don't believe our team uses Cuban at just yet. So that's something I can investigate. But either way, kubernetes is like the way to go anyway when it comes to deployment in the future. Right, cool. And maybe one question for me then is if I bring your platform to the team, your system is currently marked as early access or better, is there certain concerns or known issues with it that we should be mindful of?  



Yeah.  



So the thing is that we have not exposed the platform to folks because our goal is to only work with six companies altogether for the next one year and our team will entirely work with these six companies. Almost think of this as an extended platform team to these six companies. So the thing is that all the features that we're talking about only get built for the requirements of these six companies and the platform basically we will not make it generally available until then, basically.  



Yeah, that makes a lot of sense.  



The Gmail thing about the beta where we don't want people to sign up on the platform, start using it's more like you have to work with us closely to deploy things, which makes.  



A lot of sense. You are still in the discovery phase as well as kind of.  



The idea here. And obviously our pitch to these six companies is that you essentially get.  



ML.  



Engineers who come from Facebook or Amazon working for your use cases without you needing to exclusively pay for their salary, essentially. And you have a head start for the platform that has been built out like their full time job is doing this thing, essentially. So that's realistically our pitch to the companies and then we figure out the rest.  



Yeah, makes a lot of sense. Cool, well, I look forward to kind of monitoring your progress and seeing how it goes. Anything that I can help you with?  



Yes. So I want to actually understand in terms of your internal teams, what do you think we should do as a next step? What's your recommendation?  



And I have to jump in two minutes, but basically in terms of for your discovery, kind of talking with ML engineers or just getting to understand the problem a bit more, is that what you're looking for?  



Yeah, understanding a little bit more in technical detail. What kind of things are you looking for, what matters to you? And we also want to evaluate maybe you care about features that is three months down the line for us. Then we're like, okay, go ahead with the platforms that you want. Or maybe you're looking for things that we already have ready and we can ship it in like get started in a week. So then a good progress. Right, so that's what I want to figure out by talking to people. So ideally, if you believe personally, by the way, this could be a meaningful collaboration, then it would be great if you can loop in a couple of folks and then we do a call, join call together.  



That makes sense.  



Cool.  



All right, I pitch it to the engineer manager, and we can follow up on that then.  



Got it. So let me follow up on LinkedIn about next steps, then you tal internally.  



I'm happy to let me reach out. I think it will ben. Yeah, I can do that directly, internally.  



Okay, got it.  



So you reach out and then let me know about the next sep. Is.  



That yup, that's right.  



Okay. Got it. Awesome.  



Thank you so much, then.  



Yeah, thank you.  



Take care. "
9902102078,Polymerize,Pranjal Biyani (pranjal.biyani@polymerize.io);Deepanshu Singh (deepanshu.singh@polymerize.io);Vaibhav (vaibhav.mule@polymerize.io),,Polymerize,<50,<10 Mn,<10,AWS,,Software Development,Singapore,Anuraag Gutgutia,Polymerize_Pranjal 23-08-2022,Polymerize_Pranjal 23-08-2022,"23-08-2022

Brief background (if discussed)

""Deepanshu - working with MLOps team (Model Deployment, retraining) , adding more features, improvement of features.

Pranjal - Works on Data Science problems

Solving Client Problems ""

Use Cases for ML - types of models (Is monitoring important etc?)

Model Complexity: Simpler models - collection of models => multiple simple models and averaging them out to generate results. What we have different types of data => we get different versions of Models (each version is trained on a different dataset).

We have multiple level of versioning in models. How many models get generated in 2-3 iterations => got dataset from client that has 10 data points. For each output, have 6-7 internal models.

Current Stack for ML Deployments and pipeline

We have to configure each model - configurations related to each one so that we can train/re-train. We create configs for each structure - these are the models we have trained, these are the S3 location. If there is an optimized way to handle this.

Does training happen parallel? No its sequential. Once we have this, how do we deploy them.

Customer makes API calls to the models. For hosting these models, we create custom APIs? If we have 2 customers, are we going to create 2 different learners for them?

Models are not hosted directly - they are not directly deployed. How many models would you have that are serving inference?

Problems being faced where looking for solutions

We are in an EC2 instance - facing scalability challenges.

If multiple users start training simultaneously, we have issues.

We also provide Optimizations - you want these sort of properties, you can use these sort of things.

Right now, we are upscaling our resources. Looking for microservices etc. If we can separate out the training part separately and if inferences can happen at a separate place. Training is asyncrhonous => we let the user know that the model is trained and you can use them now.

Expected Inference time: 2-3 seconds.

How do you do re-training? Hyper-parameter tuning - when we train the models . When we train, we get the logs => Single place for monitoring all the training logs

Questions asked wrt Product

""How do we handle if simultaneously trigger predictions?

Where are the models stored? It is stored in a model registry. We provide Python APKs.

Right now, it is deployment focussed?

COULD WE UNDERSTAND WITH AN EXAMPLE: Start with a User flow - Linear Regression model trained => What are the different ways in which the pickle file is available to me? You can call our SDK and tell MLFoundry.save . It will automatically store model in SDK bucket. ""
"
9885101374,Productiv,Ashish Agarwal (ashish@productiv.com);Mengsu Chen (mengsuchen@gmail.com),,,50-100,10-50 Mn,<10,AWS,,,USA,Nikunj Bajaj,Productiv_Ashish 09-08-2022,Productiv_Ashish 09-08-2022,"Call 2

- Kill GPUs periodically.

- Sagemaker notebook - train models and save on S3.

- For demo of a model, we spin up a Streamlit. PM consumes this demo to give feedback and evaluate if the model is working. One time username and password.

- Ideally, one developing the model can put to production in a few clicks- current approach is data scientist builds the model and ML engineer rebuilds it for production. Data scientist does not want to do anything besides model builing. ML engineer needs to do both preprocessing and post processing logic.

- Right now, the models are deployed using Elastic Beanstalk. Customisation on Sagemaker was one concern and the other was cost. We already have all infrastructure as code so we can spin up easily an EBS as an instance so we deploy there. For scaling, we scale automatically based on CPU and network. [When Nikunj dug deeper, there was no use case of customisation right now but just prepping for longer term]

- We have a mono repo across JS, python, unit testing, CI/CD etc.

- Getting an endpoint on Sagemaker or something is much better.

- Sagemaker is based on container which means even batch inference is based on API which may not scale.

- Example models- mostly, simple NLP problems. People who have named application differently- zoom meeting might be called zoom for Sales team. We need to map them to a normalised form. Sometimes we also have expense reports of the company so we want to understand if some expense for a particular SaaS application. Third one is we need to automatically extract information from PDF. Eventually we may want to do realtime OCR but right now everything is batch.

- While it is tedious, it is a one time thing. Once you make it templated, it becomes very easy for DS to use. We havent done it so far and it would take 1-2 weeks. The problem with Sagemaker is that it requires us to use a separate infrastructure. Separate infrastructure would want to avoid. We dont have enough people to maintain.

- We choose to use existing AWS services and dont want to use K8s. Even for ML we ideally just want to use APIs.
"
9855605802,Affine,geekyrahul (rpalsaxena@gmail.com),,Affine,500-1000,50-100 Mn,>50,,#N/A,Business Consulting and Services,USA,Anuraag Gutgutia,,,
9855467553,GoodRx,Bing Liang (bing.liang@goodrx.com),,GoodRx,500-1000,500 - 1B,Oct-25,AWS,Yes,Hospitals and Health Care,USA,Anuraag Gutgutia,GoodRx_Bing 30-08-2022,GoodRx_Bing 30-08-2022,"Andrew: Can you show me what it looks like when a job fails and what it looks like when it works

Andrew: Being able to do error handling on our side

Haining: Versioning is very good. Can I quickly see the performance of my model versions quickly. app

Action Item: To send out a video when deployment is a bit more ready."
9855467015,GeekLurn,Preetham (tgpreetham025@gmail.com),,GeekLurn,50-100,<10 Mn,25-50,,No,IT Services and IT Consulting,India,Anuraag Gutgutia,GeekLurn_Preetham 08-08-2022,GeekLurn_Preetham 08-08-2022,"08-08-2022

Use Cases for ML - types of models (Is monitoring important etc?)

""We have built a web applications, haven't deployed yet.

Currently we have 1 model - want to deploy it.

Total of 7 members are there in team.

One of the pickle format models - done for the sensor format of data. Live streaming data => Need a cloud platform and can track it. Currently its batch mode - but will want to be Real-time mode. ""

Questions asked wrt Product

""- ProcFiles and all : Do we need to write our own or will it work automatically?

- Could you show me the CLI demo? Folder structure etc. ""
"
9855466074,ClearFeed,Ankit (ankit@clearfeed.ai),,Clearfeed,<50,,<10,AWS,Yes,"Technology, Information and Internet",India,Anuraag Gutgutia,ClearFeed_Ankit 17-08-2022,ClearFeed_Ankit 17-08-2022,
9855466074,ClearFeed,Ankit (ankit@clearfeed.ai),,Clearfeed,<50,,<10,AWS,Yes,"Technology, Information and Internet",India,Anuraag Gutgutia,ClearFeed_Ankit 08-08-2022,ClearFeed_Ankit 08-08-2022,"Brief background (if discussed)

IIT Patna - One of the founding member at ClearFeed. Extract info from different frameworks and present insights to customers.

All models are NLP Models.

Data Science - only 1 person, Engineering team: 12 members

Use Cases for ML - types of models (Is monitoring important etc?)

Most models are classification, Other is text generation Model.

Current Stack for ML Deployments and pipeline

We have 3-4 models in Production.

Deployment side: We deploy to ECS. Docker image => Push to ECR Registry => Deploy the ECR image into a ECS Task.

Already we are using AWS - we actually took service from another company - they implemented the DevOps side of the framework. Button in the AWS Cloud build. (Key Value systems - for DevOps) .

Model Training is on GCP and deployment on AWS side. We started with GCP and actually Software dev part shfts to the AWS.

All models are real-time. We process messages in Real-time.

Data Pipeline: We use DVC => inHouse datasets we have built. S3 bucket is managed by DVC.

Problems being faced where looking for solutions

Main pain-point is monitoring of the models.

We don't have access to the customer data and we don't know what is going on the customer side.

Other than storing the data, what all things do we need on top of data? Text data is unstructured - most monitoring tool is for Tabular data. What are the things you will need to make sense of the data? Model output probabilities across the distribution, Compare distribution across ground truth. Want to be able to detect if something wrong is going on.

You want to be able to monitor at the label as to what is predicted.

Currently, we are using DagsHub to manage the experiments and they connect to the GitHub. They monitor the training of the model.

Questions asked wrt Product

* DEPLOYMENT OF COMPLEX ML MODELS: While its actually very simple function to deploy. If we have a complex Deep Learning model and it has a weights file associated., how will it work?

* In this Product, do you also have experimentation side of the product? We can showcase you the demo for the same as well.

Would this work if we manage the models through MLFlow registry?

* We have test, staging and Production environment. How do we manage Dev and Production Services separately ?

* Does the product contain user level access control so that one one can touch the other Parts?

* Could we connect TrueFoundry with the GitHub Repo? Most of our code is on GitHub repo. We are working on a feature where you give us a repo link.

* Does this also support the monitoring part by itself?

* Our data is sensitive - how will it work for us then?

* How do we manage Keys in your system ? We have a way to store secrets!

Feedback wrt Product - what would make them possibly adopt it

Deployment framework is really good.

Is it possible to try it out? There is a lot of complication when we deploy at our end. In the way we deploy, there is a lot of steps.

We can manage multiple models like v1,v2, v3 etc. In current deployment side, if you want to create another environment, it is very difficult.

* Could we copy the workspaces? Like create a clone of the services to different workSpaces. We can map the WorkSpace to different Environments. Other than environment level, you can also divide at team level.

* How do we manage Code? Is their any system for reviewing the code etc

* One more problem is there: How do we store/log the model as a python function so we can directly call the model as needed. We allow you to store it as a serialisation function.

* MLFlow supports logging the model. Does TF support logging the model/artifacts etc?

Concrete Next Steps

* The person came in on lookout for monitoring.

* When we showcased, got interested in deployment as well. Wanted to see if we can directly connect with the GitHub repos to then deploy.

* Wanted set-up for Test, Staging and Production Environment.

* Deployment: They want to use deploy by using the Source Code => that's how they do it currently.

HIGH INTENT USER"
9837232688,Simpl,Sheekha Verma (sheekhaverma@gmail.com),,,500-1000,,<10,,,Consumer Services,India,Nikunj Bajaj,,Simpl_Sheekha 20-09-2022,
9764105609,Etsy,Mert Sanver (msanver@etsy.com),,Etsy,>1000,<10 Mn,>50,Multi Cloud,Yes,"Technology, Information and Internet",USA,Nikunj Bajaj,,,
9672381074,Tiger Analytics,Erima Goyal (erima.goyal@tigeranalytics.com),,Tiger Analytics,>1000,10-50 Mn,>50,AWS,Yes,Business Consulting and Services,USA,Nikunj Bajaj,Tiger Analytics_Erima 29-07-2022,,"Doing more engineering less in DS, might change in 3 months

Share with team and see if we can experiment

Immediate no problems that she can suggest where she can use us

Example of Data engineering - End goal - Build a customer 360 for a customer, goal to know customer and figure aspects to it. Touch points of customers

Stages of problem:

Stage 1: Structured Data in warehouse biggest problem - whether to have typical database or graph database

Stage 2: How are they connected to each other

Stage 3: Deriving customer kinds and analyse them

Stage 1 is the most difficult and Erima is trying to solve that

Asked about Difference from postman - end points

Postman visualisation toll, its a tableau, we help you with storage and tableau

Erima suggested she can actively reach out within tiger, and see which system can bring a use case here

Enquired about No. Of customers - relatively early, design partners

Similar setup with Tiger Analytics - as a design partner

Erima will keep in mind and reach out to people within Tiger

Business will come from domains and consulting folks - Not DS and ML Engineering teams

What can we send across to Erima?

A 60-90 sec video of what we can do that can be circulated within Tiger

Please add if I missed something

Erica's reference from KGP - https://www.linkedin.com/in/sheekha-verma-28365615/"
9662135838,Lending Katalyst,Surya Prakash (surya.prakash@lendingkatalyst.com),,Lendingkatalyst,<50,,<10,AWS,Yes,Software Development,India,Nikunj Bajaj,,,
9662317658,ActionIQ,Nitay Joffe (nitayj@gmail.com),,ActionIQ,100-500,10-50 Mn,<10,AWS,Yes,IT Services and IT Consulting,USA,Anuraag Gutgutia,,,
9662272573,Holcim,Kashif Saiyed (kashif.saiyed@holcim.com),,Holcim,>1000,10-50 Mn,Oct-25,,No,Wholesale Building Materials,Switzerland,Anuraag Gutgutia,Holcim_Kashif 28-07-2022,Holcim_Kashif 28-07-2022,"28-07-2022

Brief background (if discussed)

""MLOps at Holcim. I was at Unilever - DS there. Moved more towards MLOps. I have been working in this initiative - Plans of tomorrow. Industry 4.0 vision => large line of products. TrendSetters in Industry.

Sold off the Indian arm as well as Russia arm.

Current Stack for ML Deployments and pipeline

""Some on AWS as well as some on GCP.

Rest Dockerized and not on Cloud.

Using BigQuery etc . Also have a global data center - we push data output to GCS/BigQuery for pushing the predictions etc.

Model is in the edge. Compute is in the edge environment but batch output gets pushed to a cloud environment.

For one product, for training: using AWS. Rest, DataRobot and scripts.

Flows of deployments -

* SAGEMAKER: SageMaker pipeline => it has SageMaker experiments.

* Edge Deployment: Test environment and then Edge deployment.

Only problem with Edge deployments - experimentation tracking is a problem.

How is the Edge deployment done? Sensor data collected.

Problems being faced where looking for solutions

Deployments being edge oriented.

How we can use the plant's infra in their virtual infra. We cannot move completely to the cloud as others are able to do.

* DataRobot - being used .Sagemaker - for model monitoring

Have had conversations with Weights and Biases, Neptune, Truera. We have been in conversations with lot of vendors but haven't found a solution that fits our needs.

* Monitoring is one key aspect where they have explored solutions. Experimentation and tracking is version 2 => that's second priority.

* Reason for Weights and biases not getting up: Not being able to be flexible. We have variety of different infrastructures. Not very PLug and Play. Good for Citizen DS. Not ready for an enterprise application.

Monitoring: To be honest, still in talks with vendors.

Questions asked wrt Product

""* Are their enterprises using the Product?

* How do you package the Product? => It depends on the number of Users.

* What's the experience been with AutoML Solutions like DataRobot. How is it different from MLFlow? => What will be the benefit over that?

* Can I run some tests using the library? We will send you a demo account where we have logged a few things.

* Could I integrate the deployment piece with my own cloud provider? We are using Kubernetes, so everything is dynamic. We are using Google Artifact registry - could you integrate with that? ""

Until now, we had public endpoints - Enterprise customers care about data privacy. ""

Feedback wrt Product - what would make them possibly adopt it

Besides the deployment, what would be the benefit for me to not use MLFlow over this?

Lot of benefits we provide on top of MLFlow: 1) MLFlow doesn't have multitenant system (RBAC first class), 2) Dataset logging, Image logging is not in MLFlow, 3) One Click Deployment

Concrete Next Steps

Will like to try it out.

If some sort of documentation, will like to give it a go => Compare it with MLFlow and see what benefits are over that. I like some of the features + enhancement to what I have seen in most of the tools.

Its a good start. Will be using the Dummy data.

Mid/End August: Connect and discuss the experience.
"
9662134384,Xebia,Ananda Roy (roy.ananda@gmail.com),,Xebia Nederland B.V.,>1000,500 - 1B,<10,AWS,Yes,IT Services and IT Consulting,Canada,Anuraag Gutgutia,Xebia_Ananda 27-07-2022,Xebia_Ananda 27-07-2022,"Brief background (if discussed)

Growing organically and inorganically - Sizeable presence in Middle East.

Personal Employee of the company. We offer Consulting and professional services - Financials and Telecom. We do a traction in other services as well.

Working majorly as a ML Engineer - we have good presence in Data Engineering.

Use Cases for ML - types of models (Is monitoring important etc?)

Present Use Cases that the team is working on - building solutions that are similar to Google lens. When open, access the objects => Similar kind of product for one of our clients.

B2C Product: For that, we are trying to get the data and building a solution that can detect household objects. Object Detection Part.

Building ML is a pain point - Lack of annotated data is the biggest Problem. We tried to use data available in Open Source. Accept somewhat okay model - we have to deploy this to mobile. We have to deploy it to applications that are built using Flutter. Considering how do we automate that.

Current Stack for ML Deployments and pipeline

We are using AWS in the current project - that is the platform where we are building our solutions. Model deployed in the Mobile application - using Flutter application, deploying it. We are not exposing model as a web srvice.

Deploying the model as a service will come much later. On the edge model - very light model. We can't have a big model here.

We are using PySpark and Kafka. A lot of data is tabular.

We are using MLFlow for Experiment Tracking. For Orchestration purpose, we are considering to use KubeFlow. We deployed it in AWS. We are getting different tracking information.

Right now, we are training on AWS GPU. That process, we want to also go to KubeFlow. We are going to make it as Cloud Agnostic as possible.

Problems being faced where looking for solutions

If we get a lightweight system that can help us orchestrate, that will be great.

We are using Kubernetes, but not for ML WorkFlow. Using it for Backend Meta Services. The way - FastAPI, Docker etc and deploying using Kubernetes.

The reason for not using Kubernetes? This is still in progress. We considered running in different CPU/GPU instances. We might have to use KubeFlow for that.

We are planning to Deploy the Model: How do we get all the information and how do we re-train the model. We are using KubeFlow but not sure where it will be useful.

Questions asked wrt Product

* What are the advantages of using your Platform over MLFlow as Anuraag said?

* Is there anything we are providing for automating labelling?

* Someone can use the service for free?

* Are you using platform as well? Say entire service.

Feedback wrt Product - what would make them possibly adopt it

Wanted to recommend it to his management and team. We can get members from the team and give them a demo.

Where do you see utility? Is it possible to provide only specific Piecemeal support. We can provide individual parts and not want the infra.

Feedback on the deployment System: I wouldn't be able to take the decision from my company's perspective.

Should we connect in 2 weeks ? I may have to push it by sometime.

Concrete Next Steps

Interested in trying out the Product - we would want to use it out of your company account. More interested in Experiment Tracking part.

Send out the account details - do Onboarding and set-up a follow-up call."
9595274299,Instawork,debarshi Kar (debarshi@instawork.com);Angela Han Han (ahan@instawork.com);Eric Hagman (ehagman@instawork.com),,Instawork,500-1000,10-50 Mn,<10,AWS,Yes,"Technology, Information and Internet",USA,Anuraag Gutgutia,,,
9595273362,Wakefit,Puneet Tripathi (puneet.tripathim@gmail.com),,Wakefit,>1000,100 - 500 Mn,<10,AWS,No,Furniture and Home Furnishings Manufacturing,"Bengaluru, Karnataka",Anuraag Gutgutia,Wakefit_Puneet 25-07-2022,Wakefit_Puneet 25-07-2022,"Brief background (if discussed)

Based out of Bangalore - next time can meet in person in office. Started going to office. Only shut down when there was peak. Lot of business depends on operations. // Data and analytics background - primarily worked in retail for 10 years or so. I have been working on retailer side or CPG side or manufacturers etc.

Heading DS for 2 years - 5 year old company.

Significant share of market in furnitures and home decor. Working to establish ourselves as a complete home solution.

Use Cases for ML - types of models (Is monitoring important etc?)

Tech as a vertical is not very old - before that, the scale was pretty low and hence not worried about how to process the orders. Data as vertical is only 2 years old. Journey - adhoc requests to a lot of collusion of requests and blending into reports - tableau for the same.

TYPE OF MODELS:

Forecasting and estimation models - used for capacity planning, strength planning, raw material planning. Daily models for Last mile logistic planning - how many days do we need for handling? How many inventory days are there? Daily fulfillmnet models are also there. Minimum inventory needs are also there with 90% adherence.

Intent Model: Working with Product team to deploy - possibility of a customer to conversion.

Product affinity: Probability of conversion from X to Y.

Recommendation Model: Product ranking models, recommendation models.

Impact analytics for models that have gone live => models for this. A/B Testing is not posisble in many cases.

Current Stack for ML Deployments and pipeline

Replicas created for Production - we started looking at data warehouse, resides on - RedShift. Not using kubernetes.

The Production: OLTP - Legacy (more of a solution etc) ==> RDS (AWS primarily)

Every order that comes from marketplace comes from RDS. Dashboards - tableau.

We have onboarded another tool called DataChannel to pull data from various APIs => allows us to download data into our system and give us a 360 degree view.

Library - supports Data Engg and dev needs.

Model building came 1 year back.

Do you run models in a run-time mode or Live ? Process is there - Sales and Ops planning. That is monthly exercise. Everyone meets and agrees on a demand. Every month we project for next 3 months. Everything is stored in database.

Personalization is restricted to the CRM - minor tweaks here and there. Search analytics is part of development.

(MOST Models are offline - but pipeline is ready and it is deployable as an API etc)

What is in pipeline is Product based recommendation.

Dedicated EC2s and managed services like DynamoDB.

Problems being faced where looking for solutions

* Roadmap I have: Driven based on needs of the organization => a lot is happening on pipeline side. A lot is happening on CX.

* Other problem we face is optimization of budgets. Recently on-boarded a person as brand ambassador.

* Forecast models were not working well - all our parameters - MAE etc was too high. We used Prophet and that was giving a high RMSE. WakeFit FB Prophet.

Members in DS Team: 12 members in the team. About 4-5 months back, make it 20 members. After 6 months, all of them are data engineers.

Questions asked wrt Product

* As of now, we don't have support for Offline. Deploying as a Cron job and deploying as a job - that feature is coming in 2 weeks.

Feedback wrt Product - what would make them possibly adopt it

All the use cases I asked is asked because team has built a lot of APIs and all of them can be deployed as a rest service, even if its not a model."
9591062320,iSchoolConnect,Himanshu Maurya (hmaurya@ischoolconnect.com),,iSchoolConnect,100-500,10-50 Mn,Oct-25,GCP,No,Higher Education,USA,Anuraag Gutgutia,iSchoolConnect_Himanshu 27-07-2022,iSchoolConnect_Himanshu 27-07-2022,"Use Cases for ML - types of models (Is monitoring important etc?)

215 People team in iSchoolConnect. WebDev - 25 members, ML Engineers - 15 Engineers. + Huge data annotation team.

In 2020, very few companies working in inference side of things:

1) Candidate Recommendation Engine: 2 sides - * When students come, similar to what Yocket has. Where you are likely to get admitted.

Currently, 5 verticals that work on ML.

2) Search:

3) VIA: Video interviewing analyser => Better speaker in interviews etc.

4) DWM: I lead currently. Document writing mentor. For admission purposes, students hvae to write essays.

5) Proctering Engine: Eg - GRE etc. When Covid hit, all the businesses went online - ETS. 30 Mn students apply every year. The entire load came to us. Built the systems, scale them. 10Mn load. 3RD Party to Procter U. All the load that came - backend went to us.

6) Analytics: How products are doing in market etc

Current Stack for ML Deployments and pipelineStarted in 2017 and been here since the beginning of iSchoolConnect.

Most of the things are in GCP => We are official Google partners. We don't have internal platforms.

We used to use VMs earlier, now making move to VertexAI.

1) For Model training, we use VMs. We don't have to train models so frequently.

2) For serving, depending on use case => * Recommendation engine => API Servers and scale horizontally. * Procter Engine =>We used to work on 6 underlying models => Orchestrated it through Docker Compose => 32GB of GPUs. Lot of data movements

3) For monitoring, Data Drift and others => EvidentlyAI => recently tried to work with them => has a huge potential for us. APIs also allows to dump output in JSON format. Build separate dashboards for every purpose

4) Model Cards and training: Used MLFlow but even though we went into establishing practices => not a lot of models get screened. DONT REQUIRE TRACKING MODEL VERSIONS much. Use a combination of DVC and then use GCP.

5) Internal demoing part: Built on StreamLit. LabelStudio - did POC with them. Spent 1.5 months with them => won't waste my time. Started with streamlit and built the entire platform.

Problems being faced where looking for solutions

Monitoring was a big need - started using Evidently.

Before Evidently, was building own using PyChart etc. No point of building anything from scratch. It took a lot of engineering efforts otherwise- UI was also ready. There are places where it is not able to track Multi-variate (Milti-label or multi-classes) distributions, only binary distributions. (OPEN SOURCE PROJECT => If using for commercial purpose, you need to open source)

Questions asked wrt Product

Have had discussions with other start-ups on trying to use the platform.

There was this issue of Data privacy => What are the options that will be available? Just like we use GCP, we can spin up instances and we can use it as a cloud. While inferencing, code is also pushed and deployed in one of the pods in Kubernetes.

What are the options to keep the data in particular region? Eg: We want to keep the data in USA and Europe. Can we keep data in region? 2ND and 3RD OPTION work best -- We will want our GCP. You take the infra and set-up whatever you want to set-up. Updates can be pushed.

Feedback wrt Product - what would make them possibly adopt it

Honestly, all the 3 components you are integrating - it will be very useful for us. At iSchoolConnect, we don't have big DevOps teams. A lot of time gets wasted asking for permissions and all. You have terraform and spin up things, we also use Terraform to automate a lot of internal things.

If we want to spin up an instance just like we do in SageMaker, I will spin up and works. Something could integrate all the things we have. My role should be solving problems and ship things faster => Solving problems. Staging, Production, Release.

If someone new comes, with their access keys, we can restrict permissions for resource => No worry about Cost.

Last bit: UI side => it depends on project to project. Different dashboards built and gets integrated at the end. Used streamlit and built everything including annotations etc.
"
9591002216,IN-D.ai,k Narayansamy (k.narayanasamy@in-d.ai),,IN-D - Power of AI,<50,,<10,AZure,No,Software Development,India,Anuraag Gutgutia,IND ai _Arvind 27-09-2022,IND ai _Arvind 27-09-2022,"2nd meeting

Date: 16th September 2022

Use Cases for ML - types of models (Is monitoring important etc?)

""We have clients take out data from unstructured data - documents, invoices. We understand data sources and try to create Models.

Challenges: 1) Figure out which model to choose 2) How do we re-train or orchestrate it?

(We do our cloud, client's cloud and orchestration)

Whatever learning we get from a customer while working with them and not re-discovering.

RAHUL - Customers who are using the code in Production. How do you showcase automated training is happening etc.

We are aware of most recent things. Best Practices in terms of continuous training and deployment happens in UAT.

OUR: Its a truly horizontal capability. We end up targeting BFSI because of the background that Rahul and Abhishek has. ""

Current Stack for ML Deployments and pipeline

""In case a client is okay to use our own environment - samples are hidden. Everything has to be done in their environment (quite often). We do entire dev of the model in their environment. Except for the code, everything remains there.

How is the Deployment Orchestration? Jyoti can elaborate better => Entire solution we deploy using Docker. We develop the Docker containers. We don't have much expertise on Kubernetes clusters. Mostly limited on model side. Mostly deployment is via docker, either on client environment. CURRENTLY - Stack for Model building: Notebooks, ETL Pipeline etc? => We have dedicated notebooks on the dedicated environment and for running the traning. Our prediction environment is nothing but our Products. Evaluated models are deployed and integrated with the workflow.

Flask API is the major way of deployment. ""

Problems being faced where looking for solutions

""We have moved to Docker based environment. We can support all te environments.

Do you use any tool? We have automations in our existing flow, but not 100 %. Annotations, Model training and deployment etc => not automation.

Base models are there: How the CI/CD could be integrated using MLOps. Do you use GitHub?

Training code is also pushed to GitHub.

Model Registry: Using GCP as a model registry.

It is a SaaS product - so there is a GUI. Some cases, could be through that and some through API. ""

Questions asked wrt Product

""Offline and Online Training: Offline is what is happening in a batch. Online is close to real-time use cases.

Best cases where Online can happen.

Is the platform support both offline and online ?

How do we see if the model is performing as per testing.

How is the testing managed?"
9559498413,Polynomial.AI,PRAMOD BHARADWAJ.N (pbharadwaj777@gmail.com),,Polynomial.AI,<50,,<10,GCP,No,IT Services and IT Consulting,Singapore,Anuraag Gutgutia,Polynomial_Pramod 15-07-2022,,"15-07-2022

Current Stack for ML Deployments and pipeline

""1) We are more of a MSFT shop => GitHub, we are on Azure, etc. We have GitHub actions and things coming in, Deployment has become easy. Until and unless we run out of GithubActions free quota.

2) We are working on CLoud Native architectures - Kubernetes, Docker ==> removing dependencies ==> Latest tech direction.

3) We need more people to know about Deployments - coders who haven't been trained on deployment. How easy is it for developers to come and start using the Product.""

Problems being faced where looking for solutions

""""""Wants to move to Cloud Native way of deployment - cross-cloud strategy is very common these days => We used to be on app services. Connect GitHub to apps service => costs significantly went up so we had to take up another strategy (PRE-SALES etc) ==> Customers are on different cloud. We allow you to go cloud native!

(1 effort for deployment of a containers - 3 to 4 days) ==> It could go up if people don't know.""""""

Questions asked wrt Product

""DevOps is becoming an integral part of every service provider. How are we adding value in terms of differentiation from Azure?

Most platforms will let you do one click deployment and will allow things like Splitting between Models, A/B Testing, etc. We are trying to make that experience as fast as possible.

We support 14 programming languages and combinations.""

Concrete Next Steps

""Three possible modes of Collaboration in terms of using our platform for deployment for their clients.

Pricing needs to be worked out better"
9560226332,Greenhouse Software,Michael Boufford (mboufford@greenhouse.io),,Greenhouse Software,500-1000,10-50 Mn,<10,AWS,No,Software Development,USA,Nikunj Bajaj,,,
9559782444,Emaar,Rajesh Vepakomma (rvepakomma@emaar.ae),,Emaar,>1000,,<10,AZure,No,Real Estate,UAE,Anuraag Gutgutia,Emaar_Rajesh 21-07-2022,,"Brief background (if discussed)

We went on to introduce ourselves. IIT Madras - 2009 (13 years) ==> then in Airbus, started DS in AirBus. Then Emirates for 6 years. Emaar - owners of Burj khalifa. Lead the entire DS agenda (10 colleagues) ==> 1.5 years

Bossed kept changing in Emirates: Not a stable team.

Use Cases for ML - types of models (Is monitoring important etc?)

We were attempting a start-up sometime - 4 business lines (Property, Malls, Hospitality, Entertianment) . Biggest chunk is with properties and retail, which is malls. Record sales in properties ===> How do you price the properties?

Property sales team wants to know how the sales is going to look like? Property sales forecast.

When want to sell properties - there's a customer acquistion channel. Which agency is likely to increase leads.

How to generate leads - marketing campaigns etc.

Dubai Mall has a marketing platform => how do we bundle offers, how do we personalize offers. How do we forecast footfalls in the malls? Shopkeepers- how do you price the shop rent. Forecasting problems.

Personalizaiton: Which offers is going to be more appealing to the users? Its' not distinctive.

Current Stack for ML Deployments and pipeline

* Tech Stack: Plain DataBricks ==> We have made tech investments ==> Migrated to real cloud solutions. AzureML Studio - migrated.

* Azure notebooks. Deployment also happens through Azure platform.

* To save costs, instead of real time, we run it in batches. We pre-catche what is relevant to customer.

* Do a lot of A/B Tests ourselves . Have some bit of monitoring

* When we choose Azure v/s DataIku, Oracle, AWS are competing heavily ===> Enterprise level architecture ==> there is a bias to choose MSFT.

* Do you use Azure Studio - Drag and drop? Yes. We use the feature store concept on Azure Platform ==> 80% time goes not in model building, but doing things around it. Feature engineering, pre-processing, etc to making models production ready for deployment.

* We use SnowFlake and Azure Synapse too. Data Cleaning and customer 360 degree - Informatica is being used.

Problems being faced where looking for solutions

* I have kind of bootstrapped - run in start-up mode. Person is able to do end to end.

* No dedicated DevOps team. Structure is more aligned so that folks are focussed on delivering 1-2 projects. As of now, this is how it is.

Questions asked wrt Product

* If I had to compare to an open source framework like KubeFlow => it has its own logging and monitoring. It has A/B Testing etc. It took me 3 days to deploy Kubeflow and you will spend time in understanding the system. Developer Experience. We also have model registry and things that integrate tightly with training. (Our Learning curve is very small - User journey is defined to be very simple)

Feedback wrt Product - what would make them possibly adopt it

If I am a cloud resident, will TrueFoundry help me in reducing the cost of my cloud?

For the parts of the platform that Anuraag showed, I will have to lean back.

I will not be able to do away with my cloud costs ==> think it as a question. If somebody is already invested, then how would you get adoption?

* If TrueFoundry can show a great pre-processing part, as to how do I reduce the pre-processing time, I will want to buy.

* Models in Prod. Different versions - how do I do versioning on top of that.

* If you are only giving me monitoring, then my main cloud costs are still there ==> the business case of monitoing then is not very useful.

* If the Product is too close to Open Source and very complicated to the Cloud base, your set of customers will be restricted.

Concrete Next Steps

* Want to understand the pricing of AzureML ==> What will save the cloud costs?

* Set-up call next week and see how we can work together
"
9492822147,Quantum Metric,David Murphy,,Quantum Metric,500-1000,10-50 Mn,<10,GCP,Yes,Software Development,USA,Anuraag Gutgutia,,,
9464831632,Copy.ai,Philip Jama (phil@copy.ai),,Copy.ai,<50,<10 Mn,<10,,No,Information Services,USA,Anuraag Gutgutia,Copy.ai_Philip 07-07-2022,,"07-07-2022

Use Cases for ML - types of models (Is monitoring important etc?)

""Natural Language Generation - blog article generation.

How we use large language models - HFace and OpenAI models. We are also in a lot of their data programs""

Current Stack for ML Deployments and pipeline

""Largely self serve. Team of engineers who do the backend etc.

ETL Pipelines - GCP, Kubernetes Cluster that runs and scheduler. Unconventional.

Goes into the data warehouse that feeds the lot of research stuff.

R&D and ML - A lot of predictive models around user conversion and user churn etc.

We also have proprietary online experimentation engine that we use to split test variance of models. Randomization and allocation of users. We have a handful of metrics that we monitor + secondary and tertiary metrics as well.

We use custom analysis as well.

Problems being faced where looking for solutions

""Experimentation is what is interesting for us ==> That is of core value to orgs.

One of the early employees.

We are better than most other competitors.

Increasing the cadence of the tests on the experimentation side of ML ==> from 1 to 10 to 100 of experiments per day.

Scheduler - built it out with KubeFlow as it supports experimentation

When you get generative models from GPT, you handpick? Does it run offline or the end user has application? ==> No these experiments are all online.

Questions asked wrt Product

""We limit the blast radius if things get wrong. We are in 1 Mn user base range.

KubeFlow does operates in batch. Well designed, scalable. If it needs to be distributed, it has the capacity to do it.

Metric is - is the user happy?

How do you host these models? Backend - OpenAI but support 3rd party vendors for that. We manage a lot of models overall - managed at the source control, etc.

Typically do 2 variants for a model ==> Is it like a control plane, data plane ==> Where the control plane decides which one goes to Model 1 or Model 2 ?? Allocation to expriment groups is done randomly. ""

Feedback wrt Product - what would make them possibly adopt it

""KubeFlow - have you put that up? Or planning to put. I just push code and it fires off.

What is the blocker in going to 100 experiments? What is the blocker? It is just time constraint and in meetings all day.

> Firing the model

> Fine-tune the model

> Deployment : Its not yet integrated into my application. No end point ==> Engineering resources that need to be allocated

> Analysis => Observation

Concrete Next Steps

We need to send him the live recorded demo of the platform as to how it works.
"
9407967410,SquadStack,Pranav Bhargava (pranav.bhargava@squadrun.co);Anit Bhandari (anit.bhandari@squadstack.com),,SquadStack,100-500,<10 Mn,<10,AWS,No,"Technology, Information and Internet",India,Anuraag Gutgutia,SquadStack_Pranav 01-07-2022,SquadStack_Pranav 01-07-2022,"01-07-2022

Brief background (if discussed)

Working for 5 years => Lead the efforts on DS side. Lead efforts on DS Side. How to go about product, features to helping deploy all those pieces.

Use Cases for ML - types of models (Is monitoring important etc?)

""QUESTIONS: * Is it batch inference? You don't host the model as an API => We will do it when we make things realtime.

Current blocker is actioning system itself is not good.

* Old or new architecture? Signal processing or neural Nets? Some are Deep Learning Models. ""

Current Stack for ML Deployments and pipeline

""When we start an account with an idea, whatever use cases => Jupyter Hub. Has security access.

Start with experimentation of data on Jupyter Hub. Audio needs to be brought on the server itself. Files can get corrupted.

Exploration: 1) Experimentation => Start with analytics => DeepDive using ML. Start using simple models and then do state of art (Hyperparameter tuning starts coming into play)

Best Model: Set out to put into Production ==> How we do it? GitHub Pipeline. Use SageMaker to push model into it. Goes into Engieering team. They use spot instances to run batches of pipeline. PostGres SQL DB. Workflow starts with sometihng ==> else we pick up calls from that through sampling algorithms. (5-7 PIPELINES that are live)

Tracking: * Things in AWS Configured * Are the models running fine. Error messages etc.

Actionable part: Data also needs to be presentable. DS Team does the monitoring => use MetaBase for ML Monitoring. Coverage . ""

Problems being faced where looking for solutions

""1) As we scale => we have to keep changing the batch sizes to make it optimal. X Hours have to be maintained as we grow.

2) As number of pipelines increase, ML Engineering effort keeps on increasing.

3) We did explore ML Tools but never went into moving forward => Engg team will take the chalelnge""

Questions asked wrt Product

""Questions:

* Do we have to expose the data to our servers?""
"
9407966873,OncoImmunity,Elena (elena@oncoimmunity.com),,NEC OncoImmunity AS,<50,<10 Mn,<10,,No,Biotechnology Research,Norway,Anuraag Gutgutia,,,
9407683477,XOPA,Kenny Chong (dewei274@gmail.com),,X0PA AI,50-100,10-50 Mn,<10,,No,Software Development,United Kingdom,Anuraag Gutgutia,Xopa_Kenny 06-07-2022,,"June 22,2022

Current Stack: Using TFServe. Want to keep models as simple as possible. TF 1.0 is a bit of a pain. Now, they have added Keras. We are very traditional - mostly excel for tracking. Report all the findings. We just have TFServe. Serve is as a GRPC Object. Still trying to figure out. Earlier, was not using docker. Right now, dockerizing it. We have a simple docker compose file and we run those commands.For Inferencing, use CPUs. For training - we have own set of computers to train models.We don't do tracking on the DS side. When engineering has issue calling our models, they call us. TFServe - GRPC report has been quite sable.

Problems: * When we create a model, too many parameters involved and we do Brute Force manner. One pain-point is what tool to optimize. The way we have data, it is not well. For new data coming in, hard to marry the data. Optmizing the model is problem. We are Microsoft partners and hence it makes sense to use Azure products. Cost and logs is quite easy to see. Applications Insights => quite useful to us. Validation is very tricky. We don't have the global minimum but we have the local minimum.

Questions wrt Product: * Code syntax in MLFoundry seems to be similar to Keras. Do you use Keras under the hood? Sign-up onapp.truefoundry.com and will try on free time

Concerns/Questions: Major concern for us is private cloud. That will be a huge cost for us. Pipeline for creating models and deploying models. * Even when we ingest data, you can use our own storage ==> Control plane or Data plane.

Mid Next month - will tinker and play around it. Have to send him access and documentation. If like, will share the demo with the team."
9407683220,MindsAI,Abhinav Kaushik (abhinav.kaushik@gmail.com);Abhinav Kaushik (abhinav@minds.ai),,minds.ai,<50,<10 Mn,Oct-25,,Yes,Software Development,USA,Anuraag Gutgutia,MindsAI_Abhinav 21-06-2022,,"Call with Abhinav - June 21st

ML Engineer & Lead // Bengaluru: 10 people, 20 worldwide

Mostly a backend engineer till i joined MindsAI. Build a prduct similar to New Relic

Earlier on system side.We have our ow product which is a training platform built on top of Ray.On top of that, building domain specific product for Semiconductor fab scheduling problem.

Taking our platform and customizing it for them. Our product is called DeepSim.

More work was on training side as well as inferenceStack: RayServe => Inference. Not yet anything for Monitoring and deployments.

Kubernetes - Cloud agnostic. - Big part of problem for us is that its windows. Support of all these things on Windows. Most of customers tooling is on Windows.

Support for Ray is not great on Windows. Have to do hacks like keep minimal stack on Windows and use GRPC kind of thing.* Do you handle fractional GPUs. Can see multiple teams using it. For a staging kind of a use case, they may want to share GPUs within the team.

If comparing to our platform, it is nor productised and its not API first. Use case is for internal usage. We don't offer the platform as an offering on its own.

It looks really cool. Can be used in places that are ML Shops.* I may not be the right audience as I don't work in ML Company that is on that kind of workloads.

CAN check with Arun for platform efforts ==> Will have to get sign-off from Bus Dev."
9407683116,Enquire,Arnaud Jaspart (arnaud.jaspart@enquire.ai),,Enquire,<50,<10 Mn,<10,AWS,No,Information Services,USA,Anuraag Gutgutia,,https://app.fireflies.ai/view/Enquire-TrueFoundry-Follow-up::gzPOcfyPoD,"Yeah.  



Good. I'll just turn on the recording for this call. Is Kaan also going to join?  



I hope so.  



Okay. I wish I can deal with how's it been going. Did you all get a chance you maybe try out Kaan? Kaan?  



No.  



Okay. Who will give 1 minute for Kant to join as well?  



Hello?  



Or not? Hello.  



Hi.  



We can get started. Maybe this is just a follow up from the last call. I think we had few things that were missing in terms of the discussion. One was with regards to security that you wanted clarification on. The other thing was with respect to the pricing that we also wanted to quickly touch was the appetite. I think that you mentioned like we Karen already created. So we can quickly showcase that if that is needed. But before going into that, I wanted to kind of check I think Kaan accepted and Kaan was also flying out. He faced an issue with respect to using the Develop versus the App Store countries. So would be great if you can tell if the issue is resolved or.  



Yeah, so I think it's result. So I'm following the commands on the GitHub page called the Enquire Park. So I was able to login but I'm kind of confused in that this GitHub repo is not exactly the same as the documentation. At the documentation, we're deploying something called a run. And I think what it does is it first trains the model, right? And then it saves the model. Am I right?  



Yeah. So basically in the quickstart page you will see that okay, yes. It trains the model and the job is basically deployed as a job. And after that it basically deploys the model, right? So in this particular case, we are not training any model. We already have the model from the hugging fish model repository and we are just deploying the model in this case.  



So do you suggest that I follow any other documentation or should I follow the GitHub directory that you shared with us? What do you think is the best way?  



Yeah, I think you can follow the GitHub repository that we have shared. I think that will be the best way here.  



Okay. All right, thanks.  



It will be great if you can try that out. And if there are any issues, you can let us know. I think the documentation part, because we also have a module on the training side, the documentation part encompasses that. But for your specific use case, we try to create everything in the GitHub because at the same time we will add specific parts for documentation so that you don't if you are even following the documentation.  



So if you just want to kind of try out how to deploy a service. So this is the documentation link that I have shared here in the Google chat. So there are multiple like some examples that we have already put up and you can also follow this.  



Okay, so first I try to follow the GitHub repo and just deploy a hugging pace model. Sure. And then maybe come back to this and deploy a service.  



Yes, definitely makes sense.  



Okay. And what about models that we train locally but we want to a model we saved as a PT file and then deploying that to the True Foundry platform?  



I can answer that. So you don't upload the model to S Three or anyone else, right. The model is just there in your local system?  



Yes.  



Okay, so in that case, if it is in the local system, you can directly load it from the local path because when we are building the image, we will be copying the whole local directory. As long as it's not excluded by Tech Node, we should be able to pick up the model file also. So that's one easy way to go and deploy it. Another way would be saving the model with us. And that's basically we provide a storage system for model also. You can use that and later you can basically refer it using Sqn and download the model in your service. But I think you can try out the first one, which is essentially having the model in a local file system, ensuring that it's not getting ignored by Git.  



And then you can just load the model from your local system and do SFI deploy.  



Okay. All right, thanks.  



Yeah, go ahead, Kaan.  



Yeah, I'm just going to say I'm having some issues with the ML Truefoundry module. I'm using a Mac M One and I think I don't know, I saw that on your documentation that there might be some issues with that. But the areas I'm getting are not the same as the ones you documented.  



Yes. The big problem with M One max here is that it can be different for different people based on what you already have installed. So that I think if you can share the trace backs with us as to what you are getting on your system in the Slack channel, one of us will be able to help you out.  



Okay, that'd be great. All right.  



Yeah. If you want, we can also set up a quick 30 minutes session and get all of these things done. That will be much faster for you also.  



Yeah, that'd be great if you can.  



At least share the feedback with us beforehand because we need to do a bit of study. Otherwise we'll be just spending time on the call and we'll do a bit of study beforehand and then we can get on a call, basically. But if you just want to kind of try out deploying a model, you do not have any dependency on ML Truefoundry. The repository I have shared also do not have any dependency on ML Foundry. They just download the model from huggingpace and deploys it. So in that case, you will not need to install ML truefoundry at all. But let's say if you are following the documentation that were following, which is training our model and then deploying it, in that case you need evil foundering, then we can kind of help you with installation.  



Okay.  



Then I say I'll try to just as the first step I said let me just try to deploy the first model from hugging face.  



Make sense?  



Follow that link that you shared on the chat here and then wayve train and deploy site. I'll get your assistance and the model site.  



Makes sense?  



Yes.  



I mean, we can set up a quick 30 minutes meeting whenever you are free. Maybe today, later or tomorrow is a holiday here. And get this going. It will not take more than 30 minutes to get everything running.  



Okay. All right, yeah, sure, that works for me.  



Okay.  



Did you have the trace ready to be sent? Can you send it on slack trace? The air logs?  



Yeah, so I'm getting completely different air logs from two different versions. Like the amount truefoundry. As you said, if it's not a priority, maybe I'll just focus on the deployment side hugging Face model and then when I'm ready, maybe we could work on that. If you're okay with it.  



Yeah, it's just like if you have some logs that you have on your screen, you can just dump it and after focus on the playing tomorrow so Deborah has some time to.  



Take his.  



Code and fix it.  



And we should anyways try to set up one call whenever possible, maybe on Thursday so that we can ensure that you are well on Boarded in terms of being able to use. We understand first time, even for us, as different users are using, we also learned and it helps us improve at the same time we can get you on boarded on the call. So that is something we have been doing with a few other folks.  



Thursday works for me. If you want like for a 30 minutes call.  



Does this time work?  



Yes, it works.  



Okay, perfect. I'll send out and calendar invite you.  



Okay, thank you.  



And I think we can hold on. You can probably get one to one chat, the faster the better. Like little part is on the pricing and security. Security seems fine for these models and for the pricing. I mean, it's close to the AWS now. It will still duplicate because we already have that machine for us, for the material and so we'll be paying the double. Like no offense, it's just a little fact. So anyway, that will need to be approved. I think we can reduce the size so to not impact your cost. Because during that time you pay. So you can resource and reduce the size of the machine. And even if the answer takes like I don't know, like 1 minute, it's okay, we can still work with that. It's not a production machine.  



So let's make it very cheaper test machine in the meantime, if it's okay.  



So, just trying to understand when you say that you will have to pay the bill, the cost like one day to understand is it because this will be a test environment, you'll be running kind of feed.  



So it's because we have like all models are running on the server currently and we have two environments, three, but one is for training on Saturday morning. So let's not talk about that. It's out of scope. But we have two matching, one for the two servers that are running, one for the staging and one for production. So these two matching are running and we're paying for them. There are a few models that are on these two machines. So we talked about like the summarizer and the takeaways. These models were going to take them out. So we are planning to take them out and put it on. Truefoundry, if everybody is happy, but that will still increase the cost because we'll still keep the machines the way they are.  



So now in the strategy is to put two fundraising behind, call it and so on from the API and we'll need to develop that part. Then we will have to set up the, how do you call it, like the gateway will need to set that up and put it on. So anyway, that being said, we have a few big projects that we are working on and that will take two months and then we'll have some mental and financial breathing room then to be able to actually industrialize that solution.  



So.  



To propose we can scop it in time is to let's have can and the solution working. Then you can switch off the machine. So once we are happy about the solution, we give you the feedback. So we are both happy about the solution. Then we can switch off the machine for one month and a whole two months. Then we'll go forward and I'll ask.  



For the.  



Extra cost and we can actually implement the solution.  



Do you have any thoughts on how we can make it easier for a note in terms of the cost side given like.  



We can discuss regarding the pricing or not, if pricing is the concern, like there's room, like we can initial customers. We Karen anyways offering some discounts. That is the thing even for them. We can give out like a smaller machine that will allow you to test fast. It won't be like at high level. I guess you are using it only for testing now, right?  



Correct. Yeah, like two or three months away from production solution.  



Yeah, for testing. I think we can definitely reduce the price. I'm not sure what's the final quote like, but we can definitely work out the price or not, if that is a concern.  



I thought we can be like an active solution, like an active client on your database, but have the machine shut up for a month or two until we have the breathing room. I just wanted to have the proper concept, the fact that it works and how it works and we build our knowledge. Once we do that, then we need to focus on finishing the project we're doing right now and have the mental space and the breathing room to actually talk about that with my boss. Should make sense.  



Got it. Cool. So I know we can definitely do that. We can still provide you the access kind of we can mention it like a POC and we can think of some conversion date so that it works for you. Because what we wayve seen is sometimes like this, I don't know the process you follow, like in your.org like if there is a big process in terms of getting an agreement or is it like a simple thing like based on that, we can at least initiate that discussion. But more than that, the more important thing is we definitely want you to be using the platform.  



So it will be great if Kaan and Omar and other team members are actually first happy with it and then this discussion we can definitely have after that in terms of what's the best way forward for both of us so that we help each other at the same time, so that constraints are being met on both sides.  



Yeah. I'm just conscious of cost for you, for ups time for you and for us and the quality for you. So that's what the partnership is about. Yeah. It's just a matter of like, let's build the knowledge, let's give the feedback and once we have that down and we have happiness, then we need the mental space because right now it's not the case. Can we switch off the environment, like from our user account? Like, can we stop the machine?  



Sorry.  



That is fine. We'll do it. And then maybe we are testing with Kaan on Thursday and then we can show the demo. Yeah, we can also do something along those lines or not. Like if you're only planning to use the thing that is currently you Karen using, we can also give like one month of free usage and then from the next month you pay something like that we also do for some customers. So the first month is free so can't continue testing. Like we can give a certain size of machine that you kind of get free credits for the first month and from second month you can start.  



Yeah. And then when we resource the machine but let's make sure that we start the machine for the period where we're not using it.  



Okay.  



Can we do that from the dashboard? Can we switch off an environment?  



It will automatically only incur cost when you're using it. I mean, it will shut it down in the dev environment, don't worry.  



Okay, so it's automatically shutting down when it's not used. Yes.  



The dev environment is automatically shutting down in the production environment. Usually people don't prefer shutting down machines in production environment. So that's why we don't do worry about the cost.  



Yeah. As long as you are not running anything in that environment, you are not incurring any cost.  



Okay, that's good news. All right, cool. Because it's a dev environment, if it was prediction, it will still be running.  



You'll not delete it.  



Yeah. Okay, cool. That sounds good. Any question, command concern?  



I think the only thing about rapid API, if you want to kind of quickly see that is that something that is fine, like you'll be able to do it. You want us to just focus that part?  



I don't know if you tried, but it would be great to have a documentation and probably like on your website, like a little image. You can plug payment API gateway to our environment.  



Can you demonstrate that?  



Yeah.  



So basically we hosted it on our API, on our servers and integrate with rapid API. Anybody who wants to call your API snippets in every language and they can call this and it will work. This is how people can actually use their API. Then there is a test end point.  



So they should give the result wayve.  



You can just walk. I think our Nord and team over the workflow for this. Just so that you know. I'm not sure if they have used this themselves, but it will be helpful to just part is fairly easy, but we'll just show you. I think this has been something in your mind for quite some.  



Idea. Three years old, but one day we'll do it.  



This is the URL that we have of the deployed API model that we deployed for enquire. And all we do in rapid API is you basically go to API. You can add a payment method. I didn't add a payment method. So you can add a payment method if you want. You can see the analytics of how many times the API is being called, from where it was called, what was the latency. So if you see the current latency that we're incurring is around 6 seconds on this model running on CPU as of now, configuration is something you can.  



Find your app name and yeah, I've been on it.  



And the securities where you can configure the API key using which clients will be able to call your model.  



Yeah. And the question is on your side on transform reside we have necessarily an API key that we can put can we put multiple API keys for one model?  



Yes, you can put multiple API keys for one model.  



Okay, so one for our back end, one for rapid API and everybody is happy. Okay, yeah, that will be cool. And that will be cool. So after you have to scaling so if the CPU on the Ram is getting out of hand, usually like the CPU or the GPU, you Karen, on production. Yes.  



It will automatically scale.  



Okay. All right. The fact that you did it is fantastic. If you want to write at some point when you have the time documentation just to show how to set it up in in D and Truefoundry on, that would be fantastic. And I think it will be like a good marketing material to share on LinkedIn and stuff.  



Yeah, definitely. We plan to do that.  



Cool.  



That is like once you're ready with the rapid pay and everything, we can definitely write the docs and share it on our website.  



That would be fantastic. All right, cool.  



Yeah.  



Anything else from Omar or Kaan site? Basically the next thing one is for Kaan thursday at the same time, we'll set up a call and enquire that he is well on board and able to use the platform. And I think from the pricing perspective, I think I'll just discuss with you once the best way to take this forward.  



Sure. That is it.  



Okay.  



All right. And we can do calls if you want, like Debah and can be one call and we can be on another so we don't disturb each other.  



Maybe Kaan and someone can have.  



A.  



Session together, like just kind of upload the module, do this, do that. Just quickly, kind of help him out so he can get results quickly and we can move forward quicker than the pace we're in. If you guys want.  



Yeah, that will be great. We really want to do that. We can do it like Thursday is the same time. I think Thursday is the same time. We can do, like, going to try a few things and we can try and resource everything there. And if we need we can do.  



Managers.  



Can.  



Thursday, Omar?  



Sorry, what's that?  



Do you need it to be faster than Thursday?  



Oh, no, I'm just wondering if when is con is free? I'm not sure.  



I mean, Thursday works for me. If it works for you, tomorrow also works for me. But you have a whole day and I don't know if we're in a hurry or anything. So if everyone's okay with Thursday.  



No.  



It should not be available.  



Okay.  



Yeah.  



Let's do on Thursday then. I think it's a holiday here.  



Okay.  



You're on mutar.  



No, I was saying tomorrow is doing.  



It.  



Absolutely.  



Thanks so much. We'll try to move fast on this.  



I didn't mean it that way. I'm sorry. I didn't mean it like, oh, we're slow. I'm excited to see the inner workings. I mean, I'm partially to blame because I kind of veered off the path of finalizing it. So we had to bring in not anything to in D and Truefoundry, anything like that.  



No, I think it's very helpful. Really helped. And we really want to see this going forward in a good way. Like we can help you.  



Cool.  



Okay, thanks.  



Bye.  



Bye.  



All right. Thank you. Bye bye. Thanks.  



Have a good holiday and enjoy. Bye.  



Thanks a lot. Have a good one. Bye. "
9407683116,Enquire,Arnaud Jaspart (arnaud.jaspart@enquire.ai),,Enquire,<50,<10 Mn,<10,AWS,No,Information Services,USA,Anuraag Gutgutia,,https://app.fireflies.ai/view/Enquire-TrueFoundry-POC-Review-Call::i1XWaB09H5,"Hi Non.  



Hey Omar. Hi Karen.  



Hello.  



Hello.  



Hi Dave. I wish I could also join in a bit. So in the meantime, while Abhishek jones, I'll quickly introduce Dave. Dave is one of our first engineers. He's an ML engine. He does a lot of work on the back end side. They may be quickly if you can give a background, that would be great.  



Yeah, you have already made actually I think Omar was there in the first call also. But yeah. Anyway, so I work in the experiment tracking and the deployment bit of the profound tree and I mainly work on the back end side and the Python client side. That's my contribution. Hoping to show you some full demo today and get things going.  



Cool.  



And we are meeting Mohan for the first time. So it'd be great if we can know a little bit from him.  



Yeah. Hello everyone. So I'm working as a data scientist and machine learning engine at Enquire. I do most of the time. I do automation stuff both internally and for our features. Mostly I work with language models I guess.  



Hello.  



Hi, Arnold.  



How are you?  



Pretty good, you?  



Good.  



Hi Omar.  



Hello.  



Awesome.  



Thanks for the intro as well. So basically for this call the major agenda was to obviously showcase you the demo and also comparisons across different things we tried in terms of optimization before. That what we did. I'll just maybe take five minutes to just quickly present so that we have the agenda. Basically we define the use case based on whatever you gave us in terms of the model that is to be used and whatever requirements that were there in terms of the integration for GitHub and being able to kind of reach a certain level of conference link and being able to quickly change and deploy again from GitHub and so on. And also integration with traffic API. So what we have done is pretty much like we have a GitHub repository.  



They will kind of walk you through all of that as to how you actually deploy the model that you gave us. We'll also showcase you some of the comparisons that we have done. The results of that also they will walk you through. Also we integrated the entrypointtype video, the API and we'll showcase how that can be done. And at the end we'll give you access to the platform. You would have already received an email from our side so that you can go back and actually try the things that we are demoing here. And you can also try your own models and different stress test which I think Omar wanted to do.  



And we also need a few more things from your side on the sample payload structure and the hard limit on latency which will be great if we can get a chance to go into. And then there Karen, a few things in the last call which for example you wanted the deployment to be done in US East. One, these are things that can definitely be supported. It's just that for this one, we have not used this, but these are all functionalities in the platform. And some of the other things that you wanted will already come the moment you go into actual production. So we have mentioned those functionalities also here. So that is already in our minds. It's just that for the demo, some of these are not covered, but it will come as you kind of move ahead.  



Okay. And also like the security part, how do we authenticate the end pin so when all back end calls sure.  



We'll also go through that. We'll tell you as to how the security is done. We are actually updated in the document, but we'll walk through that. Okay, cool. Do you want to kind of take it ahead?  



Yeah, sure.  



And I'll record this so that if you want to refer to it later on team, I'll send it across as well to you.  



Yeah. So when you come in truefoundry and want to deploy some application, you start with Workspace. Workspace is basically a sandbox environment where you can put a limit on. Okay. This is the maximum number of CPU someone can request in that workspace, maximum amount of Ram. And we have some other parameters also. So what we have done here is I have created an inquired workspace for this demo. And here you can see that I have set up some custom CPU and memory limit. And also I have added you guys as an editor. What that means is that you can go and deploy application in that workspace.  



Right.  



And we will ensure that no matter how many applications that you are deploying in the workspace, you cannot go over this photo. So if you go over this photo, there will be an error message which will say that you have exhausted all the CPU and run. So you can enquire that there is always a hard limit on top of cost. So this is what a workspace is and you deploy your applications in a workspace. So, for example, I have already deployed an application here. I'll walk you through as to how Omar would, let's say, deploy his code into the system. But yeah, this is how the deployment dashboard looks like. I'll come back to this and now I will quickly jump to the code as to what code I have written. And how do you even deploy that into our platform?  



Start with a simple fast API app that I had written on top of the model that Omar had shared. So I'll quickly just not go into very detail. I'll share all this code and get a link with you guys for you to go through, but I'll just go through the entry point. So what I have done here is that essentially I'm just downloading and saving the model, right. And after that, I'm just running a web server which loads the model. I am using uniform here. And if you want to run it locally, you can just switch to Bash standardsh. It will just start the server. And let's say now you want to deploy this particular application to formatry, right as a service. So you can start essentially we have our API to declaratively deploy this application into Truefoundry, right?  



I'll just quickly walk you through as to what code you need to write. It's around ten to 15 lines of code that you need to write to define your service and then deploy it in our platform. So essentially we have the primary abstraction is this service abstraction, because in this case we are deploying a service. Like when you deploy a service, you have to give human books, for example, name of the service. Now here, if you notice, I just have a requirements. I haven't really written any docker file in this case. So, because refund at the end of the day on Kubernetes, I still need to build the image. What we provide here is that you can just give us what is the command that you need to run requirements Path and build context.  



Path is basically what is the folder that I want to get from him. So I want to deploy the bandwask API service. So I have selected that and under this you have the requirements TXT here. And I'm just calling batch run Sh, right? If you have your own docker file, you can just use that. Also pin our documentation, we have pointed out that if you have already written your docker file, you don't need to do this. You can just deploy with your docker file. Now we are running Gibber and it's running on port 8000. So we need to expose that. I have created a Readiness Pro so that part status up. You have models downloading, right? If you don't have the model, generally deep in with the image.  



So I have defined the regimen to properly understand that even if the container is running when the service is ready for traffic, now you can define your resources also. And I have defined CPU request and CQ limit request is what you are guaranteed to get once you deploy. And limit is something that our system will try to provide, but it's not guaranteed all the time. So I have defined that. Okay, my CPU request is four. In this case. Four essentially means four virtual CPU cores. And I need 3500 MB of Ram at least for this particular deployment. You can pass your environment variables here. So if you look at your undersh for the number of workloads, I'm basically reading from an environment variable bureau. So you can pass inject any environment variable if you want.  



Now, imagine if you want to change the model, like you have updated a model, you have updated some model version where you can easily inject that with environment variable and then I want to run two replicas of this service so that I can meet my support requirements. And at the end of the day, you have to do service deploy. And here comes the workspace. Because we need to deploy the service in our workspace. To deploy the service in our workspace, we need to have something called a workspace Sqn which is basically an identifier for the workspace. So here we'll have the workspace sqn here. You can just copy it from here. And the rest of the code is very simple. I'm just creating an argument. Parser. You are taking the box this FQ and I'm just calling this deploy function, right?  



So now what I'll do here is I'll quickly go and deploy this. So this is the command that essentially we need to run. What it does essentially here is that it uploads the code into our build server so that we can go and build the docker image or not. You have a question?  



Yeah, why do you do so? Do you have like load balancing for the deployment or is the end point.  



Down during the time entrypointtype video not down? So when let's say you already have some version already deployed, right? And then you Karen, again deploying some other unless you have made some code changes, right? So in that case, traffic will not be going unless the latest deployment is in success mode or in success state. Essentially, if it is not successful, your traffic is still going to go to the older the latest successful version that we have right.  



Now.  



These are zero downtime deployments. It's a rolling start. So you have to have at least two replicas. There is no downtime, right?  



So what essentially we do here is that we copy the code and push it to our builder for when we build the rapper image. What we will do here is that we'll quickly go to this link and here you can see that I had already deployed six times. I was making sure that the demo goes through. So I have deployed the seven version here right now. So we are building the image. You can access the build logs here so you would be able to see what are the steps that we are executing to build the image here. And after the image has been successfully built, it will be deployed. So it will take a bit of time. In the meantime, what I'll do here is that I'll show you how the GitHub integration and the CI CD works here in this case.  



Now, let's say you are using GitHub, right? What I have done here essentially is I have written a very simple GitHub action definition here. What it essentially does is nothing but install service Foundry which is our client library. You have to install service boundary to use the library. And this falls the same thing that I've had called. Now we provide two ways of deployment one is via the Python API. We do have a more declarative ML route. I'm not showing that for right now, but I can show it later. But essentially what you're going to run here is that if I do PY and you will basically inject API key, you most probably don't need to inject force. I'm just doing it for the demo. Now, let's say I go and make some changes, right? You can make any code change here.  



So what I'll do here is that I'll just go to deploy PY and let's say I changed this account, right? And then what I'll do here is that I'll add a comment and right now I'm just pushing it to main. But you would essentially have a PR review process and this particular action would only run when you have any push on the branch. So we'll go back to the repository here and let's go to actions. So you can see that I have an action. I have an action running here. So we'll go back, we'll go to the logs here. Yeah. So again, whatever I had executed in my local machine. So whatever changes that you are doing, if you want to have that clean CI C to get off, it's possible by default also. So it has completed deployment.  



So what we'll do here is that we'll go here, click this link and we will see that we have another build process going on. So I deployed seven through my command line and now the GitHub action also had triggered our new deployment and we are building the image forward. Let's say we already have seven deployed and the eight is currently going on. Right now what we can do is the end point is still going to be up. What I can do here is I can just go here. The end pin would be printed here. I can click on this. You can see that the entrypointtype video still up. Even though there is 8th deployment is still going on. If the deployment fails, it still will be up. It will be the 7th deployment, which was the last successful deployment, right?  



So you can go here and let's try it out. So we at least have something running here. So we have deployed the application successfully apart from this get off if you want to, let's say quickly go and change any configuration, you can do it via this edit button here. Like if you want to change your CPU request, you can do so here and quickly update the thing. One more thing that I want to show here is the logs, right? So let's say you have your deployment, it's deployed on our platform. How do you get access to metrics and logs?  



Right?  



So for every deployment, we have these two items here. One is we give you a grafana dashboard where essentially for that deployment, we will have the metrics which related to whatever requestId are coming up. We do have some metrics here. And if you want to see the logs, we also have a logs button which will essentially take the application logs. So here you can see I had deployed, I was downloading the model and these are all the application logs that are getting streamed from Kubernetes. You can directly see it from here. And we will also have CLI commands to also take these locks. So I will take a pause here and if you guys have any questions, please ask and then I'll show you some other ways to reply.  



Also, it's a lot to take in. You're like me, when I put on something, I go really fast and everybody's left, which is good. Any questions? Karen? Omar?  



No, not really.  



Yeah, I never use the GitHub action. I should.  



Go.  



Any questions on what you had about the model, the conference link and everything that we deployed?  



So the Take Your Way itself, as I said before, it's something that happens in the background and then we send an email to the client to say, hey, you have like your question is finalized and you can sense it. And when they click, they see the takeaway that happens. So we have time for the takeaways. There are some other models where it needs to be almost instantaneous, within a few seconds. And for example, one of our models that we're thinking about probably deploying, but it will be pin the longer term because there are many things to think about. Like we need embeddings in hot memory, like redis, and if we put it on your side, it's going to be probably a challenge. Also we'll need end points to change that hot memory.  



So I think it's like it's a much bigger scope, so it will come much later. But we have two models that are pretty good to start with, the takeaway model and the summary model. Like these two that Karen pretty much together. These ones don't change. We have a logic, we have like a clustering of answers by similarity and then we group them and then we make the takeaway and that one never changes. Pretty much. Maybe one day we'll do something. There is one that we change every week, which is the auto approval. And that auto approval doesn't only change every week. So it's a perfect we retrain it on our side and then we need to just upload the model. We don't need necessarily to change the code. It could be just maybe at some point.  



So that one is second best model to offload. So I think let's start with that. Do we have a discussion about pricing?  



Yeah, we can do that. But I think they also wanted to showcase maybe through the Uri. So I think this was the version that we did through the notebook and the CLI. So basically if we can also showcase for Omar and through the Uri, that will be great because I think that part is also like how they could integrate their GitHub and then how they could do that. Maybe that will be yeah.  



So I'll start with this. So you start by integrating your GitHub organization or your personal account with us. So basically you click on link GitHub and it will ask you to go and install an app and it will basically give permission to certain repositories so that we can read them. So that's how the GitHub integration works. And let's say whatever I did, you want to essentially deploy the same thing by the UI, right? So this is how you will start. You'll click on the new department. So here in our platform, we provide service and job. So in this case, we are deploying a fast API service. We'll select that and then we'll select the workspace where we have to go and deploy click on Next. Now let's say we are deploying this from UI.  



So I'll just give it a new service name and in this case, I have already integrated GitHub. So what happened here is that the repository will automatically come up here and we'll just select that. Okay, this is the repository I want. Now I'll go back to the repository once and let's say I want to deploy like in this particular deposit, I have tested out three different ways of deployment and I have selected that. I am going to go and deploy this particular service that I have written here. The same thing that I showed you on my right. Now here in this case, I'll go select Piping because I don't have a docker file. The same thing that we did through the CLI. So we'll have to provide paths to build context, which is the folder that I had just selected.  



And we need to provide a command. So this is the same command that I'm going to put here. Replicas will put it to one. I have to expose for 80. And here you can basically go and change your CPU and memory request. You can see that it will show you that. Okay, the workspace, how much buffer you do have. Like I can see that. Okay, there are two CPU units still available. So what I'll do here is that let's say I'll change this to two and change this to one, and I'll change this to 5000 megabytes and change this to let's say 3500 MB.  



What is it? Some points we want to use a GPU. So here we karen only talking about deployment of pretrained model, right? We are not training on Truefoundry. Can we train on truefoundry?  



We showed you service and we also showed you a job, right when I was deploying from the advice. So training is at the end of the day, it's a job that you will deploy. But I would imagine you would enquire GPUs for trading. So that is something that is in our roadmap. We will try to provide that. But that's in our roadmap when you define your training job and deploy it directly on our platform.  



Okay, but no problem. Right now let's focus on servicing. But what if we want to service on GPUs? Let's say that we have like a big model.  



Right now in the resources, you are just selecting CPU and memory. We are working on building up another option here, which is you will essentially select the GPU limit or how many GPUs do you want your service to have access to. So that is something that we are working on and we'll let you know once that is active.  



Okay, but that's not yeah, okay. Because you will face something like if a model can run on CPU or GPU, kuda library stuff. So when you put a GPU parameter.  



Exactly. So right now we generated the docker file for you, right, essentially. But let's say enquire some drivers and all. What we are trying to do, there will be two approach. If you already have your docker file, you can go and deploy. We will make sure that the node has right drivers and all so that you can go and access the GPU. But we will also make it seamless. If you don't have a docker file, we can also make it seamless. It will still go through and we'll install the right set of libraries.  



Okay, right.  



And essentially we will just select another GPU resource from here, like how you are selecting CPU and memory. So you can just click on submit and hopefully this will go through. Okay, this has gone through. Now what will happen here is that you can see that there's a new deployment that has come up and then it's the same. So we get the image and if the image building was successful, it will go and buy the service.  



Did you have examples like stuff that we can deploy on our account and then remove it just to try set up like a GitHub setup because you have the YAML or whatever profile. If we have like a template that we can use and modify.  



Yeah, sure. So I think we can take this part of them.  



Yeah. Basically in this document we have basically written everything here. So this is the link to the GitHub repository or not. So I think once you give us your GitHub usernames, we'll just add you to it and you'll be able to access. Have you already added them? I'm not sure that no. And after that you would have received these emails on your email ID. Like all three of you, we have used these email IDs and these are your usernames. Just check in case if it's not on the main email. Like just check your spam folder. You should have these gmail and when you go into the email and when you go into the platform, you will already have access to this workspace enquire which Dave has already created and he has added you as the editor.  



And within this workspace, you can actually just in this GitHub repository or even in another GitHub repository, you can just deploy it to this workspace. You can integrate your own GitHub to your account, like however you want, and then you can start deploying to this workspace and test it out. You can make a copy of this, you can change it. Or if you want to test out your own model, you can do that. Everything is fine.  



It's a good question. It's a good question because for example, if I put my GitHub account, you will have access to all of the repositories.  



So let's say you have your organization, right? And when you are giving the permission, you can actually go and select. So if you only want to give us access to one repository, when you click on the link, it will show you options that okay. Only give access to this particular repository and not any other repository. So that's fine. Second thing here is that if you have code in your local system, like you have code on your laptop, you don't need to do GitHub integration at all. You just need the workspace estimate which we have already provided in the documentation and also it will be available in the dashboard when you log in. You just need that, you don't need to do any GitHub integration at all if you just want to deploy the code directly from the local machine.  



GitHub integration is only required if you are doing the deployment from the Uri directory.  



Yeah, you sense a zip.  



So we can either add you to the GitHub repository directly and you can do a get loan from there, or I think we can send the zip of the code also. That will also work.  



Okay, so we can send the zip of the package and it will deploy.  



Okay, I understand. So you mean that okay, you already have a zip of your code and you want to deploy it.  



Right.  



I think you still have to anarchy it and we will not be able to handle zip directly for now. So you'll have to anarchy it on your local machine so that we will do the zipping essentially and send it to our Windsor.  



Okay, all right, but I mean, let's go GitHub.  



Yeah, sure.  



Is the way to do it. Okay. And so that's the GitHub integration that handles the security about which repository we have access to, right?  



Yeah, actually I can show the process here if you want. So I'll just quickly share my screen. I'll go to integration, I'll go to get let's say I want to link GitHub exact same thing.  



Like what Davis.  



GitHub?  



Yeah, let me just put in my password. Okay, great. So what it will do here is that you will have this option here, only select repositories. So, for example, I have only selected two repositories and one of them is enquire POC. So you would also have this option and this is like the access that we are going to ask. And we will only have that access for those repositories.  



Okay, so we install into docker and we have shared resources right on CPU and run memory. But it's in the container, so it's good enough. Any question, ken.  



Do you like any storage, like external storages, like a street bucket? Or if we have to store some files, do we just store them in the EC two? Because I'm working on a model called.  



Question Generation and that one will not be on Truffle Way.  



Yeah.  



It'S more than one model working together. One model output some files and then that folder path as an input for another model. It's a little complicated, but yes, it's.  



Multiple models that talk to each other.  



But we would love to at least understand or not, like just to see like I mean, the whole reason we karen designing is to make complicated things simple.  



I know, I totally understand. So they're like pre processing models. They're like processing models. And after all, we have like a little machine learning like that picks up the best output. So it's like a three phase.  



Besides model inference, we also run other Python scripts just to create some features from our outputs. And they have to work altogether.  



So we score the output and then there is a last machine learning that gets the output and according to the score, select the best one.  



Is this happening in real time or is it more like you run the model once again. What exact flow of these models? Like, first one, can you just describe.  



The flow roughly, if we have time?  



Or maybe we can talk to Kanye.  



I think the primary question was essentially do we provide the storage or do we have to? So right now, what you can do is that if your model, let's say, is already or any other block storage right now, we don't provide a block storage of our own. But what you can do here is that you can add your AWS Credential as a credit. Truefoundry. Which is essentially we store it in an encrypted place and you can basically go and inject your AWS access key and secret key, and your Python code would be able to access both. If you're using the Boto three library. Using that, you can connect to S Three.  



That is one option. You can bring your own S Three also. Yes, and you can also publish artifacts to us. We have a library where you can publish artifacts and models in our registry. So we can maybe do that. It will require a separate meeting to go that part. But if you really want to support that thing.  



Yeah, there are a lot of input outputs. That is a long story.  



It is good to at least if you are in a separate call. Maybe just to understand the problem so that we can get feedback for our platform that we are designing the platform that can accommodate this complex use cases. You're usually looking for complex use cases.  



That we can make.  



So the part I think where it's interesting because as far as I understand, you basically have a graph of model where one model is producing some output which is going into some other model, which is again, probably that output is going into some other you mentioned that you have a selection at the end which selects the best output.  



Right.  



So we are actually also trying to think of a same use case right now. I would imagine you will have a lot of Python glue code to ensure that all this communication is happening properly. We were also thinking of literally the same use case where you have one model and the output of that model goes into another model. If we can at least at a high level in a later meeting can understand the flow chart of how does that happen, we don't need to know what model it is, but at least how does the whole process work, it will be really helpful for us.  



Yeah, I would say before going that route, which is totally like a very interesting scenario. The thing at least for your business, there are a lot of natural language processing that needs embedding if you can update embeddings real time and fast, meaning like hot memory databases that the model will use to match stuff. So at least for you have a better coverage of machine learning because it opens. Like, natural language processing is like a huge field now. So at least if you could handle hot memory embeddings, that will open more doors for you than trying to do like pipelines of different models, like in damage already. That's what I will say.  



That's a very useful suggestion. I'd love to discuss a little bit more about this later. Omar and any other questions like Omar didn't ask any questions. I'm wondering.  



Omar is getting ready for Saturday because we have a big push.  



Okay, no worries.  



Well, I just feel like for now might better just to use the platform as a place to deploy, like you said, pretrained models without me touching it. I have like three already used for takeaways, or maybe two of them.  



Takeaways.  



And summary first because they are intensive and after auto approval. I think if we do these three.  



Now, because I've been overhauling the code a little bit, I can see exactly where I can squeeze pin the so one aspect of it is this is done asynchronous and there's a lot of like code to have it running in the background in memory. We can have it replaced by just a request basically, which is the model that you guys have is already in memory. So I don't have to worry about that. It's just about the processing time rather than loading it up. So that can remove a huge chunk of problems with the seller.  



The queueing. Thank you. Because I didn't think about the queuing.  



Yeah, the queueing is like I have to do a bit of like a work arvind to have the memories. Can't talk anymore. The model is up in memory. But I'm just thinking now if I just have an end pin that I can call and get yeah, it's in queue. But the process, it's like it waits for things. Because I use another gector. I use that thing also. So there is a bit of back and forth of that. So it should be fairly easy to incorporate.  



Yes. Did you have used cases where some of your clients are using Celery queueing or rabbitmque or whatever queueing system to actually queue the request? So when I said we karen pretty patient with the takeaways and summary models, we don't need it right away. If it takes 3 seconds, no problem. But during these 3 seconds there may be other requestId coming in. Right. Do you have like cases from your presentation or previous presentation or clients where they use salary to queue pin the run to queue the request and process it one after the other? Did you have that?  



We know of one kind using a redis queue, not using salary. They use redis as a queue.  



Okay. But the local queueing meaning the container is actually like keeping the queue in its memory and it's not like a shared queue.  



Yes, I mean as of now we do container queue. Like the faster is holding that queue. But if you want it queue is large. It depends on what is the traffic because you can put it in a queue but it depends on the traffic patterns.  



Local queues, like container queues are fined. That's exactly what so this one was it one day.  



Do you have an idea on how many requestId will be expected at the same time?  



Any ideas on that? Now say the maximum is like 20.  



Okay.  



That will easily be handled by those continents. Right.  



But the size of the request let's say it's, I don't know, 5 KB.  



Fast API internally has a Q and thread pool when it gets the request but it also sends back a response. Right. What I understand from the use case here is that you just submit a job in Celery and you have a job ID and then you basically get the result back from the results store. Right. You don't wait for the request to end. So that we'll have to see. Essentially we don't have a ready use case at this point. But I think that is something that we can do.  



And actually for the deployment it raises a good question. During the deployment, celebrity dies because we restore the service. Does it Omar? A good question. Yeah, it's a container service. So you don't restart Cideri when you restart the entrypoints?  



Yeah. The broker is ready. So I think all the messages queu"
9407683116,Enquire,Arnaud Jaspart (arnaud.jaspart@enquire.ai),,Enquire,<50,<10 Mn,<10,AWS,No,Information Services,USA,Anuraag Gutgutia,Enquire_Arnaud 21-09-2022,https://drive.google.com/file/d/11FE4UMVOMd3Te7OCJHCSAkNxhZ2_lRKr/view?usp=sharing,"2nd call - 21st Sept, 2022

Use Cases for ML - types of models (Is monitoring important etc?)

1) People update resume, we have to keep the embeddings. We have the model - but we have a model. That model doesn't change. But what changes is the embeddings of the explainaton. When we want to match the question to best explanation, we get a score for each.

How many embeddings are you talking about? 20-40K embeddings. Every query we get - it will go over, rank them and return. Now, on these 20K => these are updated 100 times a day. 200 of them change each day for example.

Why do you want to use TrueFoundry? COST OPTIMIZATION

RapidAPI - We would love to sell our endpoint outside of our own universe. You have API, You have security, etc. If its a use case you already have (RapidAPI)

Current Stack for ML Deployments and pipeline

A good documentation to integrating a RapidAPI on top of our Hosted Endpoint. Could we have a paid API ?

3 Days - 1 Week to write the documentation. Arnaud - the payment per API call is the only part that needs to be handled.

How do I integrate TrueFoundry with RapidAPI?

Problems being faced where looking for solutions

Models are having memory issues: Right now - T3A, 2/8 Slacks. Memory was not that big before.

2 Models - Summary and takeaways. It takes quite a bit of our memory.

It takes around 500MB memory. We have multiple models at the same time. It is about 16 GB.

You can put multiple models in a single container or you can put 1 model in one container. If models are getting high amount of traffic, then you put it in different containers.

You are using multiple workers and then it creates the problem. Everytime, it will multiply the memory. If we operate on one worker, we lose information.

SpringBoot API and the AI Models.

If we want to do some form of clustering etc etc - we can do it at our end or different end. If I do some work at our end and then some at our end.

When I update the code => how easy is it to modify code? YOU CAN ALSO GET THE CI/CD SET-UP

Questions asked wrt Product

Deployment on our cluster for now.

Will you deploy on your cluster? Or use ours?

Security side. We are not big enough yet.

AWS Machine type: CPU, T3A 2X Large, Do you need GPU? Wondering how you would do the model without GPU that I sent.

Feedback wrt Product - what would make them possibly adopt it

* Could you give us a Dummy Model? Request that is working == > We can ask them to put any of the transformer models from Hugging Face and we can replace it.

* You can deploy it. We can give access to the account, we can play with it. If we are able to integrate it easily with what we are doing.

* POC: We re-train our models in a Weekend. I would love if it was a single repository, but inside 1 repository would be great.

Re-train the model via CI/CD. We have a FrontEnd, API Backend => the layer communicates to the AI Model.

* DEPLOY in US East one - Could IP Filter the access request? We can do that. => Not needed at the second.

Concrete Next Steps

1) Take the model - Get it deployed - Show the demo, code , CI/CD => Next meeting you do it online.

2) https://huggingface.co/philschmid/bart-large-cnn-samsum => I was thinking of Stress testing and try and break it. In reality, it doesn't get triggered that way. Would love to say how much power I could get per hour. 1000s per minute.

3) What is the Production traffic you are seeing? Reason I was asking => Flask/FastAPI, Really want to do really optimized - you can use a model Server.

How much inference time is fine for you? Say, 100 Ms is fine. If you need something like 10Ms, FastAPI will not cut it for you.

We can do the SLA. FastAPI where it breaks down, Pytorch server.

Pricing - We can discuss the Pricing conversion. It will be cheaper than what you are paying AWS.

* Need Slack connect. enquireai.slack.com

* The Model I sent doesn't work on CPU and will take more time. It will need GPUs. HUGGING FACE has their own GPU machine"
9213359389,Optum,Kirti Singh Dangwal (kirti.dangwal@optum.com);Rajat Gupta (gupta_rajat@optum.com);Rajshekhar Gadepalli (rajshekhar_g@uhc.com),,Optum,>1000,50-100 Mn,>50,AWS,Yes,Hospitals and Health Care,USA,Anuraag Gutgutia,,https://app.fireflies.ai/view/TrueFoundry-Demo-Optum::5RQ7GIBaAq,
9213359389,Optum,Kirti Singh Dangwal (kirti.dangwal@optum.com);Rajat Gupta (gupta_rajat@optum.com);Rajshekhar Gadepalli (rajshekhar_g@uhc.com),,Optum,>1000,50-100 Mn,>50,AWS,Yes,Hospitals and Health Care,USA,Anuraag Gutgutia,,Optum_Kirti 21-07-2022,
9185507450,Eddytravels/Trippadd,Edmundas Bal?ikonis (eb@eddytravels.com),,Eddy Travels,<50,<10 Mn,<10,,No,"Technology, Information and Internet",Lithuania,Nikunj Bajaj,Eddytravels_Edmundas 21-06-2022,,"- Lithuania, do NLP

- Starting to work with a new product that has a recmmender system component

- Machine Learning world.

- We have an existing infrastructure and have some experience with that

- We started on local training. Docker container, tech stack deployed on docker containers. Never needed GPU. For training we used to use local docker local GPUs.

- We later developed cloud training solution. First based on Kubernetes and then on Serverless container. A service to just spin up any container on whatever instance that you want.

- This is how we deploy and train out model. All containerized and docker. At this stage just collectin data and yet to fugure out what model and framework that we will.

- With K8s based approach we had to some managing. This is even less for our side.

- your youtube video showed all visibility and if its just 1 line of code and it was really difficult to understand your product. It was difficult to understand your product.

- about training we run once a month. Yani some models that fits our needs and we deploy a new version togeter with other services.

- about our new product, how often and what would we need there. I wam interested in model versioning. Could we change that in production fast.

- few models on NLP side, model for intent and entity classification. Raw data into structured data. Different task which is help to answer common questions which is FAQ. We train it on many examples. Try to classify which answer is best.

- application that manages your application in your AWS account. you can get discounts and take advantage of credits. You can do that on your AWS account as well.

- Qovery - check them out."
9181477197,Animall,Hemabh Kamboj (hemabh.kamboj@animall.in);Animall K (naveen@animall.in),,Animall,50-100,<10 Mn,<10,,No,Software Development,India,Nikunj Bajaj,Animall_Hemabh 19-07-2022,Animall_Hemabh 19-07-2022,"19-07-2022

Use Cases for ML - types of models (Is monitoring important etc?

Building a couple of CV models. Sort of a 5-6 models in a pipeline. What is the TAT of one model. What is the performance for 50% of the load etc, whats' the performance of 100% load.

Current Stack for ML Deployments and pipeline

""Current, we need 30 minutes delay, that is fine. So we don't need real-time.

We batch process the listings every 5 models. We orchestrate using Prefect. Flow everything using Prefect.

We have taken a VM and then we use Prefect for Orchestration.

Model runs on CPU - no GPU. You mentioned you use Docker - are you using AWS EKS. We haven't started it and moving entire thing into Kubernetes, but some cost issue came up. (GCP is used, kubernetes Control plane was charging something).

Scale - 5 minutes, 30-40 listings ==> 80*12 ==> 300 to 600 listings.

We are using label studio for labelling the data.

Problems being faced where looking for solutions

If you have any orchestration set-up for active learning, that would really help us. In active leanring, we are building for monitoring the inference data.

** We are doing everythign at VM Level, there are lot of issues. If we move to serverless, then it would also help us ==> We are using multi-processing to do it. Sometimes, the other containers are not up. In a day, it happens twice or thrice. We don't run it as a service but run it as a batch. ==> This is dynamic CPU and memory allocation. If you ahve deployed 4th model and you are loading 5th model, we will dynamically load.

** Cost of moving to kubernetes

Questions asked wrt Product

""* How many services could I deploy in a single WorkSpace?

* Let's take an example - I have a large model. If I hit a lot of load, what if it exhausts the whole 4 GB Memory. Will it auto-scale?

* Can you go to create WorkSpace? I can only see upto 8GB Workspace.

* Will this be Kubernetes or something? We can also attach it to your own Kubernetes cluster""

Feedback wrt Product - what would make them possibly adopt it

I like how easily you have orchestrated everything in the interface. I really feel this is very useful wrt deployment.

Concrete Next Steps

""I will try the workspace and see how I can deploy the model.

"""
9146737192,Scaler Academy,harshit-tyagi Tyagi (harshit.tyagi@scaler.com),,Scaler,>1000,100 - 500 Mn,>50,,No,E-Learning Providers,USA,Anuraag Gutgutia,,Scaler_Harshit 23-06-2022,
9146737192,Scaler Academy,harshit-tyagi Tyagi (harshit.tyagi@scaler.com),,Scaler,>1000,100 - 500 Mn,>50,,No,E-Learning Providers,USA,Anuraag Gutgutia,,Scaler_Harshit 28-06-2022,
9146737192,Scaler Academy,harshit-tyagi Tyagi (harshit.tyagi@scaler.com),,Scaler,>1000,100 - 500 Mn,>50,,No,E-Learning Providers,USA,Anuraag Gutgutia,,Scaler_Harshit 14-07-2022,
9133766297,ApnaKlub,,,Apnaklub,100-500,<10 Mn,<10,,No,Wholesale,India,Nikunj Bajaj,,https://app.fireflies.ai/view/Apnaklub-TrueFoundry::I9V9HPSUoH,"Hi Nigen.  



Hi Priyank.  



Hi Shaans. Hi Nicole. How are you doing? Very good.  



How are you both good.  



Nice.  



Been such a long time triangle, I think in my life I have not moved a meeting or like a meeting has not been moved more number of times that like we have, I think.  



Past few months some kind of crazy means for us. And last month I was off on vacation to that kind of had a problem. And then at the same time there were a few members of the team that we also wanted to be a part of this meeting. So that's like that happened. I know. Apologies from my side. And at the same time I think.  



It'S not just you, I've also done it. I was not hiding anything literally, like.  



Mutually something that's happening.  



Yeah.  



So I think in July and August were experimenting with first of all models and wanted to get some flavor of it and then have a call because I was following up with your releases as well to see we are progressing in a very fast mode now, which I can see totally. So are you currently india or in NSF?  



I'm in NSF.  



Oh, nice. I just wanted to know. We have Sivank in BIANC, so Bianca is also a data center within our team and Sivank just recently joined us. They are like primarily working alongside with me on some data science models that we are building and will be building within the next few months. So I wanted them to also have some flavor of the call this was about.  



Right.  



So basically you're giving us a demo of what troopont can do for data centers and how we can use it.  



Basically the USPA is suggesting that what you wanted to discuss today was kind of like get a demo of proof on somewhere. Like, how can we help? Essentially yes.  



In short, yes. Otherwise, whatever you want to drive, I'm fine with both. If you want to do it like sometime next week or if that works for you, then that's also great. Or otherwise you want to have whatever you want to do.  



Sure, sounds good.  



I think maybe a good start would be like a brief introduction between Priyank Savank and me. You and I have already met, so maybe I can introductions briefly, introduce myself. Priyanka and Sivank. I'm one of the co founders here at Truefoundry. I personally come from a machine learning background. I used to be at Facebook where I led one of their conversational AI efforts. And before that I was at a startup called Reflection where I got a chance to build out a horizontal AI platform for the company, kind of scaling it to roughly 600 million users. So that was a remarkable experience. I feel like it's one of the best experiences of my life because I really got to work on both the engineering side of machine learning and the model building itself.  



We were building a lot of personalizations and recommendation systems basically for the ecommerce industry. So I learned a tremendous amount while working at Reflection. And prior to that I was doing my Masters came to UC Berkeley here, and before that I was india doing my undergrad. And undergrad is where I met my now co founders and wagon, Abbychecks, actually. The three of us are Bachmates from It, corrector. So glad that we're kind of building out the company together with them, basically. And personally, I grew up in Calcutta. My family originated from Rajasthan, so we're it about myself.  



Okay, Shaun, go ahead. I'm working as data scientist at Apnaklub. Before joining at Apnaklub, I was in algorithmic trading. So I did not like that job. So I switched through data science and more machine learning space. I graduated in 2020 from Tal itself. Like, I'm a junior of science. You can see in life. Okay, this is a brief introvert.  



Nice. Awesome.  



Which algo trading company that you're working.  



With, I was in which one? Trexpond.  



I haven't heard of them.  



Trexpond? I think it's more like an India based algorithm ICS based itself.  



Okay.  



But it's 2019, so it started I.  



Think there is recently started coming to IIT many times. I didn't see any of those coming in. It was more or less biggermencies.  



Okay, I see. Nice.  



Welcome to the bright side of the corporate life. France, the startup life from the but yes, go ahead. Simon would love to know a little bit about you. Are we losing Sri Lank? Or is this my Internet?  



I think it says.  



Okay, well maybe Srivanka is coming back. So do you want to briefly introduce yourself?  



Yeah, sure. Hi. IBM currently working as a data engineer here.  



It's been more than one year, so I started here as a back end developer and now it's been three years here.  



Oh, nice. Awesome.  



Very good sumit, Shivan.  



Yeah, sorry, my inderjeet connected. So I was saying, like, I recently joined a club, like two months ago previously. Before that I was working in a market research form. Currently Chris was focusing on the political domain. Like we used to study about the elections going on, predict their results and all of that. And I graduated in education and have a very brief interest in this particular field of machine learning. And IBM currently living here. A great experience in.  



Awesome.  



Sians market. Sri Lanka is very specific. I'm currently loving it here.  



So he's in present in that route. I hope it's not a fast night.  



Nice. Awesome.  



Thanks everyone for joining him for the call here. Do you all have any background? Already have had a discussion from Srians about Trufaunrian, Shivank, and I think it.  



Would be great if you can provide us a brief background to them as well because compared to me, I think you are like a better verse with Trufanti.  



Sure, I can certainly give a little bit of a background. But in today's call triangle team. What I would like to do is instead of directly giving a demo of the platform. It generally works best for us to understand what are the problems that you are trying to solve and figure out if this is even a good time for us to potentially like in a showcase what we are working on and how can like what are the areas that we are helping basically because if the problems itself are not relevant. The demo is generally never useful basically. So I think I would love to spend more time just understanding the problems itself and then after that we can actually set up a follow up call where we come with a little bit more personalized demo as well.  



If I notice that, okay, there are a couple of problems that you are trying to solve that true Poland has the capability to solve. I'll show you the demo of that part instead of giving you like a full dump of the entire platform basically.  



Yeah, I can start off. So there are few models that we are currently working on and we have already worked on few of them. So one of them is what we have worked on was on lead scoring. So where we predicted probabilities of person getting converted onto a platform. So if I want to talk about the engineering side of things so it's totally deplied on Google. So it has like Google BigQuery as the data warehouse, it has Google Airflow as the data engineering pipeline and the model is based on XC boost pipeline. What we do is we create probabilities, then we integrate probabilities to event based systems and we drive events onto web engage which is like our marketing automation tool. Using marketing automation tools we drive like the strategies, right?  



So either it could be a product nut, it could be a marketing nudge, it could be also we are calling, you know, like the calling sales things that we have or could be like the actual offline sales.  



Right?  



So there are like four types of end touch points. So that is the whole cycle of one of the models that is in production right now where we take the app data. Massage it with the other kinds of data that we have. Use GCP as a full running background and then provide results to an automation tool where they strategize on how to use the probabilities and figure out what is the best way forward to continue with the engagement with the person. So this is primarily on activation. Priyank and Sri Lank, they are currently working on retention scoring where we want to prioritize ordering or at the same time basically prioritize touch points, communication touch points.  



Right?  



So suppose if the priority of order of a person is very high then we basically bifurcate them into two parts. One is on the product side which is like match them on product again and the second is the offline and online. So the inversion of this probability, like a high probability would mean the conversion is high, the low probability would mean the conversion is low. The inversion of this will give us the churn side of things which is like we are solving for and there is a churn as well. The third thing which currently is not a data science model but it's a data engineering and analytics solution which we are currently using is on the which more in detail, which is on the supply chain optimization where we are trying to predict what to procure, when to procure and where to place.  



So we do the demand planning and then we do the demand distribution within our warehouses. So last time when we had a chat like we had only few warehouses, Nicoons. Now we have since 20 warehouses, two mother hubs, right? And that's why the network planning problem becomes a big issue for us where we want to forecast demand at the same time distribute demand in a way where the overall aging of the warehouse and utilization of the warehouse is very high. Fourth thing that I am currently working with Sudhir here is on the product recommendations where we are trying to build not implicit recommendation which is like the Sudbus algorithm figures out the features by themselves.  



However, what we are doing is trying to turn it into a regression problem and then solving it because that will help us solve for GMB, we want to optimize on GMB 1st. 2nd is we can play around with the optimization function. Third is we want to solve for cold start as well in the first version itself, so that the dynamic pages and the app is more dynamic, right rather than static. Fifth is where we want to have it more explainable and that's why we wanted to use some kind of parametric model. So right now it's in data engineering phase. However, what we have scoped for the model and the features is to be explainable so that the end users are comfortable, right?  



So for product we don't need to provide any kind of explanations, but like we want to drive the same model through our sales agents offline online and also through our communication platform which one of the team members who is on off right now is currently working on WhatsApp based smart communications which like last time I talked with you, were like only in ideation phases. We have started implementing it right now. It's analytical solution where we figure out okay, this is a product which a person would be interested in. Then we send out a WhatsApp message which is optimized to the time that he is most active. At the same time the session time of that person is highest. So we look for model R.  



We look for session time and then do an optimization on time itself to increase the Chris right that is all driven through right now it's on edit and the process automation workflow but it will be moved to Google Airflow system together with web engage so that the marketing can work together with product to apply throttling limits on WhatsApp second is we can do some kind of journeys and integrate the information into the journey and third is it gives us a playground of flexibility right? So that is on the communication side of things. Apart from that there are many internal areas and use cases that we are currently exploring which is on internal gamifications of system.  



So we have like the upnake club app which is the customer app then we have the delivery app where we have our own delivery person's delivery pendo products so there we will be solving for operations and optimizational operations it is like route planning and as well as together with CTO prediction. So if the CTO is high then those kind of models will be built into those operation system. The second app that we are building within the product is the SOS app which is turns feet on street app right? So these are like SOS where you know like in tier two and three cities the app usage is increasing but it's a gradual change.  



I see a technical shift in how the app usage would increase and we'll be driving it not us, startups like us right that's why we need a combination of product which is more suitable to these people at the same time a combination of people who go and do like a hands onboarding of these guys. So these app will also be optimized for an So to just look like here suppose Chris user I have, what kind of products would you recommend? So we want to link our recommendations platform together with communications platform with the supply chain that we have all three of them together to reduce our inventory aging which is like the inventory turns would increase and the margin should increase.  



The second that we want to optimize for is like availability at the right time at the right space with a proper person who knows how to pitch, when to pitch and what to pitch. Right, so that is like a full scale like the model that you're building but talking about four motors that you're currently working on. One is supply chain demand distribution, third is product recommendations and fourth is retention.  



Probability scoring so supply chain demand distribution.  



Recommendation and lead scoring but in retention so probability of ordering I would say.  



Okay, I see, understood, sounds good.  



So this is very helpful backgrounds and basically all these four problems you are currently actively working on basically yes.  



Okay.  



One of the problem is already in production which is leavespurring the supply chain and demand distribution is also in production and currently working. Third is the retention probability scoring and fifth is recommendations which is currently in Datta engineering phases.  



The supply chain you're working on currently supply chain is also in production.  



You're saying version one, obviously we haven't solved the problem, but obviously there's like a version one and then there's like the version N which is quite far into the future.  



Makes sense. Yeah. Okay, understood. Awesome.  



And in terms of the team, is the four of you working on data science problems?  



Yeah, so there are two other people as well which are missing. So Rohan, there's another guy who is working on the communications platform where I talked about the optimization of time and the WhatsApp and the what to send and when to send. So he's currently on the Valley holidays. There's another person was Kayhan, she's a data engineer together with Sudhir. So Sudhir is taking care of our all order management system pipeline and new integrations with the warehouse management systems. Right. So we are moving towards the ERP to an ERP system which is called ERP Next, which will take care of all our centralized ordering in one place. So he's taking care of that together with helping other create data engineering pipelines for other models which I just talked about.  



Understood. Okay. Makes sense.  



The other question is that in terms of the engineering support, do you have like a separate team kind of who helps take the models to production or is it like just you all on your own trying to take it into pendo?  



Yes and no. In cases of all product driven changes we provide the data and then the back end engineers use that data to production. But just like lead scoring is something that we completed end to end, supply chain is something that we completed end to end. So it's like a hybrid. Depends upon where we need product help, where we don't need product help and where we can build our solutions ourselves and drive it forward. The idea since like it's a startup, right? So whoever has more bandwidth. Simple.  



Makes sense. Okay, that completely makes sense. Alright. And then in this entire pipeline are most of these models like bad scoring type models where your model makes predictions, you sales that save the results in a database and then use it somewhere?  



Yes, most of them will be like that for like next six months at least.  



Okay, so most models are bad scoring models and you are completely on GCP but you're not using Vertex AI.  



No, but we'll have to start using it for retention, probability scoring and sickness product recommendation. The data size is quite huge and we can't build it on our own systems.  



I see. Okay.  



So what's the point of using Vertex in that case?  



I think so we wanted to use Manage notebooks which have a good connectivity with Google warehouse. At the same time we can deploy it in a faster way compared to doing our system then creating a report deployed.  



Makes sense. Okay, understood.  



And then if you think about the entire workflow of machine learning and in this case others can also jump in like Priyank Simon Sumit. If you want to share from your experience, what does that workflow today look like basically? How frequently do we get the data, how frequently do we train the model, how frequently do you run the inference job? When do you update the model? All of that workflow basically or when you plan CTO update the model. If like this entire effort is fairly decent, maybe you have not updated the model already. But how do you plan to do that? So we would love to understand that.  



Yes. So I'll talk about lead scoring. I think Prion can talk about one of the models that is in production and I forgot to tell which is like an associated rule mining model where we do people who bought this also bought this section within our something of that sort. So I'll talk about lead scoring. So for lead scoring we have built the model, we have integrated with our communications pipeline and then there is a program manager which is using this probability to drive the communications forward. How do we track if the model is working fine is through amplitude. So what we do is like we send events against the probabilities that we have bipartited the users.  



Then we do a sectioning of that probability greater than zero nine and amplitude has this cool features which tells you what is the conversion this time compared to last month. If the conversions is around the same number or around one standard deviation, then less than the same number, we don't calibrate. We calibrate if the conversions have drastically been changed or our marketing methodology has been drastically changed. Right. So that is one. So for this model we calibrated last on September, like I think first week of September and still working fine and we'll probably calibrate it in first week of November again and then they do it on a monthly exercise. However, I think Priyank can tell more about the ARL model that he worked on.  



For the social rule mining we have taken historical transaction data set that we had for all our orders that would ben then it was like standard machine learning process, like splitting the data into training and validation using a priority algorithm to mine association rules from both the sets and validating it on the validation data set. If it seems that the support and the confidence of those rules are high then we are adding the predicted rules onto our data set and this repetting of the model happens weekly and on a daily level. We check all the predictions that were made whether the products are available in our inventory or not. Based on that, the people also bought section is filled on the app.  



I see. Okay.  



So if these models are kind of getting updated daily or weekly, priyanka, where is this job running like the model training job itself running. Is this on a collab notebook? Is this on a Jupyter instance?  



It's on instance and GCP.  



Okay, so you have both AWS and GCP.  



Yeah.  



Interesting.  



So right now your team is actually using both the clouds.  



We are by migrating from AWS to GCP. So one of the model data creation pipeline would be on AWS which is still running and we'll have to migrate. But the model which calibrates is done on Airflow every week. So you have an airflow job which is done every day which predicts and populates the data into the database. But then there's a weekly calibration exercise that we do on airflow. So there are like two jobs, weekly job and a daily job.  



Understood. Okay, understood.  



And in this part so you said that this runs on like an easy two machine or something. So is this EC two machine like always up and running?  



Yeah.  



I see.  



And this is used only for back pendo jobs?  



Yes.  



Got it.  



I see.  



Do you know if typically how much percentage of the time these jobs running today?  



What happens is we have like few jobs which are running on the same instance but at a different time intervals. So I would say we run around ten to twelve times in a day. But that jobs only run for like five to ten minutes. So I would say around 2 hours is the running time of instance over a period of 24 hours, which is not a great utilization. But these are all like micro to medium instances.  



I see. Okay.  



So the data sep itself is not very large.  



Yes. For the data that we basically populate into the data, it's just like a one day data. So it doesn't take a lot of space on that easy CTO machine. So we are using right now small and medium instances, but known that instances on that front.  



I see. Okay, understood.  



And by the way, one more question I guess. So you mentioned that there is no Real time API deployment currently, right?  



Yes.  



And there's no plan to do that in the immediate future. Basically that's not the concern.  



Yeah. Right now the problem is that even if we do it and the product is not ready to be able to configure to be configured, this is real time API will be deployed on recommendations and as well as some other product based models. Right. For like probability scoring. We are just thinking of bad prediction every day. But like real time is more on recommendations and as well as the real time. If were to go and delivery expected models expected to deliver, then in that case but we use product kind of reading. So what we have decided right as of now is like one day is enough for them to easily use it.  



Okay.  



Other question is Shayanski monitoring or experiment tracking tools?  



Experiment tracking. We use amplitude so, as Piano mentioned, so there are like two metrics. One metric is that data datta science metric that we look and there is a business metric, right? So the business metric for any section is RBB able to increase the AOE and the SKU bit of that particular section as well. That is done on the that is done implicitly and explicitly. On amplitude, we track conversion compared to some other version of the model, more or less like Harika models are almost events and trackers. And then business matrix we are able to track on amplitude. But data science matrix is something that we do it on jupiter notebooks.  



Take care.  



Got it. And now in this entire pipeline of training retraining like amplitude based tracking, jupiter notebook based analysis, where are you all seeing any pain points? I would love to understand that in this entire process.  



Also one pain point is your interpretation. So I'm getting this number, right? So how do I inderjeet it like what actionables do I come think about to improve them? So that is 1 second is how do you basically mix two metrics, business metrics with a data science metric? If both of them are going on opposite direction, there will be few cases when it go opposite direction where datta center because app user base or some other behavior that you have not accounted for. So that is 1 second is like I think tandem. Basically, if you are able to figure out things in one place, that is like great, right?  



Figure out what kind of things in.  



One place they are like two metrics, right? So business scale is a business matrix important, right? And the DS model should be considered to play around with business. But at the same time for me, the DS matrix is more important. I want CTO reduce the variance, I want to increase the classification. Now what happens is these are all separate areas. How do you exempt together? That is 1 second is. However, it depends on person to person and company to company, right?  



What we have done is we knew amplitude is a great tool for product analytics and product management and we wanted to use it for both DS based algorithms and we are able to do that and say like rough tracking, which is necessary for right now for the data science team that we have, which is like on an average, are we doing good or bad? And if we are doing good, then we don't calibrate and don't go into the details of the S, right? But that's not the ideal way of doing things. The ideal way of doing things is like looking all of them together, then taking a call. Okay, information and what do I make out to make out of this information?  



Got it? Okay, so muthu again, anything that you.  



All would like to add here? Simon Priyanka, Sudhir, in terms of challenges that you are facing.  



I think they.  



Solved most of the challenges that we face apart from it in the ERL model. So the challenge is more on the implementation side on GCP, where the code that we have requires a lot of memory usage. Optimization is required for that.  



I see.  



Which might be true with a lot of the modeling work that you're doing, like more CPU or whatever, more memory usage, essentially, depending on the type of model that you are building.  



I see. Okay.  



Yeah.  



Understood.  



And how are these machines allocated to you all? Like, the EC, two machines that you mentioned where the things are running. Is there a DevOps team that allocates certain machines? What happens if you need to increase the size of the machine? Decrease the size of the team machine? How does that all that happen?  



We are only the DevOps people. We see that after three A and between office B will have a DevOps team which will be dedicated and at the same time optimizing on the resources that we are using. However, right now what we are doing is like figuring it ourselves and doing it ourselves. That is something that or taking the help of existing developers from the tech team.  



Got it.  



I see.  



So at a high level, what are you all trying to optimize for? Like, are you all trying to optimize for different teams? CTO optimize different things, right? Like Amar HotTime could block an Gmail and I want to change that. Some people are like, all I care about is my model should be more and more accurate. Some people are like, oh, I want to cover more am I use sales, but I'm not able to cover that. What is it that you are optimizing for?  



I think third use cases are like so we are not trying to optimize for accuracy right now because existing so whatever we will be building will be helping. Based on version A, what difference are there? Because data science and without data science, you know, there's a considerable amount of datta difference, but version one and version 2 may incremental changes, means maybe like 10% or 20%, but not like 100%. So we are solving for that zero to one journey right now for each and every use case, rather than solving for one to ten.  



And I see.  



So what limits you from being able to try out more use cases, I guess. Where are you ending up spending a lot of your time that was saved? You'd have more spoke to do stuff, basically. What are you working on that you don't want to work on? I think that's the main question.  



I'll shy away from this because Mira calendar actually meetings, so I only get like night time. But I think Priang and Simon might have something to talk about.  



You're both on mute, though.  



Yeah.  



So I think one of the major problems that we have seen, it's not a data center, it's not. The problem that you would have. How do you make it easy for the end teams to adopt? You adopt your models. That is first and second is negotiation, right? That's like a handshake right now that handshake could be driven through dialogue and communication. But then third is like how do you make sure that you want a model output up? You are able to make the other person understand that this is the change that I want to bring in. So that is where a lot of engineering and mind focus is output account so that the business is optimized and takes care of your inputs. So that is where a lot of time goes.  



However, where it not goes right now is on the engineering side of things. Right now the engineering deploy is something that we do in tandem. So our microscomptounces then we ask like we do paralyze of pluralization of tasks, this integration of tasks. But on the that is where like a major chunk of our time goes which is like how do you explain a particular model to a person which has no information, no background and at the same time able to provide a usability and you can forecast the business value out of it model? That's a major question right now.  



Sorry, go ahead.  



You were saying now how do you.  



Disintegrate it like you can tie data science matrix to the actual revenue outputs and I think that is like hard for everyone to what did you do? I think Facebook you have to use AI and ML, right? But in traditional businesses it's still a quantum leap where you want to drive that forward. At the same time it's like comfort space which you have to move the other person from.  



Makes sense actually.  



Like I've experienced this problem as a reflection. Also key data science team is trying to track whatever log loss or Ndcg metrics and stuff like that. And the business team, they are like I care about my business metrics, what's the click through rate on my product, what's the conversion rate on my product and stuff like that basically. So we used to have this constant battery like what is it that we should really be tracking and stuff so completely relate with the product problem. So basically I'm gathering key problem around metric tracking amplitude and jupyter notebook. That's where a little bit of a manual work is there. And what's the problem around how do.  



You make.  



The data science and the business metrics kind of cohabit basically co exist in some way, right?  



And these I think will come after April is like how do you optimize on the cloud cost, right? We were a part of GCP Explorator program CTO kind of be Arun free trial for the next one year. So we can focus near, right? Once we start billing, TCP starts billing us. Everybody's focus would be on how do you optimize on each and every resources that we use on CCP.  



I see, understood. Okay, got it.  



So I'll give you a little bit of a background. This is very helpful, by the way, Sharla, thanks a lot for sharing this context. I'll give a little bit of a background and I'll tell you where I think we could IBM hearing where you could potentially help. So number one, that actually most of the problems that you described are not within the scope of proof truefoundry current insurance. So that's like the short answer. One thing that is within the scope is the cost that you mentioned. Because I can see that currently the way you described that you have these two machines running and you have relatively less utilization, less fraction utilization of the machine.  



Those are things that like a troponic can really help because the way we design our jobs is, let's say if you have a two minute job, it literally spins up a machine, runs the job for two minutes and then kills the machine. Basically. That's how our jobs work. So you can truly optimize for that cost. And you could technically be like spinning up a job directly from your local machine on a remote cluster. It will spin up the cluster on the job, kill the cluster. And this also becomes very interesting if you want to do something in parallel. So for example, if you were doing for experiments, you want to do hyperparameter tuning, right? Instead of running the 16 experiments that you want sequentially, actually just launch out 16 containers, get the results and get done with it, basically.  



So there's a lot of "
9108693377,FutureFit,Ria Bhargava (ria.bhargava@futurefit.ai),,FutureFit AI,<50,<10 Mn,<10,,No,Software Development,Canada,Nikunj Bajaj,FutureFit_Ria 01-06-2022,,"Labor market information- job posting,

NER- scales, location, job titles,

We do use a bunch of ML models- sklearn

We also have recommender systems- end up using a bunch of libraries.

AWS cloud based company, setting up Sagemaker instances

Mostly batch cases- eventally live model in the future.

Sagemaker- mostly model training

Will like to version control of your model

Big divide in future fit between engineering and data- i come from actuarial background. Translation from heres the model we have trained. Always gets to how do we hand tha off to engineering.

For now, the model we have pushed through has been trained in silo- when we have it ready- explain what that model wrapped in an API. I have not been involved in that.

Us training our model in silos.

Next Step: Set up a live demo."
8991766346,AlphaSense,Raj Neervannan (rneervannan@gmail.com);Prashant Budania (pbudania@alpha-sense.com),,AlphaSense,>1000,50-100 Mn,<10,AWS,No,IT Services and IT Consulting,USA,Anuraag Gutgutia,AlphaSense_Raj 27-05-2022,,"2nd Call with PENG

SPELL: Highest used feature => Was there an option to have a Jupyter Notebook environment spinning up? WorkSpace could start with GPU instead of TPU.

Within Spell, there was a resources tab => for every run, there would be a training file and an input file. And then there could be some output tab. There was also a section for Uploads => multiple teams are working on search analytics. You could dump a lot of search data and you could just mount it to the experiment that you want to do or Jupiter notebook you wanted to see. Q FROM ABHISHEK: Did they mounted the AWS S3 path or you upload the file again? https://spell.ml/blog/how-spell-mounts-work-YLfQARQAACEAXZVa

PENG: Would like to also see the inside as to how it is done. WE would need more functionality here than what I see currently. COULD we make the data shareable across different projects?

Experiment Tracking: Can do multiple experiments and do it there?

We also want to use GPU for TPU training. Deployment - want to have some insights and transparency. We should be able to manage the scaling! We dont know but it has to be tied to some external application.

Streamlit demos we support more customisable functions."
8991766346,AlphaSense,Raj Neervannan (rneervannan@gmail.com);Prashant Budania (pbudania@alpha-sense.com),,AlphaSense,>1000,50-100 Mn,<10,AWS,No,IT Services and IT Consulting,USA,Anuraag Gutgutia,AlphaSense_Raj 20-05-2022,,"1st Call with Raj 20-05-2022

BLUF: Connected to his ML Team

Introduce to director Peng => Email suggestions => ML of Google. He built the auto-suggestion (GOT him a couple of years back)

Prashant: Early ML Engineer. More in depth perspective

Take: PainPoints are in having built a model, how do we make it easily available in all the content. We built a search engine quite differentiated from Google. BUILT a graph database.

Ask to be a leader/advisor etc. Google uses info to rank Million searches. Most knowledge professionals - it doesnt work.

Initially: Taking an existing search engine and make it work. TFID of basic indexing was being done. Positional laser guided shooting: Controversial ideas. Took every line and embedded it with semantics.

Eric Schidmt and all invested in the company. SEC Filling and etc => AI algos and make it work with one content types. Shrimps => Expert insights. 1000s of podcasts. AI Algorithms could come into your context, take expert insights that we can get. We didnt have benefit of user queries. (HEDGE FUND USERS , CORPORATE USERS, INVESTMENT M&A )

Worked with Surger - his sentiment was very very good. Data is data. Lots of mini deep surgical tools. We use this tool called : Consolidated all of them in GCP. AWSs vision to AI was not very good. (Kubernetes) // MLFlow - Databricks is used // a lot of tools.

ALPHASENSE: We are right that we are a start-up (180Mn $) /// Willing to try the platform and dont want to break what they have. DEVOPS team will be fighting tooth and nail. AI Infra and DevOps => We use all the things. SCALABILITY issues are from making the model etc. (Peng and Prashant - how much infra => want to make the tool work for multiple clouds) ===> AWS and GCP only. Depending on content set - it works. Gone with a partnership with GCP Marketplace. Managed by Anthos in Cloud centric. ITS MULTI CLOUD BASED POSITIONING

FOUNDER TO FOUNDER THING: Understand where it is going. Have done 2 other start-ups. Take money who actually use the Product. I will => not do in the next 6 months. Have few other things. Play the advisor role.

POSITIONING: On to a problem where people are going to have infra problem. Whats the exact value add? What is the alpha for your infrastructure problem. (showcase the positioning) WHY IS IT DIFFERENT FROM AN ASSISTED SERVICE ///// I Have heard of competition => Here is the 2 other options out there and this is how we are differentiating ///// BUYER PERSONA: DevOps, ML Practitioners, Directors of Research ( What is the value prop for a Data Scientist or Researcher, What is the value prop for ML Engineer? , Value Prop for a DevOps) ==> Director, DevOps, Res. Engineer. CLOUD shouldnt come on the way. Product positioning : Product, Assisted Service, How is it - Cheaper substitute or a cobbled up service? (BE IN MARKETPLACES - AWS is so friendly and people think its a junk. One among many ), AZURE is friendly."
8988169640,LoadSmart,Rodrigo Senra (rodrigo.senra@loadsmart.com),,Loadsmart,500-1000,100 - 500 Mn,Oct-25,AWS,,Truck Transportation,USA,Nikunj Bajaj,LoadSmart_Rodrigo 01-07-2022,,"Date of Call: 1st July

* Lower cost of APIs. They have a cheaper crude single end point concept ==> Data Science do not need to be backend engineers. Want a way to Productize some analytics. VOLA - reporting tool on top of jupyter notebooks. Still a Jupyter Notebook. Don't have that as a service

* Data Governance

* When you call deploy, how do you know the service is up and running? Is it synchronous or asynchronous Disadvantage for us: Is we are 70% of what you have.

Really interested in is the Automatic wrapping from Jupyter to FastAPI Container. Right now, you are priortizing control in the Jupyter notebook. Have you considered using the Cell magic support so that it can hide bit of the boiler plate. Offering both would be an interesting pitch.

Usage and Traction: We can give you images that you deploy on your cloud. Don't see before Q3 ==> Any clearance to buy software.

MID Q3: Defining the OKRs ==> As soon as I see the ML Team ====> We can do a SandBox. We can do a Paid experiment.

Major Follow-ups:

* Try out in August - will be difficult before that

* Take a project and get free usage for the team with installation (Okay with Public Cloud) ==> We can do a sandbox or a Paid Experiment"
8982840194,Inspektlabs,Sanchit . (sanchit@cvision-ai.com),,Inspektlabs,<50,<10 Mn,<10,,No,Software Development,USA,Anuraag Gutgutia,,,
8982839675,HyperVerge,Anurag Malyala (anurag@hyperverge.co);Aravinth Muthu (aravinth.muthu@hyperverge.co),,HyperVerge,50-100,10-50 Mn,Oct-25,AWS,Yes,Software Development,USA,Anuraag Gutgutia,,HyperVerge_Anurag 11-07-2022,
8967332380,Clear.Co,Satwik Seshasai (satwik@clear.co),,,100-500,50-100 Mn,<10,AWS,Yes,"Technology, Information and Internet",Canada,Nikunj Bajaj,Clear.co_Satwik 09-06-2022,,"- VISA issues and all.

- Moved in 2019. Travel to India once a year.

- I started out in 2015- RoR developer. Raised in Pune. Worked in tech consulting as well. Helping them modernise their data platform.

- JustEat, GrubHub, SkiptheDishes -- all same company. Interest in DevOps and helped Skip with MLOPs. Spent 2 years at Skip and now building ML platform

- A lot of our algorithms focus on time series forecasting, personalisation, some basic NLP to augment Sales & Marketing.

- We have some risk grading models which are classification models.

- ML Platforms looks to solve broad breadth of algorithms. Considering fintech, there is a gatekeeping on model. This is not a good developer experience.

- We are now building out our experimentation framework. Model performing comparably. We want to know that we are not shipping rubbish. Adding a layer of explainability. Mostly for some external stakeholders that we have. How our predictions come in handy.

- Our experimentation platform runs otherway round and wants to serve the needs of senior executives.

- We have different flows for PII and non-PII data. Former goes through a different process and we manually analyse the dataset. We try to add feature profiles - say great expectations. Basic metrics about dataset. How many Nulls are there- discrete, continuous etc.

- Centered our orchestration engine for Kubeflow. Model Serving we are using custom model servers- flask / FastAPI. We have seen the limitations of that on Canary. Was looking into SeldonCore before we jumped on our call. Looking into explainability. Leveraging model serving as well.

- Neptune.ai / WandB for experiment tracking. We dont want to justify for the cost. We will spit out the result of the data platform. WandB was $400 / month / user. Stupid price point for them to quote me. Scale we are at and experiment that we run - we would have had $0.5M. Neptune is $20k which is overhead of maintaining the infra. Neptune has fewer capabilities than WandB. The rationale was build out a data platform and enrich its capabilities. I will anyways put stuff on Google Docs for Satwik.

- Make the data scientists self suffiicient is our goal.

- Will try to set up a follow up call to show monitoring and get feedbakc next week. Akshay said will get back.

- Wanted support for forecasting models and multi-model ensemble support."
8928171817,Turing.com,Kai Du (kai.du@turing.com),,Turing,>1000,50-100 Mn,>50,GCP,Yes,IT Services and IT Consulting,USA,Nikunj Bajaj,,Turing_Kai 15-09-2022,
8928171817,Turing.com,Kai Du (kai.du@turing.com),,Turing,>1000,50-100 Mn,>50,GCP,Yes,IT Services and IT Consulting,USA,Nikunj Bajaj,Turing_Kai 17-05-2022,Turing_Kai 17-05-2022,"1. We are mostly remote.

2. Mostly working on data and ML platform. Still figuring out whats the best way.

3. We are heavily on GCP. Using BigQuery and all and we are always looking for stuff.

4. Right now Turing is in a phase where we are actively trying to build out ML systems. Data and field search ranking systems. We have quite a few use cases but search ranking is biggest. We try to hire people and want to have the python developers.

5. In terms of models we curated and team work and access and some of them is history of engagement with Turing and work. A lot of search ranking system. Much simpler than the eCommerce one.

6. It's also very difficult but very small part of the whole process. We still use the final metrics. This is a right set of development for you. Interview and hiring - we talk about ML systems. Basically, data pipeline we just do everything ourselves.

7. Right now we try to use some data fetching but is kind of manual. Our data volume is not that large.

8. We dont have a very active monitoring on top of it.

9. How to explain the model outcome. Clients want to understand why something came up. We are trying shap values. This is still not mature status.

10. How we can make sure this online system works as expected. We feel this value may not be correct. If one of the feature breaks, we haven't captured the importance of this feature.

11. We try to do something more common as well. If we can explain our model output better, that would be amazing. Our clients are not technical. They dont have a good way to understand.

12. Everything on Google Cloud. We use some features of VertexAI but we feel this is too new. We are not liking everything on Vertex AI.

13. Arize or Fiddler only focuses one part of the platform. When you really want to do something better.

14. We use feature store on Vertex AI.

15. Our case we are serving the Data Scientists and they have very limited knowledge of how to deploy features. Need to make them more productive.

16. Cross cloud is a strong value add.

17. Live demo to be set up. Maybe 2 weeks from now."
8906409473,Cellinobio,Ozge Whiting (owhiting@cellinobio.com),,CELLINO Copy,<50,<10 Mn,<10,,,Biotechnology Research,USA,Nikunj Bajaj,Cellinobio_Ozge 14-05-2022,,"Interested in seeing more details on deployment

Want to see an example of image data

Predictor image vs human annotated one

Another hour long session would be good for us

Design partner could be useful

Generic MNist

If the next call goes well we will dive deeper into what we are building

Next call keep it generic and then we can sign an NDA.

Ask more questions about the deployment side of things.

As a matter of first impression we want to talk about more.

Ozge might join as well. EVeryone is interested."
8886416703,CrowdForce,Ridwan Abdusalam (ridwan@crowdforce.io),,CrowdForce,<50,10-50 Mn,<10,,,IT Services and IT Consulting,USA,Anuraag Gutgutia,,CrowdForce_Ridwan 17-05-2022,
8871402132,attention.tech,Matthias Wickenburg (matthias@attention.tech),,Attention,<50,,<10,,,Advertising Services,Australia,Nikunj Bajaj,https://app.hubspot.com/contacts/20848121/deal/8871402132/?engagement=21670906222,Attention.tech_Matthias 16-06-2022,
8870765466,PrivateAI,Anandh Konar (anandh@ualberta.ca),,Private-Ai,<50,<10 Mn,<10,,,Software Development,Canada,Nikunj Bajaj,PrivateAI_Anandh 18-05-2022,,"Call 2

- Model accuracy and model version of the deployed model.

- When we are about to release the model, we update

- The point is do we need it- we have come to a conclusion that we will use it.

- Right now we are thinking to host it

- You are a seed stage company. Your data analytics feature is not useful to me.

- Hosting service is another headache but backend will host it and once you set it up

- anandh@private-ai.com

If you put a huggingFace example and how this can be done. If we can create a docker container of the model is it easy to do it?

Slack message on run complete

Send a jupyter notebook with an NLP example so it is more relevant."
8870765466,PrivateAI,Anandh Konar (anandh@ualberta.ca),,Private-Ai,<50,<10 Mn,<10,,,Software Development,Canada,Nikunj Bajaj,PrivateAI_Anandh 10-05-2022,PrivateAI_Anandh 10-05-2022,"- Deploy models using docker on the clients on prem

- We are exploring to use MLFlow.

- Didnt understand the difference over there especially when the use case is NLP

- Docker images

- We build models and sell the model- product based company

- Sometimes clients give us more dataset- training on those dataset.

- NLP based models

- They give an API key and they only send us how many calls are they making

- Over the releases performance goes down- we dont know how to monitor that. If the performance is going that.

- They have their own sample set which they validate over time

- Twilio data itself is changing

- Docker we just release the image."
8869449064,kcc.com,Avinash Manure (avinash.manure@kcc.com),,Kimberly-Clark,<50,>1B,25-50,AWS,Yes,Advertising Services,USA,Nikunj Bajaj,Kcc_Avinash 12-05-2022,Kcc_Avinash 12-05-2022,"Went over the Platform Live Demo.

KCC wants to try it out first.

and to share the login credentials for the Experiment Tracking piece of the Product.

to share the login credentials and free-tier version of the same."
8869449064,kcc.com,Avinash Manure (avinash.manure@kcc.com),,Kimberly-Clark,<50,>1B,25-50,AWS,Yes,Advertising Services,USA,Anuraag Gutgutia,Kcc_Avinash 10-05-2022,Kcc_Avinash 10-05-2022,"- Been in the data domain for 10+ years

- Started as a data analyst in Symantek- was always interested in Data

- Analyst, BI, Last role was Publicis Sapient- manager of DS

- Recently (Dec 2021) joined KCC

- Have been hired with one goal to operationalise 66+ models in production and show value

- It will soon become 100+ or 500+. We are trying to work with someone who can help us show this value to stakeholders

- Use cases are domains like forecasting, regression classification ones.

- Churn use case where we want to achieve who is likely to churn in terms of our customers- whole-sellers, middlemen, take proactive measures to prevent that

- Sometimes we need to do demand planning. How much unit of Huggies might be in demand.

- What do you use for experiment tracking

- We are new, trying to explore a lot of models. We are not even 10% there. Most of them are in production, some in production.

- We have Azure (almost 98% models deployed there). Few deployed on VMs or specific machines within factory. If I talk about the process- it is a straight forward one.

- There is a DS, MLE who take care of building the model. Everyone uses Databricks within Azure. Model objects are saved as Azure blob- they have started using MLFLow for experiment tracking and artifact storage.

- Now we are also migrating to Snowflake for any relational data

- Most of them are offline or batch predictions. Hardly 1-2 use cases are realtime ones.

- We don't use Kubernetes so far.

- Overall goal for us is to show the value of the model. It was able to show the business value of churn - help connect the gap between how the model is genrating business value

"
8823797373,G2,Chirasmita Mallick (chirasmita.mallick@gmail.com),,G2,500-1000,100 - 500 Mn,Oct-25,AWS,,"Technology, Information and Internet",USA,Anuraag Gutgutia,G2_Chirasmita 29-07-2022,G2_Chirasmita 29-07-2022,"29-07-2022

Brief background (if discussed)

Past context from G2 Calls there. Using this to get a progress update on how they have progressed on this.

Use Cases for ML - types of models (Is monitoring important etc?)

These are already discussed in the past.

Current Stack for ML Deployments and pipeline

Progress in ML Pipeline from last time we spoke

Problems being faced where looking for solutions

""Progress from last time:

* From the last time we met, not much has changed internally => Still deployment is using AWS Data Pipeline.

* Stakeholder Mgmt and Experiment Tracking side: Lot of mgmt => consistent feedback. Don't know exactly what each person is doing.

* From a deployment side, since we don't have a lot of models, it is not a problem much. Gradio and StreamLit interaction. ML Team is still using AWS Data Pipeline => there was a delay as many folks were on sabattical. ""

Questions asked wrt Product

""Mgr is not hands on in Data Science but good at setting up metrics etc.

It becomes difficult to manage all the projects => Log files don't help him much => Tracking the experiments help him.

>> FIGMA Design - we have done. We could have something similar. Track all the experiments.

All our Recommendation models are based on Language Models -> what does Hugging Face mean?

MLFLow : Siloed activity that we are doing => Using it locally => Plan is to deploy it in G2. ""

Feedback wrt Product - what would make them possibly adopt it

""One thing MLFlow doesn't have is Role Based access Control.

How is it different from MLFlow?

* Not everything can be handled by the framework => whatever framework magic you can provide.

* It becomes very difficult to understand.

Current ML Team is very small: Experiment issues + Stakeholder issues.

What kind of drift do we do in terms of NLP Models today? Just basic data drift.

* We can visualize nouns etc. and Docan - Annotation platform ""

Concrete Next Steps

""SEND OUT THE PRESENTATION - For sending to VP

August 15th -- Create a case. Its a tough thing to convince Senior Mgmt.

One Suggestion:

* If we can include Monitoring in the Presentation => if there is a data of certain distribution and if we move away from that distribution. If we go into categorizing shirts into more shirts => How will the models overall behaviour in categorizing changing?
"
8823797373,G2,Chirasmita Mallick (chirasmita.mallick@gmail.com),,G2,500-1000,100 - 500 Mn,Oct-25,AWS,,"Technology, Information and Internet",USA,Anuraag Gutgutia,G2_Chirasmita 06-05-2022,G2_Chirasmita 06-05-2022,"1. Intros

2. Started in DS by accident.

3. It was analytics domain. IBM was launching big data.

4. I started learning recommendation systems.

5. Post that I worked at Gartner. My roles have been very different from each other.

6. Very heavy on DL Inra part of things.

7. I have never worked at startups and G2 has worked because they have data from all categroeis.

8. They are oricing SMB, Midmarkt and Enterpruse very differently.

9. We saw 7000% MoM increase in traffic. We never thought building for them and we realised we have a huge opprotunity, sending out reports.

10. My work is heavy on NLP. When we started we were 6 people, there are not one templatised way. We were doing things in silos.

11. When we started thinking about system design to make DS workflow easier.

12. One of the plan is to move from Airflow to Dagster.

13. From a stakeholder standpoint, they dont know what is happening.

14. The entire team we are trying to bridge that gap.

15. Some system for me to give a hard number and why we should take a certain action involved.

16. We use CircleCI which pushes things to AWS Data pipelines.

17. Once this process is deployed.

18. Data collected is through Snowflake. DBT labs trying.

19. Data processing is happening through Snowflake itself.

20. Software seller, software buyer, Investor solutions - these are 3 main

Most ML things are happening

21. Linkedin profile based monitoring.

22. Manual approvals

23. Had connected with Baseten. That time we"
8817285983,Elpha Secure,Amir Raza (amir9ume@gmail.com),,,<50,,<10,GCP,Yes,Insurance,USA,Nikunj Bajaj,Elpha Secure_Amir 05-05-2022,,"1. Learnt a lot of practical stuff during Masters.

2. Roorkee we

3. How to do experiments - combined some application of application development. Timestamp, which machine.

4. What we dont have currently - deployment on Kubernetes.

5. Model parameters for Redis, cron job for retraining. Data is elastic- someone has to pick their data. Looking for DAGs basically.

6. Model inference is in real time. RabbitMQ which does message passing. They are real time- we are not planning any real time updates.

7. Once every few weeks we trigger for offline training.

8. We dont do MicroService for models. MS won't allow you the feedback cycle.

9. For experiment tracking, we tried TensorBoard. WandB is expensive. Right now, experiment tracking is folder structure.

10. Optuna for hyperpparameter tuning.

11. For inference monitoring, we use Grafana for systems. We dont have for drift montioring. We are yet to figure out.

12. For model inference, we have some R&D going on for CI/CD. Someone has written a script for container. It can be improved further. We are working towards adding some testing."
8815275563,nRoad,Hrishikesh Pathak (hrajpathak@gmail.com),,Nroadcorp,<50,<10 Mn,<10,,,IT Services and IT Consulting,USA,Anuraag Gutgutia,,nRoad_Hrishi 10-04-2022,
8814139237,ShopUp,Navaneetha Krishnan J (navaneeth@shopup.org);Muthuselvam Selvaraj (muthuselvam.s@shopup.org),,ShopUp,500-1000,50-100 Mn,<10,AZure,No,"Technology, Information and Internet",Bangladesh,Anuraag Gutgutia,ShopUp_Navaneetha 26-09-2022,ShopUp_Navaneetha 26-09-2022,"2nd call - 26 Sept, 2022

Brief background (if discussed)

Use cases : We repriortised. Focus is on reporting and Business Intelligence. Did deployment for models in Text recognition.

We revamped some of the busienss Operations.

Focussing on getting the things done - see need for Location based prediction. Start with BI and then go to building Models.

Central Data team - work on. Citizen data scientists.Current focus is on dashboards and BI. Trying to get some of the data that we might need for models. Customer data etc.

Use Cases for ML - types of models (Is monitoring important etc?)

Stack remains the same - GCP over Kubernetes.

Application side - MySQL DB / MongoDB as well, GCP - Kubernetes (Central data warehouse) , Using BigQuery as primary data base, Use MetaBase for Dashboarding. For deployment, we do batch deployment on Kubernetes. Text AI and Custom models.

VertexAI is used to build models. Why not use Vertex AI for hosting models. We have a Production cluster and hence it is easier. Kubernetes is much easier.

We are using HevoData for most of the Pipelines. For data warehouse, we use schedule before usign Big Query.

Many platforms are targeted for large teams. Both from cost and operations perspective! Makes sense for a bit larger teams.

Current Stack for ML Deployments and pipeline

Mostly use cases will take time to evolve - Doing only based on Need basis. Some of the Products are evolving and still evolving new features.

We are taking any critical need. Otherwise focussed on data Ops.

In 3-4 months, wil see more traction on the DS side.

BUSINESS: Don't understand the use cases. Being ops driven business. Constant thing between Business and tech.

A number of companies invest in Platform => 9 months. ML Engineers - start building the platform themselves.

When data collection is itself becoming an issue => Basic Data Platform or ML Platform.

Problems being faced where looking for solutions

A little bit about the Deployment Stack: Mohan and team currently working on the deployment side as well.

We have some ML Engineers - kind of data analysts trying to learn ML.

There, some of the data tools we are trying to bring up to speed.

Till now, our idea is to use off-the shelf ML Models with our customization. Next step - identifying a lot of use cases, pick up models and then start using them. DEVOPS Team - Core thing is in kubernetes.

There is a centralised DevOps team that handles that. We have mainly 3 frameworks - Go, Ruby, Node. All frameworks have been set-up.

When it comes to ML or so, its collaborative etc. SRE Team - collaborative thing => JIRA task etc.

Questions asked wrt Product

* Could we use it for any deployments or is it generic to Python? (Service, Job, Model => If you select a model, library of models we will keep adding)

*What do I need to provide to deploy a Model and create a service on top of it?

* How is the cost controlled for the Infra?"
8814139237,ShopUp,Navaneetha Krishnan J (navaneeth@shopup.org);Muthuselvam Selvaraj (muthuselvam.s@shopup.org),,ShopUp,500-1000,50-100 Mn,<10,AZure,No,"Technology, Information and Internet",Bangladesh,Anuraag Gutgutia,ShopUp_Navaneetha 17-05-2022,,"Discussion with ShopUp: 17-05-2022

Dataiku and DataRobot : I didnt get much context 

Currently, doing a few ML Projects: Using GCP OR in Laptops. 

Currently, based on mostly what GCP Offers. Mohan - lead the Data Science projects at ShopUp. Muthu - leads the overall data of Data Platform. 

What kind of Models? Currently,focussed: 1) Core Business (Sales happening and also have a logistics platform) + manages logistics for us and external. 2) Kind of Sales and demand forecasting. ==> Only joined a few months back. Trying to build the team here. Other side of the use cases: Operational side (Invoice scanning and document scanning) =.> They are not tech savvy. 

Currently, entire data team is 10 members. Pipeline: GCP? We tried DataRobot - found some challenges with that. Didnt put it in Production. It is not well integrated with other Pods. Experiment pods is sitting outside. 

Environment creation, User Mgmt, Package creation. Etc. Some of the tools. ClearML, Converge : Simplify the deployment. They would be easily. 

Muthu: * Flexibility => How much tools are helping up speed the entire process. There is a huge expectation from the business. * Product stack is on Kubernetes => DevOps * Anything for Monitoring? => Couple of tools. Moving from one to other. NewRelic // Mostly batch use cases. "
8811616336,Pencil,Sumukh Avadhani (sumukh@trypencil.com),,Pencil AI,<50,<10 Mn,<10,,,"Technology, Information and Internet",Singapore,Nikunj Bajaj,Pencil_Sumukh 21-07-2022,Pencil_Sumukh 21-07-2022,"Use Cases for ML - types of models (Is monitoring important etc?)

SaaS platform => Companies can geenrate content using AI Algos. Generative kind of models - not a GAN. Very hard to get perfect fidelity.

Multiple AI Models which do different things. We don't scale very well: 1) Narrativ Model - what's the narrative telling to users. 2) What kind of content should go into what sequence

Current Stack for ML Deployments and pipeline

Predictive system => The ad which the system has proposed : Is it going to be a winner?

Off the shelf solutions.

Even in these, its not 1 Uber Model. There is a model from our side which works from our side. Lots of decisions coming from different things.

AI Pipeline: Small and niche: 1) Pytorch 2) Tight control on what kind of models we develop. 3) Have own way of versioning the models and things. 4) How do you deploy? AWS mostly but most solutions are not asynchronous. Its not a HTTP Kind of time frame or response time. Job is created => job runs, soluton space exploration. Simple EC2 machines through Kubernetes.

Is it mostly batch inference on the models: Actors come on what kind of space we explore. Models give decisons on discrete points on the space. Do a guided generation. Push the models towards certain areas in the space.

Training is offline, GPUs in office. System itself has a large re-inforcement loop. Re-inforcement learning at a higher level.

** A lot of things we have done is manual - everything is not automated. Collectively brainstorm => we compare -> etc. What I see imissing is DATA VERSIONING For reproducibility.

Problems being faced where looking for solutions

* Where do you host the model and load it? Model stored and hosted in S3. By the time model is laoded, it seems the model is not responding to health checks. S3 becomes too slow to pull the model and load it from there. What we have done is we use EFS and mount directly on to the machines. And then you have to do a lot of jugglery to get the model to be loaded. (HEALTH CHECK TIMING is customizable - AWS will not wait, AWS API Gate will crash)

* Inferencing Code: We could write anything in the inference code? There is a lot of pre-inference call that needs to be done. It runs through a pipeline of things and then take a decision.

Feedback wrt Product - what would make them possibly adopt it

* What I see imissing is DATA VERSIONING For reproducibility. How is the data changing etc? Need to connect the loop back => how do you connect the data, how do you loop back to the model.

* There seems to be a bias that every model is a classification model => Eg- generative and ranking models which are hard to fit. The thing is most teams tend to have logging and monitoring by themselves? If this can fit into existing workflow, then it becomes easy. When you have an issue, you don't want to be scrambling here and there.

* If you have issues, you don't want to be keeping track of service or model. Right now, the dashboard feels a bit of flat. If you want to be dealing with 4-5 projects within it, it might be easier to visualize it. And then again you are lost.

* When you say Service foUNDRY, or could you configure it to deploy in our own VPC? We can ship it as a Helm chart and you can put in your own Kubernetes cloud. Not grant public access etc. How do you do security? How would you configure those scenarios? If you are already using VPN, all endpoints are in your cluster. We are using ISTIO as a load balancer.

Concrete Next Steps

We are not a very good fit. Our Product has reached a stable changes - not looking for algo level changes. Integrations etc. Still a very small team => don't want to fix it.

As a founder to founder to have a clear idea of Pricing. Get a good handle of pricing => its a very powerful tool. We are going to start is 3rd week. ASK MY ENGINEER: Start using and deploy with it. Think of pricing or even Per User.

Can connect you to people who are more early stage - stable and haven't changed the generative model. One project coming up soon. Unified Prediction Model => Its a predicton model. What is your pricing on?

hAVE TO WORK WITH THE MODEL SIZE: We can work with the same. For training time, its a slightly sensitive data. Actual ad spend and money => touchy feel.

NEW WORK: Trying to pool across all data from FB Ads, split by sector etc. Cold Start Problem. Data is n number of frames. Waht will be interesting is to see => if it's Image data, how do you comparisons or visualize where is the issue or Outlier."
8811616336,Pencil,Sumukh Avadhani (sumukh@trypencil.com),,Pencil AI,<50,<10 Mn,<10,,,"Technology, Information and Internet",Singapore,Nikunj Bajaj,Pencil_Sumukh 29-04-2022,Pencil_Sumukh 29-04-2022,"Pattern recognition engineer, Amalgamation of multimedia. Statsitical signal processing, Machine Learning, Deep learning

Tangential direction, web development, backend.

At Pencil- culmination of different experiences

We have built a SaaS platform. Content for Ads platform for brands.

Directly integrated with Facebook.

Built a generative AI model. Not a single model - real world purposes. It basically in the ads space, details, nature of it is very exact.

It generates a narrative of how this sequence is generated. What angles it generates- it does a story generation. Outline of story. Images or videos used in sequence.

Underlying templating engine - Ai gives the story, outline and structure. Contemporary and output. We do a prediction on whether it will likely wokr. We overgenerate and pick the best to make sense for the user.

Based on their own past performance. They run the model, derive insights on what actually works.

We have been very scrappy- not mature. When I say our whole system is conclomeration of AI models. Off the shelf models available. When you upload a video, we need to transcribe. We dont use translator of our own. Our core expertise was a core natural language generation model. They have much bigger budgets.

We have some core models, in terms of narrative AI. We craft that into our own narrative. Different models have their own deployment- API calls. Some places we have used Clip. For our own models we use Pytorch and AWS.

We have to go and firefight that. We have 4 engineers on the algorithm side. Prediction is our own models which are standard.

In terms of model infrastructure we are not mature at all.

We have custom model loader at the start of the model. There is a redis that notes down which is the latest model.

In the code itself, we have things monitoring - our 75% accurate on inference. We have built dashboards where we manage the performance. We have separate models for each client. Every 2 day it autoretrains. Autodeploy model.

Couple of engineers who are backend, but very good as IaaC

Monitoring via metabase is really hard

I hate Sagemaker

Big painpoints is to track experiments. Most of the time data is in a common pool. Data warehouse. Experiments are run and track.

Not a training infrastructure. Track your experiments all the way from data. Have a common shared view - of all the common views. Datadrift is actually a big problem in production. Your actual production one has a different distribution. Nobody realises data drift. Many ML engineers are oblivious to it.

I do think there are other solutions available online which are solving MLOPs- its beyond deployment and scaling but also observing real world problems. I used to work on antiabuse infrastructure. Most of my time was not on ML Infrastructure.

Most time was spent in dipstick samples to see if model is still working correctly.

Secondary validation process. Keeping this updated in Google Sheet. I dont know if this fits in your

Changing existing things would be a bit of pain. For anything new I would love to use this platform. Would love to try out this for new model. We should sync up next week."
8811515315,Frame.ai,George Davis (george@frame.ai),,Frame,<50,<10 Mn,<10,,No,"Technology, Information and Internet",USA,Nikunj Bajaj,Frame.ai_George 29-04-2022,Frame.ai_George 29-04-2022,"Issue that companies are communicatiing in many more different ways with their customers. Lots of people working on structured data. Not nearly as much about unstructured data.

Every communciation platform like helpfesk, surveys, social, public communities, user interview platforms. Metadata associated with the conversation and we annotate that using Semantic Embedding model. We use those annotation to score for autoation. We can do things like  what things customer talk about before they churn. What kind of issues are customers facing at the time.

We are only 15 people, good set of logos- Nvidia, Autodesk. Series A. Rapid growth stage. Some applications are real time and others are weekly report. We do the inference in a streaming context.

Models are integrated  containerised service workers with postgress database. A typical worker that have picked up. Models are embedded in that workloop. All python code.

For primary Saas deployment we are on ECS. Customers we do on k8s.

Only self made tools. We have evaluated a couple of things. Tools like Prodigy for labeling tasks. Looked at DeepNote or Domino Data Labs. We have never evaluated MLOps platform. I have a head of data who made the decision. I dont recall. I dont think it is about MLOps functions. We were organising it as a way of sharing notebooks. We were able to solve for development and repo.

Not much of a trouble. The part of peipleine we worry about is traininig process. Custom models for clients. Thats why we evaluated prodigy for training platform. Allows the tool for active learning. Psuedo labeling and semi surervised labeling. 2-5 hours of laeling for producton quality accuracy. Very importantly we dont have a direct revenue relationshp. Accuracy is not actually. We hae to tune for specificyity and sentitivyty. For our application areas it is a reasonable solition.

That has not been a focus of ours.

Monitoring is not needed in the beginning.

We dont iterate fast enough with our models to deploy.

To get advantage of your deplyment- microservice architecture might not be clean.

Adopting a microservice too ealry is an antipattern.

Once you have lots oflots of organisations are much more likely to be working on a monolithic architectrue.

You dont have to have the biggest TAM in the beginning. Have a small niche that absolutely loves you.

Maybe people who are deploying their first model, you can help them. Maybe we would have designed our thing as a microservice. Building a developer community around.

Consider me as a resource. Set up some time with me in 2 weeks timeframe.
"
8680224891,The Yes,Amit Aggarwal (amit@theyes.com),,THE YES,50-100,<10 Mn,<10,,,"Technology, Information and Internet",USA,Nikunj Bajaj,The Yes_Amit 22-04-2022,The Yes_Amit 22-04-2022,"Highly personalized eCommerce 

For music - Spotify is highly personalized to you 

Dynamic experience to shopping 

We built as a marketplace- supply side we connected with brands for inventory. 

We have a big catalog because of a low touch model. 

A lot of our ML application is around personalisation. 

Use CV to extract information, use NLP to extract descriptions. We also do user activity based models. We are on GCP. Heavy consumer of Google AI tech. Moving over to Vertex AI. It does offer a lot of capability for training, model serving and infra. 

High level perspecive, my push has been to use Vertex AI. Build as little as we need for MLOps. Simple reason for that is thats not our differentiation. 

We have a single team and basically the philosophy is you own the proble e2e. 

Google has great tech. It might not be perfect today but I believe as we scale, they will keep building the platform and I see no reason to think about it. 

Lets say we were to replace Vertex AI with something else- I think the bar would be high. Its just hard. We have the same billing, commitment from Google. its going to be  a hard decision. Its going to be really hard to replace. 

I hear a lot of pitches in MLOPs platform, it is too broad- my reaction is we have Vertex AI- the right pitch is, here is a plugin in your Vertex AI, Sagemaker and do one thing really well. You really need to integrate with existing setup. Those platforms dont do this well. We will come and do this well. Can Vertex AI is better. Model serving is not a painpoint. Monitoring might be a painpoint. Versioning might be a painpoint. 

Google is going to be continue to build on top of it. If an engineer talked about model serving, 

At a high level, the pitch is attractive. It really depends on the value prop- whatever you will do, is it solving a painpoint. #2 is how much of an investment are you asking for? 

I think monitoring is a painpoint and its also very crowded. Lots of companies talk about it. If you land on the right painpoint, its an easy conversation. If that painpoint is not top of my list. 

Connect in a few weeks out. Maybe next month or so and that might give you some data. 

Narrow down on painpoint- pick one. There is a lot of companies in the space and you have to figure out how to differentiate. 

There is clearly a lot of opportunity here 

You need to have more of these conversations- after you pitch what you are doing, yeah I want that. If you build this, I want it. You could get a friendly design partners you need to collect a bit more data. There might be a big market need in monitoring and something. One interesting trend is why do you need engineers and devops to do these. You can sell to product people. "
8658240013,Kevala,Troy Hodges (tjhodges12@gmail.com),,Kevala,<50,<10 Mn,<10,AWS,Yes,Software Development,USA,Nikunj Bajaj,Kevala_Troy 19-04-2022,,"Grid prediction problems.

Our founder came from energy regulator world. And we have been lucky to get in PGE.

Time series modeling,

Basic ones use ARIMA, little bit of deep learning, some other combination of classical models

Mix of deployment types- some kind analysis and some other.

Platform in heroku apps. Instead of having models in their own servers, containerised etc.

Papertrail of models, we do some logging of results - another area we are wanting to ramp up with. Want to do data drift.

Vision is for same set of people to do both but DS handle when things break. DS hands over working code and they deploy it.

They are pretty well suited because they dont have good deployment or monitoring pipelines and have decent number of structured dataset ML probles.

We have set up a meeting mid-may with them to review with leadership team so we can mutually evaluate the design partner relationship."
8413504291,Mavenir,Pankaj Kenjale (pankaj.kenjale@mavenir.com),,Mavenir,>1000,500 - 1B,<10,AWS,Yes,Software Development,USA,Nikunj Bajaj,Mavenir_Pankaj 31-03-2022,,"Pankaj Kenjale

Shrinidhi 

Associate director for AI engineering. Manage R&D from Bangalore. 

We have conversational AI, time series forecasting models. 

We have 5G core products, - regression task, classification task etc. 

We have edge use cases using Computer Vision 

Service Assurance - which is TimeSeries Classification and regression 

Pankajs vertical is about 60-65 people. 

As a whole combination of researchers, data scientists, UI developers, backend- 40-50% are Data Science and ML engineers 

Pankaj 

Group VP 

Lead Data Engineering and DS. Mandate is to apply intelligence in all portfolio products. We have quite a large big data infra. 250+ telecomm providers. 

Network optimisation on telecomm side- product is in conversational, CV space etc. 

Vijayant

technical architect on ML 

Pankaj

ML Dev and ML OPs 

We cannot deploy without a lot of testing. What we have is in house MLOps framework. After model is deployed in production. MLDev platform we use Dataiku.  Jupyter Hub for prototyping. Do it more on the on prem environment. Once the models are build we embed in the micro services which have KPIs. Those MS are built for serving - also has observability. Once the model is tested locally- same product goes to multiple customers. We also retrain the model before it is deployed. We have a few micro services part of MLOps models. Once the platform goes above certain threshold- we trigger new model. CI CD pipeline is there. If the new model is better - we replace the model. Based on K8s.

Model building itself is a huge cycle, Telecomm domain experts have to work with DS. 3-6 months to come up with a good model. Once it is developed, it is deployed as a MS- some customers will deploy immediately in a few months. Some other customers will take multiple months. It typically takes 3-6months cycle. It also requires end to end testing. 

We started looking at a different MLOps platform in 2020. Datarobot, Dataiku, Seldon - evaluated a lot of those. We concluded Dataiku meets our requirement better. We dont use it much for MLOps. We have a lot of custom parts of micorservice. Our observability is there. This is not completely scalable and optimal. I wanted to understand what is the maturity of your solution. 

80% OnPrem and 20% OnCloud- telecomm has restrictions on Cloud. AWS is major for cloud but some on GCP, Azure etc. Each cloud provider has their own thing. We have our own K8s platform. 
"
8379292246,Migo,Valmik P (valmikkpatel@gmail.com);Madhura Joglekar (madhurarjoglekar@gmail.com),,Migo,100-500,10-50 Mn,<10,AWS,No,Financial Services,USA,Nikunj Bajaj,Migo_Valmik 30-03-2022,,"Madhura Jogelkar

Working at Migo for 2.5 years. Lots of theoretical models.

Partner with telecom companies and banks and define credit score in Nigeria &???? Brazil.

Currently 8-9 people. 5-6 DS and 3-4 engineers all in one team. Headed by DS manager. At present all of us are in the same team.

Switching to WandB now.

Sigma for inference monitoring. Also use SumoLogic.

Sigma is used by business focused people.

Data drift monitoring we dont have a specific . Deploy models frequently. Even if we dont change features.

Different for different models. Average once a week.

We use AWS notebooks and pycharm. Also system integrated with gitlab. We deploy to package cloud as a package.

K8s is used in production. Package cloud models is pre production.

Service that is used in production is K8s. Bnch of models in package cloud. Data scientists push to package cloud and we sometimes work on services.

Explainability is not a bottle neck for us- given the markets.

Talked through the product and she got super interested in the webapp side. She said she will work with us to try out the webapp project.

Meeting is set up 1.5 months out in mid May.
"
8374483593,ConstructorIo,Sasha Aptlin (sasha@aptlin.com),,Constructor.io,<50,<10 Mn,<10,AZure,No,"Technology, Information and Internet",Mexico,Anuraag Gutgutia,Constructorio_Sasha 24-03-2022,,"Notes: 24-03-2022

ClickStream Data for recommendations 

Search, AutoSuggest, Browse, recommendations and collections 

Interfacing at a lot of APIs and fix 

Constructor: Havent used a lot of readymade products. Do everything from scratch to save costs. 

The latency constraints are huge with Constructor and a lot of companies are out of questions. 

Imagine I am an engineer on call with you. What is the pricing structure or explore? 

We use CometML for experiments and save them on their own infra. How can we prove that we can make a switch very fast? 

Something minimal to prove. Lets focus on Experiment tracking part and compare our product with them ===> ** Experiments required a lot of hacks in the code to make it work // ** Sometimes the experiments dont sync.

Group call with Engineers => 30-45 minutes => trying to prove that we provide more value than what it costs us to switch

Selling Point will be providing the SandBox, convincing DevOps to round the TerraForm etc. 

HTML Demo of what it is like to track an experiment 

What functionalities we provide to ensure that the experiments are not lost? 

Comments: * Use only 2 functionalities: Comparison of runs and metrics * Just looking at Shap values and all the charts for that (WhyLabs/Evidently) 

Can give contact of Product Mgr? Just someone who decided to leave. "
8374282582,Doxel AI,Anupam Chakraborty (anupam.chakraborty@gmail.com),,Doxel,50-100,10-50 Mn,<10,AWS,Yes,Software Development,USA,Anuraag Gutgutia,,Doxel_Anupam 02-05-2022,
8374282582,Doxel AI,Anupam Chakraborty (anupam.chakraborty@gmail.com),,Doxel,50-100,10-50 Mn,<10,AWS,Yes,Software Development,USA,Anuraag Gutgutia,Doxel_Anupam 24-03-2022,,"Date: 24-03-2022

to follow-up with Anupam in a week wrt more details about TrueFoundry and their Experimentation Platform.

Doxel is currently using W&B for its experimentation Platform

They believe in using things as a SaaS and outsourced instead of building it.

DOXEL AI: GCP is cloud infra - DB and storage also there. DB is postgress. Pipeline - Apache airflow. API - Flast and Rest API, but currently moved to FastAPI. ML: Use PyTorch => Focus on core business objectives. Integrate whatever can be got from SaaS. Experiment tracking - W&Biases, Aquarium - Helps us with monitoring and error analysis/explainability. Construction site => Lot of geometry. Postgress - postGIS.

DOXEL AI: With experiment tracking, the bigger problem is in the data side. Data version control side of it and artifact version control => currently another team looked into VertexAI and tensorBoard offering. We almost selected Weights and Biases. Also did comparison with MLFlow. We wanted more DataSet Versioning. One of the reasons is it is very flexible. Less dependency on the infra they are providing. What we find better in MLFlow than W&B => How you promote a model? Eg docker registry => someone has to go and tag the docker as production. W&B - have to tag it as release. In MLFlow, there is a lifecycle. We thought of implementing that concept called Retool. Its like a visual studio in a Web. (MODEL Lifecycle Mgmt)

Aquarium Learning: Similar to what our Model Monitoring has. Upload prediction and ground truth. Can cluster the failures and can do error analysis and enhancement of training sets. Growing very fast.

: Need to figure out more contacts in Doxel and reach out. Their use case is very relevant."
8374320504,Intellimize,Jing Conan Wang (jing@intellimize.com),,Intellimize,50-100,<10 Mn,<10,AWS,No,Software Development,USA,Anuraag Gutgutia,Intellimize_Jing 21-03-2022,,"Meeting on 21.03.2022

7 ML Engineers working on Infra and 2 DS working on models 

CR/CR

Mostly use Jupiter notebooks for model training. For training jobs, use Airflow. Hoping to use TF-Serving 

Deployment: It is on internal serving on Radis 

Anything you use for Experiment tracking => building inhouse 

7 Engineers working on the platform side 

Did you explore other tools? Or decided to build inhouse. 

Running TF in AWS. Not very common ==> AWS does;t have any support for Cluster based training. 

2 PainPoints: Experiment Tracking => build //// 1))) Training is the no 1 pain point - it doesnt scale. 2))) Cant use the service of serving on aWS 

SageMaker is also evolving : It was not very mature when evaluated in 2019 . Fast Forward to 2021 

One quick thing: Serving? Managed solution API ==> Some parts - managed service. Model Engineering: will want to control "
8374284044,IQVIA,Sunil Singh,,IQVIA,>1000,10-50 Mn,>50,AWS,Yes,Hospitals and Health Care,USA,Anuraag Gutgutia,IQVIA_Sunil 27-03-2022,,"Date: 27-03-2022

to send details to Sunil on the Experiment Tracking with a Video

to follow-up with him in 2 days if needed

Biggest supplier of Pharmacy data. Data privacy is a big concern, currently everything is in their own on prem cloud

Deployment layer is built well and low chances for anything there - the platform is well built on top of Kubernetes

Explainability is the biggest problem and team is working on that currently.

Experiment Tracking is also done for a certain number of use cases, but currently looking for a better platform for integrating all use cases together. Overall, the platform sounded exciting but chances of conversion according to me are low

Sunil to discuss with other Director Heads and accordingly come back to us."
8359169716,VoiceFlow,Keyhan Babaee (me@kayhan.dev),,Voiceflow,50-100,10-50 Mn,<10,AWS,No,Software Development,USA,Nikunj Bajaj,VoiceFlow_Frank 12-10-2022,VoiceFlow_Frank 12-10-2022,
8359014838,Cognitops,Kevin Safford (kevin@cognitops.com),,CognitOps,<50,<10 Mn,<10,GCP,No,Software Development,USA,Nikunj Bajaj,Cognitops_Kevin 02-03-2022,,"Call 2 

History of Databricks decision. 

Fundamental dataset- datalake is logs from application event streams persisted indefinitely. Logs are any chance to any collected or calculated data in any databases. 

We make software for warehouses. 

We can replay each change event and recreate the state of the warehouse at any point in time. Its difficult to deal with  from an exploration or modelling standpoint. 

We run spark jobs to produce aggregate files that represent different things like- roll up to current state. Create a time series that we are interested in. Tabular structure that would be better suited for ML or Data Exploration. 

This was painful from an EDA standpoint. We needed to writing spark jobs and deploy them on a cluster. And then Airflow DAG schedule. 

We realised most time goes on EDA process and most time resulted in such jobs. 

This is a problem that we wanted to pay money for. 

Databricks allowed us to create tables using Delta Lake format and allowed us to create easy job and near 0 amount of infrastructure work. 

Python, Sql, Minimal Pyspark and schedule it. As a matter of advantage we decided to write airflow which is a nightmare but better. 

Flow control would happen within a notebook. This gets us to current state because we move our data. Now that we have data in deltabrcks table, that resulted us to move our processing into Databricks because its easy to access that data. 

Our production job also lives in Notebook. 

Data engineering jobs change more rarely so they cause less problems for us. 

What would you ideally do? 

Something like Databricks version of Kubeflow could be nice. We can easily seamlessly interact with Databricks tables from a docker image. Databricks could deal with provisioning of servers and scaling. This could be managed through the same scheduling tool. Some equivalent kind of environment where we could mirror our git repo- point the scheduler to a module in that repo. If that could be wrapped up behind a rest end point. 

Easy integration with Kafka. 

Map a rest end point to a mirrored module. 

Call a rest endpoint. 

Not a 100% sure of how Github repos work with Databricks notebook.s

We publish our repos on a private pypi server. Thats where our CI/CD pipeline kicks in. We provide Databricks requirements to pull. Import from this module in this cell and not cell run it. 

Data Scientists dont want to write docker containers. 

Heres my python module, this file has the main module. Call this main module when you receive an http request. 

That environment where it runs has access to databricks tables and can publish to other artefacts in our cloud environments. 

Literate programming- certain sections of file are executable. Its basically an executable documentation. You can export that raw file- out into individual files which could e version controlle.d You run a function to send snippet to this file. In some way this is a precursor to notebook. 

If there was a capability to specify taking cells, sending them to a file structure, a version controlled environment could reduce that point of friction. 

Have experimented with mlflow deployments. Databricks solution architects told us not to use it. Designed for testing but not for production. Send in different rows to see what comes back- that is fine but not okay to use it for production traffic. 

Models that we are using in databricks today, we are using it for batch predictions. Notebook runs and we write those to a database. 

For online predictions we still do the old thing. Move it to python modules. Deploy them using cloud functions or cloud runs. 

Databricks is large spark which is a batch processing distributed computing tool. 

Call 1 

Started in Academia Statistical Physics. 

About 10 years ago left academia and joined a startup. 

Brought me back to Austin. I have worked entirely in startups- varying size and stage, founding early stage to later state - heading into acquisitions. Never worked into large enterprise. 

Worked in multiple capacity such as SWE, DS, Manager, IC, Executive etc.. 

At Cognitops my title is Chief Data Scientist- now around 60 people. Over the course of 3 years- software for physical warehouses. We ingest data from multiple sources and provide real time information whats going on in the building. Also forecast and recommendations of actions they can take. 

Increased throughout, reduced cost, leading into black Friday- might want to prepare for demand surge. Taking into account whatever KPIs- if you move 3 people from this part to that part and turn on and off certain equipment. 

Data driven, prescriptive analytics are front and centre. 

4 Data scientists and 1 analyst and me. We will add ML engineer Data engineer and a data scientist in the next 1-2 quarters. We have tried a lot of things in the MLOps space. 

We have explored working with various vendors- Comet, Neptune, Saturn, W&B, Algorithmia. Piloted with several of them. Ended up rejecting all of them. Chose instead to manage our own MLFLow clyster and built tooling and support needed for ourselves. We have since moved past that and are using databricks as our ETL tool. MLFlow is still our preferred method for serving out of the box python models. We track in the MLFLow registry. For model deployment monitoring etc. we are kinda out on our own. 

A lot of our models are more statistical in nature- MLFlow is just not always a good fit. We have been using it a little less recently. 

TimeSeries forecasting recently. Some things that we dont have and we need candidly are better model performance monitoring- across trained version of the models. We retrain every day every hour, are we seeing a trend? Sensible alerting around absurd predictions is a challenge. Prediction serving is a challenge. MLFlow has a Model Serve feature. Even Databricks recommends against using it in a production environment. 

Working with Databricks, everything is notebook driven, code review, versioning, deployment, access control- I dont understand industry-wide movement, I want unit tests. 

Why not choose WandB

We were more cost sensitive at the earlier stage. We were willing to invest in a tool 

We noticed high price tags for modest functionality. Comet- 60k for the year. Practically like a notebook hosting. They have added a lot of functionality since then. 

We got pretty far along with Algorithmia. They were focused on model marketplace. Model serving and deployment capabilities was an afterthought. Even still they had a pretty tidy way to wrap a model and a small amount of code and turn it into an endpoint. 

What we discovered that the environment that you had access to was very limited. 

We couldnt install Profit in AI platform. They dont give you where you could hand over a docker container. We could not push around 2Gb docker constainer. 

Some issues that we encountered on what was available then was easy to use. We found a lot of point solution

We needed to stitch together  5 different tools and everything became expensive and we didnt know how well they would integrate with one another. 

MLFlow you can get a plot of training vs validation for a particular epoch. You cannot get the plot for model version. 

We really wanted to buy instead of build. We couldnt find anything that solved our problems. 

With Databricks- they offer managed MLFlow. We dont have to do our own clusters. We are on GCP. Our customers are warehouses- Amazon is dirty word. 

We were managing our own Dataproc. clusters- they were a big pain remover for us. Now we have to worry about notebooks. We kind of switched to worrying about deploying models for online predictions. Rest Webapp engineers. We are doing as much s we can and schedule batch jobs. Predict the world very frequently and publish the results to an application. 

Keeping the data models for the predictions that we are creating. 

Asked about pricing model- pretty short on time. If you could follow up email with pricing and also "
8359014838,Cognitops,Kevin Safford (kevin@cognitops.com),,CognitOps,<50,<10 Mn,<10,GCP,No,Software Development,USA,Nikunj Bajaj,,Cognitops_Kevin 15-02-2022,
8358014474,Rakuten Rewards,Manoj Dobbali (manojraj.dobbali@gmail.com),,Rakuten,>1000,10-50 Mn,>50,AWS,Yes,"Technology, Information and Internet",Japan,Nikunj Bajaj,,Rakuten_Manoj 11-03-2022,"ML Team is not tied to product per se. Working several different projects within the company. 2-3 years ago- initiative happened.

Worked on 4-5 projects  everything is producing some value for the company.

Good encouragement in the company to solve problems.

Contributing to data products within the company. Now transitioning to Platform team. We will be working more on the developer platform as well.

We are fast in adopting shiny and new stuff. Using Ray for distributed systems. Deploy everything on K8s. Everything is managed by AWS. Airflow.

Artefacts we use S3. PoC research we use Sagemaker.

We are a small team- myself and 2 more engineers. My lead.

Read a bit about what we you are building. Always wanting to try new stuff.

Do everything from data engineering to ML model.

Data scientists were our core customers. 7-8 Data scientists. 2 different teams.

Data Scientists within US and across Europe.

For experiment tracking- we evaluated 2 tools. I evaluated MLFlow, Neptune & WandB. We finally got WandB into our stack. We havent educated DS thoroughly into the experimentation. I am working on a project which has integration of WandB.

MLFlow seemed interesting initially. I dont remember much. It does too many things. I just wanted an experiment tracking tool. Did not want artefact management

The reason we chose WandB was because it is specific for Deep Learning.

Enterprise support for WandB was good for us. We were a small team- DevOps team is 2 people.

We are adopting a tool called Ray and exploring enterprise version of it.

Currently we have a project based on conversion rate. We have neural network based projects. DS works until a stage this is a valid approach and we will pursue this. From the point we come in and work with them closely to turn into a product.

During the process we find many other potential bottlenecks.

Tuning model was an issue- training jobs took 1 hour or so on small datasets. Ray was a good tool to use for that.

How hyperparameter sweeps resulted. Started integrating in WandB code. Using WandB to track those metrics. Integration is super easy between Ray and WandB.

We have data dog for monitoring and stuff. Nothing ML specific.

When Data Scientists are in active research phase- Sagemaker is used. Once idea is validated and we want to turn into production ready code, we move away from Sagemaker. We design and build the systems- DS contribute the system.

Our integration and deployment is completely via Github action. Airflow job is via Kubernetes cluster. We only have batch processing currently.

Everything runs on airflow.

Manoj will make intro with his manager."
8357973048,Trilliant health,Sandeep Kothiwale (kothiwale.sandeep@gmail.com);Matt Hawthorn (matt.hawthorn@trillianthealth.com),,Trilliant Health,50-100,10-50 Mn,Oct-25,AWS,Yes,Hospitals and Health Care,USA,Nikunj Bajaj,Trilliant Health_Sandeep 07-04-2022,,"Matt

Works on DS, SWE, DE etc.

Comes from Math background. Came to DS for a lot better job.

At Trillian- what I do is SWE in service of DS. Building a lot of infra. Pretty early on in DS journey. We need a lot of systems in place.

Lack of ground truth data- thats the biggest problem we are trying to solve. Getting high quality training data. Setting up infrastructure for the rest of the team to iterate quickly. We are on a K8s cluster.

Some work that is being done is to make it easy for DS to interact.

Our deployment is pretty straight forward. We are not serving live APIs. We are doing big batch transforms. Anything we do with trained model is importing it inside of a short lived process.

Most well developed ML models are trained in virtual machines. We are not using docker for that. We would like to move to dockerised training on K8s wth an interface that is easy.

Because training is not dockerised, its manually kicked off on a virtual machine. Deploy docker image on a machine so its manual.

We have some internal tooling - attempts to distributing your model to K8s easy.

Experiment tracking for CI right now - reporting evaluation metrics for change. Just for business model. We do some logic change- painstakingly built. Want to follow a similar structure. Code change you get a report on how accurate you are. Want to have that stuff persisted somewhere.

Interested in Webapp as a Data labeling tool. Will send out an email with manager and Data Scientist.
"
8357973048,Trilliant health,Sandeep Kothiwale (kothiwale.sandeep@gmail.com);Matt Hawthorn (matt.hawthorn@trillianthealth.com),,Trilliant Health,50-100,10-50 Mn,Oct-25,AWS,Yes,Hospitals and Health Care,USA,Nikunj Bajaj,Trilliant Health_Sandeep 15-03-2022,,"Worked at a large healthcare company. After my PhD. 

Early 2015

Building dashboard on feature importance and why certain predictions come in certain way

That was really the main thing. There is additional work to show value of the model to stakeholders. We didnt have much monitoring. We did work on container platform - OpenShyft called Redhat. 

Model ran at a particular time. If it didnt run- it showed us during this run. 

We actually dont know the actual results until 2 days later. You have to have another pipeline what happens. 

Since then I moved to a healthcare startup- early stage in ML. 

Actually building ML models, some business logic that is in use. We are actually in fact looking for ways to have a CI/CD pipeline. Different ideas floating around. And thats where I thought there is an opportunity to collaborate. 

In CA healthcare I could give to physicians by trade. They are VPs in different org. Why did this patient go to ICU. If the model did predict ICU. Even on revenue cycle management side things- there his whole payment side- coders who are trying to code what happened in the hospital so insurance company pays adequate amount. She had background in that. Why was this patients insurance denied. this is what our model says. She was convinced the model does have some value. 

In this company it would be internal facing products that we build but we need to verify. Product and QA team verifies. 

I use Dash in python. You have to build up from scratch.

If its a simple dash dashboard it would take a day or 2.  Happening once every model. Once a month. There are more people who face this problem. They have to bridge that gap. Here I have convinced 

For me, webapp is most interesting. 

We have about 10 people in ML family. There is no platform team internally. 

Lot of focus is on CI/CD. Thats where the team is focusing now. I use it for my personal use case. 

If we can get the CI/CD stuff working. I can connect you 

Will connect with my colleague. He is trying to write something from scratch. "
8357691771,Fetch Rewards,Kumud Chauhan (k.chauhan@fetchrewards.com),,Fetch Rewards,500-1000,100 - 500 Mn,25-50,AWS,Yes,Software Development,USA,Nikunj Bajaj,Fetch Rewards_Alec 05-05-2022,,"Call with Alec -

Nikunj to get back after we have an easy version of deploying the UI locally.

Rewards users for snapping pictures of their receipts

My team does algorithms for document understanding

We have some systems that can add product intelligence.

We pass that to other teams. The whole business model is right downstream our models.

Computer Vision to NLP to Graph-based models. We also have DS on other teams who do some recommender systems, churn predictions, XGBoost models.

We have quite a few people use MLFlow. Our team and one other team are the heavy uers.

My background is in Mathematics and Economics- experimental design, econometrics. Finacnial modeling.

We are on AWS, we are using Sagemaker. We have MLFlow server hosted internally. Everything hooked up.

We are a huge startup- 700 people. 20 people.

We dont have 100s of models. Sometimes, we will do MLFlow tutorials and 10 new experiments show up. Its not a big deal.

We just use MLFlow to monitor models. We do a lot of Streamlit apps to do one off inference.

I use Streamlit for a lot of things personally-

Validate a model,

Compare the prediction with annotations

Does this tensor make sense

Is this the right graph structure.

We dont use MLFlow Model registry. Its either in production or not in production. We have implemented our won one

Will probably end up using MOitoring.

Have a lot of custom.

Basically, Sagemaker script mode is awesome. Any model from Github, nice and modular, dataset file, training file, I like to write code like that. What I can do is Jupyter Notebook- launch training job.

Invoke pytorch estimator. Heres my entry point. Open the docker contrainer. Inject in the training image.

You need to get the == Sagemaker SDK. We use HuggingFace estimators. Its very little work."
8357701287,ClearCover,"Jerry Claghorn, PhD (jclaghorn@clearcover.com)",,Clearcover,100-500,100 - 500 Mn,<10,AWS,Yes,Insurance,USA,Nikunj Bajaj,ClearCover_Jerry 24-03-2022,,"ClearCover - PM for AI/ML projects 

Single team - 5 DS, 5 MLEs

Oct 1 we had only 3 people in the team. 

We intenionally had our platform capability- reusable assets for ML flows. 

Did that very intentionally. Never  hire enough DS. DS is becoming more commoditised. Easier to incorporate as part of an engineering stack. 

Joined about a year ago. Had to run ML models every week in the process. If I deploy 5 ML models, my entire job would just be running ML models- this was at a previous startup. 

How do you have ML as part of the tech stack. We take MLOps very seriously. 

We were a pilot partner with Arize for monitoring. We got connected with them. When their product was nothing- we started working with them. Paid relationship recently. On the services monitoring we already had DataDog. 

For our feature store, we built on top of Neo4J. We had a lot of graphs problems. We can do everything we need with the graph. We had our own deployment framework- that starts from a FastAPi wrapper and becomes more capable overtime. 

Once we have our pattern of being able to deploy services and now we made it really easy to have our partners integrate with our services. 

All of our models were batch- send payload and get the score back. Every time we got new features we had to reintegrate. Now we have a single entry point for all models. You just enter at one point and give us very minimal information. 

Not using anything for experiment tracking so far. After company has Optimizely. We encode A/B splits into the services- sometimes Arize helps with that stuff and sometimes in Looker Dashboards. We just had to really quickly launch a new feature and I got feedback on Google Sheet.

We version control notebooks. We explicitly have a purge and phase. We have a data source associated with it. You have all these artefacts to rebuild the model. We are tracking on S3. 

Face got lit for model demos. We have actuary types and that process is very manual. We build all our predictors and outputs and they slice and dice that. 

We are experimenting with trying to create with Streamlit Apps. 

We built  a model that makes a decision. All the ML models there is a tradeoff to be made. If you are very strict you might divert a lot of business. We play claims and we are not profitable. It hurts our company to sell to business people. 

We take more risk that is optimal for the sake of growth. We have a hard time for that. 

Share https://truefoundry.gitbook.io/mlfoundry/

https://www.youtube.com/watch?v=oXbplOCL_74

Initial version of their software was not value for them. They were developing very quickly and every week they had new feature to show us. At a certain point their product reached a maturity that was adding value. 3-6 months of regular working with them an hour a week. 

We signed NDAs and couple other piece of paperwork that protected both sides. 

They have us login information. We started piping data in. Give them feedback on product. 

Value to us for that was they built a product that fit our need very closely. The thing we most valued got built very quickly. 

Free pilot that turned into a paid engagement. 

They offered us free scope of work for a limited scope. They got access to our executives through that. They showed off what they were doing. That was pretty intense on our part. That was HCSE. Working with them there. 

Work with tech companies not with 100 year old company. Its tempting to work with health insurance company but legacy system will kill you. 

We didnt talk about pricing until later in the process and we got favourable price. It helped them fundraise. Price realisation takes a while. Takes longer than proof of value. 

Follow up call set up to discuss experiment tracking, web app and some more of design partner relationship. "
8357768821,AbleTo,Ning Li (ning.li@ableto.com),,AbleTo,>1000,50-100 Mn,Oct-25,,,Mental Health Care,USA,Nikunj Bajaj,,https://app.fireflies.ai/view/Nikunj-Ning-Reconnect::4BvMSwkQdZ,"Hello.  



Hey, how are you doing?  



Hi, Ning. I'm good. How are you?  



Good. Good to catch up again.  



Absolutely. I've been such a long time since CTO, say, caught up. I don't know. Was it in March?  



Yeah, it's been quite a while, to be honest. Some of the details from my memory. But I remember you raised a new round of funding that you told me in LinkedIn. Congratulations.  



Thank you, Nimar. I think things are moving forward very nicely for us, so we are very happy. And I really appreciate you taking the time early on when we did not have a full project product and everything and helping us with some of your feedback that I really appreciate.  



Absolutely. Yeah. Mutually beneficial experience that IBM also learning things from you guys. And in the meantime, my feedback can be helpful in your product development. IBM more than happy to help.  



Appreciate it. So, yeah, like, maybe I'll just give you a little bit of a rundown of what happened in the last five, six months since we connected. I would love to learn a little bit more about how have things been at your end as well. So from our side, I think we continued building out the product that's Ben going quite well, actually. The product has become a lot more stable now. A lot more. We have CTO cover actually a lot more ground than what we had around Amar April Time France. Since then, we have also gotten a bunch of enterprise customers using our product as well. That's working out nicely. A bunch of funding related things that we mentioned to you. We also got some nice public, like, basically some opportunities to present in good forums.  



Like, for example, I don't know if you know of TechCrunch Disrupt, but we actually got selected as one of the top two United sales presenting in Tech France Disrupt. We did that. So team grew quite a bit. So a lot of nice things from our side. But yeah, I am very curious to learn. How have things been at your end as well?  



Fantastic. I'm glad to hear that things Ben gong great for you and team. Yeah, well, we've been having a really exciting year in the past. More than a year, I would say. I think last time when we met, I might have shared this with you, that we've started building our in house ML Ops platform. Of course, it's not as polished as yours, but we started building our own Mlaps framework since mid last year, and now it's this year that's been one of the top priority for the team and also the company. And I'm glad to share that there's still a lot of details CTO be refactored and polished that we kind of launched.  



We deployed the Revamped models for one of a major client last month using that new MLS platform and actually running a pretty tricky A B testing cross dual system because before we retire the legacy pipeline for this client, want to make sure the new model and the new pipeline are really solid. That's why we'll run them side by side and do a B testing for a couple of months. So things are going well so far and the model lift is in the right direction. And also the Ops part, it's working nicely, but we're also identifying new issues that we're kind of further polishing it into a more mature platform so that we have dozens of clients. We're going to migrate over to the new platform throughout the rest of this year as well. Probably the majority of next year will be our focus.  



I see. Congratulations. That's very exciting news. I think when you joined able to I think you were like one of the first few people in the ML machine learning and the infrastructure side of things, right. If I September correctly and you actually built the team ground up, if I remember correctly.  



Right, right. Back then there was an ML apps or machine learning engineering that's a team I built within my data science team. Back then there was basically just one data scientist that's in the back end and now we're kind of stepping out of our comfort zone from building models. CTO working very closely with DevOps and data engineering that learned a lot as well as Google Cloud team was providing a lot of support that we use the bare bone of our in house frameworks built on the Shane they rebranded to wear the AI. But it's essentially just Kubeflow pipeline, right?  



Yeah. And I think you're already on top of taxi like last time we spoke. So the investment has been going on for a while, it seems like.  



Yeah.  



So when you Akshay Nikunj that you decided to build an inhouse pipeline and on top of Vertex AI, I'm actually curious what are the things that you found missing in Vertex AI that you had to get a team together to build it out and how long did that take? Like, I'm very curious to understand what are the things that are missing in Vertex AI that teams are having to build violence. Because at least Google's promise is that Vertex AI is the ML Ops platform that you should be using. Right. And not like needing to build anything on top of it. But of course I understand from other customer conversations that the promise itself is not met. So I won't understand practically from you experience.  



Yeah, that's a great question. Same question I believe, asking Google team early. The dream. There's some distance between the dream and the reality. We realize one major thing is the fully managed, for lack of a better term, than the model management. Especially when we built it last year that wasn't quite there. I think now they're getting more mature, the model registry or the metadata part, but I feel like the metadata and model management is still something. I feel like we now have something that works, but I can see it would continue to evolve, that our platform will evolve together with as Google's product get more mature. We couldn't really leverage their full kind of the AutoML benefit because we purposefully chose to design and build our customized model because healthcare is a very domain knowledge specific thing.  



We frankly couldn't really trust just auto ML. So that's why the nice graphical interface and all that not so much attraction for us. But really what we the main reason we use Vertex AI is to essentially we need like a serverless workflow pipeline, CTO glue all these components together. And that actually worked out pretty well with all the basic elements like training, prediction or even incorporating a service Dataproc component. Doing the feature engineering part, which the reason we use Dataproc is CTO Spark actually handles the bigger data slightly better than us. But then it's like what's missing a little more is when all now that the model is done, but then how do we manage it more in terms of maintenance and or even more like automated QC, which I believe is still lacking.  



So we kind of build our own version of the QC monitoring datta drift and for metadata we're also using that their model registry. Now looking back, there are certainly things that are not quite ready. We realized that's that is a part they acknowledge that's something they're still working on. Actually we have the biweekly touch base with the Google support team and there were nimble to kind of making supporting and making adjustments.  



They were not sorry, what did you say? They were nimble or they were not nimble to support?  



They're pretty nimble in terms of trying to support but not always being able to because sometimes the products are just not ready. But they're honest about that. So that's one big part about all these features that I would say nice to have, but not the core feature like training and all that they already have. So it's like the model management and the QC that's the part they can do better and they are making improvements. And another major area I also told them directly and they took this feedback very well, it's the quality of their documentation. It's like that their products are developing so fast, the documentation just couldn't catch up because you can find all sorts of documentation online.  



But then when you bring it into you own pipeline built, it may or may not work because that might be something from a year ago, the things that's already changed. But for that's where the value of their very kind of customers overview comes in. They have dedicated personnel, CTO support us, not just Wordx. But I need Google because everything, our data engineering team, the whole data platform is on GCP. So they would provide kind of the enterprise support.  



I see. Okay, so one question like the part that you mentioned about the entire model registry, model maintenance, etc. For there are tools like ML Flow, and I think you guys already use databricks you mentioned. So like ML Flow is kind of designed for that experiment tracking component of it. Right. Was that not sufficient or are you using that or you did not try that. What was the status there?  



We actually tried a few. We don't use databrick because we have the databrick is essentially just a Spark notebook, to my understanding. But we have something similar for Spark, which is the Dataproc in Google. So that you can use a notebook, but then you can also build pipeline to run production scripts. But the ML Flow is something we did explore. I couldn't remember all the details, but eventually we end up not using it. But I remember we did explore it a little bit, because even with ML Flow, my main takeaway was that it wasn't quite there wasn't like a lifesaver. CTO resolve everything, but on the other hand, it brings the burden of maintenance. Now you have not just Cook flow, now you also have ML Flow.  



So we try to think about not just the works, but also going forward, what's the level of effort in terms of maintenance?  



I see. And how about some of the other paid solutions, like weights and biases, for example?  



I don't believe we've explored other paid solutions. We've only explored several open source solutions. But of course, the only paid solution is the Google platform.  



It makes sense. Understood. One other question is are you currently doing anything for model monitoring? Is that something that you built out internally?  



Yes.  



What was the solution that you ended up building on top of your monitoring systems?  



The monitoring comes into the data queue, which is relatively more straightforward. You monitor the train versus test, you monitor the drift. We receive medical claims history month over months would just say volumes steady. So in the normal days those things should be steady. It's nothing really fancy and serves the part purpose. If it sees something, it will stop the based on the different level of risk. If it's a minor, just keep going, just send a note. But if there's a major issue, just stop. And then that can connect to slack, which is pop up, I noticed, or something like that. But the model monitoring comes to the training and the prediction. And the prediction side is also relatively straightforward. We just monitor the outcome, not just the scores, but also population profile. And you can either hook it to a looker dashboard or whatever.  



That's pretty straightforward. But the training is in terms of not really monitoring because the main purpose for the training dashboard is more to serve as a model intelligibility. It's like we don't want it to be a black box. Right. We wanted to talk with our clinical partners either within or outside the company. Like, what does our algorithm do exactly? Like, inside the box, we build this in house training dashboard. We call it a dashboard, but it's essentially just streamlit and then hosting an internal dashboard. We can review some basic model performance, and beyond that, we can report not only a model level or a population level of insight. For example, that tornado plot with Shapley. But we also can do an individual level. What's the top reasons?  



We're saying this person has high risk and that actually has great clinical value that when intervening a patient, for example, that could be the factors. Or we can match that patient to the best provider because of why this person is targeted. So that's one good potential usage. It's Streamlided. Think it's also open source for surge week.  



Now, it's acquired by Snowflake.  



Okay.  



So it went through like, a big acquisition. Like, Snowflake paid $800 million to acquire STREAMLET.  



I didn't know. Interesting.  



Yeah. Okay. The other question that I wanted to ask you, Naing, is seems like you've made a lot of progress from the first time we spoke in terms of building out the platform. And I heard that you're also shipping new models on top of this platform now. But one thing I was curious about, you mentioned that you are shipping the platform to your customers. Is it such that actually you third party, the customers themselves are building models on top of the platform? Or you mean, like, you guys are building the models and shipping the models to the customer?  



No, this is more of a figure of speech. We don't really deploy them the client side. We deploy this in house and we take their data in house.  



Okay.  



And then we generate targets. We're able to, as a company, will do the outreach and the intervention. So everything is greenhouse.  



I see. Understood. Okay. And rafi models, how many machine learning models are like your team and all have built out? Like, how many are in production right now? Love to understand.  



The quantity doesn't really matter at this point. A new model, they find what you want to predict as a model, you just tell the feature engineering module, what do you want your population look like? How do you want to define your endpoint, what kind of inputs you want to use, then automatically it would train a model for you. But in general, I would say for each client, for each product, we have multiple kind of mental health intervention products. For each product, there will be a set of three or four models from different perspectives. Some from clinically mental health needs, clinical preferences, some from medical cost saving or all right angle, some from engageability. That if we outreach, is this person likely CTO engage with us or not? So from multiple more like then each model would have a score at the pendo.  



There will be an optimization process to target. Which one you can think of it as a composite score. And in that part actually we tried several strategies. Turns out some integer programming gives better result than just research throughout the car threshold or score threshold.  



I see. Understood. Okay, so you mentioned that each client actually have their own set of models. So basically the number of models scaler with the number of customers that they want to help.  



Typically, yes. Because of the nature of because this one client might have a commercial population, another client might have a Medicare population. The commercial population would average age of 35. Well, Medicare might be average age of 75. So the different model is the best outcome or best quality. It's best to have it trained for its own.  



Okay.  



Providing the sample size is larger. Shane that makes sense.  



Yeah. So would this go in the models itself would go in the range of hundreds. In that case thousands.  



No, we don't have that many clients.  



I see. Okay, understood. And then the other thing is to estimate Nat, you team building out this in house platform. How many developers, how many months did you all end up spending to get to a point where you feel more comfortable with where you are now?  



Yeah, that's an interesting question. I were to do this all over again I guess I'm just thinking about machine learning engineers or data scientists. Forget about the DevOps or other teams for a second. Yeah. I would need a team of ideally two to three machine learning engineers. I think two should be sufficient for initial use. Two to three and then one to two data scientists depending on the level of seniority, maybe one senior or two not so senior, which is pretty much just thinking about my team in the past year then the process. Because for us and for myself as well, there's a lot of learning. So we took about a year from knowing nothing to now build this thing and deploy it.  



But if were to do it again, probably won't take as long altogether it's like four to five FTE over nine to twelve months of time.  



I see. And if you think about now, the maintenance of this with other use cases mature roughly how much resources, how many resources do you estimate on an ongoing basis to maintain this? Maintain and grow the platform?  



Of course it can always break, anything can go wrong. The client can throw some dirty data, can mess things up. But if things goes well, actually the maintenance requirement is going to be much lower than development. Mainly because for healthcare we don't have to retrain the data so rapidly. So once every couple of months and then it's sufficient. So after the initial deployment, basically we just for the data scientist, just monitoring the outcome. If it looks fine, maybe that's like 1 hour per month kind of thing for that account for MLE then because things will be eventually set to a we use like a cloud scheduler or that kind of thing.  



This ML Ops is embedded in a bigger data operation pipeline that we actually listen to event queue that say the last event is the data warehouse or data mart has curated the raw data to standardized form. Now they're going to tag and say hey, datta science already. Then we'll be listening to that event. If that new event comes up, say standard claims ready, then we will start to automatically kicks off the engineering and then running prediction and then things like that. And then this whole process, if it works without issue, it should be kind of pretty much hands free for the MLB. We just need maybe look at a log or something.  



I see. Okay, so almost no maintenance effort basically.  



But that's like the ideal case. I'm sure there always will be something a little glitch here and there. Then that's when I would say at least one ML to be a standby sam, you say, oh, let's make a little trick here then if we need to change the pipeline, then kind of rerun it.  



Got it? Okay, understood. And one last question link, if you Shane to think about your next year's goals in terms of the platform, right. What are some of the other top of mind problems that you intend to solve? Like where do you think the team will focus on.  



Majority of next year is probably CTO take full advantage of this platform because now the majority of our clients are on the Legacy platform which is like Luigi embedded in an airflow and using Spark ML. Which is nothing wrong with Spark ML. But that convoluted pipeline is very even maintaining that kind of needs a lot of baby sealing model improvement aside. It's just for the machine learning operation. We would like to move away from that to this new Vertex AI pipeline. So next year the main focus for us would be just migrating most if not all the clients to the new platform.  



I see. Did you have any questions for me, Nain?  



Yeah, I'm curious to learn about the kind of as you now having more engagement with companies and also as the Google Cloud, Amazon and Azure and in the past there's also I said in the past, but they're still there, the data robot and things like that. IBM curious to learn the kind of the latest trend, if you like. What are the companies looking for in terms of from ML Ops product such as yours when they have the options of GCP or Amazon AWS or also other what are the main things that I feel like this field, even though it's a very hot field, but they changed so rapidly. So I appreciate it to learn soumen of the latest trends from you.  



Yeah, for sure. So what we're observing so far is of course, by the way, the answer depends on the type of company that we are speaking with, right. For some companies, if they're able to increase like 3% of the developer productivity, like platform like ours are like a no brainer at that point, right. For some other companies they don't optimize for the last 10% 5% of their developer today also it's fine, as long as my models are getting shipped, that's fine. So it really depends, but at a high level, what I'm hearing is the following that companies who are investing very heavily in sage maker, Vertex, AI type of pipelines are actually doing something similar to what you described Linked, where they're actually building on top of that platform.  



And I think your team actually built it out fairly fast, like nine months to twelve months is actually on the note. We hear some teams taking this long to build it, but more often we hear companies taking roughly in the range of 18 months to build out the platform on top of. So for example, but they also add a little bit more work on the monitoring side. For example, what you describe monitoring on top of stream rate and all is like a more getting started version, but you can imagine that more sophisticated dashboarding and all that you can build on top of monitoring, right? I think that gets included a little bit.  



But roughly we hear that it takes 18 months to build out a platform on top of the Vertex or the maker for the world because the direct developer experience is actually quite bad for each of these platforms. Like data scientists are practically not able to use it. Maybe Emily's can use it and there's almost no customizability. So if you want to do anything custom on top of Chris, you have to build out your own internal platform. Right? So that's like one of the things that we hear. The second thing is we also hear companies actually, like you mentioned that there's almost no time from the maintenance.  



But actually one of the, again pleasantly surprising things that I've heard that maybe your team has actually spent a lot of time doing the right engineering around it because typically the ratios that we hear is that companies end up needing one is to follow ratio of people maintaining the platform to people building out the model basically. So like 25% roughly of their resources go in just maintaining the platform in terms of the number of people building out the models as well.  



That's sort of like the case for our legacy. That's what I call it, babysitting.  



Okay. I Sep. Yeah. So maybe you have improved quite a bit on your platform and that's something that we hear companies, those are a couple of things right now, people who are doing things like data about our data, IO, etc. For that segment of market is actually fairly different. That segment is more like a no code tool almost, right? That's what it ends up becoming. And NOCO tools are great, but companies which have little bit more sophisticated engineers and data scientists and all, they actually don't prefer using NOCO tools because the end of the day it becomes more limiting. If you want to do something more sophisticated, you are not able to do that. But if you're doing simple things, I think it works quite well.  



And nothing against that platform or the audience itself, like our platform actually receives a lot of reception, gets a lot of reception with companies who have to care about developer productivity, right? And all companies who actually don't want to invest all this time that you mentioned, like investing on top of Sage Maker and Vertex AI and stuff like that, they actually want something which is much simpler, something which is much more integrated and they expect that their use cases will also evolve. So for example, like today they are shipping, let's say ten models and they expect in one year they will be shipping hundred models. Today they are using maybe one library of framework. They expect that tomorrow they are going to be using three libraries of framework.  



Because once if you know that's going to happen, the first level of platform that you build out internally almost never scaler because as newer use cases come, you always have to adapt your platform, essentially. So I think that's the segment where we see much of the traction that we are getting essentially. Basically the way ML Ops landscape is evolving, it's really hard for completely building internal platform that continues to be state of the art. Like you could always build out the state of the art at a given point in time, but the space also is evolving. It's very hard to manthan that to be state of the art. I think that's the thing that we hear.  



Yeah, that makes a lot of sense. That's exactly where companies like and products like yours provide value, which, you know, I don't know that 18 month company like how big of a team they have.  



Yeah. So again, we have done quite a bit of math here. So we cannot give an absolute number because an absolute number in this context does not make too much sense. Right. So usually what we have heard is one is to four for the maintenance is what I mentioned. And then roughly one is to three is for the build phase, basically. So like my data scientists are building something on top of whatever, on top of the Sage Makers and Vertex players of the world. They devote almost one third of their resources and building out the entire platform. So let's say you have a team of ten people in you machine learning team, roughly three to four people are working almost full time on building out the platform.  



If you have a team of 30 people, you actually develop eight to ten people building out the platform. Who is going to serve that 30 people?  



That's what we end up hearing that's about. Right. Actually is relatively smaller than our data scientists. But yeah, that ratio sounds about right. We have two, three ML kind of developing, and then in the future we'll be maintaining the pipeline rather than one to three, maybe one to two or something like that.  



I see. Okay. Yeah. Literally, I've actually talked to companies it's surprising how this ratio does not change. Like, I've talked to companies which have like 200 plus machine learning engineers, and I've talked to companies that have like five or six machine learning engineers. Right. And this ratio of it remains the same. It's Qovery interesting how this just continues to scale with customer development, basically.  



Yeah, no, take a note of sense because they decide CTO need a few more because they're more like a consultant talking with business stakeholders.  



Exactly. Yeah, I think that's right. That's where we are. So glad that I got a chance to catch up with you, learn about the platform that you build. One thing that might be interesting is I'm very curious to see what kind of things you have built on top of Vertex AI. Of course, I got some sense of talking to you after talking to you, but if you're open to it, I would love to see how the improvement that you have built and how the new platform is a lot more stable or what kind of features have you built out just for me to learn about. Okay. These are the problems that Ning and his team were facing and they solved it this way. Basically, it gives a good mental map.  



And I'm also happy to show you what we have built out and potentially even get your feedback on the platform if you are open to those two things.  



Yeah, no, IBM happy to exchange ideas at some time.  



Yeah, so maybe what we can do is if you are available then maybe not next, actually next week only because after that I'm actually going to India. Actually either next week or early next to next week. Whatever works out for you, maybe we can set up some time CTO do like a mutual demo.  



Sounds good. Maybe we can for this conversation, maybe we can involve some of the technical folks that are my team that knows more details that I think this might be a good learning opportunities for them as well.  



Yeah, for sure. Yeah. I'm happy to share what we have built out and show our product CTO them and also learn from your side, like whatever things that you built out. So would you kind of loop and like if I send out an email to you right now, like following up on like a timeline, would you look into a couple of folks that you think are relevant for this discussion on the email?  



Yeah, sounds great. Let's see, in terms of timing, next week maybe not great. I don't know what. We can discuss timing afterwards. Maybe sometime after Thanksgiving?  



Oh, I see. So after Thanksgiving actually I'm actually going to India on November 10 and this time I actually plan to spend about three to four months. I actually have a lot of different meetings that I'm taking. Singapore, Dubai, India, etc. So actually and want to spend three to four months in Asif, basically. So I was wondering if we can do this before I leave, just so we have more overlap.  



I guess I see. November 10. Yeah, because next week is probably gong to be a crazy week. We have a few new client launches.  



Sure. Like no, seven or eight that you think is good for you.  



Yeah, maybe the week of November. Yes, seven or eight. Around in the afternoon. Four ish. I think that might work. I'll look in one or two of the team members and see if they're interested.  



Perfect. Awesome. Then let's do that. I'll actually continue to block at least your calendar for November 7 at 04:00 p.m.. But we can always move over email if you feel like there's a better time along with the team, basically.  



Okay, shoot me an email. I'll look the team in.  



Sounds good. Yeah, awesome.  



Alright, great catching up.  



Good catching up. You have a good one. Bye. "
8357768821,AbleTo,Ning Li (ning.li@ableto.com),,AbleTo,>1000,50-100 Mn,Oct-25,,,Mental Health Care,USA,Nikunj Bajaj,AbleTo_Ning 26-03-2022,,"Call 2

Monitoring we use Streamlit, Dataset for model and the model itself.

The outcome is Streamlit Dashboard and we host that in a Google internal instance. You can share and link to someone and there is an interactive dashboard.

For prediction we are using a Looker. That is just easier to share with the product.

Stremlit is more technical audience focused.

There is also a lot of data operations stack.

Use Google Query and orchestrating DBT.

Call 1

Been around Boston since Grad School- working remotely

7 people in the team, hiring couple more this year. Still a fairly small team for a company this size. 200 FTE, 1000+ therapist in the country.

Built the team ground up when there was only 1 person.

As a tele health provider, focus on mental health. Serve the health plan pairs by providing intervention. Obviously we cannot outreach every member. Classical targeting problem. Which small subset will benefit most from our program.

Reduced physical health cost is also amazing.

We bring in the claims data. There is an Airflow pipeline. Client sends over SFTP every month.

We have an ETL pipeline that loads and processes this data and ranks this patient by score. Only a select subset is outreached by engagement centre.

Participate in an 8 week long session and they graduate.

Processing this data properly and targeting the patient.

We have been using existing pieplein which uses BigQuery and SparkML. We have been rebuilding MLOPs framework which leverage Kubeflow to build new models. DataProc which is Google version of Databricks.

The reason we are working on new one because we feel Spark is missing some key DS features. VertexAI - want to orchestrate more python native packages.

For the version we have used, the strength of Databricks is clear- it is fast. The weakness is also clear- the available algorithm is not as rich. Some ells and whistles are missing- what does the model learn. XGBoost can easily apply Shap package and see the performance metrics- for spark its possible but not easy. XGBoost isnt bad in terms of speed.

We use VertexAI by ourselves but not AutoML. We need a lot more customisation so instead we use the really use Vertex AI to have Kubeflow pipeline is server less. I say its mainly in-house. I dont know what Google claims that to be but from a user perspective it doesnt finish the last mile or even last 5 miles.

We use their images for XGBoost and copy that but we should still need some customisation.

Lot of features like metadata tracking, logging etc. is still lacking.

Mentioned we are almost done with the stack. Dont want to drop stuff we have already built and dont want to swap to.

Sent out a follow up call just for
"
8173019537,Beyond Limits,Majd Jamaah (mjamaah@beyond.ai);Hussein Al-natsheh (hal-natsheh@beyond.ai);Gerry Rizzo (grizzo@beyond.ai),,Beyond Limits,100-500,10-50 Mn,25-50,AWS,Yes,Software Development,USA,Anuraag Gutgutia,Beyond Limits_Majid 28-07-2022,,"Quick Call with Majd

How are things going with you? All good. Long holiday for Eid

Gerry and everyone had 2 weeks of annual vacation => This got us delayed

Complete the deployment: Hussein will come, Gerry will go

It has been delaying our communication

We have finished the minimized version : Can't deploy and keep it running for months

ML Guys have a free time slot : Shipping this next week

Majd and are teams aligned to work internally: I am leading this project to enhance our MLOps Infra => I need to co-ordinate with all teams

ML Engineer team => Use case that we will apply

We will be free from ML Team : 1st -10th August

Majd, Hussein and Anuraag to have a call"
8173019537,Beyond Limits,Majd Jamaah (mjamaah@beyond.ai);Hussein Al-natsheh (hal-natsheh@beyond.ai);Gerry Rizzo (grizzo@beyond.ai),,Beyond Limits,100-500,10-50 Mn,25-50,AWS,Yes,Software Development,USA,Anuraag Gutgutia,Beyond Limits_Majid 25-03-2022,Beyond Limits_Majid 25-03-2022,"Meeting Date: 25-03-2022

Very good meeting. Asked a lot of questions.

Let's compile a list of all questions asked here and put it into a doc: https://drive.google.com/file/d/1620kyJLIvwpcyU-SXu3uvW45BoAN8Utc/view?usp=sharing

Follow-ups -

to send them an agreement for Design partner

to send them a follow-up email by Monday with the presentation and the Product Architecture included.

to follow-up with them on the next steps."
8173019537,Beyond Limits,Majd Jamaah (mjamaah@beyond.ai);Hussein Al-natsheh (hal-natsheh@beyond.ai);Gerry Rizzo (grizzo@beyond.ai),,Beyond Limits,100-500,10-50 Mn,25-50,AWS,Yes,Software Development,USA,Anuraag Gutgutia,Beyond Limits_Majid 08-03-2022,Beyond Limits_Majid 08-03-2022,"https://drive.google.com/file/d/1wP3t1AIPVQzJN67EIGt7oVBdMOI2z7_C/view?usp=sharing => Link to the call. Hussein had invited a new teammate from DevOps team as well.

What are the biggest Problems you are facing?

MajD : DevOps Manager

Use a lot of ML and AI => Type of data is structured as well as unstructured

Use Cases: British Petroleum - Health Mgmt. Take data from sensors

How many models in Production? // Data Pipeline?

Tools you are using?

Looking into MLOps // Its not standardised

Use MLFlow Extensively => got into Production // Kubernetes - Cloud based ==> Multi Cloud // Monitoring - System Monitoring only.

Observability: Going back to data pipelines? Monitoring => Data Monitoring

Follow-up

Need to suggest a few times for a follow-up call, preferably Monday to Thursday

Share the 2 Pager, also send a brief of the Experimentation Platform and benefits as compared to MLFlow, share the small presentation I had shared.

to share this - latest by March 9th

Thanks!"
8077328669,Fundbox,Artyom Tochilovsky (artyom.tochilovsky@fundbox.com),,Fundbox,100-500,100 - 500 Mn,<10,,,Financial Services,USA,Nikunj Bajaj,,,
8023082830,Tifin Group,Mike Lewis (mlewis@distillai.com),,The Tifin Group,100-500,50-100 Mn,<10,,,Financial Services,USA,Anuraag Gutgutia,,Tifin_Mike 24-02-2022,
7992269109,TradeShift,Mike Williamson (mike.williamson@tradeshift.com),,Tradeshift,500-1000,100 - 500 Mn,<10,AWS,Yes,IT Services and IT Consulting,USA,Anuraag Gutgutia,TradeShift_Mike 22-02-2022,TradeShift_Mike 22-02-2022,"Only been running it for 6 months and mostly IC roles. TradeShift - recently was a start-up.

B2B Marketplace => easing invoicing and payments. 40K Suppliers.

USE CASES: 1) Menus are dynamic 2) Product Categories - what kind of suppliers and producers are a good match (Depending on the behaviour on the past, menus will change) 3) Manage pre-funding => loans

Relatively slow team and growing: 4 ML (DE and ML) //// People send in that arent digital // We have OCR => didnt build inhouse. /// Dont have a very strong

ML Pipeline: We use AWS for cloud for most things // Either own EKS or own Kubernetes clusters or Sagemaker. Couple 100K Unique models and allows people to custom build: 12 repos.

1) We are using KubeFlow Pipelines right now. 2) Have a pretty strong and robust SRE and platform team. Have to do all the Helm charts. 3) We will use KubeFlow pipelines

Grafana v/s SuperSet => Generic apache visitation

Data Security - its still taking us weeks to get this done. /// DPA Agreements etc.

FOLLOW-UP: * Send across the Experiment Tracking Solution part * Send a token of 100$"
7992268684,Bloomreach,Samit Paul (samit.paul@bloomreach.com),,BloomReach,500-1000,100 - 500 Mn,Oct-25,AWS,Yes,Software Development,USA,Anuraag Gutgutia,Bloomreach_Samit 01-03-2022,Bloomreach_Samit 01-03-2022,"Use AWS +GCP. Build things in a cloud agnostic way.

Some inorganic acquisitions which have some cloud already

We are an engineering heavy company

The DS works in an isolation, building models etc. The e2e requirement is needed. Most of my folks are hands on backend engineer. They can write production level pipelines.

What is my major areas or engineering side. In the centre we have model building- none of them are old school models.

Most models are NN models. There are some light weight architecture as well. Just a DL architecture with CNN flavoured things. We also have various kinds of metadata so deal with multi modal problems.

Structured, unstructured and image as well.

Left side we have data pipelines. Some orchestration engine. Then we have model part of it. Deployment is interesting as well. We have real time models as well. Model pruning, ONNX format.

We know the overall lifecycle. We dont use standard MLOPs tools but not still there. We can stitch the pipeline, write the entire flow.

Down the line, Ill be interested.

Is there a way to make this formal.

IDE is also a question mark. Some people decide to go in Sagemaker mode. Certain things come with Sagemaker. Some other people go EC2 route. The latter is economical but you need to take care of a lot of things.

We have some important models which are key models. Customer specific model is our trade secret.

We use K8s from an engineering point of view. Within DS we are starting.

Within DS we dont use any formal orchestration framework. Predominantly we are into GCP. Moving away though. Its a company decision to move everyone to AWS. Dont want to do a lot of tunnels.

We have categories of models are limited but variations are many.

Everything is on cloud but end of day it is somebodys responsibility to log it.

Asked about our cloud vs your cloud.

Get to see all the infra to provision.

If its our cloud, we do a markup based pricing.

Pay the infra + SaaS or markup on the cloud.

Think about the compute cost and pay it based on that.

To have an effective conversation.

Ideally, people who should listen to the depth are people who are doing things day-to-day.

State of the company- state of the company. Product is in building stage. Experimentation platform is being used by 4 months. Mentioned about design partners.

Do you believe in partnership or get done.

Also interested in monitoring as well.

Amex has a lot of governance restrictions

We have GDPR requirements as well."
7984043912,Stord,Mohnish Chakravarti (mohnish.chakravarti@stord.com),,Stord,500-1000,50-100 Mn,<10,AWS,No,"Transportation, Logistics, Supply Chain and Storage",USA,Nikunj Bajaj,Stord_Mohnish 28-02-2022,,"Working at Stord in process automation.

Columbia Operations Research degree,

Niklas- Stord for 9 months- setting up data Warehouse.

Getting to modelling decision making type projects.

How to forecast for certain things- starting to get into ML supply chain projects.

Adhoc data team prior and now transitioning to robust team which has systems and processes in place. Want to follow the MLOPs process, CI/CD. Putting cloud functions together. Put it on Github, version control, cloud correct practices in simple way.

Apply unit tests to do things the right away as opposed to the fast way.

Worked at TI various roles- bootcamp at UT Austin and did undergrad in Supply Chain.

BigQuery. 5 people working in ML people. 3 ICs 1 manager 1 director.

We are still super early, scoping exploring etc.

Primarily ingesting data in the warehouse.

Inventory optimisation, demand forecasting, supply demand-

Some of the models could be online as well. We have to price opportunities- looking at market rates.

We deploy most of our processes using Google Cloud functions. Trying to do everything through Git and Terraform. End result is a cloud function.

SHARED EXPERIMENT TRACKING DASHBOARD ALREADY"
7983848751,Zeonai,Biswa Singh (biswa.singh@zeonai.com),,ZeonAI,<50,<10 Mn,<10,,,IT Services and IT Consulting,India,Anuraag Gutgutia,Zeonai_Biswa 28-12-2021,Zeonai_Biswa 28-12-2021,"1st call: 28-12-2021

Co-creating with a few clients right now

Blue Yander - Doing Consulting with them. They have built their own DS Platform => Mostly working on Demand Forecasting there (JDA Software)

Most small companies are struggling to build platforms in ML. One partner was at Genpact and Product Managing QuoraML.

Capillary - still not evolved much in ML. We talked to few companies - built data Labelling but customers saying that MLOps is the problem. ** 3.5 years back => Everyone was building 1-2 models in Notebooks. SparkML Lib. (Batch Inferencing) ** No Schedule, someone will run it anytime ** Many requirements coming from Product Mgrs : Churn Prediction model ========> We had 300+ customers and no way to scale any model for any customers. Even train for those customers. Biswa and Subrat built a tool for auto-retraining, monitor. We started exploring => How would we scale? First decided to make different services - Training service, Deployment Service, Feature Gen service etc. Again not scalable => Find out Workflow mgrs - Luigi, AirFlow. Found Airflow to be mature. Most micro services running on ec2 machines. Kubernetes based platform came up. Capillary decided to move to DataBricks 1 year back. Went for a managed service. Moved a lot of micro services and put into a notebook based service in DataBricks. Orchestrated everything through AirFlow. Still had service running in Kubernetes applications. Didnt leverage MLFlow of DataBricks. Some realtime churning product that Capillary is building. Inferencing is daily and training is monthly. Also did thresholding kind of thing - 1) Model is evaluated every day as well - Monitor. Trigger training or 2) 1 month based training. (UI Built by Capillary) => 2-3 years to build it. Upekkha Accelerator (B2B SaaS Accelerator) //// Product: How do I manage my models? /// If someone can build a templatised stuff: Data Integration Service, Training Service - support pytorch, etc. Shared Resource as well

Blue Yonder: Built a platform where // DataBricks has no concept of Service deployment.

DATA BRICKS: Prabhu da will tell - Did you know about the costing of DataBricks? How much is the mark-up?

Too many frameworks - which ones to choose etc?

Zeonai: Generic models and small number of customers. Training we have not started. Using SageMaker to deploy models. Havent faced that problem yet. Later on, will build automated training workflow. Focussing: Summariser model, QG model, QA Model => EdTech // Zeonai is into Knowledge Discovery Space => Build Enterprise Search. Documents in Audio, Video, Internal Search. For Litigation company, building on top of the documents.

Havent scaled much to build a ML Platform:

Use cases: People give feedback - the feedback mechanism feeds into the model. Edtech companies - exposing their API.

Short Term Goals/ Longer term Goals:

One of the partners is a product mgr and he built it and deployed in system.

Mix of Stratosphere and Databricks // Different services: AdHoc Service, HTTP Permanently running service - autoscale, 

TrueFoundry Deploy: We are building and should be there in 2-3 weeks

Is the models cloud agnostic? By design, it is cloud agnostic but by implementation, it is AWS.

People might say dont want to go through AWS. CHALLENGE. One click move from one cloud to another. Can something be deployed in ServerLess?

Reason for talking about Serverless: Main thing is indexing of documents. Have written Lambdas to create this and managing and monitoring that is a pain.

Give us a function => Tell us what we need => We will find the cheapest and best mechanism for the same.

AGNEXT: Code here and there, Models here and there. (Talk to them) => Agritech Company (Quality assessment model)

2 weeks from now => Connect to see the Demo // Will need training the models continuously - That use case will be there

How does the library installation and Support is there? We deploy any code you write.

BISWA: Any companies you can connect us to?
"
7983895871,IDFY,Gurudatt Bhobe (gurudatt.bhobe@idfy.com),,IDfy,100-500,10-50 Mn,Oct-25,GCP,Yes,IT Services and IT Consulting,India,Anuraag Gutgutia,IDFY_Gurudatt 13-03-2022,,"(Anuraag) Intro of founders.  

Starting with Sales & Marketing- we were doing Sales Analytics.

Working on a whole bunch of DS initiatives. Started getting into ML and Engineering.

At IDFY- we had a small practice initially. 3-4 products initially. We were doing about 100k requests a month back then. Now we are doing 10s of millions a month scale.

Straight forward engineering initiatives- nothing to do in ML.

(Anuraag) Intro of TF.

(Guru) IDFY intro

Focus is computer vision. We are looking at building products that solve use cases with Vision. We use ML only when it is required.

Where do we acquire data. How we annotate those data. In parallel we research solution and figure out if we need to bring something from scratch which is generally not required.

We usually set up experiments- have a lot of boiler plates now.

We usually discuss what needs to be done. 2-3 people working on it- set up multiple experiments & run them, Can be run on individual computer, colabs, or specialised VMs.

Slice out dataset- train, test, validate and blind for QA purpose.

Again, we have a bunch of homegrown scripts for those.

Once we have a certain baseline that is achieved, we move it to production.

Otherwise, we continue iterating.

We have a bunch of boiler plate for application development. Well defined structure when it comes to code.

Mostly we go with Flask. Homegrown telemetry which sends events to Data Warehouse.

Deployment is within containers on GKE.

We have applications which invoke the ML services which is then updated.

Then handed off to Infosec team will do their own testing

After this is done, we use Kustomize and Argo for deployment. Merge your code deploy with a click.

Experiment log metrics- store in one place and pull it in one place in S3 Buckets. We dont use any formal Experiment Tracking Framework. We export to Jupyter notebook and view it from there. Occasionally we use TensorBoard.

For monitoring- it gets exported into Data Warehouse and we use Metabase to visualise it. System metrics go to prometheus grafana.

Mentioned just getting started with Data Drift- end of April we should be done with that.

There is no such thing as a DevOps team. Every team is responsible for their own applications.

Our teams are MLEs and not DS. Full LifeCycle of ML development.

Logs show up on StackDriver.

InfoSec team just does the application testing. Models are not packaged along with the code at Deploytime.

Evaluating Vertex for now. Eligible for free credits."
7937830254,MathCompany,Abhishek Srivastava (abhishek.srivastava@themathcompany.com),,TheMath,>1000,100 - 500 Mn,>50,GCP,Yes,Business Consulting and Services,USA,Nikunj Bajaj,MathCompany_Abhishek 05-05-2022,,"Use cases for ML models in prediction.

We have a data lake now.

In production system they have created an experimentation layer.

There was sudden need for ML applications.

Data Scientists will start building models and business will start consuming that.

Now that IT has understood that this is a requirement that will scale up, IT is looking into how to deploy this into proper manner.

You do things in experimental layer which gives you an output.

Not talking about real-time use cases. Say weekly or monthly cadence. Even if pipeline ways- you have time to handle.

When you try to do SDLC - it is not required. You have a capability to get done there itself.

Some people are saying does not require SDLC. Package upgrade. This is happening everywhere.

There is a people aspect to it- business got what they wanted. Business wanted who will pay for SDLC. IT is saying SDLC is required.

They have created an experimentation layer which has access to prod data. One database which they write.

Engineering Org  TheMathCompany

All are alma mater of MuSigma.

1 person is from Kgp, other from Madras.

They formed this company 4-5 years back. They were more into Data Science and now they are building Data Engineering and supporting MLOPs. The experimentation layer is there so IT sharing resources. Getting output much out of it.

Batch job, monthly job weekly job etc.

General Electric, Home Depot, KDP, Abbott, Merc  doing lots of healthcare.

Competing with ZS as well

Engineering

30 -40 % of whole company

Last I know we had 1000+ folks.

500 person Data Science org

50 fund raise."
7937752205,Iterable,Ankur Mathur (ankur.mathur@iterable.com),,"Iterable, Inc.",500-1000,50-100 Mn,<10,AWS,Yes,Software Development,USA,Nikunj Bajaj,Iterable_Ankur 10-02-2022,,"We do most data processing feature engineering in Databricks

We use Spark and Scala APIs. Some python APIs. Most framework level code is in Scala.

We have a fairly large scale data- process terabytes of data. B2B marketing platform. Lot of engagement data. Lot of custom data as well. We accept almost any data.

It allowed us to grow into a Customer Data Platform. They mine their data using visual tools that we create for personalised campaigns.

We are trying to raise the level of abstractions. No intricate personalisation. Pretty lofty goal but things that we are optimising for are segments. Help the right consumer and audience.

In terms of model management and all we are somewhat nonstandard at the moment. Business KPI management system. Dont have classification of regression monitoring system. Lift in open rate. Sometimes its even a human in the loop. Talk to customers, product managers and built something custom.

We are heavy users of MLFLow. We use it for even log data pipeline runs and all. Building data observability framework sort of thing.

If Databricks provide something we do it but only.

We are building instrumentation within our pipeline. Own model serving services. Instrumentation sends monitoring services send things to DataDog. This is for system monitoring. Guidance for SRE team is to keep most of the things there. We havent felt the need to create own dashboards.

Data drift hasnt been a huge deal for us. Tracking seasonality and all is not a big deal Product recommendation would have required more of data drift.

We were abusing it a bit. They now have a security model that allows us to have an encrypted key interaction with their MLFlow API.

We get away on model demos by doing notebook deployments. We dont need to formalise that. For explainability we are working on some product that require explainability. Building our custom dashboards.

Somewhat interested in serving model. Trying to do multiple model serving portions of traffic- that can be . A/B testing system. Things like multi arm bandit. Automatically promote better models."
7937732149,Varo,Viral Parikh (vparikh@varomoney.com),,Varo,500-1000,100 - 500 Mn,Oct-25,,,Banking,USA,Nikunj Bajaj,Varo_Viral 09-02-22,Varo_Viral 09-02-22,"Home grown feature store  evaluating Tecton

Model building  looking into pipelines. At Credit Karma used Airflow

Inference - there is no way we can leverage 3rd party. Not exploring any 3rd party. Still using hosted solution AWS Sagemaker.

Model Experimentation  MLFlow will evaluate later. Not worried about right now.

Model Monitoring - Internal Dashboards around that. We have Arize and all. It is top of mind as well.

Wanted to evaluate workflows.

Mentioned will get back to me.

Definitely aligned with where you are going.
"
7937617669,Cherre,Misha Sulpovar (misha@cherre.com),,Cherre,50-100,10-50 Mn,Oct-25,AWS,Yes,Real Estate,USA,Nikunj Bajaj,Cherre_Misha 18-02-2022,Cherre_Misha 18-02-2022,"Bye. Hello. 



Hello. Hi, how are you? 



Hi Jeff. I'm good, how are you? 



I'm good. Thank you for choosing this. 



Yeah, of course. Are you dialing in from New Jersey? 



Are you in which location, where located? 



Sorry, right now I'm traveling to India. My co founder and Rag is based india. 



Oh hi Ragla. How to pronounce him ragna. 



You can call me ANU. 



ANU. Nice to meet you. 



Nice to meet you. 



Jeffrey so your company is in San Francisco or india? 



San Francisco, basically like our company headquartered in Delaware. 



Okay. I need to meet with you guys india. 



He just meant for us to take this call and figure out some good next steps in terms of like the ML of Set, merck and stuff like that. That's the context. I can give some more context. Do you work very closely with Suman? 



Yeah, I work under Suman's organization. 



Okay, understood. So the context here is that Summer and I had a chat about one of the platforms that we are building. I will give you some background about our team as well. Jeff this is around operationalizing your internal machine learning processes. So imagine like deploying models, deploying batch and print jobs, monitoring your models and stuff like that. And someone really liked building based on the discussion. So he was like, we should definitely do a POC so we can figure out how can this help merck in general in his process. Right, so that's where someone connected us with you. Let me share a little bit of my background and rock and also tell you some details. 



So I personally come from a machine learning background jeff I did my masters at UC Berkeley and since then I've mostly been working in the Bay Area in ML. So initially I worked at a startup called Reflection where we built out a lot of recommender systems for the ecommerce industry. So there I got a chance to scale our machine learning systems from 100 users to like 600 million users a month across hundreds of ecommerce websites. Most of I joined Facebook to lead one of their conversation AI teams. So if you have used their virtual assistant product called Portal, which is like the Alex or Google home of you, that's the team where I was leading out of the conversation AI efforts. 



And between quitting Facebook and starting True Foundry, actually Anuraga, Vishek and myself, the three co founders here also did another startup that got acquired by the largest HR tech player india. So that's a little bit about my personal background. Also telling you about one of my co founders, Abishek, who's not joining this call right now. Abhishek actually joined Facebook and spent like about six years there. Three years out of New York office and three years out of the Bay Area office. And before he quit Facebook, he was leading the entire videos.org as an IC basically. So actually one of the fastest growing engineers at Facebook. He has also led some of the Caching and Preoptimization teams as well. 



Okay, I can add maybe briefly just like again batchment of anikunjana Vishek from Karakpur. After that I worked for a hedge fund called Worldwide. We used to do asset management across global markets. I was managing around 600 million in assets for them primarily again across global markets trading equities and during that time I was also a member of the CEO of Is looking after various strategic initiatives for the firm. Spent part of my time in Mumbai and then around three and a half years between us and Singapore myself. Used to also angel invest into a number of startups and then finally that Mercedes into our journey of finding our startup and then now building through foundry with them of optimizing ML models. 



Okay, I see. Yeah. 



We would love to learn a little bit about you as well Jeff before we start. 



Yes, sure. So I need engineering team here and also have a couple of projects like MBX and Business Engine and mostly so I'm on the engineering side, I manage the product development deployment like going out to the market. My background, I jumped different health care company mostly in the healthcare AI space. Before Merck, I worked for clarity and voxo cloud. They all healthcare AI startup. And also before that I have spent half year in Cisco. I work as a software engineer and I graduate as a commercialized degree. So would love to hear more about your products for sure. 



Thanks a lot for sharing your background Jeff. Actually in this call we will obviously share a little bit about our product as well. But we would love to spend some time trying to understand a little bit getting more context on some of the messages that I think someone sent out. Like he mentioned a few names that obviously we don't understand because they're kind of internal names right now. NBX and all that's one thing. And the second thing is we also wanted we had a few specific questions about how is the team at work structured in terms of building and deploying models, what are the tools, technologies, cloud on prem, et cetera that you all are using. So we had a few questions as well, if that's okay with you. 



Yeah, maybe before that it might be good to kind of give a two minutes overview so that context then you can focus on telling problems around those areas. 



Can you share me some of your solutions, what you are selling like the product or service? 



Let me give you a little bit of a background very quickly. Okay. 



Just give me 1 second. 



I'm going to pull up one slide and show you that. I think that should clarify and Rogue, you have that slide right where we talk about what roof on Is building. Do you want to quickly share that and give him some context on what we're building. 



Okay, sure, I can do that. Are you able to see my screen? 



Yes, we can. 



Jeff. So basically, like, the ML workflow, as you know, it involves like data and feature engineering to kind of get the data from different silos into one place. Then there is the model building where data scientists are building their own models. Then you need to deploy it, which could be in production environments, and you need to scale this deployment system. And then at the same time, you want to be able to monitor it in the way that you can ensure that there is any data. Then you can retrain if something is going wrong. So we are in this part of the pipeline. So first you have built out the model, the entire deployment, the entire monitoring piece. 



So basically the thing around operationalizing ML, if you think of our platform, we have built a platform to speed up the developer workflows with full security and control for the Internet. We make it very easy to kind of deploy models for data scientists or ML engineers. We make it also like the platform works in a way that it is very simple to learn for anyone in the team. Like even if they do not have knowledge about complex infrastructure, or if they do not have engineering skill sets, it works. And even if they have engineering skill sets, the platform kind of speed set up, it kind of ensures the best si practices by default, and at the same time, it ensures that everything is done on the same infrastructure that you are currently running. So roughly, basically, there are three major parts. 



One is related to setting up your infrastructure in a secure way. The second is making the model deployment of different types with the right scaling, with the right practices in the right way, and then finally being able to do basic monitoring on top of that. So, just wanted to kind of like this picture so that you have that. And then we are working with companies and customers to build around this as well as at different solutions around this, so that we can help them speed up their overall deployment or the operationalization challenges that they are facing. And that is where someone had connected, as Nicole mentioned. Probably. I would love to hear about the kind of challenges you are seeing at Merck and then very specific to also some of the things that okay, so. 



Are you trying to provide service or this is like a platform. We can use it's. 



A platform you can use, yes, you can use platform. And if you want, you can build on top of this as well. Or we can help you build on top of it, working closely with you as well. 



Okay, how to use this platform? If there is a data scientist they want to deploy the model, how to use it? 



Yeah. So basically what will happen is that we provide like one of our client side library so that you can install do your data scientists frequently use like Jupyter notebooks or like Python? Okay, so you can actually directly install that client side library that we provide on your Jubilee notebook itself. And imagine that you have like written a training script that you wanted to run on a cloud machine or you have written like an inference function that you wanted to expose an endpoint for. So for each of these we provide you APIs as part of that library. You can invoke that API directly from a Jupiter notebook and it will quickly deploy your model on a remote server. 



Basically let's see which infrastructure you require, what kind of infrastructure do you need. 



So we work with pretty much overall any infrastructure right now. The platform supports Kubernetes and can work across multiple cloud on premise as well. But we are open to working where people are using specific clouds as well. 



Plan with the infrastructure inside work you need kubernetes, I'm not sure, do you have like infrastructure? Basically we need to deploy your package or deploy my system right here. 



So you will generally deploy the package in your infrastructure. So we make it, we work with your infrared team to make that happen. And that's why we wanted this call to understand a little bit more about the entire pipeline and we didn't want to give you the full product because a lot of times there are nuances in the product that is on your deployment. So I think just wanted to give an overview so you have a sense of the problem. But if you can tell us a little bit more about the ML pipeline at more about the specific problem areas, that will help us also understand your problem and then we can probably give you a better suggestion to how you can use it seamlessly within more as well. 



Okay, yeah, I can give you some overview of the team setting here. So, basically, we have multiple products like MBC stands for Next the Best Experience. Next Business Experience. So It's recommendation platform, it's already been built and served for different countries. And we have piercing inside engine. We have business. Engine. Those are pretty matured products. Another part of the large group, they are technical analytics and they do like ad hoc stuff and they analyze the patient journey or market size or just commercial business analytics and they provide the insights back to the stakeholder so they can decide how to sell more job. And there's a data strategy team, they onboard the third party vendors. So we have a lot of vendors like Ikea, Komodo, those are the data vendors. 



They provide the claim data to us and we need to onboard those data set and standardized data set. Yeah, that's basically the settings here and there's the It team, so we need to communicate with them. It's called HSID, they provide AWS environment. So I assume if we use your product, we have to deploy your product. Because we manage the Python package, we have the internal pipeline, and we only can install from internal pipeline. We cannot download from outside. I need to think about how to sort of onboard your package and deploy your system. Because we have limited sort of budget, if your system costs too high, the item won't allow this. So that's why I need to understand your infrastructure by battery so I can get sense about how to talk with the It in order to deploy your system. 



So I'll give you one context here. Just like we have been working with another really large company, two very large companies in the enterprise space similar to yours in different domains, and they also have like a pretty stringent It team. So we have been able to work with them to deploy on their infrastructure pretty simple constraints with regards to VPC or with regards to downloading an external package. And all of those will not be there. Like, we'll work around the constraints of your It and will kind of work seamlessly with them to make that happen. So that is something you should not be worried about. But Nicole, do you want to share more around that so that concern at least is allocated? 



Yeah, for sure. So like you mentioned about the internal pipe package, of course there will be some security. And by the way, this is all when we get to that stage of deploying truefoundry on the platform, right? So far, we still have to validate if the fit in terms of the project that you guys are working on, what we are building, et cetera, is all good. So we have to spend some time there. But imagine that we get past that then exposing our package to be deployed on your internal pipe by going through the security reviews, going through the compliance reviews, et cetera, something that we have done a few times before. Our platform has already gone through stuff like VIP testing and stuff like that. 



So basically we have already gone through a bunch of the hoops in terms of It, security, compliance, et cetera. And also our team is flexible to work with your constraints as well. So if there are some other things that we need to take care of, happy to work together and make that happen. So I guess that's where we stand. If you had other specific. 



I need to understand your solution better. So can you share some details about the solution architecture or something like that? Because even like, if I on board you guys, I have to socialize with the It team. So they will ask me what's the solution? They need to understand what you guys are using with service like that. 



Okay, let me maybe spend like five minutes and give you an overview of the product itself. Okay. Just so you have a good mental map of what we are doing, how we are doing it, and if you have any more questions on that, we can actually take some time today to answer those questions and then we can get to some questions that I'm ragging I had for you. Is that okay? 



Okay. Yeah, sure. Okay. 



I'm going to keep it fairly high level given that this is a first call that we are taking. You get an overview of the product itself, right? Do you see my screen? 



Yeah. 



The way you should think about the demo is imagine that Merck already started to work with True Foundry. What would happen first in terms of like whatever installations and all. And then what would the data scientists or machine learning developers within the company, how would they interact with our platform? Right? So like, imagine recommended system, how would they interact with the platform? Those are two parts that I will show. So first thing is, let's say if you have already a Kubernetes cluster running within your company, that's what you want to use truefoundry on that is deployed through. The way that works is like you connect that cluster with our Truefoundry dashboard. Now, connecting up cluster itself does not do anything. It just tells us that there is a Kubernetes cluster that exists like this. 



But then what you would do is you would actually install a couple of helm charts on your Kubernetes cluster. And that's basically going to set up the infrastructure that is required by Truefoundry to run your machine learning models. And at that point, you will start seeing that this cluster is connected on the Truefoundry platform and it's available for the data scientists to be able to deploy their machine learning models via the Truefoundry platform. So that's pretty much like the setup. There's a couple of other things that you can set up. Like if you wanted to connect your docker registry or get and stuff like that, you could do that. But I'm going to skip all of that for now. Now imagine that this place is set up and I'm going to give you a quick overview of what your developer experience would look like. 



Right. Imagine you have some data scientists in the company that either want to deploy any services, which are like, imagine any API endpoint for preprocessing or postprocessing of your machine learning models. Or they want to deploy some jobs which could be like a training job or a batch inference job. So something that runs once spins up a machine would execute that piece of code and then kill the machine. And lastly, if you wanted to deploy a model where let's say you have a model file saved anywhere, like an S three or something, you just provide us the Uri of the model and we will just deploy that and give you an end point of the model. So these are the three things that a machine learning to represent deploy on the platform. They can do it directly from the UI. 



So, for example, if they wanted to deploy a model, all they do is on the UI, they tell us the name of the model, they will tell us a Uri of the model itself, and just hit Deploy. 



Okay, if they want to deploy a model. So can you show me an example of the model? 



What an example model looks like? Is that the question? 



Yeah. 



So in here, basically, we support different types of models. For example, let me actually show you how a model looks like. So, for example, let's say we have, like, whatever, a basic Churn prediction model here. So, like, this model itself, if you think so. These are all the models that are logged on our platform. We have version control of each of the models. 



Here. 



You see that the way this model looks like is it takes like a few of these inputs as arguments, right? Like, these are the features that this model takes as argument. And then the prediction that it makes is like a categorical prediction, for example, right? And then if you wanted to understand how this model looks like, you can actually look at the actual, whatever. This is a psychic learn model. It is saved using a pickle format, for example, right? And then you can obviously download the entire model if you wanted to and stuff like that. Basically, we can also track some important things like metrics and stuff. 



I'm just trying to understand the workflow. So data scientists can write anything in jupyter notebook and then what they need to do. Okay? 



So I think the way this conversation is going is we are practically giving, like a full demo of the platform in some way. I'm happy to do that, but I think usually the only problem in giving, like a full pleasure demo without us understanding some of your requirements is we end up covering, like, all functionalities of the platform that may or may not be relevant. That's why we generally like to personalize the demos for the problems that people are facing. But in this case, because you're curious, I'm going to give you a little bit of this workflow. So, for example, let's say a data scientist wrote, like, one training script in their Jupiter notebook. This training script is very simple. They read their data, they instantiate a classifier and the trainer model, right? 



Now, let's say they want to run this particular training script on our platform, okay? So there are two different ways in which they can do it. Number one, they can directly have this training job deployed from their Jupiter notebook. So basically, like, this service boundary that you're seeing is true as our SDK that a data scientist would put on their machine. And then they write like, these five lines of code. So they need none of these, actually. They just write these five lines of code and when they hit a job deploy it will directly deploy this particular training script on our platform. And they will be able to track that on the dashboard here. Okay, so they can actually see this on the dashboard here. 



I see. Could you go back to the notebook? There's this. 



Like, be the same script. 



That the data scientists wrote here, and they just. 



File. 



There are also ways on the platform where they don't need to dump it to a file and they can directly deploy like a function as an end point as well without actually writing it to a file. We also have that API. 



But how about the data if they want to connect with a database? 



Yeah, for sure. So remember that this is like a vanilla Python script, right? So in this case, they are getting the data from this read CSV, like a CSV file that's hosted on GitHub. Practically, you can actually implement a simple thing like read data from that function and we would be able to read that. And we also have full permission controls as well built into it. So if you wanted to provide, read or write access to the data scientist about a certain database or whatever, you could do that on the platform as well. 



I see. Okay. So what kind of service do you need to deploy this? 



Sorry? 



What kind of the service? You said you need a Kubernetes, right. How large is the cluster? 



So that really depends on the kind of loads that you want to deploy on the model. Like, we have a couple of infrastructure elements that we need to have deployed on the Kubernetes cluster. So, for example, we need a postgres database and one file system itself. So that's what we need. Like you're deploying on AWS. If you're deploying on an AWS cloud, then we need S three. If you're deploying on GCP platform, the minimum requirement is this one. So usually some of the Kubernetes customers that we have worked with are like nine CPU and 3GB of Ram in terms of sites, just to make sure that we have enough things to deploy the infrastructure that we need to run all these systems. 



You use the apex provided. Kubernetes. 



It usually depends on the type of usage. So I think without actually understanding the kind of usage of the platform, it's actually really hard to provide the cost because is it constrained on CPU? Is it constrained on GPU? Is it constrained on number of developers? How many models are you deploying? 



You charge by usage or you charged by user. 



So user is one metric, but we also have different levers on the number of applications that you are deploying on the model. It's not necessarily about how much CPU are you using, we charge based on that. But like, number of applications that you're deploying, number of models that you're deploying, the size of your set that you're dealing with. So there are a few factors that we need to consider. So it's really hard to give up charging. 



Okay. I need to have a rough idea about how much it costs. We need to report the budget. 



How big is the team. 



Which have copied 50 I think at least 50 people developer at least 50. 



Okay, got it. Let me just think once. 



So if we did not assume anything, if you had to really guess based on how many models are these developers deploying and stuff like that. If we really had to just index on that one number and give you an estimated price point, that would be roughly in the range of 200K for deploying for that many number of developers? 



Two hundred k per year. Okay, that's based on what? Based on users or usage. 



That's the only information that we have. So like if you had to guess a price based on that, we would say something in the range of 200 kwh. It can be more or less depending on the usage. 



Okay, so you need Kubernetes because our It teams, they don't like Kubernetes. We mostly use service tool. Can you not using communities instead of serverless? 



We can support serverless. So basically, Jeffrey, at this stage, as Nikon mentioned, there are some organizations we are working with Q and It's requirement is there. There are some organizations where people are asking us for new things. So if serverless is a requirement, we can definitely build on top of it. But we'll need a little bit more understanding whether of this are all models going to serverless? 



One question on the serverless, when you say serverless, what is it running like? Do you mean serverless? Like lambda serverless or running it on? 



We can use Lambda glue, job orchestration tool like Step function, Airflow, those are what people frequently use in our organization. And if you I don't think anybody use communities. 



We can do Far gate Fargate is something that is already getting used on our platform. 



Okay. Is there a way like we can have like trial version that people can test it? 



Absolutely, yes. We can also spin up an account for you that you can try the platform out. Get a feel of how the platform like deployments and all work on the platform. Absolutely, we can do that. Yeah. 



When you say open up account, it has to connect with your server. 



So if you wanted to just try out the platform, we generally recommend using not using any sensitive data for the trial itself. Right. You probably don't want to use any very sensitive patient data or something. Then you can directly do it on our cloud, like with some public data sets to get a feel of the platform. That's a very quick way of trying. We can open up like a user account for you. If you wanted to do more immersive testing of the platform, like trying out, then you could also experience deploying our platform on your cloud or something and testing it out there. But usually that process is a little bit longer and we recommend trying it out on the public platform. 



I see. 



In the long term, my understanding is that you would not want to use like public cloud for training your machinery. Models of the sensitive data that you deal with and everything would probably need to be by design within the ecosystem. 



Yeah. Okay, so what kind of information you need? You said you want to understand better of what yeah. 



I think you mentioned number one, that is like you are using AWS as a cloud. First of all, two things, that is how is your team structures? You mentioned you have 50 to 100 people. Do you have like a lot of data scientists who do a lot of experimentation but don't deal with like actual productionization of these models? Or do you have ML engineers who do like both kind of stuff like a platform team? We'd love to understand your team structure and then also would love to understand about what is production for you. Like when you deploy models, what does deployment mean? What kind of models are getting deployed? Where are they getting deployed? Would love to understand the deployment workflow as well. So yeah, if you can start with these two questions. 



So, as I mentioned, there are three products. So for the product project, so they have entire product team like data scientists, engineer, MLPs and they build a model by yourself and also deploy the model by themselves. They use mostly firm data like step function, like Airflow, good job, those things. And then model deployment. Sometimes they use Sage maker and then for the technical analytics, we sort of want to deploy products as much as possible. If your platform can help us deploy their work, probably it's meaningful for us. For that technical analytics, they just write the script in Jupiter notebook and conduct analysis and then the slides put into a slice and send to the business stakeholder. So you can help them to maintain their work and also connect with we have internal asset management tool. 



Can browse the model through internal website. I'm not sure the progress there I need to connect with them, but it seems like someone want to better manage those assets. It seems like your platform can connect with that asset management tool. Nice. 



Yeah. So actually I want to understand a little bit more. You mentioned about the technical analytics like where they write jupyter notebooks for creating a presentation for the leadership, right? Maybe help us deploy that. What do you mean by that? What kind of deployment there are you talking about? 



I mean probably just trigger the Jupiter notebook in a certain frequency. Sometimes there's a report that has to generate like monthly, weekly like that and that's probably one use cases. 



Understood. 



Okay, got it. 



And. The second thing is, like you mentioned about the asset management tools. Are you using like what are you using for your model and asset management right now? 



I'm not sure there's a team to do that. I will meet them next week to talk about how to collaborate, certain things for that. That's the internal engineer to sort of do this. I don't think they can connect with the model. It's just a place to put a link. There is a model name and it can link to the gift repository document like that. But it doesn't have a way to trigger the model. 



I see. Understood. Okay. So they don't have the full loop closing where the model itself is saved, but they can't actually trigger the model or get an API inference or something from the model. 



Yeah. 



Okay. Actually that's a good .1 of the other thing I wanted to ask is you mentioned that some teams use Stage Maker. Do you know which parts of Stage Maker are the teams using? Is it like more on the training deployment? 



Deployment. Usually they don't train on a Sage Maker, but they will deploy their training script using Sage Maker. So for the MBX, they will train the model, like once an update model and then use the back interest job to trigger the model. 



I see. Got it. Okay. 



For those products, I don't see any need for your platform. But for those ta work, they probably need some way to maintain their work and deploy put into the product phase. That's what suma want, I think. 



So just to make sure I understand this right, you're saying that for the training jobs that they run on Sage Maker, there is not value to be added from the platform, but basically managing your like, the model one place is something that we can add value. 



Yeah, managing those analytics work. 



Oh, I see. Okay. 



Basically I think some I want to build more like production life more work. Like those tape. 



Work is right now not getting productionized. You're saying Jeffrey? 



No, this ad hoc. 



And you want to get to a state where all of these are also productionized so that someone else access. Out of the 5200 members you mentioned, how much part of the team actually works on this technical analytics work? 



I think at least half of them. 



Half of them. Okay, fair enough. Understood. Okay. And curious to know about. I remember someone also referred to some projects like I think they had cryptic names, four of them. 



Which project? In WhatsApp group? 



I think the projects that you mentioned were like NBX CTAP. 



Yeah. Okay. If you want to pilot for this, I need to talk with C Tag and Pi team, see if they have the need. And also for MBX is very matured product. They have well maintained infrastructure. It's hard to break in. I think for C tub and Pi or bre. It's very new. But for BR e, it's a different story. It's not machine learning model. This is all business model, all the SQL. This is more focused on ETL part, but for the CPAP and Pie probably there is some opportunity there. I need to talk with some. 



Also out of time currently, but is it possible to have another call with you? We would love to understand a little bit more on the technical analytics part, like exactly what is happening, how many models probably. And then also if you are able to get some more information from CTAP and Pi and if need be, like we are also open to doing a more detailed presentation for this folks or folks in this team if that helps in some way. Like we kind of. 



Not in the merchant network, so I'm not sure, I don't think we can talk more of that part of the work. I can just tell you that they write into the notebook. Some of them are machine learning model. I think that's all. I don't even know the detail about what they are doing. The model won't be too crazy. It's not a deep learning model, it's just even simpler than machine learning model. In our work, the most heavy part is ETL part. The data is pretty noisy most of the time they spend based on the GP part, I can tell you, but I sort of understand your technology. But I haven't jumped into more detail about the infrastructure. But I need to talk with a different team and to see if they need this kind of tool. 



If you can you set up meeting, talk more about the infrastructure? Yeah, that would be more helpful. 



Yeah, I think we can do a second meeting where we can tell you more even about the product. Like I think we kind of gave a high level "
7931906589,Curai,Luke Diliberto (luke.diliberto@curai.com);Anitha Kannan (anitha@curai.com),,Curai,50-100,<10 Mn,Oct-25,AWS,No,Wellness and Fitness Services,USA,Nikunj Bajaj,Curai_Luke 19-02-2022,,"Picking

ML Problems - pretty far from them.

What I do is - terraform to manage infrastructure, Github for CI/CD. WandB, Sagemaker- model retraining and deployment, Dagster for data orchestration

Configuration file and a library. Put all model components in one repo- has its own repo and it handles the slice of the component. Depending on the use case DAGs to pipelines. All TF modules, docker containers that get built in that repo.

On my Radar, what am I getting for monitoring- for model performance monitoring.Endpoints we can monitor using Sagemaker. For each one of these tools, we deep dive. We revise Sagemaker enough to fit in our use case.

To some extent we know the interfaces of Sagemaker. We will be able to move out but it will require building an adapter. I am not very scared but people have mentioned it.

ML Research- close to data and product. Researchers are engineering competent. Then we have ML Apps- They give us python package. MLOps team is going to move that over to Sagemaker. How we wind up porting that in. Adjusting the containers as we need to. Customise docker containers for their models. Retraining happens in Dagster. Open up a PR for a new model. Hyperparameter optimised for MLR. We retrain it in dev.

We get a Trainer class that we put in our training container.

Inference

Standard pytorch structure

5 models in production.

The only problem I see- (sidechat) you need to dive deep into tools to extend them. If we were using MLFlow I need to understand the source code to handle the corner case. When I am evaluating software, is the code open enough that I can extend. Can I read how it is made and able to extend. That approach is not easy to people to grasp."
7922298281,Jupiter,Deepanshu Wadhwa (deepanshu.wadhwa@jupiter.money);Vivek Y S (vivek.ys@jupiter.money);Jitendra Gupta (jiten@jupiter.money),,Jupiter,50-100,10-50 Mn,Oct-25,AZure,No,"Banking Mumbai, Maharashtra",India,Anuraag Gutgutia,,Jupiter_Deepanshu 03-03-2022,
7918569001,Corel,Teresa Joy,,Corel Corporation,500-1000,100 - 500 Mn,<10,AWS,No,Software Development,Canada,Nikunj Bajaj,Corel_teresa 08-02-2022,Corel_teresa 08-02-2022,"Meeting 3

Some bullet points to be sent.

Distribution ot sentiments

Actionable insights

Questions for Teresa

Need employee IDs so as to be able to get themes out of where people are presented. Some ID that connects answers in a group.

Meeting 2

Sending out data, presentation and NDA

Nikunj to get back by Tuesday with the models.

Legal team will take it to get it signed.

Meeting 1

Work for 3 timezones. Report to CEO who is based in PST.

Team in EST, contracting resources in Sydney

Some analytic things that we are thinking about. Leverage AI/ML for a niche greta.

Do have it in my OKR for something.

Overview of my role.

Context about Corel-

Umbrella is a house of brands. We have 14 brands under Corel. If you are a mac user and windows app is part of corel brand. If you use Adremover which is part of Corel family of products.

We are a truly product company- mostly engineering.

We are rebranding ourselves in March. Awareness about us is always in the context of graphics. Massive rebranding.

Lead platform data analytics. Typically Chief Data Officers. All of our Digital Execution platforms.

I have a group of engineers who work on that component. Developers who work on website. I lead data strategy and management. Data architecture for Corel as a company. Lead our analytics function. I lead digital Sales and marketing analytics or B2B analytics as well. We do have enterprise products. 60% of our revenue comes from B2C and 40% comes from B2B. In what market does Corel Operate. We operate in 250 countries. 90M active users across the globe. Products we dont call home we dont know how many people are using it.

Platform dev is product R&D Engineering team- they develop integrations. How our in product technologies. In Data Engineering and management team have mostly Data engineers or DBAs. On analytics team I have people who know how to do data sciences but they are not ML engineers.

There was no sense of understanding our own data. Basic analytics or attribution or spend. We do have some core infrastructure- multicloud. Have footprint in AWS and GCP. We are pretty hybrid company. Hybrid data platform. Both have some AI/ML and we havent dabbled with it. We were doing fundamental ground work.

We are working with Google. CorelDraw is our flagship product under graphics portfolio. Has about 40k features embedded in them. R&D team loves to add features. Nobody really knows what feature drives the conversion. Then we can see what converts. Graphics portfolio- feel trial is a product itself. Our trial conversion rates are very low on the graphics side. All these features we want to figure out drives conversion.

We built it out in Jupyter notebooks. Somehow it didnt give us a lot of details that we need to. That is a perfect use case for us to understand the conversion of in product usage, for retention or usage or churn. What drives trial conversions for us. What drives retention when you look at in product behaviour.

This year we are implementing Amplitude where they bring some predictive modelling. We are trying to track in product behaviour.

Spend about $60M in media. Would be curious for that. If you have a use case for digital marketing. Havent seen a lot in that space.

We do an employee engagement survey. One of the ask for the CPO is which has a lot of unstructured text in it. Can we get insights. Each question is accompanied by a free form in some way. As of last week we have about 900 employees. 2800 comments that we got. The ask from my team is unstructured data- can we mine it to get some intelligence.

We can walk you through the models we created. Send you a deck that we have. How we look at data and how we look internally. Working with leader of analytics. Trying to figure out what we can do with this data. Usually this is not numbers. Completely want to help us. Still waiting to see actual data and just got the attributes.

Corel is interesting in a way that there is a shit load of legal stuff. There are a lot of text mining culture surveys.

Churn prediction is a really important problem for me. Corel has an execution infrastructure isnt the most robust. Execution and recommendation isnt the most robust. When we know the churn, what behaviours drive and somehow"
7918569001,Corel,Teresa Joy,,Corel Corporation,500-1000,100 - 500 Mn,<10,AWS,No,Software Development,Canada,Nikunj Bajaj,Corel_teresa 11-02-2022,Corel_teresa 11-02-2022,"Meeting 3

Some bullet points to be sent.

Distribution ot sentiments

Actionable insights

Questions for Teresa

Need employee IDs so as to be able to get themes out of where people are presented. Some ID that connects answers in a group.

Meeting 2

Sending out data, presentation and NDA

Nikunj to get back by Tuesday with the models.

Legal team will take it to get it signed.

Meeting 1

Work for 3 timezones. Report to CEO who is based in PST.

Team in EST, contracting resources in Sydney

Some analytic things that we are thinking about. Leverage AI/ML for a niche greta.

Do have it in my OKR for something.

Overview of my role.

Context about Corel-

Umbrella is a house of brands. We have 14 brands under Corel. If you are a mac user and windows app is part of corel brand. If you use Adremover which is part of Corel family of products.

We are a truly product company- mostly engineering.

We are rebranding ourselves in March. Awareness about us is always in the context of graphics. Massive rebranding.

Lead platform data analytics. Typically Chief Data Officers. All of our Digital Execution platforms.

I have a group of engineers who work on that component. Developers who work on website. I lead data strategy and management. Data architecture for Corel as a company. Lead our analytics function. I lead digital Sales and marketing analytics or B2B analytics as well. We do have enterprise products. 60% of our revenue comes from B2C and 40% comes from B2B. In what market does Corel Operate. We operate in 250 countries. 90M active users across the globe. Products we dont call home we dont know how many people are using it.

Platform dev is product R&D Engineering team- they develop integrations. How our in product technologies. In Data Engineering and management team have mostly Data engineers or DBAs. On analytics team I have people who know how to do data sciences but they are not ML engineers.

There was no sense of understanding our own data. Basic analytics or attribution or spend. We do have some core infrastructure- multicloud. Have footprint in AWS and GCP. We are pretty hybrid company. Hybrid data platform. Both have some AI/ML and we havent dabbled with it. We were doing fundamental ground work.

We are working with Google. CorelDraw is our flagship product under graphics portfolio. Has about 40k features embedded in them. R&D team loves to add features. Nobody really knows what feature drives the conversion. Then we can see what converts. Graphics portfolio- feel trial is a product itself. Our trial conversion rates are very low on the graphics side. All these features we want to figure out drives conversion.

We built it out in Jupyter notebooks. Somehow it didnt give us a lot of details that we need to. That is a perfect use case for us to understand the conversion of in product usage, for retention or usage or churn. What drives trial conversions for us. What drives retention when you look at in product behaviour.

This year we are implementing Amplitude where they bring some predictive modelling. We are trying to track in product behaviour.

Spend about $60M in media. Would be curious for that. If you have a use case for digital marketing. Havent seen a lot in that space.

We do an employee engagement survey. One of the ask for the CPO is which has a lot of unstructured text in it. Can we get insights. Each question is accompanied by a free form in some way. As of last week we have about 900 employees. 2800 comments that we got. The ask from my team is unstructured data- can we mine it to get some intelligence.

We can walk you through the models we created. Send you a deck that we have. How we look at data and how we look internally. Working with leader of analytics. Trying to figure out what we can do with this data. Usually this is not numbers. Completely want to help us. Still waiting to see actual data and just got the attributes.

Corel is interesting in a way that there is a shit load of legal stuff. There are a lot of text mining culture surveys.

Churn prediction is a really important problem for me. Corel has an execution infrastructure isnt the most robust. Execution and recommendation isnt the most robust. When we know the churn, what behaviours drive and somehow"
7918569001,Corel,Teresa Joy,,Corel Corporation,500-1000,100 - 500 Mn,<10,AWS,No,Software Development,Canada,Nikunj Bajaj,Corel_teresa 16-02-2022,Corel_teresa 16-02-2022,"Meeting 3

Some bullet points to be sent.

Distribution ot sentiments

Actionable insights

Questions for Teresa

Need employee IDs so as to be able to get themes out of where people are presented. Some ID that connects answers in a group.

Meeting 2

Sending out data, presentation and NDA

Nikunj to get back by Tuesday with the models.

Legal team will take it to get it signed.

Meeting 1

Work for 3 timezones. Report to CEO who is based in PST.

Team in EST, contracting resources in Sydney

Some analytic things that we are thinking about. Leverage AI/ML for a niche greta.

Do have it in my OKR for something.

Overview of my role.

Context about Corel-

Umbrella is a house of brands. We have 14 brands under Corel. If you are a mac user and windows app is part of corel brand. If you use Adremover which is part of Corel family of products.

We are a truly product company- mostly engineering.

We are rebranding ourselves in March. Awareness about us is always in the context of graphics. Massive rebranding.

Lead platform data analytics. Typically Chief Data Officers. All of our Digital Execution platforms.

I have a group of engineers who work on that component. Developers who work on website. I lead data strategy and management. Data architecture for Corel as a company. Lead our analytics function. I lead digital Sales and marketing analytics or B2B analytics as well. We do have enterprise products. 60% of our revenue comes from B2C and 40% comes from B2B. In what market does Corel Operate. We operate in 250 countries. 90M active users across the globe. Products we dont call home we dont know how many people are using it.

Platform dev is product R&D Engineering team- they develop integrations. How our in product technologies. In Data Engineering and management team have mostly Data engineers or DBAs. On analytics team I have people who know how to do data sciences but they are not ML engineers.

There was no sense of understanding our own data. Basic analytics or attribution or spend. We do have some core infrastructure- multicloud. Have footprint in AWS and GCP. We are pretty hybrid company. Hybrid data platform. Both have some AI/ML and we havent dabbled with it. We were doing fundamental ground work.

We are working with Google. CorelDraw is our flagship product under graphics portfolio. Has about 40k features embedded in them. R&D team loves to add features. Nobody really knows what feature drives the conversion. Then we can see what converts. Graphics portfolio- feel trial is a product itself. Our trial conversion rates are very low on the graphics side. All these features we want to figure out drives conversion.

We built it out in Jupyter notebooks. Somehow it didnt give us a lot of details that we need to. That is a perfect use case for us to understand the conversion of in product usage, for retention or usage or churn. What drives trial conversions for us. What drives retention when you look at in product behaviour.

This year we are implementing Amplitude where they bring some predictive modelling. We are trying to track in product behaviour.

Spend about $60M in media. Would be curious for that. If you have a use case for digital marketing. Havent seen a lot in that space.

We do an employee engagement survey. One of the ask for the CPO is which has a lot of unstructured text in it. Can we get insights. Each question is accompanied by a free form in some way. As of last week we have about 900 employees. 2800 comments that we got. The ask from my team is unstructured data- can we mine it to get some intelligence.

We can walk you through the models we created. Send you a deck that we have. How we look at data and how we look internally. Working with leader of analytics. Trying to figure out what we can do with this data. Usually this is not numbers. Completely want to help us. Still waiting to see actual data and just got the attributes.

Corel is interesting in a way that there is a shit load of legal stuff. There are a lot of text mining culture surveys.

Churn prediction is a really important problem for me. Corel has an execution infrastructure isnt the most robust. Execution and recommendation isnt the most robust. When we know the churn, what behaviours drive and somehow"
7904663670,JusPay,Magizhan Selvan (magizhan.selvan@juspay.in),,Juspay,500-1000,100 - 500 Mn,<10,,Yes,"Technology, Information and Internet",India,Anuraag Gutgutia,Call did not happen,,
7904657541,Zeta,Ramki (ramki@zeta.tech),,Zeta,>1000,50-100 Mn,Oct-25,,,Embedded Software Products,USA,Anuraag Gutgutia,Call did not happen,,
7865040558,Theremin,Mridul Khurana (mridul.khurana@theremin.ai);Hemant Kothavade (hemant.kothavade@theremin.ai),,theremin.ai,<50,,<10,,,Capital Markets,India,Anuraag Gutgutia,Theremin_Hemant 15-02-2022,,"02.15

Experiment tracking is very useful

10 years of market data  dataset is vast.

Monitoring intuitively it sounds very good.

Where we struggle a bit - say we monitor accuracy of a model. Average accuracy is 54- when you get 52 number you dont know if something is broken.

Outlier detection is useful.

First Call

Theremins whole mission is ML powered investment in the market

Most focus has been on alpha generation.

We spend a lot of time in what to invest in. Modelling and evaluating models

For platform we have built various tools and toolkits.

Data side- bunch of web scraping, Sql + mongo database.

Extracting data and featurising it. Modelling side we have a mixed bag.

Some tools to help things move faster. No platform on modelling.

For back testing we have some important. Bunch of libraries or toolkits we have. We inherit what exists and

There are some analytical toolkits for evaluation side. Performance evaluation or something. We have played with Lime and Shap. Only for feature importance.

Once test is passed we go into production oriented side. Its a simulation or paper trail. We have developed something for ourselves to see that.

Then we have an order processing system - converting ML rex to real word requirements. Order processing system has a feedback loop. Small website or dashboard where we can track whats going on. Operational stuff are more command line.

We have been grappling with how do we manage environment on our local servers.

Some of these things have been of interest to us. Everything is OnPrem.

Cost of cloud always holds us back. 10 months vs 3 years. I get data centre free from

Grid search takes 1-2 weeks. It has also constrained our model architecture choices. Can you do distributed training in one shot.

Experiment tracking would be helpful

Dockerisation question

For evaluation purpose we predict all the data we have.

Live price data is realtime

Trying to solve the code as is. We are working on the cloud. We can figure out something for on-prem."
7865017491,AlphaStreet,Srinivas Kadaba,,AlphaStreet,<50,<10 Mn,<10,,No,Financial Services,USA,Anuraag Gutgutia,AlphaStreet_Srinivas 09-02-2022,,"Watched the entire Intro Video: As someone in that space.

Tell you about Srinivas: I graduated with my PhD in 1996. Masters in PhD and Purdue. Spent in BellLabs for Wirelass communication.

Always been an infrastructure guy

CTO at AlphaSteet and recently moved out.

Its not the maths that matters but the systems that matter :)

Distributed Systems, Scale, Infra challenges!! Actual work in the AI Space itself. CTO Responsibility => these algos we need to use

Extremely Small Scale company in terms of using ML/NLP: No AI. Take up the aspect of MLOps well.

Problems: 1) Did experience deployment challenges => entire engineering team is in India. 2) ML and AI is a crucial part of Data 3) Want the MLOps aspect of work solved

OBJECTIVE:

Experience doesnt go away. One of my strengths is able to connect the dots.

LOGISTICAL: ** Havent got any model into Production. We know what we need to do. Actually put it into Production ** They will hire anytime => will connect you with

CONNECT WITH ALPHASTREET: ** Potential Users of what we are doing. ** Building the APIs and serving infra for data we have

I re-architected the Cloud native data platform - team built it.

PERFECT NEED of MLOPS ==> Abhishek

Experimentation TRACKING: By itself - slowed down a little bit in terms of experimentation // RETRAINING PIPELINE

I dont know how many companies will have people with that maturity. Very little computation, little amounts of data

Send an email to take it ahead.

2 THINGS: 1) Facilitate some connections for us => Some connections might be immediately useful. 2) Advisory Role loosely speaking"
7821961144,Agnext,Subrat Panda (subrat@agnext.in),,Agnext Technologies,100-500,<10 Mn,Oct-25,GCP,No,,India,Anuraag Gutgutia,Call did not happen,,
7821267820,Faire,Wayne Zhang (wayne.zhang@faire.com);Daniele Perito (daniele@faire.com),,Faire,500-1000,500 - 1B,25-50,AWS,Yes,"Technology, Information and Internet",USA,Nikunj Bajaj,Faire_Wayne 21-01-2022,Faire_Wayne 21-01-2022,"Faire-

- I think they are finalising WandB for tracking

- Built out internal feature stores.

- Java based deployments for faster inference time.

- Monitoring will do in Q4 and have already talked to Arize.

- AppFoundry - they seemed to have a use case for but could not visualise with the basic demo we showed. Also mentioned this is not a high priority for them so we should not spend time preparing new demos for them now."
7821370708,Reflektion,Brahm Kiran Singh (brahmkiran@gmail.com),,Reflektion,50-100,10-50 Mn,<10,AWS,Yes,Software Development,USA,Nikunj Bajaj,Reflektion_Chandu 21-01-2022,Reflektion_Chandu 21-01-2022,"Chandu

Lot of manual stuff and trying to build our generic models that can be tested deployed and improved over time.

Build as much as generic models and improve those models. Deploy specific version of the model as opposed to change the code.

The team is under me. German and Brahm are trying to drive initiatives.

Content search is the next big thing. Relevancy is trying to solve how can we return relevant results.

eCommerce product search can be handled.

First phase would be search related stuff.

Right now using AWS services like Sagemaker and all. Rather than trying to find solutions outside. Makes sense to colocate with AWS. Around 6 months later, we might want to gentrify we want to support other vendors as well.

Multi cloud and multi region stuff.

Mostly hosted models on Sagemaker. That way we dont have to have models as entities on S3. With hosted models anybody in enterprise can use them

Not so much of experiment tracking and model building.
"
7821337487,Zest AI,Sean Kamkar (sean.kamkar@zest.ai),,Zest AI,50-100,50-100 Mn,Oct-25,AWS,Yes,Financial Services,USA,Nikunj Bajaj,ZestAI_Sean 12-01-2022,ZestAI_Sean 12-01-2022,"Zest - Use ML to disrupt the FICO industry

Looking at different ways to host our models

Main competitor is Sagemaker, Vertex

Serverless platform made us interested- out model dont take much resources to run. Process intensive than memory intensive.

Looking to go into Sagemaker serverless route.

Right now our deployments take about 4 hours to multiple environments. We are hoping that the Sagemaker will change that for us. Terraform deployment would probably fix that.

What interested me about your platform  the timing.

We have our flask setup and used to

We converted our PoC for Sagemaker invocations.

We are using XGBoost these days. Used to use some scikit-learn and tf.

Sagemaker doesnt have size limit- bring your

Spin up is about 1.5 mins so we will have to do keep warm functionality.

Goal for getting the deployment for our new model. Nuts and bolt to monitoring system down to an hour.

We have an IT monitoring system and log monitoring and model monitoring system.

Data Scientist handle ML & AI and MLE handled scalability.

Model code is separate from deployment stuff. We had an issue where we cant really upgrade models without redeploying the whole thing.

Everything is version controlled on Github. Model artefacts are stored in S3.

All configuration is in Github and moving to Vault."
7821337487,Zest AI,Sean Kamkar (sean.kamkar@zest.ai),,Zest AI,50-100,50-100 Mn,Oct-25,AWS,Yes,Financial Services,USA,Nikunj Bajaj,ZestAI_Sean 14-01-2022,ZestAI_Sean 14-01-2022,"DevOps- 5 strong  DevOps is serrated and trying to embed them in automation teams. Our big push this year is to automate.

Delivery is to build model using credit cards.

Automate that pipeline

Third side of it is  how customers come in to help build their own ML models.

We have got your model

25 MLE + DS  switches daily

Mostly based in the US. Expanding into foreign countries.

Next Steps

Give partner Sean (Data Science) and Lenz (ML Engineering)"
7821289144,Unqork,Piyush Satapathy (piyush.satapathy@gmail.com),,Unqork,100-500,50-100 Mn,<10,AWS,Yes,Software Development,USA,Nikunj Bajaj,Unqork_Piyush 01-02-2022,,"Low code platform

Build Native applications on ML

Third party applications can be on our platform

Named W&B, Seldon,  what if I want to build this in-house

Wanted to get the presentation and the NDA"
7821248546,VoxelAI,Diksha Gohlyan (diksha@voxelai.com),,DAN.COM,<50,<10 Mn,<10,,,Software Development,USA,Nikunj Bajaj,,VoxelAI_Diksha 14-08-2022,
7821248546,VoxelAI,Diksha Gohlyan (diksha@voxelai.com),,DAN.COM,<50,<10 Mn,<10,,,Software Development,USA,Nikunj Bajaj,,VoxelAI_Diksha 10-08-2022,
7821248546,VoxelAI,Diksha Gohlyan (diksha@voxelai.com),,DAN.COM,<50,<10 Mn,<10,,,Software Development,USA,Nikunj Bajaj,VoxelAI_Diksha 05-02-2022,VoxelAI_Diksha 05-02-2022,"Did not work with ML in Facebook or Google

MLOps is generalised programming.

At Google I was an application developer

Voxel is warehouse safety. We ave cameras, run video store reams, bunch of algorithms to figure out safety and all. Safety vest hard hat. No stop. Piggy back violation. We use detectron and have our own classifiers.

Violations work on video. Do our tracking on videos. Trued to use 3rd party tools for development- WandB, built an internal tool called symphony. Previously we had all labels for Google Cloud files. Used CVAC earlier and now use Scale. There are models, CI/CD pipelines and other MLOps tools. We extensively use Pytorch.

We have a simple way of deploying a model. We have model artifacts and use those artefacts themselves in Gitub.We sync our code to head and that runs out system to models. We push the artefacts to code base.

In our code we define config on which models. It helps us visualise experimentation but dont connect to our.

We have an instance running in camera and we sync that with the correct commit sha. It doesnt even need a binary file.

Being able to experiment with models at the same time. Being able to run with new data. Being able to run models with the dataset. Most of the MLOps tools work on images and we rely exclusively on videos.
"
7819022152,AccreteAI,,,Accrete,100-500,10-50 Mn,<10,AWS,Yes,Software Development,USA,Anuraag Gutgutia,AccreteAI 05-05-2022,,"Kubernetes cluster along with Sagemaker => Few hours it takes

How do you compare with SageMaker sort of Functionality

Data Sits in S3 Queue => entire pipeline ready by connecting to S3. Server less and server ful architecture. Real time and batch

Strength => Providing Monitoring which I havent yet seen in SageMaker as a functionality /// Data Monitoring => If we are running on Streaming data - Concept Drift and Data Drift

For Accrete, we are in the process of building similar functionality like what I said. There is definitely a need for such a platform in the market.

A good Product: UI for the product => We can have sort of No Code platform to build ML Platform. We could come in where we come in as a ML Platform.

Users can provide their data end points, people can provide their Model locations and have it in a more seamless fashion. FB has so much more resources than any of us => if we are able to democratize that architecture.

INTRO to friends in other companies. Follow-up
"
7797789303,Balbix,Muthuswamy Sankarapandian (muthu.sankarapandian@balbix.com),,Balbix,50-100,<10 Mn,<10,AWS,Yes,Computer and Network Security,USA,Nikunj Bajaj,,Balbix_Muthuswamy 01-03-2022,
7774463498,Thoughtspot,Utkarsh Ohm (utkarsh.ohm@thoughtspot.com),,ThoughtSpot,500-1000,100 - 500 Mn,<10,AWS,Yes,Software Development,USA,Nikunj Bajaj,Thoughtspot_Utkarsh 04-02-2022,Thoughtspot_Utkarsh 04-02-2022,"Without Sql you can do queries.

When you point to data can we tell you that is statistically significant.

What caused this number to jump so much from today.

Alerting the user for anomalies

Based on historical number that is expected we figure out what is the expected and observed number

The prediction does not need to be accurate but high confidence

Skillwise we are backed engineers system engineers. Do a lot o server side things and have limited data science.

Typical use cases is that you have daily data. You have sales data where new numbers are ingested daily. And you have history for 5-10 years.

100s of attributes can result in gigabytes of data.

For daily granularity it wont be terabytes."
7748743236,Udaan,Kaushik Mukherjee (kaushik.mk@udaan.com),,UDAAN,>1000,10-50 Mn,Oct-25,AZure,Yes,"Technology, Information and Internet",India,Nikunj Bajaj,Udaan_Kaushik 12-02-2022,Udaan_Kaushik 12-02-2022,"I head SWE at Udaan. Been for a couple of years.

Built out Flipkart discovery platforms

Category in OlaShare.

Was one of the early developers at Nullsoft.

Worked on App Development in Windows and

PSIT Bangalore

What we are doing right now  a lot of our infrastructure is a bit of hybrid.

We have our own CDN internally

Our first traffic hits cloud fare and redirect happens CDN. We work on Azure.

We use a lot of databricks infrastructure. We built our custom feature store.

Wanna be data scientists to keep mucking around engineers on our platform.

We have feature store which is not evolved at this point. Databricks is fine but we continue to face challenges.

Amount of experimentations that are possible

Mohit talked about feature stores using Feast.

Databricks is used for analytical queries which are canned. Queries running on a certain frequency.

We have real time model serving use cases but that aspect is not yet mature.

Infra stack is largely on K8s.

For observability we use prometheus as a store and theanos for observbility. Incident managemt using Com-cod

Inciden management

Dashboarding layer using Grafana

Log Aggregation using LK

Batch is largely crons- we use EventHub  kafka

Use MixPanel and have a construct of dogfooding

Nikunj to send the deck."
7748676647,Kissht,Neha Shivran (neha.shivran@kissht.com),,Kissht,100-500,10-50 Mn,<10,,,Financial Services,India,Anuraag Gutgutia,,,
7740377133,Netmeds,Sumit Rao (sumit.rao@netmeds.com),,Netmeds Marketplace,500-1000,<10 Mn,<10,AZure,,Wellness and Fitness Services,India,Anuraag Gutgutia,Active Customer,Active Customer,
7677937509,OnlineSalesAI,Nitesh Garg (nitesh.garg@onlinesales.ai);Shantanu Harku (shantanu.harkut@onlinesales.ai),,OnlineSales.ai,50-100,<10 Mn,,,,Advertising Services,USA,Anuraag Gutgutia,,,
7677797490,WhatFix,Khadim Bhati (khadim@whatfix.com);Vara Kumar (varakumar@whatfix.com),,WHATFIX,500-1000,100 - 500 Mn,<10,AZure,,"Technology, Information and Internet",USA,Anuraag Gutgutia,WhatFix_Khadim 15-09-2022,WhatFix_Khadim 15-09-2022,"Ateendees from WhatFix

Abhishek, Rohit - Head of Data Science

Brief background (if discussed)

Been at WhatFix for 10-12 months. AI Consultant to a Data Engineer. Leading team of DS and insights team.

Abhishek: Senior DS with Whatfix for 2 years. Engineering side of things. Use models and monitoring - Deployment/ Retail side of things.

WHATFIX

Use Cases for ML - types of models (Is monitoring important etc?)

Kind of Use Cases: We don't have Supervised learning use cases. Most use cases are Recommendation (but doing it in an unsupervised learning way) or NLP Based models. Use cases for Supervised models are minimal.

Today, we don't have anything that runs on client side. Tomorrow we might have.

Current Stack for ML Deployments and pipeline

Multi-Node Hadoop set-up to do all the development. All models had Batch models deployments. Artifacts are scheduled, they run overnight and they are used in the product.

Batch Inference - do you use Spark or something else?

A lot of the use cases we have - We need a architecture: 1) CREATE easily deployment code and package it well and create service out of it 2) SERVICE should be fault tolerant.

STACK ON CLOUD: We have an infra where we do experiments (Hadoop clusters) // All the VMs are Azure (I think we would want it in Azure) // For same company, it could be different account.

Availability of the infra and engineering team - we have to wait for months to get their availability. If there was a method through which we could generate the APIs. Model goes through the changes. I want to make sure we have the ebst things avaiable at hand to reduce time to production.

B2C World - this is very relevant.

Problems being faced where looking for solutions

Aim is to give Data Scientists an API. Supervised learning model, Python Script, Unsupervised learning model.

Inferencing framework. Have you seen any solution - almost about to finalise a vendor.

It is about the use - what other vendor?We can do a Quick POC - Azure POC was not smooth.
"
7677698187,YellowAI,Esther Chinnappa (esther.chinnappa@yellow.ai);Raghu Ravinutala (raghu@yellow.ai),,Yellow Messenger,>1000,100 - 500 Mn,25-50,AWS,Yes,IT Services and IT Consulting,USA,Anuraag Gutgutia,Yellow_Esther 25-05-2022,Yellow_Esther 25-05-2022,"Use Cases for ML - types of models (Is monitoring important etc?)

Conversational AI Platform - automate conversations that Enterprises are having with consumers. SDK that can go inside Apps, etc. Along with the bot building platform, complementary product for Contact center agent.

Outbound communication - Campaign Mgmt etc and personalisations => Campaigns can be delieered

Current Stack for ML Deployments and pipeline

100 Member Engg team, 10 member DS Team - 3 persons more on research and remaining doing both research and engineering.

Models: BERT and fine tune it for our use case. Also create classifications model. Bot level models are there - which sentence lead to what.

Using Collab, have custom dashboards where pump in data into Open search. Deployments are all containerised - Kubernetes and cloud - GPU based. Support all 3 clouds - because of data localizations and loss in diff geographies, have to support all clouds.

DevOps team maintain kubernetes cluster and developers have access to it. Healing etc are taken care of by DevOps. For CI/CD pipelines => all the code goes into BigBucket => Container registry and then Jenkins triggers the rest. Thinking of using ArgoCD.

EKL Jobs - are based on AirFlow => data goes into blob stores for Azure and S3 for AWS. So far, have exposed spark clusters. Experimenting with SnowFlake - it can be the tool that other teams can use to extract data for needs.

They were trying versioning using MLFlow => but it hasn't been operationalised. It was a need. Once model is built, ML Engineering team converts the model into services.

For DB Mgmt and Kubernetes monitoring => L1 alerts from NewRelic. Kubernetes metrics - go into Prometheus. Model Performance => how diff versions are working, for that, we are pumping data into Open sEARCH and have built custom logs.

Have one common Jenkins to control all regions. Prometheus - data is stored separately. Thinking of making the entry points common. Open Search - they are currently different.

Problems being faced where looking for solutions

They were trying versioning using MLFlow => but it hasn't been operationalised. It was a need. Once model is built, ML Engineering team converts the model into services.

* A lot of 0 to 1 thing happened before I joined. Building repeatable flow would have taken 6-9 months. Automation keeps on taking the backseat. A lot of things we haven't handled => Drift analysis eg. (KUBERNETES ADOPTION - We started doing in 2019) => Developers were facing challenges in scaling up and down. Microservices side and it got propagated to ML Side.

* Currently struggling to be GDPR Compliant. If they have to rely on us to expose all the controls, it will become difficult for them.

* HIGH USE CASE: Real time Monitoring for a Real-time use Case. Have only real time. Batch use cases are not there.

Questions asked wrt Product

* Is Deployment FastAPI based or FlaskAPI? You can use anything like PytoRCHServe, TensorFLowServe etc

* Every single Deployment => you can deploy wherever you want.

* We can expose the part of cluster creation automatically => want a cluster in XYZ. SLEEP WorkSpace => its constantly incurring cost. It will re-start the machines as well. Each Dev can also track the billing part of it.

* Are the deployments happening in our Kubernetes cluster or TF Cluster? (Collab ++ Kind of thing => You can do whatever you want). You can get a copy of the entire Infra on your own cloud => Only your own workloads on your side.

* 2ND QUESTION: Let's say we have currently 6 different regions: Will we need to have 6 diff dashboards or could I go to one single UI and see? There will be a clusters tab => There is a Control Plane. You deploy CP on one of the clusters. CLUSTER LEVEL Access and WORKSPACE Level access.

* What is the current Stability of the Product say, a hem chart you have to install

* Will MLFoundry only work in training? Or even after depoyment, you can continuously keep tracking. Real time inference monitoring will not be handled by this"
7487458991,Stylumia,Jugraj Singh (jugraj.singh@stylumia.com);Sharath Puranik (sharath.puranik@stylumia.com),,Stylumia,<50,<10 Mn,<10,,,Software Development,India,Anuraag Gutgutia,Stylumia_Jugraj 12-01-2022,Stylumia_Jugraj 12-01-2022,"We train model on a regular basis.

We check logs in our terminal.

If the model is working better than previous experiment.

Just loss or accuracy.

Sometime we wait even 10 mins to see how the model is working.

When model is ready we try to run everything in production and see the performance.

We wanted some tool that can show up this.

We tried MLFlow and we have to explore a bit more.

We have to share credential with everyone to use that.

We are constantly updating our data and we dont have anything like a data versioning.

We wanted to try that also.

Every time we add new data how the model is working.

One thing we want to track here is a certain amount of data for retraining/ Lets say 1000 images for training data- we can automatically trigger retraining the model. If we keep the data in a single place in terms of version control. We have line created, trained deleted etc.  as soon as we have new data points added, we can trigger a new training of the model.

We have accuracy of the older model with us already. And if the new model is better we can easily provide that.

Lets say we are training 5 models of different splits of your datasets- which of the 5 models is the best in validation. Feeding that back in the data creation is useful.

We tried DVC but not getting properly adopted because of complex nature of it.

Our data creation is not centralised and we dont have a common bucket. Everyone is working in their own systems- Jupyter notebooks and all. We dont have a centralised machine or cluster for experiments. Download data in their own environments.

Training jobs dont run on laptop because we dont have local resources. All training is on single machine.

How critical is experimentation monitoring.

Retraining again and again is a lot of manual work and it slows things down. We need to start from a black box machine and it slows things down. State of the machine is lost.

We would like to account for gmail. Experiment tracked with login and access to the whole system will be taken away once login is taken. We dont want to expose GCP details via a service account.

Visibility of a feature is not a problem. Who has created the last step or change- we want to know that. If we can isolate experiment based on prefix of user name so buckets dont have a problem.

Is there a concept of VPN

They directly login to the machine using SSH.

We rely on Google CLI security.

Jugraj Intro

Stylumia is a forecasting prediction company. Started with Fashion industry using public data, eCommerce data. Good analogy will be Google Rank pages. That is our USP. Which e-commerce products are trending what is the trend value.

We have ranking algorithms. Also, we use ML to extract attributes from apparels.

In terms of a team structure- Sharath is one of the founding members and is Director of Engineering. I am responsible for the architecture from storing data to handling multi cloud infrastructure. Avinash is in MLOPs whose responsibility is to keep models up 100%

Overview

Different models some run on GPU and some CPU.

Models are API based- scale up or down.

Some are batch based.

For example attribute / color extraction- we try to do it as an API service and ranking is batch processing.

We have our SaaS offering. We allow for inference attribute. Model directly is not exposed to customer.

Do you also have proprietary customer data?

Sw in general availability has data from our own sources.

What types of customers?

Anyone who can be a brand, retailer, use case is mostly batch based. We do the inference one and hardly the model value is changed.

If things are not good we can update the model on a case by case basis.

Multiple team use data for that inference so it is deployed as a service. Pixel level segmentation of a dress. This is a GPU based on Kubernetes. GKE.

For some machine we have custom autoscaler written.

What is the tech stack?

For data and monitoring- Elastic Search. Framework we have tensorflow, pytorch. Python as a core. Not using any paid monitoring service.

Healthcheck setup for the machine.

TorchServe for model service

Size of team

14 people in tech team as a whole. 5-6 people in the ML team.

Key challenges

During model development we have some road blocks. Model training has to be done manually. Parallelization is hard to get. Hyperparameter tuning is one. Deployment is well taken care of.

Future plan - something the may not scale

We are not stuck in the near future.

1 year earlier we had some problems related to autoscaling

Monitoring

We use Sentry.

We also use ELK stack.

Data / Scale

8B entries in Elastic Search for documents. Some of them are not inference data.

Vision we have 20-30TB of images. We are done extraction on that.

Bucket size that I checked was Aug-Sep.

How Frequently do you update your models? How many models?

Few models are updated on a monthly basis. We have more than 20 models in production.

Sometimes it can take weeks to test out new improvements. If it downgrades the performance of actual model.

Problems

Experiment tracking

Developer resource management

Next Steps

Send out the experiment tool

Sharath join in as well.

Have to check with Sharath who is traveling to the states. We are free around 2 PM-4PM."
6688376748,Housing.com,Chirag Sharma (chirag.sharma@housing.com);Anil Goyal (anil.goyal@housing.com),,Housing.com,>1000,>1B,<10,AWS,No,Real Estate,India,Nikunj Bajaj,Active Customer,Active Customer,
6688341264,Innovaccer,raj.singh@innovaccer.com,,Innovaccer,>1000,100 - 500 Mn,25-50,AWS,Yes,IT Services and IT Consulting,USA,Nikunj Bajaj,Innovaccer_Raj 26-03-2022,,"Platform product at Innovaccer.

Developer experiences. Analytical platform. Healthcare is new to me.

B2B enterprise platforms.

Started at SalesForce, worked with ML models at Lyft and noticed the pinpoints.

I used to discourage my team to not use ML and the complexity is too high.

Now at Innovaccer- ML is still a new thing for us. Our predictive models are more statistical, metrics, key indicators etc. Slowly, we are seeing opportunity. We are absolutely seeing opportunity. One of the biggest area we see opportunity is in unstructured data.

Actual Care pathways, guiding activities is harder. We are relying on other people to do that.

Our job is to see ourselves as a platform. Enable experts in the industry, domain industry knowledge. Care team space and provide them the right toolset. Want to be mainly a platform organisation or platform company. ML is part of it to enable them and giving them some statistical and predictive guidance- either financial, cost out come etc. Also for supply chain management- prediction those challenges can be really.

We have used it through partnerships with other organisations- unstructured notes from providers. Parsing those notes out, ey diagnosis notes, medications etc. Even in that case we are partnering with external organisations. Amazon HealthCare NLP engine. Given unstructured data, it gives out diagnosis code which is standardised in the industry- semantic understanding of the content.

It is still digital test.

Roadmap planning is happening. I can bring in some people like Vibhuti who leads our Data analytics team. Her team is responsible for building and maintaining these predictive models. This year we want to start working on.

The biggest problem often is- access to the right data. Deidentifying some of that data is also important for us. Vibhuti will answer this better.

Design partner relationship could be interesting for us. My hypothesis is its better. What is a bottleneck- how we might deal with regulatory scenario.
"
6688307367,Capillary,Piyush Kumar (piyush.kumar@capillarytech.com),,Capillary Technologies,500-1000,50-100 Mn,Oct-25,AWS,Yes,IT Services and IT Consulting,Singapore,Anuraag Gutgutia,,Capillary_Piyush 18-08-2022,
6688307367,Capillary,Piyush Kumar (piyush.kumar@capillarytech.com),,Capillary Technologies,500-1000,50-100 Mn,Oct-25,AWS,Yes,IT Services and IT Consulting,Singapore,Anuraag Gutgutia,Capillary_Piyush 06-06-2022,Capillary_Piyush 06-06-2022,"Problems being faced where looking for solutions

* We are a Multi-tenanted system => User see only models triggered on their side of the system.

* Operators see all the side of the system

Questions asked wrt Product

* What volume of data is your platform capable of handling?

* Is there a dashboard where we can correlate - outputs and also comparing with different ORGS or diffeerent verticals? (Is there we can compare different points)

* Could we compare the data statistics between models and see?

* Is there a way to get holistic view of alerts in terms of training?

* Purely in terms of Pipelines? AUTO-RECOVERY => infra level re-price => We don't control any of that.

Feedback wrt Product - what would make them possibly adopt it

* We have a model that does Propensity calculation => Example which customer has higher propensity. Want to enable brand users to see. There have to be a MLOps pipeline that makes it self-serve for the user? If it fails for some reason => what all feedback will go back to the user.

* Prediction to show how much time the model will take to learn? User is running a long user call. Feedback to the user => Expose the APIs from our system.

* Why is it a long running job? Because its not just model inferencing that is happening. But there will be training process also initiated. Whether the process is successful.

* If N is running, X is failed => We can do easily.

Concrete Next Steps

* In terms of tech metrics => Holistic view will help debug. Collective View.

* We cannot get rid of the DB and the API Layer.

* CORE OF THE VALUE PROP: 1) What are you optimsing for?

FRom environment perspective.

* There is an opportunity cost of trying something out: Explain and run a bunch of these things => Discuss with Saurav and we can see. Can bring in value.
"
6688265378,Level AI,Sumeet Khullar (sumeet@thelevel.ai);Ashish Nagar (ashish@thelevel.ai),,Thelevel,50-100,10-50 Mn,<10,,Yes,Software Development,USA,Nikunj Bajaj,,Level AI_Sumeet 09-08-2022,
6688265378,Level AI,Sumeet Khullar (sumeet@thelevel.ai);Ashish Nagar (ashish@thelevel.ai),,Thelevel,50-100,10-50 Mn,<10,,Yes,Software Development,USA,Nikunj Bajaj,Level AI_Sumeet 27-01-2022,Level AI_Sumeet 27-01-2022,"What I took from the video was if I am an ML engineer who trains a model- you get an endpoint, with operational metrics.

Not clear to me about where is this getting hosted and what is the virtual environment management.

We do some text preprocessing and stuff in the Jupiter notebook. That part was not clear to me.

Where is the operational endpoint getting hosted- will there be a configurable.

One part that was interesting was the feature part that you were describing. Question that I had was will they work for linear models only.

The science phase is Jupiter and engineers have their own VMs. They will build models, they have connectors. Some of our jupyter notebooks they will do a PG fetch. Finally they will export the model. That is the experimentation phase. They build a repo which is a much cleaner version. One is model generation and the other is model hosting.

For hosting we have a simple API layer- the deployment on that is pretty straight forward. The scale is small and we have monitoring built into it.

Our operations are significantly better now because we have done this for applications and services.

We hire more MLEs - we also have an operational ML person. Lot of our work goes in understanding- instantly our costs have increased, some of our costs have increased. We have an MLOps person whose responsibility is scaling ML inference. He also helps with quantisation and all.

For some the refresh rate.

We have our own versioning. For others refresh rate is so low that we didnt care.

We are managing experiments at the server layer. Our staging and dev is on the same server and our prod is in a different server.

I am not aware of the space in general. Not aware of the space in general. If we were starting out- this would have become a no brainer.

With postures we are using an easy way of doing it.

We are using data dog for grafana. We might start using prometheus. It also ties into other metrics that we have. A lot of this thing on the deployment side that is gone there.

0.5- 1 day is what it takes to deploy a model.

We are investing in improving some of those processes.

Uses GCP.

This is based on FastAPI.

At our scale drift doesnt matter so much and developing features matter more. We use large scale pretrained models.

We have put in a lot of effort on an annotation tool.

GCP, some AWS- tensoflor + pytorch +huggingFace

My bigger worry right now is interpretability - models that we use are are pretty sophisticated NLP models. When things go wrong that we are not doing as much.

In terms of quality of training data we worry about as well.

My previous Bloomreach people have done this as well. Explainability has some opportunity for innovation. That is almost a research problem at this point. Its a bit of an unknown. But if I see something that is really innovative that would be very compelling.

Your deployments has to be - your deployment touch points have to be very clean. Your configs have to be managed in a very clean way.

Idea

Can we make dockerisation as an AWS service and launch it?"
6688357893,ProjectPro,Kedar Kanhere (kedar@projectpro.io);Binny Matthews (binny@projectpro.io),,ProjectPro,<50,,<10,AWS,,IT Services and IT Consulting,India,Nikunj Bajaj,Active Client,Active Client,
6688366661,Bluevine,Gilad Even (gilad.even@bluevine.com);Brian Jackson (brian.jackson@bluevine.com),,Bluevine Capital Inc.,500-1000,100 - 500 Mn,Oct-25,AWS,,Financial Services,USA,Nikunj Bajaj,Bluevine_Gilad 09-12-2021,Bluevine_Gilad 09-12-2021,"We have been investing in AWS and we are married to them. We are utilising their Sagemaker pipelines. How is TrueFoundry different from Sagemaker?

We invested quite a lot in this to make it seamless for Data Scientists. We wrapped Sagemaker itself. Even Amazon has a tendency to change their API- especially, if you start to use new technologies. We wrapped their container that they are suggesting to use. Actual APIs for Sagemaker to create a later between Data Scientist and Sagemaker itself. All of our models are using our wrapper which is wrapping AWS functions- it allows us to control for AWS changes

We also automated our infrastructure deployment process. Internal scripts that create new terraform code- a seamless one click task. We have our own infrastructure- security groups, networking etc. We invested a lot on this. Obviously not every company can have that investment. We are proud that we did but this could have been something that Amazon or startup could have provided.

Ds create model using docker container, dump model in S3, go through process of Sagemaker automation, it goes through our CI/CD process - goes into staging automatically. The entire pipeline we built automatons around it.

When we started this was a big time consuming task- we had DevOps, engineers, DS and miscommunications early. Once we got here- life has been awesome.

This took us about 1-1.5 years of man work despite we had Sagemaker. Wrapper, pipeline, CI/CD etc.

We have been always using Jupyter Notebook hosted on Amazon as well. We use Sagemaker a year and a half ago. Once they have the model they dump it to S3.

We have a tendency to limit the set of packages we have and keep things aligned so DS dont choose dependencies for each model. We try to create a base where more models are using the same version of packages. We do have a way of exceptions and overrides in the wrapper. We have a base wrapper and most of our main package is setup. While we built that we prefer not to have different versions that much.

Our wrapper have a lot of default mechanisms and most models fall into our float and dont need to do a lot. It pretty much sets up itself. Folder, model, versioning etc. They do have the opportunity to add more logic and a lot of times Sagemaker allows to put the logic in python but the main aspect of running the model is preset in the wrapper. We try to minimise the similar work needed across models. If they leave it empty everything is default and they can add pre or post processing. We literally have analysts which are less experienced in python and even they can just write the logic  they just have to add python logic and it works.

There are 2 main aspects to monitoring- we have a structured logging mechanism in our Sagemaker wrapper. Different information that we decided was crucial to monitor = we have our entire streaming logic. Data Scientists can analyse this. We also have the ELK stack set up. We also have prometheus and grafana setup. If there are 10 different options that the model can output we track that as a separate mechanism. Logging goes to Elastic Cache and we also ingest everything back to database which allows the data scientists for bulk analysis.

Analysts requested some sort of pipelines- kind of what you have in AirFlow. Does not solve our real time flows for the analysts. We put that inside our Sagemaker containers- multiple objects running in a single flow. Logging into a Database to help us analyse. Once we know we need something- we build an internal framework. We dont like too much complexity.

We are very security oriented.

We do not send our data outside at all. Everything is within our VPC and we have total control. We do not use 3rd party services in the cloud that need our data to be sent internally.

We have also built a data connection library.

Most complications was around DS and Eng not being in the same department. This was an issue and was a big mess. Everything was disconnected, communication was complicated and deployment was complicated. No DevOps intervention is needed now and made DS very independent. Everything CI/CD has drastically changed our work. We invested a lot to make this work. That itself includes different aspects like networking. CI/CD, monitoring etc."
6688366348,FortressIQ,,,FortressIQ,>1000,10-50 Mn,Oct-25,,Yes,IT Services and IT Consulting,USA,Nikunj Bajaj,FortressIQ_Pankaj 26-10-2021,FortressIQ_Pankaj 26-10-2021,"The core ML models- object detection, OCR models, some NLP models, some more algorithmic stuff which is not ML per se but data science at scale.

We tend to have a mix- try to find open source / public implementation of something and optimise to our use case. We are using Efficient Net detectors, OCR times we are using Paddle and iterated 3 times. Just getting into contrastive and metric learning.

Half a dozen people in data science, maybe grow by 50% in near term. Another DevOps/ MLOps has 1 person and data engineers are separate and they are 3 people. When people look at the level of automation from checkin to Github to deployed in Kubernetes. Numbers would double if we count offshore.

We have a weak debugging and monitoring solutions. We looked at a couple things and we didnt notice enough value. We were iterating faster than our drift. At least a quarterly basis. While its all manual - we drop images in buckets and it gets picked up. Its not something that takes a lot of efforts.

We have got a fairly good way of partitioning production data into a way that can be accessed easily. This is more for security, tied to compliance- PIC, Hippa etc. The model can get into production but getting the data out of production is hard. That is also done via Jupiter notebooks.

We use GCP more than we use Azure. Using hosted Jupyter notebooks.

The area we are not as good is the active learning- how to get data back into the system and redeploy the model.

We dont have a good way of estimating the impact it would have. I just dont know how much we would gain.

The problems that I run into the most and the areas that are most annoying are that- our production infrastructure has very good ways of handling security, monitoring (systems). Getting a notebook into production - there is really no way to manage Secrets. We use pythons decouple - using shared libraries . Doing that in notebooks is a pain in the ass. Code reproduction from production is a huge mess for us. We import libraries that is old.

Compliance is important for us  we want to make it as easy for Data scientists to run experiments etc.

I will share the feedback I shared with Hadley - I would challenge you on is the first principles kind of scenario. Adhering to engineering best practices- engineers dont follow either. If an engineer wont even do it in their own world- getting a data scientist do it is harder. Impendence mismatch between security compliance etc.

CI/ CD is not compatible with PCI auditors. Data scientists getting access to data is something that they flip out.

All of this generally sounds useful. How does it integrate- what are the migration paths. We probably have half a dozen different models. Because we try and adopt the best we have and optimise it. It could all be running on different platforms. OCR on pytorch we host ourselves. Object detection is tensorfzlow and hosted in vertex AI. Vertex is pretty good. If you are willing to subscribe to what we are doing. Well integrated package from experimentation to deployment. When you want to mix and match it gets screwed up. Can we build that killer function it would be great.

What you are trying to do is big- there are companies trying to do is big.

I found that the dashboard we have kibana and elastic in there. The performance astounded me.

If you have some way in telling people in understanding the RoI is worth from your side. How much OCR a page cost from T4GPU, to M8 CPU. If you can explore the latency or data you might be able to reduce the cost of infrastructure by X this will help me get notice.

Something in the active learning space can be helpful from our side. We do that tensorflow.

Nikunj to shoot an email and we will plan some next steps. Will involve pulling in some people."
6688349274,Coulomb AI,Khushboo,,Coulomb AI,<50,<10 Mn,<10,,,Data Infrastructure and Analytics,India,Nikunj Bajaj,CoulombAI_Khushboo 29-05-2022,,"Hired ML from next month

Currnetly checking data quality and all

Everything we are doing now is python code

Were planning to use MLFlow- planning to use EvidentlyAI

Stilll in an experimental phase. Whether to go with most solution in house built.

Do some tooling

Time series data

Both Real time and Batch and on AWS

They are currnetly tryng to sort out their Data pipeline dealing with terrabytes of data. Will deal wiht ML later and keep us in mind. We did a design level demo today heavility focusing on experiment tracking and they liked what we are building. Later, we can do a live demo with deployment and all. Currently, they are too earlu."
6688349274,Coulomb AI,Khushboo,,Coulomb AI,<50,<10 Mn,<10,,,Data Infrastructure and Analytics,India,Nikunj Bajaj,CoulombAI_Khushboo 26-10-2021,CoulombAI_Khushboo 26-10-2021,"The network you get in YC is hard to get anywhere else.

DemoDay is certainly very happening. Access to bookface is one of the most helpful aspects of YC.

Adoption of analytics space for mentorship.

Someone working with Steve Jobs is leading Series A program in YC. Its a lifelong engagement.

Valuation part also increases quite a bit because of YC. Capillary CTO started a new company who is our mentor. He is starting up and mentioned to us about the 3 networks- YC, Accel, Sequoia are important.

Talked to Shailesh Lakhani and mentioned that given that you have raised $1.2M - we havent mentioned anything to them.

Overall product is a batter analytics - giving it to fleets to optimise their battery performance. How can we give them the predictive analytics on batteries. It is intensive on predictive analytics rather than real time. We have large amounts of data coming from vehicles.

Mahindra Logistic is one of the companies. Electrical vehicles around fleets. The data is coming real time.

Weak sounding Metrics are coming out of ML & DL both but we are still exploring what kind of models work out.

We are using mix of Jupyter Lab & Sagemaker. We are trying to get intertwined with Sagemaker which is a lot more comfortable.

We have full time and interns working with us full time.

As of now its not productionised. We are intervening and still doing training and stuff. We are also building our dashboard which will get consumed by a fleet manager.

The clients dont have access to our models but just insights. Lets say they have to see if vehicle will reach destination or not? Internally, monitoring will become important.

Learning curve is pretty small because we are experienced with Jupyter and Sagemaker is easy to learn.

Number of models that you will end up building  not sure but startups business perspectives we will try to keep 90% of models common across customers."
6688331933,Intellect,Anurag Chutani (anurag@intellect.co),,Intellect,50-100,<10 Mn,<10,,,Mental Health Care,Singapore,Nikunj Bajaj,,,
6688331812,Passbase,Mathias J Klenk (mathias@passbase.com);Stavros Filippidis (stavros@passbase.com),,Passbase,<50,<10 Mn,<10,,,"Technology, Information and Internet",USA,Nikunj Bajaj,,,
6688298855,Safebeat,Rachita (rachita@safebeatrx.com);Kunj Patel (kunj.patel@gmail.com),,SafeBeatRX,<50,<10 Mn,<10,,,Wellness and Fitness Services,USA,Nikunj Bajaj,,,
