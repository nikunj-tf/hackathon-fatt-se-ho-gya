Sorry about that. Oh no, it's fine. I think it happens sometimes. New Year. Sorry guys, I'm actually driving. So I realized that the invitation was sent from my zoom until I joined you guys couldn't get started either. Okay. Balaji, how are you doing? Hey guys, sorry about the delay but would love to jump in. I know we were connected and I believe you had something that could be of interest to uniform and I wanted Balaji in the conversation because he obviously owns a technology vision and roadmap. But let's just get better acquainted and hear about what you guys are doing. Yeah, maybe I can introduce myself first and then undergo you can go next. So very nice to meet both of you, balaji and Vinod. My name is Mikkun. I come from a machine learning background myself. I used to work at Facebook in conversation with AI, and prior to Facebook, I led the machine learning team at a startup called Reflection where we built out a lot of recommended systems for the ecommerce industry. So personalization algorithms, personalized search recommendations, all of that. And as part of Reflection, I also got a chance to spend about a year and a half building out a horizontal machine learning platform that served the developers on five teams that were building ML models. Basically between Facebook and starting True Foundry, Anraga, Bishik and I, the three of us got together and built another startup in the talent space that got acquired by InfoAge, which is like the parent company of Nokri.com. And generally my educational background is I went to the US for my masters at Berkeley, UC Berkeley, and Karappur is where I did my undergrad and that's where Anragh, Abhishek and I actually met. The three of us are bachelor's from Karappur 2013. Nice. Yeah, I can go next. Basically same schooling as Nikonj and post that. I spent the majority of my career working with a hedge fund majority of my career working with a hedge fund called World Fund. We used to do algorithmic trading using a lot of data to build trading models. Spent four years in India and then spent three years between US and Singapore, during which time I was doing portfolio management for them, during which I got a chance to manage around 600 million assets. That was a good experience and at the same time was a member of the CEO office looking after various strategic initiatives for the growth of worldwide. During that time, also started investing into startups as an angel and then was curious to kind of build something myself. And then Nikkun Janaishek kind of went on the same boat and then continued on the journey with them and I think thanks Vinod, for giving us a chance to talk. Like WADA is one of the investors as well in our company, so got connected by him and at a high level, just what we kind of are trying to do. At two foundries to enable every company to make their machine learning pipeline very seamless. So when Nikon Jana Vishak were at Facebook facebook has an internal ML platform which we can tell more about, and that kind of makes it very easy for data scientists and ML engineers to easily test out models and then release it to production and actually see huge ROI because of the speed at which they are doing things. And when we came out in our first startup, we were building models as well. And what we found is it's really hard to kind of get a model to production in a way that will impact and that is scalable, that is reliable and that is cost effective. You had to add a lot of understanding of the cloud infrastructure, you had to piece in a lot of open source systems and so on. And we felt that if we can bring what existed at Facebook to other companies, like, we'll potentially be able to speed up what impact I could have on the world. And that's where we started the journey of profound awesome. Maybe Balaji and I can introduce ourselves quickly. I don't know if you know how much you know, you might know something, but maybe Balaji just hears a very interesting background and I'll take maybe just one line to introduce myself, but Balaji with you and then we'll just jump to your content guys. Sure, sounds good. One question before I jump into the introduction itself. Like when you talk about Facebook's platform are you referring to or is this a different platform that you're talking about? Yeah, so a combination of EBIT, Earner and predictor. Okay. I actually have a good context on pretty much everything that's built inside Facebook because one of my friends is leading the machine learning teams over there. Anantheband name and some of the people that worked on FB learned previously were people I was trying to hire, at least when I was needing over there. To give you a little context on myself, I worked at Google for over a decade and built Gmail alongside Paul Bukite and a few others from the early stages, 2004 through 2015. I kind of led that effort and. While building that we built a common. Platform that actually could then become the storage and search engine behind Gmail Google Drive Calendar, like all of the Google Suite products. Then after eleven years it just felt. Like I needed to do something new. So I did a startup for a year and a half in the social third space. But like at some point I realized I haven't changed enough capital. The main area that I wanted to I should have spent money on was marketing, which is where I spent the lease. So I decided I needed to find an exit for the company, where Lift was an interesting company, which was very aligned with my principle, where I decided to kind of take my team to and I founded the machine learning team. For Lyft and built equivalent to FB Learn. I actually built something called as Lyft Learn internally within Lyft, which is the machine learning platform, one click deploy of models, everything, right? Like the whole shenanigans of feature engineering, feature repository to one click serving, so all of that with the experimentation platform as well. So that was my journey through the five years I met the ride share marketplace team and the machine learning team and then decided it's like inclined to do something new. Which is when I ended up coming to uniform about a year ago and engineering teams. In the Merchant Navy. The only time we went to Karatpur was by train. My father is posted in the northeast so we used to go that way. So spent nine years in the Merchant Navy and then we randomly joined a friend's startup in Fintech, which was really my learning experience. Spent seven years building his business. Globally we reached 30 millionaire. It was nice experience and it was pure sat. Like 2009, we were doing a dollar 25 a transaction, right? So it was really nice. We did the first mobile stock trading applications on a SAS basis for Tdmitrade Charles Schwab even today show up options express their mobile stock derivative trading platforms are still built on our platform. Right. We had a middleware which integrated all the back end systems, uniform headphones, so auto management system, risk management system, rationalizing the portfolio and then we actually gave trading tools on top. Imagine 2008 streaming charts on the phone, the ability to do 50 technical and fundamental analysis on the phone and trade from chart right, for a retail on a palm tree or 650. Right. So since you said you get I mean, I'm sure you're exposed to that one. So 2008, this was dramatic stuff. When Tdmitrade signed up with us, they put out a that they partnered with us and we were like 22 kids in Chennai. The oldest person was like high twenty s, right? Yeah. This is amazing. So this is incredible. It's truly my learning in tech and cutting edge tech and that company could have become anything it wanted. And there's also my introduction on how to f*** up a great company full cycle. I had to apply that to Cloud Cherry, which is my startup that I founded in 2014, sold to Cisco in 2019, went to Cisco. I was the chief operating officer for the contact center business, the entire webex customer experience suite, which is also interesting because did some MNA there after getting acquired we did a billion dollars of acquisition which was fantastic to learn from the other side. And then I was all set to join a fund or do my own startup and then Omesh said you're not doing any of the above. He kind of grabbed my neck and said come here, we are going to go public, we're going to do great things and when I went for Greece's freshwater's IPO, I had a tinge of sadness that I could have been part of Grish's journey multiple times and I chose not to. So I said, okay, if there's a pre IPO opportunity, it's a great learning experience, right? It's a very different thing from zero to one. So that's why I'm trying to do what it takes to help go public. Amazing. This is very good. I have one question for you, Balaji. When you were giving introduction about Lyft, you mentioned about building out this ML deployment side of things, was it not Flight? Flight was more pipeline side of things. Yeah, correct. Flight was actually only the workflow in this. And Flight was also initiated by Jason and from my team and I. So we met in the first week of joining Lyft. We actually went to Seattle and met with Jason, who's the founder of Flight. But at that time, it was a. Very dedicated and custom workflow engine. And as I started building the machine learning platform, we kind of pulled it out of like the CTA team, which is where Kathan was working. And I said, this is going to be part of the machine learning platform. And it wasn't built on Kubernetes at the time, so Lyft did not have CAES, but we ended up kind of like pushing lift towards building Kubernetes. Flight became like the first engine that was built on top of Kubernetes native and then eventually became what is known as Flight. Today. You should talk to Kathan about it. He has a better story to tell than I do about it. I do see Kaitlyn a few times in some of the ML Ops meetups and all, given that he is building union that day. I know, correct? Yeah. When he was thinking about moving to Kubernetes at that time, he said, let me take a longer migration path of taking our current customers, taking one company at a time and building it out on Kubernetes. I told them, don't take that path, build it from ground up. Again with Kubernetes native paradigms. And that actually expedited being able to take this Flight to open source, which is actually what gave him the ability to start unit. That's amazing. Wow. Good to hear this story. So vinod. Balaji true. One thing before we get started with an introduction of a true foundries. We would really appreciate if both of you can think of this more about us telling you what we are building, getting your feedback on where we are and how can we move ahead in this journey as opposed to really evaluating a vendor that you can immediately use. Or not. So that's the way we would love to take this call. And Rock, maybe you can give an introduction if you want. Here. Mandrag you're on mute. Mute. Sorry. So we know that basically what we are building akshay Truefoundry is imagine like your business is itself an AI driven business. And when it's an AI driven business or for other businesses where AI is an enabler, everyone is going towards data. So they would want to use a lot of data to build models that they can take into the hands of the user very fast. Now, building these models is generally done by data scientists. And data scientists do not have the best engineering skill set. So they would rely on ML engineers and the infra teams to kind of enable them to take these models and build the right infrastructure structure to put it into a scalable production mechanism. When teams like when teams become bigger, they build platform teams, like in case of Lyft or in case of Facebook, or when AI driven teams would be building the platform teams that would enable these data scientists and ML engineers to be more productive and enabling them so that they don't have to worry about this infrastructure and they can do things with a few lines of code. So instead of building that platform, what we are building at Prophet is kind of building a replacement to what teams would build inhouse pretty much like an FB learner or a Lyft learner equivalent or other teams. That is. Built on top of Kubernetes, where we try to enable the entire flexibility of developers, so that whatever kind of models they are building, they are able to actually test it out, they are able to deploy it to prod, and at the same time ensuring that there is full security control for the infrastructure team. So that's at a very high level, I am happy to dive into maybe a small presentation as to how we have built this and even showcase like a short demo if it helps, and answer more technical questions as well, like biology that you might have and even take feedback based on your experience of it out. Sounds good. Yeah. Take us through first Unhindered because Care training ML model is better. Like there's so many ways in which you can approach the problem and everyone does so many things. Let's hear your POV unhindered for some time, right? And then as we know better, we can ask better questions, right? Otherwise we can take you down a rabbit hole, which is not appropriate to you guys. Sure. Okay, let me share my screen. Are you able to see my screen? Yes. Okay. So one of the things actually while Balaji was mentioning one of the builders of the ability platform, I don't know if you know him, Adity Caldro, he was also one of the initial members. He is actually one of our advisors as well. I didn't know Tall was an advisor. Tall actually worked for me at this. Actually joined us as an angel. So we actually have an open safe round where we were getting good angels who have been builders of tech platforms or CTOs or even founders of unicon companies to kind of come and join us in fact love for you both also to take a look at it later. Post this call. Sounds good. Cool. Starting with what proof? Founded so we are building a reliable ML platform. The goal is think of it like an internal developer platform to facilitate the deployment of ML models, making it very simple and forcing best practices. When we think of deployment, it's not just about taking a model and putting it into production, but it goes to different stages. So even when you are training a model as you are saying, you might have to deploy it into a cloud in order to be able to run this training jobs at the right way. So what we do is we enable the part that we don't do is building out the model. But while building out if you have to run your training jobs and you want to compare those, then our system allows you to create clusters and then run your training jobs. Once your model is built and you want to kind of actually deploy it to production, we allow you to kind of take the model, host it into a dev space or a test workspace and then from there you can promote it to production space. In that process you can also do things like shadow testing, et cetera. So all that part is enabled in the system. The first thing is you can deploy your model starting from the simplest predict function to a very complicated graph model patterns as well. We try to ensure that we make it very easy to learn by enabling like Python libraries which is there for the data scientists so that there's no knowledge of Kubernetes needed. The entire system as of now is built on top of Kubernetes as a pass and later on we'll add serverless and other things. Our goal is to also ensure that while you are deploying there's also stability. So we try and enforce the best engineering and SRE principles by default. Things like authentication, secret management and things like that. Also one of the things that we believe is in ML, like monitoring is not separate from deployment. So as you deploy your model there can be things that can go wrong in terms of data drifting or in terms of the model performance not being as per the actual trained. So what we also do is enable that with our deployment infrastructure you get a basic monitoring by default. When you are able to test the performance of the models, you are able to see things like data drift and feature drift and able to throw alerts if that is needed. And while building this, we try to follow the best practices completely GitHub's driven and so on. So the goal is from a value add to the business team, we can make the ML developers much faster. Hopefully we can save on the cost of building and maintaining. Any questions? Just one quick thing. Like if Balaji and Vino, both of you feel like given that you have been building out platforms and using these platforms yourself, if you feel like you're fairly familiar with this information, we can also show you a brief, like two to three minute demo of the platform as well, if that gives more context. Whatever you all think might be better. Yeah, I think this is the pitch I've heard from several companies. So it's pretty much the standard set of challenges you have to deal with in an MLS platform. So it's actually very aligned with the core problems that need to be solved. Maybe you are unique PO on solving the problems and then back with the demo will be great. Right? I think we are completely resonant with the problem statement. Right? Yeah. So what do you do that you think is awesome? You can show us a slide if you want to and then we can use the demo to kind of back that up. Right, okay, let me do that. And I know the hat we are putting on is in the startup ecosystem. We want you guys to do well and everything. So the hat, as you requested earlier, we are putting on a not some vendor eval or something. It's really to see what you guys do and if we have any inputs that are valuable, if you can use it, of course we'll use it. Right, so that's the hat we'll put on. Yeah. I love it. Awesome. So I'll say few of the things that we kind of try and do different from what exists in the market. One is like we have tried to build it very deeply for the developers. So like the onboarding experience and all, and the kind of abstractions we have created both at the infrastructure provisioning level and management of infra level, as well as for the deployment level, we try to make it as simple as possible. So even a data scientist who does not have any knowledge of, say, Doctor or Cuban it is anything, can do it. And even like if it's an ML engineer who understands YML, they can use YAML. So we try and make it seamless. You can do it from the UI, from the YAML, from the Python, from the Jupiter notebooks and so on. The second thing is, I think this is something that you all already resonated. Like some of the companies like Sage, Maker, et cetera, they create separate pipeline for your machine learning deployment. And we believe that ideally it's the same as software deployment, it's just slightly more complicated. So we try to kind of build on top of Kubernetes and follow the same principles that are followed on the software engineering side. So just one thing that I would add here is like Rag mentioned about the developer experience of using different interfaces that the developer can use when they're deploying models depending on their comfort level. So, for example, if like a data scientist wanted to deploy like a quick demo able endpoint just to showcase to an internal product manager or get some feedback, they could do that directly from the Jupiter notebook. Right? But when you're building like a more mature machine learning pipeline, you want to do it with your Python interfaces, like actual Python scripts proper. And when you're doing CI CD, you probably want to integrate with your using your YAML interfaces essentially right, to integrate with your CICD pipeline. So one of the things that we have done but then we also believe that machine learning should follow the same rigor as regular software. Right? So what we have tried to do is we have tried to balance between these two that we really meet the developer in their existing workflow but take the rigor of software engineering. So what we do is we translate each of these three different user interface layers, the graphical user interface, Python and the YAML ones into one new vanilla format that we have come up with. It's like a true foundry specific format and that file is always checked in like GitHub basically. So we follow Githubs at the level of code model infrastructure and secrets altogether. So when you revert something, you can just revert really everything back at one go. So that's just one example of how we're trying to balance the two things. And your target customer is the data scientist or machine learning engineer or both. Yeah. So I think ideally the way we think of this as an ML developer, we really don't want to separate because at some point whichever company that we end up serving would generally have a mix of both. And it's really hard to build out a platform that only one persona will use because we are talking about production use cases as well. So we have built out interfaces that cater to both of their skill sets. Essentially what we are seeing biology is at least in the experience currently very few companies actually trust the data scientists to deploy to Prod. They'll generally have the ML engineers deploying to Prod. So while the interfaces have been built to data scientists, what we are seeing is data scientists will generally use it to deploy it to the dev environment or the test environment and then the final deployment to the Prod environment is done by the ML engine. At least in the current state of the world there are few companies that we have. In fact it's kind of a mix and they are doing everything, but very few companies are in that state. I believe over a period of time, our belief is anyone who dies the code should be the ones to take it to lift. When I started was in that similar boat where data scientists would just train the model and throw it over the fence and then you can't even get the same features in production. So engineers used to hate data scientists, they just couldn't work with them. That's why I was asking that question like are you thinking about it early in the life cycle to enable production thinking for the data scientist? So that like the features that they are using will be pretty much like the same thing that will be available in production. So I think that's helpful. And one other thing that we have done is, and I think maybe if we dive into the architecture and all this part could become clear, we have tried to build it in a way that building any custom plugin is very easy. So what we have seen is the ML use cases tend to be very different and the tags are so different that you cannot have a single platform that can solve everything. So we have tried to build it in a way that at least for any company that wants to build on top of it, or wants to build plugins that is made very simple. So these are some of the things at a high level that we try to do different. The APIs are in Python. We have a python SDK. Yes. We also have a command line interface as well. And do you have anything in Scala or other. Nothing in Scala just only focuses Python and CLI and also the GUI. Yeah, makes sense. Okay, yeah, got it. So I can walk you through a quick demo of the platform, Nicole. It might be actually good to also more parts of the platform to Balaji because this recorded one has the small parts, I'll show one, but then maybe other parts around secrets management are the things you can touch on through the policies. Absolutely cool. So the way Balaji the platform is like there are three parts. The first part is for the intra team generally to be able to create a secure environment for the ML team. So what this part does is it allows you to connect to your existing clusters, blacklist and whitelist machines, and then be able to allocate the right resources to your multi in terms of CPU, memory, et cetera, and put the right constraints. You can do things like production environment, dev environment and so on, or you can split it by teams. And the platform ensures that at any point you do not exceed those resources, you do not exceed the cost. So the guardrails are there to prevent things from going wrong. So this allows that the infra team is in full control, they have full visibility to everything that is going on in the organization and full visibility to all the services and models deployed. The second part, which is exposed to the developers is the deployment layer where all the things around model deployment is there. And we have also tried to do things wherein you can use model servers if you want and optimization of your models. And you can do things like bluegreen deployment or shadow testing. That part is in the building process. So it will come in the next few weeks or so. And finally, the last part is the monitoring. So for any model deployed, you are able to track like model performance and drifts. You are able to ensure that there's debug ability and you are able to send these alerts to the right channel. So maybe I'll just quickly show this and walk you over. Also one of the things is we try to ensure that you are able to integrate with everything that you are using rather than so if you have docketed histories, you can integrate with that get secret stores, alert channels. If you are using certain things, you can integrate with that. So this is the interface where the infertility kind of connects their cluster. So this cluster for example, is connected. For a new cluster, you just kind of click on new cluster. You can select the cloud provider. It is multi cloud supports all of them. You have user access control. You can configure your base domain URLs. You can also configure monitoring URLs if you have certain and these are like instance families. So in case you want to configure, say, GPU or say for certain environment, you want to whitelist the GPU missions, say for the training environment and for say a production environment, you do not want the GPUs to be active. So you can configure that here and you can create multiple clusters. Once this is done, if you already have an existing cluster, you just install a health chart and then basically the cluster will be onboard into the system with all the namespaces as well. Then this is the part like within a cluster, like we have provision for creating workspaces which are similar to namespaces in Kubernetes. For a workspace, you can create it at the dev level, different dev testing production or different teams. With the right user access control, you can limit CPU memory and in the advanced fields again, you can again reconfigure and reallocate. Okay, further whitelisting and blacklisting of machines and other things. You can track the resource limits. We are going to add a cost tab as well here so that we are able to get visibility of cost. So this is the part that allows the introduction to have full visibility of what is going on in the so, any questions so far here? Very intuitive so far. I think foundational elements are there. So let's keep going. Okay, amazing. Then the next part is which is the developer exposed interface where you can deploy a service job or a model services and jobs services, so that you can also deploy pre processing, post processing. Function actually before the deployment model. Can you actually tell me more about the user model? Because you did mention in your previous slide that you've designed it around ML teams rather than individual users. I think that was a very key point because when it. Comes to the development phases, it is useful for even for example, if you're working in a jupyter notebook, to be able to share that jupyter notebook with your collaborators and have the ML team, whichever team is actually operating, have access and can collaborate within that development process. And then to the training and deployment. I can take it if you want to. Yeah, I think you'll have to. Because this recorded one was this sort. Of one sorry, never mind. No, I can do that. Unlock if you want. If it's coming in the future, then in that case that's fine. Let's go in the sequence that you have the recording in so that I'll readjust accordingly. Okay, so do you want me to complete this and then you take over? Or do you want to just kind of do everything from here? I think it's okay. You can finish this and I can show them the code. Okay, got it. Cool. So that part is there balaji wherein developers San Jose net I'll just give a high level explanation. So in the workspaces, as you saw, you have admin and so say this workspace is exposed to a team. So suppose that team, all of them are editors. Then this workspace they will have access to now when they come to this deployment and Nikonju will show the way. You can run training jobs and compare the same thing like here, as you see, you deploy services. Sorry, just 1 second. When you come to this deployment, you can deploy services, jobs. So when you deploy those jobs, if it's in the workspace that is shared with the team, you'll be able to track all the metrics of the jobs as well. So here we are seeing an example of a service. You select a workspace you can deploy from a docker image if you have. Otherwise you can deploy from a source code like a GitHub or anything. Like the Python Build pack. If you're using a Python Build pack, then you just select that and you give your command like Train PY. You can give the part to the requirements. You can also give other packages. We have also wants to like if you want to expose the public URL, if you do not want to expose it or guard it with an authentication, you can also configure environment variables in terms of secrets. And then you click Submit and literally the service is deployed with your metrics, showing up with your logs that you can track as well. You can push this to the corresponding monitoring systems you're using. These endpoints are like the API endpoints within a model. You can go and see different versions. And if you want to redeploy any version, you just go to that version and you will redeploy and redeploy. Deployment will actually ensure that your secrets are versioned to that. It will at least ask you if your secrets have changed. It will ask you if you want to use the old secrets. Similarly you can deploy jobs models and equal so your motorcycles and the monitoring part is where you can track like different metrics. This happens like with an addition of one line of code. So you don't have to do a lot of work. You'll be able to track different metrics, you'll be able to compare across different time windows in terms of actuals versus prediction. If you pause here, I'll just talk about one thing. So here we are talking about a machine learning model, performance monitoring, right? So you can generally like there's like a hierarchical overview that you see that you can track the performance of the model. Then you can track the second tab that you see. You can track the performance of any data subset, basically any features that you have and then you can get down to a raw data, which is like a single row of the data as well. Right? So that's kind of the hierarchical view that we have built out. And here this graph that you are seeing, right? I want to explain this graph a little bit better. So unlock like if you can scroll up, basically just get to the part of the window where we see the log loss as well. So essentially you notice that these bars, right, these bars are basically color coded in the sense that some are darker, some are lighter in color. So the darker ones actually have a higher log loss and the lighter ones actually have a lower log loss. So basically what this image is telling you is immediately it will tell you that a certain class of a data is underperforming compared to another class of a data, essentially. So then you know that there's a slice that is more problematic. You just click on that slice and then you get to see that okay, what's happening with this subset of my data, essentially? So the idea here is that if your model performance degrades, the debugging of your model becomes fairly straightforward as you move forward. I think that's the overall idea about. This monitoring dashboard in terms of the metrics itself, there is obviously like AUC rock kind of ways to visualize what are the standard things that you are actually looking at. Like graphs that you're looking at? Yeah, so generally what we do is biology. We have like a preset metrics depending on the type of problem that you're solving. So like classification killer, you will have things like F, one score, precision recall accuracy, et cetera. Regression problems, you'll have your own MASC RMSE type of metrics. But then the other thing that we have done is we are also building out a concept of projections. What that allows is you take any slice of a data set and the user can define like a Python function that can compute pretty much whatever you want. So a lot of times people are interested in tracking more business metrics as opposed to just the model level metrics. So if you wanted to compute that using some Python custom function that you have, you could do that and track that graph here as well. Okay. Yeah, because even for those problems, it's usually like in my past when I've worked with forecasting problems, for example, with either an RNN or using some kind of moving average like Arima kind of like solution. Typically you go with MSC or a Map kind of like but while there is bias towards certain metrics, we actually went with a non traditional SME way of doing forecasting models like evaluation to actually optimize for that. So which is why I was asking whether you actually do have the ability to do custom metrics further in addition to just like the standard metrics that are being dashboarded. That makes sense. Yeah, that makes sense. That was one of the important things that you want to build out from the beginning. So that part is supported. Okay. Maybe you can share and take through the training jobs comparison how it's shared between team and all those. So actually just quickly walk through this one part. So like we have a so like generally like when we think about permission controls, right, that's one thing that we have tried to build out fairly extensively in the platform and then we have taken some open ended calls that at what level should you do permission controls? That is like if you start doing permission control at the level of every service, it's probably too much management, but if you do it at the entire system level, I think it's very lack of granularity. Right? So what we've done is we've tried to define a few concepts like you can define teams where you can add people and then you can add those teams to a workspace level basically. So basically at the workspace level you can tell that okay, like this team has this workspace and then within the team you can say that okay, dev staging prod are basically three different workspaces of the century. So that's the level. If you do permission control, it works out, it handles most of the common use cases and still is relatively easy to maintain. So that's one part of how we handle the team's part. Was that your primary question, Balaji, or am I misreading that? That is part of the primary question, but the other part is in the development process itself. One of the things I was curious about is within the workspace, starting a new jupyter notebook and being able to collaborate within it, is that something the capability that's present in this UI or is it like something that is external to the tool? Right, so basically what we do is we work with Jupiter notebooks, but New York we allow people to use whatever Jupiter notebooks that they prefer. So if they want to use like an Amazon hosted Sage maker notebook or like a self hosted Japanese notebook or Google collapse, you could do any of that. And then what you do is you basically start by installing like a couple of client side libraries that we have built out, essentially. And then the workspace concept actually works on the Jupiter notebook itself. So I'll actually explain you very quickly how this works out to just show you like a quick developer interface, right? I don't know if you're able to see the screen in this one. Yeah. So here I'm defining like a very simple job, like something that just runs in loop and just prints some random stuff, right? Just a counter print basically, that's less than the job that you want to deploy to production. Okay? So the way the deployment, the developer would see the deployment would be something like this, basically. One thing is like, we don't interface with your job code at all. So we let you write whatever vanilla Python code that you want and we don't interfere with that at all. So that using True Foundry after the effect is easy. And letting go of Truefoundry is also easy, basically. Right? So then you basically define a job object where you just tell us that, hey, here's the Python file that I want to run. And then you basically do a job deploy. And this is where you will provide us the workspace argument. Okay? So, so long as you as a user who's running this Jupiter notebook is authenticated on this workspace, this job will just directly go through, it will deploy this job on this workspace, and you will basically see it already getting deployed. You will basically see the full thing coming live on the platform, essentially. So that's the interface and the same thing, we have tried to do it for any arbitrary Python function or your direct model deployments and stuff. Basically all the interfaces look very similar at that point. One question, I think biology, which is like whether we allow like a jupyter notebook to be spawned. Right now we don't do that, but basically we have the support for custom home charts. So at any point, if there is a need, like we can work with some customer to actually get it done, we never prioritized it, but that is possible within the workspace. You can actually just like deploying this. You can do anything. Yeah, I mean, we did that for a lift line where in the UI itself you could actually set the secrets, then create, pick an image. We also did custom images because we at lift in the autonomous vehicles group, because they had a specific set of TensorFlow version that they were using on the vehicle they wanted to replicate, pretty much like the older version of the development environment wanted the same images to be present. So we let them create these custom images that mirrored what they are going to have in serving on the car itself to be created, which instead of installing individual libraries, they were able to install the whole image. Okay. And then start a new instance with that image for where they can actually start kind of like developing and testing their models. So that's why some of the learnings we got, I'm just kind of sharing those out which might be helpful in the future when you talk to some of the customers as well. Got it. Interesting. Yeah, I mean that is something that we can actually build. So if it will enhance the experience, we can try to do that, maybe. Nick, one thing that will be also good right now you are seeing this Jupiter notebook separate. Like once they have people have said run different jobs, what we support is like the comparison. So suppose there's a team that is working on a problem and they have set out different training jobs, then in the jobs metrics, they can actually compare this runs and see the metrics as well. Yeah, I can show that very quickly. Actually even that experimentation part is there. But at least what we saw is like people are using generally a different experimentation tool so far like at least weights and biases and all and generally when they use anything that is deployment, they want the training part and other things. So we have tried to integrate experimentation within the training jobs. So I think liquid yeah. And by the way, one of the other things is also around model registry. Like we have tried to integrate like a full fledged model registry as well on the platform. We'll show that to you. So this is what I think and Rag is talking about that let's say if you have like a retraining jobs that keep running once every day, right, you probably want to be able to track the metric that your retraining metric is not falling right? Like after every retrain of the model. So actually do that. You can actually compare all of that in the UI as well. That okay. After every run, here's how your model metric is looking like, basically and if you're also tracking data drift across different runs, basically that's the other thing that you can track. And the good news is that you can then compare that if my data drift increases, my model performance would decrease. So these two things are correlated. That kind of gives you confidence that the same drift tracking technique would also work on your inference data set at that point. Does that make sense? These are all the runs that you are seeing that is happening across different days of your model training, for example. And for HPO, I see you have some of the hyper parameters listed. Does your SDK also do provisioning for hyperparameter optimization or is that something that. You. Like TensorFlow and others to actually support? Yeah. When you say that provisioning for hyper parameter optimization, do you mean in an automated way or do you mean in an automated way? Because with Flight, for example, we actually had a custom plug in for hyperparameter optimization that we created to enable multiple jobs to be done with different hyperparameters and get comparative model metrics across them. Got it? Yeah. So I think we don't do an automated selection of hyper parameters as yet. What we do is we allow the user to quickly and in parallel spin up like your hyper parameter training jobs, essentially. So just like you would have like, let's say a grid search or a random search or a bayesian search that you're doing over hyper parameters, you provide us that list, let's say 16 jobs and immediately in parallel, you can run those 16 jobs. The metrics will be corrected in one dashboard and then you can compare all of them. Here are the metrics, here are the hyper parameters. And then if you wanted to compare the actual learning curves and everything, you could do those things as well. Basically, you can actually compare any plots overviews that you might have seen essentially. Like these kind of plots. You can also compare me. No, this is great. I think that actually helps because in general you don't want unbounded auto spawn, right? Especially with complex problems in pricing or things like that. You can actually have a continuous hyper parameter space and you want the programmatic way of creation of hyperparameters also kind of like more thought through and controlled. So I think this is right. And then just showing this model versioning basically that every model that you have registered would have like an FQ and fully qualified name associated with it. And then you have certain things that you log about the model that is your model schema and the metrics, any metadata that you might have logged about the model. So you have all of that available. So what that means is you can finish some workflows. Like you basically ran the hyperparameter tuning job, you logged a bunch of different models, you like a certain model, you quickly take the FQ and deploy that model. As soon as that model is deployed, you have your monitoring dashboards come up and running, basically. So those are some of the loops that we are trying to close through the platform. Is this UI implemented in react? Yeah, implemented in react. Okay. I actually like the fact that the transitions are very smooth and pretty much instantaneous. So it's actually nice to see non janky UI. That's good to know. One more thing that I wanted, because you mentioned about the secrets magnetic and GIF you can share. Basically when you are deploying the model, we kind of allow you to configure the environment variables and basically if you have configured secrets, maybe I san Jose. Show this very quickly. So one thing that we have built out here is that we realize that given that we are building like a multi cloud product, that different people will use different people also end up using different secret managers, right? Somebody's using a vault, somebody's using an AWS parameter store and stuff, right? And then people generally end up building some developer layer on top of it. So what we have done is we have basically built out a generic Secret manager layer that actually integrates with whatever secret manager that you are using. And then as a user, as basically an admin, what you can do is you can configure a lot of secrets here overall and the developers would get access to the key, they would not get access to the value, basically. So they can actually refer to any secret with this key that you have provided and they can just refer to this secret as a key here, even when they are doing the deployments, essentially. Does that make sense? So they never actually see the secret value, but they can just use it simply from a secret that was configured. In the UI, essentially a named way of doing it. Yeah. Makes sense. Yeah. And that just works across different types of secret manager side. By the way, is Snowflake a customer yet? Given Tall has actually invested and he's leading machine learning now at Snowflake. It's too early. Just invested. Okay. He was leading the Lift Learn effort as well. For me, when he was at Lift. He has the full context of what we did in Liftlearn as well. And he was amazing. This is really, I think, one question. How long have you guys been building this. In the building journey, maybe? Awesome. This is quite a bit of ground covered in that period. So it's kudos to that. This is a very complex space. I don't know if you know Rajith Munga. We do, yeah. So Rajith is also kind of like building a similar kind of company. And I actually had a conversation with him, I think about two and a half weeks ago. He's definitely also keen on solving the same set of problems, a similar set of problems, especially on deployments. And I was telling him about Abacus AI? Yes. Which is also by another ex colleague and friend who was my tech lead at Google, like in Gmail. So Arvindrajan, who's the CTO for that company, they started with a similar theme, but then they went into the direction of focusing more on custom environments for specific problem domains, where you just pick the environment. You just say, I'm solving a forecasting problem, and they actually handle the they bring in their models to come in with and solve the problems. This is a pretty hard space and I know your experiences is going to help you a lot in driving kind of success in this space. But definitely I think your direction is in the right direction of tackling the developer problem rather than actually trying to do only the data problem or only the serving problem, but thinking about the end to end driving that outcome. Looking at the problem domain itself, strong problem statement, right. Investment in the right places, creating the end to end framework I think is the right place to go. And this is where I think the biggest benefit for enterprises will be, in my opinion, because most of the enterprises struggle with enabling just the data scientists, but then not knowing how to ship to production or enabling only the production environment. But then the data scientists are still struggling to actually build towards that and so on. I would say uniform is kind of in that latter bucket where we've got the production environment kind of nailed. But the data scientists taking their models to production is something that we are still building the tools towards. Okay, I think that's an area where we could potentially even have an evaluation right now and see whether you can be a good partner to actually enable that. But as Vinod mentioned earlier, this conversation was not meant to be. Only do we evaluate as a vendor. We have two minutes. I know we had 45 minutes schedule. The only reason we went ahead is because it's going well. And then after this biology, you're in. My. Use the 15 minutes filler that Brenda had left. We both have a common admin, so our schedules are synced. So let's do a couple of things. You guys should do a follow up note to maybe Balls will will bring folks from his team on the eval itself. There's no reason not to also prioritize that. But the other thing, I hope you also realize that the other reason I wanted Balaji to come is his network of people who work for him, he's worked for his peers, is very strong in the Bay. So surely as you guys go ahead, you'll be able to you should definitely talk to Balaji more as someone who can help guide you folks too. Sometimes there are some incredible people in the bay who have done just mind blowing work, right. And having them on board as advisors or this or whatever right. Is super valuable. Right? So definitely, please do lean on Balaji. You have an email, you have full freedom to spam him, because when you. Work with FBI, learner was Ashwin Barambe on the team? So, actually two things. I actually did not work on the Epiler team. I was more of a user of the Epidural. Right. And honestly, the name does not ring. A bell at this point. Okay, no worries. The reason I ask is because Ashwin was at Facebook for 17 years, I think 17 years, something like that. And he recently left Facebook, and I think he's trying to explore his own ideas and so on and so forth. But I don't think he's finalized what he's doing right now. But he is one of the most phenomenal kind of engineers I've come across. He's in the ML platform is kind of like domain in some ways, he's worked mostly at Facebook in that space. So if he's in the market, he might be a good candidate to kind of see if he's excited about the problem space and wants to kind of come and become a part of the team. That sounds yeah. Some of these things. Balaji and we know we would love to lean on to your network and your expertise and if there are some introductions that we can get to just speak with people, get their feedback on what we're building. I think we are so early in the, in our journey that I think one of the most important things that we can do is just learn more about the problem and the potential solution. So we'd love to seek your help there. Actually, if you can introduce with your friend from Facebook as well, that would be very nice. We'll just chat with him and show him the product, get his feedback and see if he's excited about what we're building. We would love to explore that, basically. Yeah, that's his LinkedIn profile. I will be meeting him next week, so I'll actually also chat with him and give him a little yeah, please. Do mention about us. And he's also open to having a conversation with us. Sounds good. And besides that, we'll follow up. Just a couple of quick questions. We are actually over time, so I think we'll call up in the next call. It's fine. Thank you so much. I really appreciate the patience and the time, really. Thanks a lot. We know. Thanks Bellaji. You guys are doing great. Happy to help us support as we can. Yes, thank you very much. Yes, folks, take care. Take care. Bye.