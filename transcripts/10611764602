Are you in office? No, I'm at home today. I went to office monday, Tuesday. Will you come tomorrow? Are you coming tomorrow? Can come if you want to tell you. Okay. Hi guys. Hi Nicole. Hi, Andrel. How are you? Good, how are you? Yeah, we are good as well. Thanks for taking out time. I was just going to write an email if you karen joining me. Yeah, I'm just switching between calls. That's why I got a bit label. You guys are based out of India? Yes. Like me. I wish I can on the call. We are in India. Like Nikkong I think had reached out to you. Nikhonge is based in the US right? So his time zone was not matching. So we'll be adding the call instead. Hi Abhishek. Kancina as well. Great. Awesome guys, let's get started. Awesome. The context of this call was the following basically, as you know, we are building in the machine learning deployment space. So we wanted to reach out to understand a bit more about upSTOCK ML needs currently in terms of what are the use cases. What is the info like understand a little bit more detail around that and based on that see if there's any way in which we could help in the journey or anything we can do to potentially even make it better from what it is currently. So I wanted to talk to you to understand the infrastructure and the current state of DML at upSTOCK. That was the context of the call, if that is fine. What we can do is we can maybe quickly introduce each other. Love to hear what you look after at stock site in the ML side and then we can also give a quick introduction and take it from there. Sure, yeah, sounds good to me. I think I would also love to know more about your offerings and what is it in the ML of space that you guys specialize in that will also help me set the tone better in terms of what we are actually looking at. Right, so yeah, you guys know my name already. I'm a principal ML and data engineer here at Upstocks. So I was the first hiring of the data drive year back in June 2021 and a half years back. I pretty much set up this entire right from scratch, right from the data engineering practice to your bi practice to your data science and ML everything pretty much. I transitioned out of a principal engineer role this year June into a more architect role. So my day to day responsibility is more on the overall strategic initiatives around data and data products in the organization rather than day to day deliveries. And recently I have also been so basically I report to the chief architect here at abstracts and recently not just in the data domain, but I wayve been looking after some of the other aspects of platform engineering like chaos engineering and all those sites as well. So pretty much my job is to wear architects hat, review things, set up and architect things, set up practices, set up things from scratch and then leave it up to the individual teams to take it forward. In terms of what we have done since setting up the data tribe around a year and a half back, we kind of started from ground zero where we had absolutely no way to interact with the company's data. Something as simple as how many trade orders were placed in the last one day. This kind of questions required a very painstaking approval process and all that, which took around three to four days for you to get access to that data, which was nightmare. I mean, imagine you trying to run a fast paced startup with this kind of processes in place, which is typically what you call the historically what the banks have been doing, right? Yeah, there was a big gap there and I think the gap was produced because of a data exposure that had happened with the company before I joined. In fact, my hiring also was a response to that. So we wanted to set up a data practice because of one of the data exposure revenge that happened. So yeah, we ended up setting up a lake house from scratch. We are completely on AWS cloud. We do not use any out of the box offerings or solutions. We use native AWS services and frameworks that we have built ourselves to set up this entire label house. It's a combination of your data lake and data warehouse. From the ML side, I think we picked up ML actively since the April, May, June quarter of this year. There are a couple of use cases that we have been building and working on. One is Associate Partner Incentivization. So let's say there are two partners that I have and Anurag. And Anurag is giving me on a monthly basis a business of one Cr and anura Abhishek is giving me a business of, let's say one lot. It's important for me to compensate or give more incentives to Anura rather than appreciate because that partner is bringing us more business and more logins or customers. Right? So that's one program that we are working on, there are trading nudges. Program that we are working on, trading nudges. Let's say we want to push the user towards doing the let's say if you are essentially placing F and O order where we know that definitely you will end up being pin loss at the end of this order placement. Right? So maybe you are a newbie investor who is not aware of all of these nuances of FNO markets. So those kind of nudges that we want to trigger to the users on real time saying that hey, this is potentially a loss making deal, are you sure you want to do it? Something like that. Then there are loyalty programs, there is RFM cohert RFM is basically what you call the recency frequency and monitoring, right? So how much of an active particular user is depending on that. So your frequency of trading, how recently you're traded, how much monetary contribution you're making to us in terms of revenue per user, right? So all of those analysis depending on that, you basically classify your users into diamond, gold, Silver category. And depending on that, there are certain marketing campaigns which are run specifically for our diamond users. There are specific marketing campaigns that are for your Silver users and so on in terms of experience also, right, you cannot today we have a user base of what, 11 million users. It's not practically possible for our customer service team to prioritize all of these 11 million users at the same time, right? So you have to be really smart about which ones are your really critical customers which are contributing a lot to your revenue and target them for a specific kind of experience, right? So that's where the RFM analysis comes into the picture. Then there are use cases like Churn detection, which is pretty much every company does that reactivation activation campaigns, sentiment analysis, personalized watch list, revenue forecast models, pretty much CLDP prediction, customized cycle value, right? So pretty much everything on this front that we are doing in terms of our ML, Ups, India, we are so. One question in Daniel here before going further, I also saw that there's some recommendation, like in one of the JDS it was mentioned that recommendation and newsfeed personalization. So is there like something you are doing? There also like newsfeed because these users are mostly trading. What does this news feed is it like which soft news they want to. Read or something if you ask our management, right, so they are very heavy on personalization of things, right? So I'll give you one example. So take an example today if you search on our platform, the search is painstaking. I mean, this is also one of the problem that we are solving with the search part. So it's a very painstaking process. Imagine Abhishek is someone who just trades in the equity market, searches for bank and Abhishek ends up seeing bank nifty and things like that at the top of the search recommendation. And imagine how broken experience that is, right? Abhishek is not interested in ETFs, man, he's just interested in equity. Just know that HDFC, bank, it bank all these stocks, right? Those kind of things that we want to do. This is one part. Then obviously there is another part that we want to also give make the search experience better in terms of what kind of filtering on search. Then there is a uniform experience of the search. So today if you search the same part in our web portal and if you search the same keyword in your Android, those ends up showing two different results, then these are not typical tolerant searches. So all of those experiences are also something that we are working on. This is just one example on the personalization front. There are a lot of other personalization related objectives also that we have taken up which are all eventually going to happen over a period of time. But some of the use cases that I discussed right now or proposed, those are the ones which are basically our tier one priorities that we are working. Okay, understood. Thanks for that information. I would love to kind of understand a little bit like given you started the ML journey this year and it's been a few months, quite interesting to see that there are a lot of challenges and different use cases that your team is working on. That's really nice. Love to understand the stack perspective and the ML perspective. Like the ML perspective. Do you also use Cuban? It is like actually one of the JDS I mentioned about. We are very heavy on Kaan. In fact, we call it what we call a greenfield project. It's one of the top priority initiatives as An.org that we have taken up, which is migrating all of our workloads to Kubernetes. If you ask me specifically from the data trip perspective, to be very honest, we don't use Kubernetes. The reason being we are serverless anyway. And when I say we are serverless, we are essentially working out of the data engineering team is working out of Blue ETL notebooks, the data science team is working out of Sage Maker notebooks, and the Bi team is working out of Lambdas and Sage Maker notebooks. So we are serverless anyways. We do not have a single VM that is allocated to us. So for us, it's a low priority migration to Kubernetes. But I think for all the other application services, for them, it's one of the top priority migrations that are happening. But we are very aggressive on the fact that we want to use Kubernetes only going forward for each of our company wide workloads, irrespective of whether from data or anything. Okay. The reason why this is actually our platform, like it's built on top of Kubernetes at this point. So it's really good to see again an organization making their effort to move to Kubernetes given it's more scalable. I wish you had any questions. Yeah. So internal like you mentioned that the host is serverless. What is it? So do you use data scientists use to put a notebooks on Sage Maker and then for running training jobs also you are using Sage Maker, then hosting models? Like, do you have only batched or realtime models also? No, I think all of the models that we have hosted until now are batch models only. There's no realtime model that we have and all of these workloads are hosted on Sage Maker. You are heavily using Sage maker. Very heavily dependent. Okay. And any reason why the ML team basically decided to use take a note the general recuperator because it's not there yet. No, I'll tell you the reason. So it's also on the comfort of the team. Right. So within the team, so we have just one MLPs engineer, by the way, rest. All the guys are data scientists. Right. So in terms of skill set alignment, also, it made more sense for us to go for a Sage Maker out of the box kind of offering rather than setting it up, because the time to market increases. Right. At the end of the day, companies interested in seeing ML based solutions out. Right. So they are not interested in knowing how sophisticated or well managed your ML option price under the hood. Right. So, because one of the other things also that pin this kind of setup, you also have to give the ROI to set up a new tribe. You have to show them that impact what this tribe brings on the table. And then you can focus on building that long term platform architecture in terms of building everything in the house. Okay, makes sense. How has the Safe Maker experience been so far? I guess you're only in the batch. You have not gone to the real. Time experience entrypointtype video. But so far, how has it been the experience overall? Look, I think I would say pretty much what we expected. There have been bumps on the road, but it's something that we expected anyway. We do not expect everything to work perfectly fine. Okay, understood. Cool. And this cognitive migration effort that you're saying this has started, this is you have to go or it has already been done. Roughly. What is the current yeah. For the rest of the company? Oh, that's already going on. I think our tier one services, I think more than 50% of them have already migrated. Okay, understood. Any salesmen like a data scientist only do the deployment of this notebooks and everything? Yeah, the training jobs and everything. Okay, cool. I think that's all for my side. But there are two other teams. Like, I think the Bi team is using something else, and there's another team that was using something else. I just wanted to hear that as well. So the data engineering team is using Blue retail. That's the first thing that we have started using in a stack when we started off the data type work. And eventually, I think when the data science practice was being set up, we started using Sage Maker notebooks. Bi team were using our studio for a long time during the initial days. But I think we transitioned out of R studio. I think last year in around December, January period, we transitioned into VM based approach. In one of the VMs, we were hosting these jobs and we were triggering those jobs through run decks. It's a scheduler platform, basically. Then we eventually moved out to a lambda based approach because again, we wanted everything to be serverless and only pay while we use it. Yeah, because most of our workloads are like that. Right. It's not a consistent workload. It's something that mostly our batch processes, which runs for five or ten minutes in there. And do you use anything for tracking requirements like ML flow equivalent and for monitoring of models? Anything like that? Everything out of the sage maker itself. So we don't use that. Okay. Our ML of practice, as I said. Right. So it kicked off in the April, May, June quarter only it's not very what you call in a steady state till now. I mean, it's something that is still undergoing small changes here and there. It's not what I would call our ideal place where we want to be, but it's something that we want to somewhere that we want to reach by end of this year. Okay, so one question I have pin general is, apart from the test and the segment, like, is there any other thing, like in ML of pipeline, any other tool that you, Karen, using? What is the workflow like for a business case coming in to finally a model going into production, starting from maybe product manager to a data scientist to how it flows to the infertium block to hear that a little bit. So, look, in terms of our workflows, right? So it's pretty much initially whatever the request comes in. Right. It's a discussion between the product team and the data strategy team to identify that what's the best way to achieve that, which is essentially me, a couple of other guys, and the product team sitting together and deliberating on it. What's the best way to achieve it. Then comes what you call the exploration phase, wherein you figure out what's the right model, the right data to work with, if there is any feature engineering involved, what is it that you want to do, how is it that your pipeline is going to look like? That's what we establish in that phase. And once you have that part done, I think our deployment strategy is pretty standard. There is what you call a feature generation job that is scheduled, which kind of runs based on the batch trigger that you pick up, that kind of populates your feature store. And from your feature store, you basically consume the data into a model on a daily basis to produce whatever results are enquire. It's a pretty standard listing. We currently don't have any ML offerings exposed through an API endpoint as such, which is what you call the next generation integration. Right. So wherein your ML offering is directly integrated with the front end feature. Right. So we have not had those kind of productionization done yet, but it's something that we can support easily using sage maker endpoints. That's something that we have already explored and we have it in our offering list. When the use case comes up for that, we will take it up. But there are also challenges in serving that kind of responses, right? Because challenges like typically if you're exposing some kind of model to a front end system, one is you have to really make it responsive. So you have to look after a lot of other optimizations as well. Right. Because you can't wait for your ML model to return a response after a couple of seconds or something like that. It has to be within milliseconds. One of the features, front end facing features that the data engineering just made life last to last week, I was also part of that. We had a P nine of 45 milliseconds. So that's the kind of performance that we want to achieve if we are exposing anything to the front end system. Can you come again? Like on that I'm sorry, can you come again on the exact number? 45 milliseconds? That's the latest number that we got during load testing. So. I can give you a quick overview of that, what that is. So typically any user when they open account with us, right? So as Pin, when your portfolio holdings values keep on fluctuating, right? Your valuation also keeps on fluctuating. Let's say if you hold TCS and Infose and the TCS and Infose stocks are fluctuating, your valuation as a user who holds ten stocks of TCS and ten structures also keeps on waiting on a daily basis. So that's the data that we are plotting, it for our users in the front end. And that's something that is coming through this interface that I was just talking. And right now, like this deployment, this is still being done by the data scientist, is it? Or the ML engineer who is there? ML engineer is the one who does it. Okay. This is not a custom deployment made for each of these models either. So these are all standard deployments. You essentially have to just configure whatever the notebook or the final script that the data science team has given. And you basically we have a CI CD pipeline through which it is automatically production is. Unlikely. Like you mentioned that these people are writing code on the Jupiter on the sales maker notebooks, right? Right. And then you commit them to GitHub. Also not GitHub, we use bite bucket. Okay, but do you commit the notebooks. To be bucket or you first converted into a script and then the script despite okay. And for CICD or using bit bucket pipelines? No. Okay. This is automation jobs. It's a standard CI CD pipelines that is used across the organization, not just what you get. Okay, understood. And is there any like from this? What are some of the goals that the ML team is trying to hit? Understood from the use case perspective, but anything else, say from there is a bottleneck in terms of either the install or in terms of latency needs or in terms of the kind of models that are supported, et cetera. To be very honest, we have not yet reached that stage. Right. So, as I said, we are at a stage wherein we are proving the ROI, right? So we are kind of giving it to the business that okay, this kind of problems can be solved using ML in our company. And then comes a stage where you Karen, just optimizing, optimizing and optimizing everything because we have gone through the change. And in the data engineering program also, it all started with just somehow give access to data to everyone. Then it started with optimized, optimize, optimize, then it started with a cost optimized system. Then it went to a point where finally, okay, things are stable. Now you build a next generation data plan. So similarly, the ML and data science team is also going through our same journey. I think it's pretty much in the first part of the journey that they are in the kind of transitioning into a point where pin the business is starting to get more value out of it and they are going to be more and more aggressive and demanding in terms of what they want now. Right, but it's still a process that's on. One last question, is the cost that is being incurred in the infrastructure from the amounts? Assuming that a lot of the bad. Jobs yeah, they're bad jobs. Cool. I think get a good sense of the overall pipeline in the middle. Thanks for that. What I can do is I can give a five minute overview of what we are building and then we can schedule a follow up call. Like given the Use case right now is not necessarily instant from your end. What we can do is we can give an overview of the system and try to kind of just showcase you how when you're thinking of a scalable ML pipeline journey, how the system could be useful. And maybe based on that, you can kind of think of if that is something you want to experiment with right away or think of it later, but at least you'll have a picture of what we are doing and if we can find some ways of collaboration, that will be great. Sure. Okay. So just at a high level, part of the pipeline that we are focusing on is more towards the deployment infrastructure. So think of it in this way. Currently you are deploying batch models. Tomorrow the Use case will move towards real time models. And when you kind of have a decently sized, you would want to be able to test models before releasing them to production. In terms of if there's one model already running, then if you have to release a second model, whether this model is better than the other. So the goal of the platform is it enables your data science team in itself to be able to deploy to production with minimal lines of code, most of it being like boilerplate code that they can write on top of their inference code or on top of their models that they're building. And then the system will allow you to containerise and then deploy to the corresponding infrastructure, which could be anything. Like it could be AWS, GCP or SEO. We are built on top of Kubernetes. So by that virtue, like all the things that Kubernetes offer comes by default. So the flow basically becomes a data scientist or an ML engineer. Whoever is deploying, they kind of have their model, they bring it to our system, they add a few lines of code and then they kind of do something like a twoFounded deploy and it will deploy to the 11th infrastructure that is there. And this infrastructure can be managed by your DevOps team, where they connect to their own cloud or the corresponding Kubernetes clusters. And they are able to allocate resources to the data science team. So if there's multiple members, then multiple sets of resource can be allocated. Say there's one workspace where people are working on two sets of problems, there's another workspace where people are working on three sets of problems and each of them have limits so that no one can exceed the cost and all. And as you deploy anything, you can do like both the batch time deployment that is auto schedule and you can also do real time deployments. And as you deploy, you get your monitoring. Basic monitoring does both automatically and you can set triggers to auto train like retrain your models from there. So that is roughly the part that we are focusing on. So the idea was like as seems kind of evolved. They will kind of build their own internal platform for this. Instead of building and investing a team of like six, seven engineers to build this, you can adopt that platform and then you can build on top of it for more customized use cases. So it's built in a way that it's not a black box for your team. So you can actually make modifications to it and everything is API driven, so you can use it in the way. Got it. Just one quick question. So look, I understand typical ML ups platform offerings and I think there Karen, multiple other competitors also out in the market, right? I mean, if you consider that with seed Maker is also another competition in that space. Right? So exactly what Sage Maker is trying to solve, which is give everything that is there to offer in the entire MLO pipeline out of a single platform service. Right. What's the differentiating factor? How are you guys different? What is it that you guys do differently from some of the other platform offerings that are there in the market? Yeah. So in the middle it's actually very similar to Sage Maker if you think of it like that. Right. The only differentiation is Sage Maker is like a black box you'll not be able to make customizations, whereas we build a platform on top of Kubernetes. So if you want a single stack. For the whole confuse on a single. Interest stack and everything. So that way it's fully open, it's built on top of open resource. So if you want to customize anything like people run into problems with Sage Maker where some timeouts and all which are configured in Stage Maker will not be able to overwrite that because that comes from AWS. So those things are there and cost wise it will be at least 30% to 40% cheaper. By the way, you guys are aware of NimbleBox TM? Yeah. So I think they had also reached out to me sometime back. I think they've been in touch with me since the past year. Now something very similar is what they have also built, right? And I think sounds well to me the way you guys are also projecting Truefoundry as a solution, right? So I'm just curious to understand how is it that your offerings are different from some of the other guys as well. So, look, I understand some of the Sage Maker shortcomings that you're talking about. Some of the shortcomings are not even relevant to us. I'll give you insight. So basically in the data platform also while we were setting up, right, there were multiple multiple bottlenecks that we faced from the AWS native services and has reached to a point wherein today we kind of work with the AWS service teams directly. We have a say in prioritization of some of the feature releases also. And we also have a say in terms of contributing to those feature releases. I mean, I can discount features and double digit numbers that we have kind of worked together with the AWS team to get it out deployed and rolled out to all the AWS customers one bit because it was something that kind of sold our problem also. So yeah, we do have that leverage in terms of as a company as a whole, in terms of working with a S. However, I'll tell you the reason why I would prefer to use a solution like two foundry rather than a Sage Maker is something what we call how much percentage or part of it is out of the box offered by you guys directly, right? So for me, I think why would anyone go for a buy versus a build kind of this thing, right? So if you're taking a call for a buy against a build, it is always because there is a time to market factor associated with it. So right now, if you ask me, the speed or the level at which we are maturing is at a certain pace, right? If you go with the out of the box solution like Roof foundry, you kind of accelerate that entire process wherein you reach that mature space in a much lesser span of time, right? Which is exactly why I would like to talk to companies like Truefoundry and Akhil. See. What is it that whether it's something that is relevant to us, that can totally add value to our journey that we are on. But however, I think kind of repeating that my question is very simple, that how are we different from some of the other guys also that exist in the market purely in terms of what's our X factor? That's all that is. That helps me also get some clarity in terms of what are we exactly looking at. So we can take this question in more detail as well. Pin the nil in a follow up call but at a high level, the major difference with respect to other places is one is in terms of the developer experience. So like onboarding into the system and getting started would be very easy for any developer. So today, like if you have a team of seven people in the ML side, tomorrow you kind of have 15 people. You won't have to retrain them and tell them everything about a new solution. It's almost like they can just read the documentation and get started. The speed will be much, much faster to start with the system and all the complex use cases that might come in will be automatically supported in the system as well. That is one part. Second is second is in terms of obviously the cost part that we check was mentioning that if you look at say a Sage maker kind of a solution, we will turn out to be cheaper than Sage maker in terms of the cost as well. The third part of the differentiation is ultimately everything is tying into the developer experience and making it much much more faster for your team. I think that is where we have tried to build it in the right way. And I think one thing that is not necessarily relevant for you at this stage is probably being able to be multicloud and use that as a leverage on top of your vendors today, like your software engineering stack is on Kubernetes. If the same stack for ML is also on Kubernetes, then you have a leverage over AWS and all because then they know it is very easy for you to step at one click from AWS so you can leverage that to even reduce your cost on other sides in terms of the net Pin store cost. So what being tied up to one thing, right? Like once you're using Page maker you're tied to it and they know that after a certain point it will be difficult to move and if you have to move like a vertex or as your ML, it will not be an easy process. So you get tied in and then your advantage on the cost side you will not be able to manage like later on. I think that part making that part scalable from get go gives you a lot of advantage over this cloud plus later on. So I think that is one thing that kind of will also come into the. Make sense. Would it be fine like if we can do another follow up call in that what we would love to do is we can walk you over the two from the platform, showcase what we have built in terms of and what is the roadmap. And based on your internal use case, I understand that your use cases are still evolving. But if you get like once you Karen, able to kind of see the platform, maybe you can kind of tell us if there's some part of the internal store where you see this fitting in. And that will be great to kind of even try and see if there's a small DLC that we can do to kind of see if you are actually able to drive value pin terms of speed. Sure. And we have just one quick question I have so your platform is hosted on our infrastructure or is it something that's going to be outside our infra? So is it going to be deciding in our VPC is what my question is. Yeah, we can do both. So we have a public cloud version, pin the name where you can host your models and use our info, which is basically our own Kubernetes Clusters. It has the ability to kind of use your own infra and we make it very easy. Like we can skip it as a help chart. Let's say your platform offering Giri Truefoundry upstarts or something like that. So it's typically entirely encapsulated in our infrastructure itself, right. The data or anything never leaves our boundaries, right? Yeah. It will be entirely in your VPC loading. Cool guys, I think one more thing. I think in terms of the offerings that you're going to show right. I would love to wayve a live demo if possible. Okay, we can do that. Is there a particular type of model where you would want to see this live demo? I am okay with any example that you want to take up. It could be any model, but however I would like to see. So let's say you have a data residing at a certain place. How do you go from building features on top of that data to training your model to evaluating which one is the right model to deploy it in production, then productionizing it or basically exposing your model through an API, something like that, that end to end cycle, how does it go through in your platform is something that I would look. Sure. So just one thing to clarify is the part of building features, et cetera, that is not something that is a part of our platform. At this point in the Mohan that continues to work in the way that you are doing correctly, the platform is still more focused on the deployment of the batch and the real time around that we will demolish. What you're doing like a simple batch of generated features that we can do. Yeah, that's not a deal breaker for us. Right. So whatever offerings you have, my only request is just show us that entire lifecycle of productionizing, a model in whatever platform offerings you have. So let's say you don't have a feature building platform offering that's okay, we'll skip that. But whatever you guys have, just align it with that kind of demo which is helping us through that entire lifecycle. Okay, cool. What would be a good time for this? Maybe sometime next week, early next week or so. Let me just check. Next week is little hectic, guys, I would prefer if we can do it this week. Friday afternoon. Friday afternoon. Okay. Yeah. So sometime between, let's say three to 430, something like that. Okay, cool. We will block some time there and I will follow up. Yeah, I think that would be great. And I'll just invite my ML engineer also during the demo so he can also have a look at it. That will be good. Like we can also get to learn a little bit about maybe hands on problem that he might have also seen. Okay, thank you. This was a good discussion. Okay, thanks. Have a good one. Bye. Thank you guys. Take care. Bye.