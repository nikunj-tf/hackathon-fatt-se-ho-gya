Hey guys, can you hear me? Yeah, hi. Goldie, nice to meet you. I can hear you. Yes. You are chill, Manny. Right. Yes. Okay, and you are the founder? No, I work in the founder's office, so I'll just introduce myself, I think. Nikonjan Raghur, also to join shortly. So. Hi. This is Jimmy. I work in the founder's office team Chinmay Truefoundry. Before Truefoundry, I was working with Mckinzien Company as a management consultant and I also graduated from It like the rest of the founders. So all three founders of two Truefoundry are from It. Okay, just give me 1 second, I'll just check the joining. So you guys have a product already or you're still figuring out what to build? No, we do have a product, we have a platform. We also have a few customers. You're working with a couple of enterprises as well as some multiple small companies. Okay, got it. Hello? Hi Gouli. Nice to connect. Hi Nikon. Can you hear me okay? Yes, I can hear you. Yes. Sorry for being a couple of minutes late. I was just wrapping up printing before this. Yeah, no problem. Were you able to give an introduction? Yes, I just introduced myself. If you could also introduce yourself. Sure, yeah. So, Goldie, just a little bit about myself. My name is Liquin Bhajaj. I am co founder CEO at Truefoundry. I come from a machine learning background. So I used to be at Facebook where I was leading one of their conversational AI initiatives, working on their virtual assistant product. Prior to Facebook, I was leading the machine learning team at a startup called Reflection where we were building recommended systems for the ecommerce industry. So there for the first couple of years, I got a chance to work on these algorithms recommendations and personalization algorithms. And the last year and a half I basically worked on building a horizontal machine learning platform that built some of the operationalization elements like feature stores, model deployment, layer, model monitoring layer, et cetera. So we scaled the system to roughly 600 million users. A decent scale, not a crazy scale, I would say. And prior to that, I was doing my Masters at Berkeley, and before that I was in India doing my Bachelor's, went to ID Karakpur. And that's where I met my now performers and Raghan Abhishek as well. So the three of us are patchmates from Kadappu. Okay. 13 graduate. Okay. ML platform is very much the reach these days, right? It is, and it has definitely taken a lot of importance in all the unicorns in India. Right. So anytime A, B, two, C company, which is a billion users sorry, a billion valuation, they need an ML platform, they need a data platform. Right. So that becomes table stakes. So interesting what you guys are doing. Are you targeting mostly unicorns? Yeah, I think so. I think and likely when larger companies generally like, basically the idea for us is to build an ML platform that anyone and everyone can use. But in the beginning stages, Goldie what's happening is companies that have used ML platform beyond ML, machine learning beyond the threshold are the ones who realize the importance of operationalizing it. Because until at least one model breaks in production, or at least until one developer spends a week trying to debug what's happening or just write the boilerplate code around it, people don't realize the pain point of it. So that's why we are targeting relatively larger companies. And to be honest, company being larger is not very interesting. What's interesting is that company has invested a decent amount of time in machine learning generally. There is some correlation that once I have to bring ML models, et cetera. Nicole, your voice is breaking up, at least for me. I don't know. Yeah, it's breaking for me as well. Okay, can you go now? I'll turn off my video. Yeah, let me turn off my video. Yeah, it's better. Yeah, it's better still. Give me it just give me 1 second. Can you answer the question for Goldie? Yeah, so basically Goldie the kind of customer that we interact with. So because if a person has like or a team has operationalized a model or let's say they have like a DevOps team or a platform team who is catering to the organization, they then begin to realize that there are lot of these cases in which you need to support the operationalization. Like you could have different requests coming in from different teams. Like someone could have been asking for auto scaling, someone needs like cabinet deployment, someone needs something else. Right. And then there are different cloud setup setups as well that you might need to support. So typically when a person has deployed a model or like a few models, that is the place where we have seen most companies realize the problem and the actual need for a platform. So typically that could even be like a smaller company, but the chances of it being like a unicorn or an enterprise that's larger. Right? Okay. Hi, can you hear me now? Is this better? Yeah, it was. So far so good. Okay, nice. Hoping this is better than the last one. All right, actually a couple of things we would love to learn a little bit about you as well. Whether you based what's the portion of machine learning team and all that you lead today. And then after that, the intent for this call is that we wanted to dive a little bit deeper to validate a certain thesis that we are going out to the market with. So we had a few specific questions to ask from you just to learn the set up a little bit better. And after that we can also tell you a little bit about what we are doing at Proof Foundry. If you have any questions, we can also answer that. Does that sound like an okay agenda for the Call today? Sure. I have done startups before and I have mostly a data science background. Right. But I also built systems, distributed systems in my past. So to some degree the data platform and the ML platform that we are talking about, I have built those platforms in the past in multiple places, both in companies like Oracle as a past solution and in companies like Flipkart mentra where we were doing it more for our internal consumption. Right. So I definitely come from this background and in fact, you are not the first startup I'm talking to when it comes to ML platform. I think I've run into quite a few of these over the past two years. Some of them have pivoted by the way, but still trying this. But right now I'm basically building a travel fintech within Make My Trip. Nice. I also come from a fintech background. I have been part of lending companies before and have sold startups. So right now we are focused on building a neo bank and fintech specifically focused for the travel segment. Right. And there also to a large degree I handle both systems engineering and data science. So whatever risk based underwriting we do today, based on credit bureau reports, based on behavioral signals, we have some semblance of a data platform and we have a feature store and we do deploy our models through a standard recipe. Wow. Okay, understood. Yeah. So currently for this, basically, first of all, congrats on leading a new vertical within a large company. It's like a startup within a large company. So sounds like a very nice set up. And then besides that, you're saying that you are already building out or have built out a feature store and standardized some of the model deployments within the company. Is that a fair understanding? Right. So Make My Trip already has a data platform team right, which does not roll up to me and to some degree they have standardized this. Right, got it. Okay. Most of it is boilerplate nikunj, to be honest, something that's open source by LinkedIn or over. Right. And we have put this together to sort of gather clickstream data gather database through CDC and then transform it and store it in a data lake and then a lot of data processing pipelines sort of take this and compute the features and put it in a feature store with a catalog. So to a large degree we are mostly relied on open source to build this out for MakeMyTrip. And what I'm using for even the printer is pretty much a replicated set up of what we do at Makematrip. Understood? Okay, that makes sense. So there's like a central platform and then like all the other verticals and views are basically using the same thing. Make sense. One question Goldie, about the cloud. Do you all use like a specific cloud provider or there are multiple clouds. Within make metric so that's a good question, right? We are currently using AWS and yes, so there is no multicloud provider as such. We used to have our own data center, but we realized it was too much headache and maintenance for very little. ROI moved away from our data center and are 100% based on AWS. And AWS. By the way, because of data localization requirement, we were also waiting for AWS to set up multiple data centers in India, both from a late reduction perspective and also data localization perspective. Once they set up multiple data centers in India, it became easy for us to just 100% rely on AWS. Understood? Got it. Okay. And within AWS, do you have the intra primarily based on top of Kubernetes or VMs? Or do you also for machine learning, you end up using Sage Maker? Yeah. So as far as the micro services that all the different engineering teams write, the typical CI CD pipeline that we have is sort of an ECS set up containerization through ECS and deployment through Jenkins. As far as Kubernetes go, we are not too big on Kubernetes, to be honest. As an orchestration engine, it is a pain to sort of pain to maintain, to be honest. And that has been our experience so far. So you typically gravitate it towards using ECS and the likes of that software based containerization. Got it. And is this the same pipeline that is used? So, like, for machine learning, do you end up using something different like Sage Maker something, or they also use Sage Maker sorry, ECS. So Sage maker is heavily used. Sage Maker sits on top of our S three data lake, and typically the data scientists do their EDA exploratory data analysis and a model building on Sage Maker. Right. Once the model is built and once it shows positive results, then typically they take that code and sort of we have an airflow cluster. So they typically put it on Airflow and run it as a bad job to do the retraining at a periodic frequency. And once that model is built, we typically have a Flaskbased setup to sort of do the deployment. So sort of a deployment framework through Flask that takes this makes the model available for prediction and scoring. And we also have a feature store where you register all the features that you are using for your model, and they make it available both for near real time and batch use cases. Got it? Okay, understood. So I think this is a great background. I have a few questions here. So you mentioned that the model prototyping happens on Sage Maker. I'm assuming Sage Maker studio is what you're talking about. Yes. Okay. Now, after the experiments are done, the batch jobs are run on airflow. So is that airflow, first of all, managed by your team, or do you use some kind of an air managed Airflow? That's number one. And number two, that who writes these airflow jobs? Like, is it like the developer of the model itself or the model builder? Like ML developer data Scientist works with someone from the engineering team to write these airflow jobs. Right. So I think you asked me two questions, so let me answer it one by one. Right. So the second question, as far as who writes the airflow jobs, it is typically the job of the ML engineer. And that's why we encourage our data scientist to be a full stack ML engineer and not be just limited to doing data science. Because if you can just do data science, it's not very effective. You are still dependent on somebody else. As far as airflow management is concerned, it's typical. We have not done a managed service. We have not outsourced it to somebody outside for managing that service. Our DevOps team sort of manages the cluster. Got it, okay. So actually, you mentioned about data scientists and ML engineer. And that brings up another question about how is the team structured, actually? Do you actually still have data scientists within the team or there is some level of separation at least today between data scientists and ML engineers? And if yes, what's the split like overall? So, typically, even if somebody's title says Data Scientists can Make My Trip, they are supposed to be ML engineers. Right. Okay, understood. So the label does not matter in ML parlance, they are supposed to do the deployment as well. Got it. Okay, understood. Makes sense. So basically, you have like, the one standard. And how many machine learning developers data scientists are there today working in the company? And how is the team structured? Also? Like, basically, is there like, only a central ML team? Are there machine learning engineers embedded within different parts of the product lines? Right. So there is a horizontal data science team, right? There is a horizontal data platform team. So both are horizontal functions. The only exception being the travel fintech that I am running right here. The data scientists are embedded within the fintech division itself because it's a very different business than what Make My Trip typically does. We are doing a lot of risk based underwriting. We are doing a lot of we are handling a lot of financial data for our customers. So it's a completely separate subsidiary of Make My Trip. Only for my the setup is different, but for the rest of the Make My Trip lines of businesses like flights, hotels, cabs, buses, trains, you would see that it's a horizontal team that shows their needs. Interesting. I see. And how many people are there on the central team? And like your specific vertical goldie, there. Will be total in total, between the two teams, there will be less than 30 data scientists. Understood. Okay, sounds good. The next question that I wanted to ask is your data scientists are basically writing these airflow jobs. Do you get a lot of complaints from them? It's hard to manage it's hard to do this thing or like people are now used to it. I think we do a very good job, Nikkunj, to train the people that we hire. Understood. To be honest, we hire a lot from IIT, especially the junior folks that we hire. Right. They are very untrue about sort of learning all the tricks of the trade, including how to do the deployment and ML ops. Understood. Nice. Okay, sounds good. And the next question that I wanted to ask you was around the next part of the pipeline that you mentioned. There are some models that are real time or near real time for which you use a Flask based setup. Right. Where is this Flask app running? Like, is this containerized and running on ECS? Yes. Okay, got it. And what does that pipeline look like? Your model gets stored in S three or something and then you have a Flask app that like a data scientist has built out and they themselves will containerize it. Basically, what's the hand off process from the machine learning team to the intra team that's managing it? Basically. See, to a large extent what we have done is we have created a recipe in terms of how the deployment works. So for example, we have a sample demo ML model running through Flask. Right. So somebody new building, a new data science model will just take that recipe, control C, control V and then change whatever they need to do to sort of make the deployment work for their model. Understood. Okay. So basically in this setup, what will happen is every model that is getting built is getting developed as its own microservice in some way. Right, understood. By the way we are building, what we realize, Nikkung, is a lot of the things that we do ends up using multi arm bandit, right. In quite a few cases we are using both Linear CB and Epsilon Greedy. So what we have done is we have tried to platformize MAB. I see that even a non data scientist can sort of apply MAB for that use case. So to some degree that platformization journey is also on. Interesting. I see. Got it. Okay. I have a couple of follow up questions on that. But before that, just to close the thought on the close the loop on the previous conversation. So there's a Flask, there's one model, one microservice. And is there like a CI CD that is happening with respect to these models? Like these flask apps. Is there a CACD which is happening with respect to these models? Flask app basically is the developer pushing the code that they control C control read into a GitHub repository? And then is there like an automated pipeline that creates the docker containers and deploys that? Yes, understood. That is managed by DevOps. Okay. So the developer would push the code to get it up and from that point onwards, the DevOps will take over in some way. Yeah. So they just have to file a jira with DevOps saying that they want to deploy this into a container and they would suggest the number of containers they need and the QPS, the queries per second or predictions per second that they would expect this model to serve and the DevOps sort of take it from there. Got it. And what part of Sage Maker do the DevOps interactive or the developers directly interact with? Is it just the studio and the notebook part, and is the rest of it abstracted? Is that understanding? Correct. So what do you mean by abstracted? You mentioned that there are a lot of pipelines set up, right? I think there was some issue, but basically what we are trying to understand is that does the ML developer interact only with the notebook parts of Sage Maker directly, or there are other aspects as well? Because you mentioned that a lot of the deployment part, et cetera, is put in like these cookie cutter kind of pipelines. No, they must interact with the notebook. Okay. They just interact with the notebook. Let's say the developers have to set up monitoring and all on this. Do you have standard monitoring dashboard if they want to include new metrics or something? How is that handled currently? That, again is our doctor eml recipes that we have for deployment sort of has a sidecar implementation for monitoring. So it comes as standard for all micro services, including the deployment micro service that we use for data science models. So that sort of takes care of monitoring. It pipes the CPU usage, memory usage, hard disk usage to scale it, and ultimately is protein on grafana. Specific metrics like drift, et cetera. Drift, et cetera. I think, yes, we do monitor drift, et cetera. Again, to some degree. It depends on the use case. Jane Mai. Understood? In some places we don't expect a drift to happen, and so there we don't monitor the drift, but like I said, we have airflow based, regularly scheduled retraining jobs that always has the feedback loop running and retraining the models. So to some extent, the data drift is taken care of. Understood. Just trying to understand, you mentioned the developers open like a jira ticket. How much time does it usually take for a developer between when they've developed the models and for it to get deployed and picked up by the DevOps. And deployed on the same day? Okay, so it's fairly quick then. Yeah, and about that as well. Let's say the developers want to do a quick testing deployment or create a demo or something. Does that also flow through the same process or is there like a separate. Process set up for that? I mean, they can deploy it locally as well on their development setup and then they can ping the APIs. So one of the ways we do the QA testing, for example, of the deployment models is we have a QA pod where they can deploy the same thing and then hand it over to QA for testing. Understood. The development that the developers do, it's also like locally, is it? Or a lot of it is also on Studio. Where is the development environment currently? It is on the Sage Maker notebooks and studio. Understood. And let's say, do you guys also encounter like multi file code or something which would not work with Sage Maker, Studio, or how is that handled? Sorry, I haven't encountered that problem before. What is that problem? Can you explain? Yeah, usually what we've seen is that in deep learning code, like, people write multiple files instead of a single file. So then maybe managing it on the notebooks becomes a bit difficult. Do you guys also face that or it's fairly managed by singular notebooks code. To be honest, we don't use a lot of deep learning. In fact, our GPU usage is close to zero. And in fact, the same was true at Flipkart Mantra as well. We have realized that as long as the data we collect is clean and of good quality, even the off the shelf traditional ML models do a very good job of taking care of the problem statement. Right. And also interpretability and explainability is also quite well understood when it comes to traditional data science models compared to deep learning models, which is a key criteria for us. Understood. You mentioned you guys were also in a hybrid kind of a set up earlier. Could you just double click on that a bit? Like, why was there a hybrid setup? Was it because of security reasons, data housing norms, or what? Exactly. I think I'm not aware this predates me, but from what I understand, Make My Trip is a 22 year old company, right. So there was a lot of legacy reasons why they might have done their own data center. I mean, data centers in Singapore and India only became a reality in the last five, six years. Before that, that was not an option. Right. So you had to host it in US or Europe from where the round trip latencies would be high. So I would understand why that decision was taken to Make My Trip's own data center. Understood. And you mentioned there were a lot of operational overheads or what can you tell? Was it that the ML ops was not set up here or like the management itself, like Ubuntu, et cetera was an issue? What kind of issues did you guys face which made you move away from it? Move away from our own data center? Yeah. It's just operational headache. At the end of the day, it's just pure operational headache. Right. And if you do a cost benefit analysis of hosting it on AWS versus doing it by our own, it just didn't make sense. Yeah. Hello. I have a few more questions. Thanks a lot for answering some of these questions so far, Goldie. So one of the things that I wanted to understand from you was we had talked about this entire model building and deployment pipeline and the Flask apps and stuff, right. You also mentioned something about periodic retraining of the models. Typically, how frequently are the models retrained today? Is this set up using Cron jobs? What's the typical set up here? It's typically through airflow. So we use Airflow to schedule retraining jobs. Right. And it will depend on the use case where we expect a data drift, we would retrain. Where we don't expect a data drift, there is no retraining required. Okay. So, like, would it be in the range of, like, basically between where you expect data drift versus don't? Like, would it be in the range of days to months? Yes. Okay, understood. Got it. And according to you, Goldie, in this entire pipeline of building, deploying, monitoring models, drifts, et cetera, what are the areas where you think it needs, like the maximum amount of improvement so far? Which part of the pipeline is, quote, unquote, the most broken, according to you, or needs the most improvement? Needs the most improvement. See, data scientists are very expensive resource for a company. Right. So in terms of improving the productivity and efficiency of data science has a function I think can always use improvement. What I mean by that is, and I think that's the reason why ML Ops and ML platforms are becoming the rage these days, right. To a large extent, that take away all the grant work that's required to make data science work for a company. So anything that you can do to sort of free up a data scientist to focus more on model building, I think is something that's worth doing. Right. But having said that, at Make My Trip, given how long the company has been in business, how long the data platform and data science team has been around right. We have sold most of these problems to a large extent. Understood. Got it. Basically, like this data and productivity issue that you're talking about, basically, it's not necessarily a concern within make my report for you as a leader, engineering leader here. Yes, right now it's not a concern. Maybe three or four years back. So, again, going back to what you were saying, right. You have to find the right company at the right stage where they are in that journey of realizing that this is becoming a bottleneck, we have passed that stage. We are at a stage where we have solved those bottlenecks. And so even with a very small data science team, we have probably 6017 models in production today. Got it. I see that's actually a fairly large number of models. And actually, I have a few questions in the setup that you described. Right. So you mentioned class caps, which I'm assuming basically is working Rest API currently, right. Do you have any models that deal with larger type of data sets, like, I don't know, like large text files that can't be transferred over Rest API and you need a gRPC interface or something. Do you have anything like that? So far we have not run into anything. Understood. Got it. And the second thing is, has infra cost optimization been a concern at all? Because one of the issues that we have seen with many companies that build microservices with machine learning models is too many models, too many microservices, too much duplicated infra cost. Right. So, like, one of the setup that people end up using is put multiple models behind the same endpoint, blah, blah, blah, multi model serving kind of a thing. Right. Have those things come at all? No, I think our DevOps team has done a tailored job at infra cost optimization across the board. Right. So the ECS containerization based deployment strategy that I talked about earlier, they have figured out a way to do this on sort of something like the spot instances without compromising on high availability. So to a large degree, that is solved for you are right. That one of the reasons why we are taking the platformization approach for multi bandit models is because of the number of use case we are encountering and the number of microservices we have deployed for already running maps. So we are taking that approach that we would have a router at the front and then it would send it to the same instance and it would employ different mabs for different use cases under the hood so as to optimize the infrared cost even further. But again, these are slim pickings, to be honest. Right. So, I mean, the infrared cost optimization that I'm talking about will maybe reduce it by 10%. Okay, I see. Understood. One other question in terms of the platformization stuff is what are you doing today for model monitoring? Are you using any external tools? Have you built out something internally? What's the setup there? So, again, it's a sidecar setup. That's what I was telling Chinmay. Okay, sorry, we can skip that. Understood. Cool. I think it's fine. This is very helpful information. Goldie, go ahead. Just to learn, since Goldie, you mentioned that the DevOps team has cracked a lot of problems and you see a lot of efficiency as well that has been provided to the team. We were just wondering, would it be possible for you to connect us with someone in that team? We would just want to learn how they were able to tackle some of the problems. Would really help us in designing a few things that we are working on in our product as well. Do you think that would be possible? Yeah, I can definitely connect you to one of the leaders there. Sure. I think that would be really helpful. Yeah. So I'll drop your mail for this call. Maybe if you can look them in, that would be great. Sure, yeah, that sounds good. Maybe we can take like a few minutes to answer any basically just tell you a little bit about our platform as well. What's the approach that we are taking to solve the problem and answer any questions that you might have enough? Sure. So what made you choose this as a problem statement to solve? I know that because quite a few startups are doing this right, to be honest. And then there are a lot of open source tools like ML Flow and others. Right? Quite a few have been open source. Uber is open source. I think much of this feature stores have been open source, data catalogs have been open sourced. Then you have AWS and Azure all offering this as a service. So why tackle this particular problem statement? Great question. I think we get this question also a lot goldie. Basically, I think the story is like three different events in our life resulted into us choosing this. And I'll first describe those events just to give you a complete story, and then I'll tell you more about why we still think that there are problems that need to be solved here. But the first event, as I mentioned, that when I was the reflection, we're building a horizontal level platform. So we realized that once your company goes beyond a certain scale in machine learning, you need to platformize things for efficiency reasons. Right? And to be honest, what we had built out in about a year and a half, which already feels like a very long time for a startup. We're very proud of what we had built out until I joined Facebook and I saw what can be done there. Realistically, we were actually very happy because we did not imagine what are the other things that can be built out in this aspect. Okay, so that was an eye opening experience. We think that our platform is very mature, but a lot of times it is not. The second thing is, obviously when I joined Facebook, I noticed all these amazing features in the platform. I cannot exaggerate how much efficient Facebook platform made us as machine learning developers. Like building training, deploying models, distributed training, serving across regions, et cetera, et cetera, was just trivialized realistically, right? So that was an eye opening experience. And the last thing is, after Facebook and before starting True Foundry, we were doing another startup in the talent space, okay? And we're building our own machine learning models and that's where we found the biggest pain. Turns out we are just coming out of Facebook where we have used this amazing platform, and now here we are trying to build and deploy and maintain the models all by ourselves. And we realize that doing it with the existing tooling, like whatever, like AWS tooling, setting things up is still a lot of pain and it does not get you as far as we expect people would need when machine learning becomes more mainstream. I think the use cases of today are still getting solved. But the way machine learning is maturing, people will need a lot more mature practice like set up overall. So these are the three events that led us to believing this problem is still unsolved. Now, I'll tell you what's the direction in which I think the problem has still not been solved and why we are taking up this problem still moving forward. So, if you think about machine learning today, actually ML set up in a lot of companies is a completely parallel setup from your traditional software, okay? Like for example, companies would use a Sage maker for machine learning deployment and completely ECS or Eks driven things for most of their actual software deployments, right? In your case, you are at least doing prototyping work mostly on ECS, Eks and then your deployment pipeline merges with a lot of companies. People would also put models to production behind endpoints using Sage maker, which is actually one of the advantages of using Sage maker, right? You actually lose a lot of benefits when you don't use those features. Now, in the long term, that is not scalable because there is no other branch in software that actually creates a parallel deployment paradigm compared to your traditional software, right? Everything eventually follows the same paradigm. So is it scalable for machine learning? Our belief is not. And the second thing is, if you think about machine learning today, actually machine learning engineers get a lot of slack, I would say, okay, where it's fine. It is not following some of the best practices, it's not going through CACD, it's not going through version control, it's not going through automatic rollbacks, a lot of testing infrastructure. Basically all these good practices that we have developed in software over decades of practice is actually not really maintained in machine learning today. You actually end up doing machine learning in a fairly ad hoc way. Even today, most of the organizations are not seeing every organization. And again, when machine learning becomes more mainstream, people will need to solve this problem. Otherwise it will become a beast that is unmanageable, essentially. Okay? So for these reasons, with the goal of making machine learning truly follow the best software engineering practices, but understanding the context of an ML developer that they may not understand all of those concepts so well, we're trying to build a platform that bridges that gap. Like so two main parts to our platform a developer interface that's friendly to ML engineers and a software interface that really ties in the best practices. That's the gap that we notice that we are trying to solve with True. Foundry. Okay, so when you discuss this with potential customers, is it cost efficiency that you highlight or is it because typically either you are value based product or more cost efficient way of doing things, right? And the pricing lever that you have would vary depending on how you highlight this value prop. So that makes sense. Yeah, that makes sense. There's two things that we generally bring up in this context and actually describe you where I'm coming from. So number one is actually more opportunities. So if you think about teams that have not truly operationalized machine learning, spend roughly 60% of their time maintaining and managing their existing models, which could be used, like the same problem that you described, right, that data scientists time needs to be more productively utilized, which could be used for building newer use cases. And newer use cases are infinite potential in some way, right? So one of the pitch that we have for people is that you actually unlock much more use cases in machine learning, with machine learning that you are not able to utilize today. That's one thing. The second thing is there's like a clear math here, and I'll describe you what that math is. Actually, I'm also interested in knowing that number from your standpoint as well. Goldie but typically what we have seen is people who build out this platform to people who use the platform, the number of people who build the platform to number of people who use the platform, that ratio is in the range of one is to four. Okay. Depending on the company. In some cases where the platform is getting built, it is one is to two. In some cases where it's very mature, it goes to one is to seven or eight. That's a range that I've seen. What's that ratio in make metric, by the way? I think one is to eight, something like that. Okay, understood. So it's more on the mature end. But the more typical thing that we see is one is to four. Now, simple math says that if your ML engineer is essentially one fourth the cost of an ML engineer that's going into building and maintaining the platform, that's roughly how it boils down to if one is to pose the ratio. Right. And typically by that, it might even be more expensive because a good platform team is actually like your best engineers, which typically end up costing more. Okay, but let's assume it's one fourth the cost of an ML engineer. Essentially, from a cost perspective, you're spending 25% of the time operationalizing machine learning, 25% of your resources operationalizing machine learning, which could be used in much better way. Right. So if we can offer you a platform that reduces this 25% of our ML engineers cost down to, let's say, 2%, 5%, et cetera, et cetera, it's a clear ROI from your perspective. That's a pitch that we have. Those are the two lines of pitch that we have, depending on the persona that we are speaking with. Okay, interesting. So you guys have looked at data ikea something? Yes, they have looked at data IQ. Have you used Data IQ before? No, but it was interesting because they were offering a very what you see is what you get kind of interface to everything with respect to data collection, data ingestion, all the way to ML model deployment and monitoring right, in one platform. And also some sort of an automl capability where you could do simpler models like your random forest or Xgboost without even writing a single line of code. So they were solving for end to end. And that was interesting for me from the perspective of taking it to non tech companies because for them, they are not going to hire a data science team. They are not going to have a data platform team. Right. So if you solve for end to end, like what data IQ is doing, it becomes very interesting for, let's say, logistic companies or manufacturing companies or companies that are in non tech line of business. Not myself. Yeah. So when you said what you see is what you get, was there a particular perspective with which you said that? Do you like that approach or do you not like that approach? I think I like that approach because I built a platform at Oracle that did big data processing and automl. Right. And whenever I went on customer calls, it was very obvious that they didn't understand and they wanted somebody to solve the problem. And so a data IQ kind of approach Oracle, what approach Oracle took to solving this problem seemed more scalable in terms of the number of customers this could appeal to. Whereas your approach still requires somebody to have a data science team, somebody to understand, somebody to already have figured out that they need to gather the data. They need to have some sort of a data like strategy. Right. So you are making lot of assumptions about what type of companies you target and that's why it would be a subset of the companies you can go after as customers. That makes sense. Yeah. No, you're right. That the target segment for us. And data IQ are actually different. It is true. So, like, for example, data IQ would, as you correctly pointed out, would let's target like some manufacturing company that does not have an in house engineering or data science team. We would target, let's say, some large enterprises like, I don't know, like Airtel, for example, which is not like as tech heavy as, let's say, a Make My Trip or a Swiggy. But they have their data scientists and there are some of the platform teams, but they're also not as backward in their tech compared to, let's say, the manufacturing company that we talked about. So you're right, there's a difference in the persona that we target here. Right. And I believe that will limit your applicability. And as far as multi cloud is concerned, I think it's a general use case. Right. When I was at Flipkart, we used to spend Flipkart mantra, we used to spend roughly $100 million on AWS in infra cost and we realized that they knew that we were so dependent on them that they could extract the maximum mileage out of it. Right. Because it was not easy to move tens of thousands of micro services and then set up to another cloud that easily. So one of the projects that we had started was to build a middle layer, a middleware that would abstract out the underlying cloud from us and we would move all of the services, all of the platforms that we use on top of this middle layer and that way we would become cloud agnostic. So tomorrow, if Gcp gives us better pricing or Azure gives us better pricing, it is easy for us to switch at a moment's notice. Or we could have a hybrid cloud set up as well. So some services where we get better pricing from AWS, we run it there. And some services where we get better pricing from Gcp, we run it there. Right. So it was a member multi year project that we were embarking on, but it was for the entire company, not specifically solving for Data science or ML platform. I see. Okay, that makes a ton of sense here. Very interesting. So you see a lot of companies doing this, or at least have you seen so far amongst your friends and peers where people try to go across cloud because of this cost reason? Cost arbitrage reasons. Not really. Right. Typically, if you get to a certain scale right now, since there is tough competition between 56 different cloud providers, everybody is bending over backwards to give you competitive pricing. Right. So even the moment you tell them that you are thinking, they will make sure that they match the price that whatever the other cloud providers are offering. So it's not really a concern, to be honest. But they are also waiting for you to be so dependent on them that at some point they will start extracting more value out of you to some degree. I think somebody with a foresight of somebody like an Uber or Lyft for example, so recently Uber was in the news, right, that they are planning to use Oracle cloud as well, apart from using AWS or Gcp, whatever they are using right now. Right. So as soon as a company gets to a certain scale, they would use a multi cloud strategy. Before that, it is not even the top three priorities for them. Understood. Okay. Makes sense. This is very helpful. Thank you so much for sharing that perspective, Goldie. But yeah, we are also so far continuing to work with a few companies to figure out what's our longer term strategy, et cetera. But at least while we continue to figure out our niche, I think this problem will still needs to be solved. Basically there are going to be companies that are going to solve the problem and whoever makes it easiest for the data scientist or the ML developer in the end with kind of meeting them where they are today, but helping them scale where they want to go next will basically solve the problem. And my understanding is likely it will not be like a one winner take it all market, but it will also not be like the hundreds of startups that are trying to do it today. I believe that there will be less than half a dozen of companies that will become very big here. So the question is, who solves the problem? For the data scientist? I guess that's where the market will evolve to, right? Yeah, right. Cool. I have to jump to another call. Great talking with you guys. And we will send out an email to connect with the platform team, okay? Sure. All right. Thank you so much. Bye.