You are going to lead? Yeah, if you want, yeah. Happy to. Kind of see as to how you go about. Try to lead through the floor. Okay. On the user flow. Let's see. We don't have to give a demo. Hi Michelle, can you hear us? Hey. I was wrapping up my call. Where are you based? Auto. I'm from Bangladesh. What about you? Yeah, I am also in Bangalore. Our entire engineering team. Glad to meet you. Which part of by the way, I live in South Bangladesh. I'm in Managata Road. Our office actually is in Core Mangla, too. Nearby? Yeah, nearby. Nice. Just to give you a short introduction. So. Hi, Michelle. I look at products and customer development at Truefoundry. Previous to this, I was working as a management consultant at McKenzie and graduated out of It for chemical engineering. For that. Yeah, I can give a quick background as well. We shall also be tech from It for electrical engineering and then spent six years working with a hedge fund called World Quant, where we were using data to build trading strategies across global markets. Spent three years in Mumbai and then three and a half years between us and Singapore as a portfolio manager, trading a portfolio of 600 million in assets for them. And was a member of the CEO office as well. And my co founders, Abishek and Nikkunj, both of them were at Facebook. Abishek on the engineering side, he was a senior staff software engineer there at Facebook and Anikunj was a lead ML engineer. All three of us are, and we have been like each other for 13 years. And that's basically been the basic foundation of starting Proof. Truefoundry. Awesome. Okay, my introduction. So I'm working at Times Square as the head of engineering. Now we got acquired by Delivery. So we are more of a delivery. And I'm taking care of entry and engineering practice over here. And I haven't worked with the company like Facebook and all because for my career I work with startups only. So I started my career with Inpibium.com, which was an e commerce company. It was a startup. When I joined the company, I was Pivot engineer. If you think about any startup, I have joined them either first or second or third engineer. I see. In premium. Then I moved to Logic, first engineer over there. Then I took him on Commitment for ten of years. Then I moved to Curate. Then merkel Science and then merkel science to here. That sounds like an interesting journey. Yeah, Merkel Science is the one that was being run by Subhanker and all. Right, which one. Was with the Singapore based company? Or is it ma'am is a founder, CTO founder. Got it. I think this is different. Okay, understood. Thanks a lot for the introduction, Michelle. I'll give you a short, brief context of what we want to do with this call. So Truefoundry is building an ML platform, which is focused majorly on ML model deployment. We want to ease out this process for enterprises. So much so that the data scientists themselves can do most of the stuff for which currently they would have to rely on an infra or a type of team so that it really reduces the kind of back and forth that happened there. In this regard. Currently we are in a very early state. So what we are trying to do is to partner with companies like TabSquare and what we try to do is to understand more or less of what kind of problems they are facing, what kind of context is there, how is your experience currently in terms of model deployment and associated steps of ML lifecycle? And try to understand what kind of problems are there potentially that we could better help you solve? So with that context you want to just understand what kind of ML use cases do you guys work at and brief context about what kind of stack do you use, what the developer experience like right now for a data scientist and something like that? Is that fine? Like Michelle, if we can use this call to learn a little bit about the workflow. And also basically one thing I had is you mentioned you kind of got acquired by Delivery. Is it the delivery, the Indian delivery? No, Delivery hero. Delivery Hero. Okay, got it. So one context is like does the engineering and all integrate with them or is it like you run separately, still. Continue to run separately, but we are working with them to integrate our products with that product, with our products. Okay, so even before that, before we have already integrated with Pool Panda and few other companies to process their orders on our system. We have that order in our system. Okay. So I'm not the right person to explain it because we have a dedicated data science team, they are taking care of it and we have San Luke who is VP of Data Science and Engineering. So he's the right person for it. But from high level, what I can tell you is like we are analyzing each and every data. Okay? So whenever you click on a particular product in a restaurant, then I'm just telling you the way. I'm not ML Kai, so I can't tell you proper way. No. So what we do is we are tracking each and every event and then we are sending it to Cloud Firestore and then Firestore triggers an event to Google pops up, google pops up, send it sensor to Google dialogflow where we are doing ETL jobs. And then we are storing data to BigQuery and we are also storing data to this one elastic search and for recommendations, purposes and all. And we are running some AI and process on the top of data. And then we are providing what is it? Cross selling and upselling recommendations along with analyzing personal food, eating habits, so that algorithm I am also here to understand because I've never got a chance to get into that particular part. But we are running ML engines over there where we are analyzing persons for eating habit only through the order data, whatever order that we are getting from for the particular. And we are analyzing each and every event. Like what are the ingredients, what are the taste level, everything. What kind of poor weather is vegetable, non vegetable. So there are around 600 to 900, I think even more than that list set up battery against which we are checking the order details. And then we are giving personalized recommendation for each and every user. Okay. And you mentioned you work with your clients would be like big conglomerates. Is it like food panda or something? What you mentioned. Restaurants are our main clients, but we have created a product called Smart Connect where what we do is you don't have to whenever you have a restaurant and you want to register your restaurant on Biggie dometo and Uber, it managing menus categories, orders in all the applications, whatever they have provided. So if you want to update a menu item, then you go to update in all the three applications. If you want to market out of store or update the price, whatever you want to do. So we have created a centralized system. We manage everything and create categories, manuals, promotions, whatever you want to do on our system. It will be a full sync all this information with four pounder or whatever system. And then whenever somebody will place an order on this particular platform and if that restaurant is registered on our system, then four ponder will send those orders to our system and we will start processing those orders. So you don't have to accept orders and pool print application, accept it on sorry Tesla product and then manage everything in one go. So offline orders, online orders, everything will be managed at the center of location only. Okay. So one question that I had, what is the team size for the engineering and the data science all and also for the is it separate or is it like generally the platform team that manages it? So we have separate team for infra and separate team for engineering. So Data science, we have around 15 people in data science and engineering. And when it comes to only product engineering, then we have around 120 people. Okay. And this is just TabSquare, I'm not speaking about delivery. Okay. And why separate infra for data science and ML, sorry, for ML and software? Is there a particular reason? What is the stack like? If you can given you had the engineering side wanted to was curious as. To what it is. Totally isolated application. Right? There is no connection between product database or their database. There is no connect because they are going to store data in their system or sorry, in BigQuery or they're on Elastic search. So through pop up queue we are just pushing it through their system or either the system, we are just pushing that. There is no relation. So we have given them separate product sorry, separate Google project and they are going to take care of it from there. And what is the base layer? Do you use Kubernetes or do you use any particular cloud? No, we are using TCP and we are also using Kubernetes. Okay, got it. Understood. And I'm guessing the data science team will also be deploying on GKE then. Yeah, a few things are there on App engine and few things are there on GKE. Okay. There are Dataproc and all these systems are also there, right? Yeah. Okay, cool. So shiny. Like you had questions. I also want to ask more on the engineering side given Michelle that is where a lot of the experience of your site and then maybe try to. Kind of try to get somebody on call from data engineering but not replaying might be busy income other call. Okay, we can set up a separate call with them. That's fine. If you can just link us through, that would be really helpful. But on the engineering but we are just going to understand the journey. So as a restaurant, if you are catering to individual restaurant, they just integrate with, let's say Omato Sweet and Food Panda. Right. And how do you install your system on there? Like do you give them hardware? How does it work? We are providing the hardware. Then if they have already purchased Path machine, then we are integrating with Path machines as well. Like we are doing all kind of integration whether it is offline or online. Okay. And does your application like run on these machines or. We have applications running on kiosk machines, we have applications running on tablets and we have QR codes managed with the application. So then the curcode application will come, the application will open your mobile specific to that particular restaurant. Or if you are sitting on a table, then it will be specific to that particular table only. Okay, so it's like integrating both the live dining experience as well as online that the restaurant might be catering, right? Yes. Okay. So like a unique channel for both of these orders. And now let's say this data comes in. This data, where is it stored? Like let's say a user or a restaurant would have to create their own database on menus et cetera. This would be stored there. Is this also in GCP? Yeah, everything is in GCP. Okay. And there is no privacy concern et cetera. Right? I mean you get access to every customer's data. Yeah, because like customer data is the core thing for us. So one customer with one TabSquare customer can access Tabsco application. In any restaurant you just have to enter your phone number at the time of registration. We are taking the permission or consent like we are going to use your phone number and we will go forward we will give you start recommendations wherever tap score is available you will use the same credentials okay? And now just to connect the flow let's say a customer places an order on any of your apps let's say the QR code this event goes when, how is it processed? If you can just shed some light again on that. Once you open the application, scan up your application, go through menu items and then move them to card, then confirm that particular card. And if there is any promotion and apply that particular promotion, or if you have any reward points and you can also apply reward points and then go ahead with order process. Place the order and then order will come to your system. We will go to merchants consoles like merchant console will get an order. You have an order. Okay? So that's one flow. And along with that, whenever you confirm the order, along with that, we'll send trigger one event from front end only this data. And you receive this data and you analyze whatever you want to analyze. So now this data let's say this order comes to the console. Now this data is sent where? It's sent to your cloud it's sent to our cloud okay and analytics on. This, is it like aggregated like a batch mode kind of analytics or now. How does this order get processed after. It'S sent to your cloud? Once it's sent to our cloud. Okay, so we are triggering one event. There are two ways. If it is front end sorry, web application. Then if you go to Google Analytics we operate our own analytics. So from front end only we are sending that data to our analytics database and other option is we are sending whenever we confirm the order we are sending that order detail to that system through Google pop okay? And there is one more platform where we are supporting offline systems as well where we are sending match order because there is a possibility we are then offline set up in some restaurants where there is a possibility there won't be any internet. Connection, but we have installed it, so without internet connection also you can place an order and restaurant person will receive that order in their merchant console and they can process the order but everything will happen offline so once the system will come online. Theoretically, we are syncing data with them. Okay, understood. And let's say this data comes in, let's say 100 people order from this restaurant. All this data comes in to you. So do you have like, where does analytics come in? Where are the ML models for each customer? Like there is a separate model and is it like a batch inference or a license? So you didn't get per customer you have different models. Per customer I need to check that I need to confirm. It okay, but basically this data comes in, you have an analytics dashboard which you populate with this data, right. And they can go. So are there any predictive or classification or something kind of modeled as well rather than the statistical analysis? When do they I'm just sharing whatever information is public right now. Okay. But whatever possibilities that you can take up, everything is there in the system. Okay. Got it. So few things vishal I would love to know from an engineering standpoint, I wanted to understand what do you use for are you using Data Dog new relic or some other tool for the monitor using Data Dog? And how is the engineering workflow? Like, suppose an engineer and this is say the reason why I'm asking this, I'll give you context is while we have built an ML platform for the deployment and monitoring for the data science teams, we kind of have built it on top of top of integrated to the software engineering workflow. So for example, in your case you are using say, GKE. So what it will do is the system will connect to your GKE and it will create spaces for your data science team to work on. And then within those workspaces, then the data science teams will be able to deploy. So basically we try to do it on top of the software engineering systems. And one of the things that we have heard from companies is even for software, some kind of this system could be useful. So while diving into that, I wanted to understand once the engineering workflow, two developers themselves test out the services they are building. Is there a visibility of all services that are there in the organization? How does the engineering workflow look like? Who writes the YAML Configuration? Is it the DevOps infrastructure? So a little bit of understanding on that side will be really helpful. Okay, definitely. So YAML file, if you think about YAML file and definitely DevOps team is taking care of it. But when it comes to docker and all the files, then it is engineering team who is taking care of it. Okay, so basically engineering team will write the code somewhere, then they will dockerize it at their end. And then this docker file will then be sent to the DevOps file who will write the YAML configuration responding to the Kubernetes and then they will do the deployment. Is that it? Yes. And is the rollout generally like one shot or is it based on traffic? So it's rolled out say 1% of the traffic, 2% of the traffic and then skill or is the rollout yes. Following Canary development. You follow Canary? Okay. Who does the canary like? Is it again like the conditions for the Canary, is it set by the development team or is it set by the DevOps team? And how is it? Right now we are just setting up proper DevOps practices in the company. Previously there were limited engineers in the company. So devo engineers in the company. We are doing it through DevOps team only slowly. We are improving all the things. When you say proper DevOps practices, I wanted to ask a few more secrets management. Do you use anything for secrets management? Yeah. Okay. Is it the Google Secret store? Yeah, for some service we use it. We use something else. Okay, got it. And then the authentication layer on top of this, like it's built in house or in house? In house, currently. Okay. And then how about I'm guessing in software, the hardware would not be a challenge like which hardware to use? Mostly in most of the cases. We have with specific binders. So whenever the requirement, we are looking for kiosk in Apple tablet or Normal and a Samsung tablet or any printer concept to your device, then through those vendors, we are just purchasing them. And then we are setting up each and every country. There are different devices depending on the availability. Okay, got it. Okay, that's understood. And then how does the CI CD is taken care of, like CICD and. What do you kind of Jenkins right now. Okay. So for a developer, just try to understand. So do they do the training, et cetera, on their own laptops through a Jupiter notebook or something like that? What is their experience like currently in. The for the data centers training? No, let's say they're developing some models or trying out different stuff. Experimenting? Yeah, they are doing it on Jupiter. We are done set up for Jupiter Instruments on cloud. Okay. Also on GCP. Also on GCP. Okay. And then from migrating this code, let's say this Jupiter code to creating images out of them or writing the Python files are the same. Is this also done by the developers? Yeah, that is DevOps team. Everything is done by developer. And just sorry, database team is everything is done by developer and then help in deploying all the services. Okay. And just trying to understand in terms of issues that you might be facing with deployments, what takes up like most of the time of DevOps team when it comes to interacting with the data, serving the data science team? Are there like any particular invoice that. You setting up pipeline and all the things setting a pipeline of creating ML file or monitoring system with infrastructure set up and all this? Only those things fall under the op string when it comes to deployment. So you just have to merge your code and it will be deployed automatically to the production. Okay. And this pipeline, resource provisioning, et cetera, everything has already been templateized or something or is that a case to place? Everything is in place. Okay. Understood. One more thing like visual. As we are moving to this basically more DevOps practices, I wanted to understand, are you developing something in house from a platform team's perspective? Like basically what's the platform teams at hotstar. There is a kind of develop where they basically make it very easy to kind of put services and it automatically goes through Canada and then there's full control at the hands of the developer in terms of visibility, in terms of cost, as well as promotion flows like currently. Do you deploy it to different environments and do you kind of promote from one environment to other or is it directly into a product environment? How does that part work? We do promote staging to you, to. Production and is that easy from a developer standpoint or is there a lot of bottleneck? No, there is nothing to coordinate. You don't have to coordinate anything. You just raise your peer against a particular branch and automated pipelines are already there in place to measure and then. Okay, so do you kind of create a copy of that? Basically suppose something is running in the test environment and now you have to move it to a production environment. So are these in the same clusters or are these in the same workspaces or are there different clusters that manage the product or test? Different clusters. Different clusters. So now if you have to take this and deploy a different cluster so do you create a copy of this code in the different cluster? Yeah, we need to because everything different branches and everything like staging may have code which we are not supposed to deliver deploy. So we can't promote staging branch to UAT or UAT branch to promotion. Definitely UAT and promotion. Other production are identical, but staging to UAT is not possible. Can you use GitHub visit or like BitMarket? Both are fine. Yeah. Okay, so I got a sense of the engineering side of things. Well, what we do, I'm telling you, we are creating a release branch and that branch we keep matching to different different systems. So one, staging can have multiple release branches, but that specific relief branch will help code which we are going to read on that particular day or that particular time. Understood. Which are like two things. Like first of all, thanks for sharing the context on the engineering side and also whatever context you are on the Data science side. What I would really request is. Somebody from data science team. Yeah, if we can get someone from the Data science team it will help understand that flow a little bit better. And we'll also love to take the chance to showcase to the platform in that and specifically for the work that you are doing on engineering in terms of setting up the best DevOps practice. Actually the way we have built the platform, we have tried to build it agnostic to software engineering as well. So I'll just tell you at a high level, what happens is basically first you can easily connect to your cluster like a GKE account, you know, including authentication, access control and choice of machines. Once you have connected within that you create different namespaces or workspaces for your different teams. This come with resource constraints like fourGB tenGB, memory, whatever and then you can have control of allowing people access to it, not access to it with full access control. And once this workspaces are created, then developers individually can deploy any service to this workspace which could be a service or a Cron job. The Cron job can be scheduled or you can trigger it at your own well, the deployment to the workspace is very simple like you can do it from the UI or through a simple YAML configuration. You don't have to take the help of DevOps or infringement for the Kubernetes YAML. Basically everything is orchestrated through the platform and then the DevOps comes and is more like a security layer wherein if anything break or if you change in the final approval, they come into the play if you want it. If you are okay with your developers directly pushing then that is also fine and develops as a visibility of the platform. So that's the kind of abstraction we have tried to build out into the platform. And then once you kind of do this deployment you automatically see monitoring logs which you can push to Data Dog which you are using and then at the same time you have secrets management automatically ingrained build. So if you are using Secrets Manager you can use our interface to kind of generate secret keys on top of this so it acts as a layer to sit and then all the other best practices like CI, CD integration to GitHub bit bucket, all of that. Your authentication systems, your rollout to production in a canary shadow testing manner, all of that comes by default with the system. Okay, so that's how we have built it. And the same workflow is replicated in data science. In data science like instead of TML, it basically exposes the Python way so that data scientists can do it. And then there are certain more complexities that come into the platform in terms of doing monitoring at the level of data science rather than just system level monitoring. Okay, got it? Yes. I would love to kind of have you on the call along with the person and actually give you a walkthrough of the platform. I would love if there is a way in which you feel this could be useful given you are already transitioning to something more usable on the developer side in terms of the best practices. I would love to see if you think there could be any value from there. Sure. So as an except we'll write an email, but it would be great if you can then include the corresponding person and we can try and take it forward. I'm just checking if we can join right away, but I think I'm also running out of time because I need. To we're all out of time, but another day is definitely. Let'S do that. We will also take the time to show you the platform that we are building. Okay. Can we do things? I can set up a call with you guys tomorrow. I'm not available. Thursday around 04:00 p.m.. Yes. Thursday 04:00 p.m should work. Do you also check with the Data Science VP? If he also Data Science VP, I think he will not be available. But Data Science Engineer who is the main person who is working on it. Yeah. Okay. Then I'll send you a minute for that. Thanks a lot. Thank you. Thank you. If you can stay for a minute yeah.