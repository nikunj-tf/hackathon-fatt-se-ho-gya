Hello. Hey. Hi, how are you? I'm doing good. How about you? Yeah, I'm good as well. Thank you so much for taking time to speak earlier. Shade the same. No problem. Look forward to the discussion. Yeah. Did I pronounce it correctly? Yes. Okay, that's fine. So the main thing that I wanted to kind of do in this call was get to know each other a little bit, learn about Maggie a bit. I can give a very quick introduction of Â£2 as well without diving into maybe technical details. And what I would love to understand is the stage of the current data science project at Amagi. What is the pipeline like, what is the infrastructure like? Dive into a little bit of that and if there are any code problems that you are looking to solve and given we scheduled this for a small time, I would love to kind of then do a follow up with a more deeper technical dive, including Amish, my co founder and CTO as well, to take this forward. Does that sound good? Yeah, that sounds fine. Okay, awesome. So I can go first. Basically, I'm one of the cofounders at Co Foundry, did my undergrad from IIT correct for Electrical Engineering 2013 batch. And after that I spent time working with a hedge fund called Worldwide, where we were kind of using a lot of different data to predict stock markets in certain way and trade along it. So mostly we Karen doing global markets and equities. Initially I was a quant pin India, and then I moved to US and Singapore where I was doing portfolio management for them. Managed around 600 million in assets for Worldwide and at the same time was looking after various growth initiatives there. During that time I had a keen interest in the startup ecosystem, so started interacting with founders and then started to invest in a few startups myself as well. And that is where I realized that I wanted to build something myself. And then in 2020, my colleagues like Abhishek and Nikung were my co founders and colleagues from It correct. They were also on the same boat and they were working with Facebook in the US. Both of them was on the software engineering team, like handling the videos or system redesign for Facebook. And Nikon was on the machine learning site, so he was a lead ML engineer there. So we all kind of came together and we kind of built our first startup in the talent space, trying to help companies hire technically better talent. We sold it to Info Edge because we are facing challenges in scaling it operationally and we wanted to build more on the tech side. And that is where we ended up discussing about the state of the ML infrared today pin the world as well as what it is compared to what Facebook has internally. And that is where we realize that there is a huge gap between what tech companies like Facebook, Google have built internally versus what is available for others to use. And that's where we thought that if we can build something that can provide the state of art intro to companies like Status or mid size companies or even a bigger size non tech companies where we can actually they don't have to spend time building the platform themselves, but they can actually use it to productionize models faster, that could be a huge value add. And that is where over the last one year we started building through Truefoundry. So that's like a very quick background. Pin some of the okay, nice, glad to hear it. Okay, sure I can go next. So I have been with Amagi for about a year. I graduated way back 2001, my bachelor's. Yeah. So I was initially working in the process design, I was working intel property, then tried my hand at technical marketing in D company called a Team. I was a technical marketing guy for Codex. So that's where I kind of got to know about the, I would say the audio video space basically how is media ecosystem handled and so on. So that was my entering to media then. But then kind of realized that I'm happy to know what happens in the marketing space. That's not for me. Just like you, I had an Itch and I wanted to be more technical than I was. So that's why I kind of pivoted at that point. And then I've been working with a lot of startups since. So I basically started off with the networking intrusion detection company. We were like seven or eight people, angel funded and didn't go anywhere. Then I was with the fashion technology company for about one and a half years where I got to really work on all these vision aspects more on I would say segmentation ranking those kind of use cases. Then I was working with another startup as a chief data scientist on that was again we were into the financial market space looking at documents, extracting information from the documents, got it and selling it to h ones, people who would want to use that data and build their own models. That was prior to starting Amagi. So in Amagi it's primarily into video ML knowledge, knowledge building knowledge graphs, use cases like that, trying to look at videos and then trying to see what is it that we can say about the videos, how can we add descriptors for the videos to help in recommendation ads on the typical use cases? But it's here one question, this adding of additional meta information for the videos or descriptors for the videos, is it targeted towards a certain class of videos? Is it more marketing videos or is it like some other kind of videos? Because I was reading and what I was understanding is more targeted towards maybe more advertisement kind of solutions. But I'd love to understand a little bit more. Definitely not targeted at ads or anything. The bread and butter for Amagi is cloud based transcode or cloud based content delivery solutions. Okay? So I mean, thanks to not exactly thanks to, but one of the outcome of the pandemic was that there is a lot more video viewing that is now happening. And also it just so happened that a lot of content creators also wanted to move their on prem stuff to cloud. Right. I would say the right place at the right time and all our solutions were pretty much on the cloud. Okay? So if I'm not mistaken, I don't know the exact count, but there are over 500 channels that we currently deliver. Okay fashion. But there are a lot more fast channels, personalized channels and things like that that goes on. Right. So we don't deal only with that. We deal with the actual content that gets streamed. We stitch the ads together. So there is a service side ad insertion solution. Also that's there is a part of this video that's where we are. I love to know a little bit like this is not like something that is probably something that I'm well aware of. Suppose if you can explain to me a little bit maybe from a user's perspective like say who is the user and then what access do they get in terms of the product push of Miguel? And then which part does data science start playing a role which is then powered by your team, I guess. Yeah. So where we come pin the picture, we don't really deliver this to we don't control the final UI, okay? Right. So we deliver it to let's say the Roku, Samsung TV and so on. So let's say content owner comes in and then says I have these 400 videos that I want to monetize. Okay. Now you create a channel for it and then you deliver it to one of these devices. Okay. So they will bring the content and then you will kind of create a channel by this channel will sit on which platform? Samsung TV plus. Right? Your smart TVs. Smart TVs or Apple TV. Okay. So there is a new channel pretty much that is created for that corresponding and you charge them for that creation of that channel. And then where does the data sign come into the play here. So the data science where it comes into the play is that there is a lot of long tail content that we play. When I say longtail content, these are not necessarily the contents that are wherein you get information from IMDb of the world. Right. So there is very little metadata to go with the kind of videos that we typically did. Okay. Our use case or the solutions that we have is looking at the video. What can you infer about the video? Okay, maybe video talks about a specific war or it's a historical documentary, it's about a specific topic. What are the different topics that are there? What are the key one of the main use cases, where do you want to insert the advertisements given a video? If you are, let's say, a premium kind of a content creator, then you can spend the time and effort and the money to figure out where is it that you want to place the ads. So quite a few of our customers are also digital first customers. Okay. So for them they don't want to spend the time and effort to create these kind of ad markers and so on. Help figure out where should they place the ads in an automated fashion. This is where data science comes in. Okay. Okay. So it's mainly analyzing the videos and structured data. It could be it involves vision, NLP, speech. Understood, understood. So this is the major use case that is being done by the Datta karen, there other use cases. So this one is clear right now. I understand, but there are multiple use cases, machine learning, I would say from the video aspect and there are recommendation aspects of what you want to recommend, what ads do you want to recommend, what is the nature of the content? Depending on the nature of the content given in D inventory, what kind of ads do you really recommend from that inventory? Okay, and these are all personalized to you, specific users like this recommendation, like on the TV and all? Just a quick second, I'm getting repeated calls from a number. Yeah, sure. My God, it's a credit card. Yeah, sorry. There are two aspects, video ML and recommendation recommendations. And these recommendations are individual user base. Recommendation as well at various levels. Individual. Got it. Basically, how big is the Data science team? Is it like a few members currently or is it like sense of members? No, it is not sense of members. I would say right now we are growing. So we have about five people right now, I think offers for at least three more. Okay, yeah, we will probably quote around growth around ten people by the end of this year or three, four months. Understood. I would also call or see the sense is what we are kind of responsible is more of building containers for the solutions that we build in the sense of we do use AWS, GCP, Sage Maker, Vertex, all those solutions wherein scaling is kind of taken care of by the platform. There is a separate platform team actually which helps scaling infra in vertex a Sage maker. So there is separate team but we are more focused on delivering containers, wrapping the solutions that we have. Okay, so the containers wayve been passed on to the corresponding platform or the DevOps team or the internet so they. Would build deployment containers on top of the containers Karen deployed. And we'd love to understand a little bit of the ML pipeline. So the use case is clear. Are these use cases, also real time use cases or most of these are currently batch and there's a goal to move this to real time. Most of it is batch. We are not trying to move this to real time except when there is a business need in the sense that we are looking to work on some live content shortly. So currently all of the content is pre recorded content. So there is no real need of a real time solution. So people can give us the video files, we work on them, we develop, get annotations, and we add it to our knowledge graph and then we are done. So this is more like a batch process. There is no real time need. There are use cases around, live use cases that we Karen looking to add on where there is a business need. We have not really spent time on use cases yet, but shortly, I would say sometime around this quarter likely to look into it. Got it. Understood. And then just from a pipeline perspective, does the team use local notebooks for training or like do they use instances on Sage Maker, etcetera. For training? And are the data scientists themselves also have engineering skill sets in the team? Like are they engineers or family data scientists? Do you see any challenges pin terms of the deployment and scaling or other use cases that are currently not being sold? Would love to hear a little bit about that. Okay sense, to be very frank with you, probably people could afford to be data scientists alone three or four years back. Right now, I really do not hire people who come in with the mindset saying that I only work on a jupyter notebook. Everything else needs to be taken care of because as you would know well, none of the solutions are really one time on this. So there is a huge monitoring pipeline that goes behind it. How often do you periodically collect cases where you karen not doing well? How often do you retrain? All of these things are automated, so it's like that's a bare minimum bar anybody needs to so we hire mainly people either with the computer science skill set or okay with in the past are okay with picking these things up. To be very honest with you, I would say rarely work on jupyter notebooks except for scenarios where we want to evaluate certain things or there is a very small specific piece of logic that we want to check out. Yeah, but mostly it's all CLI within dockers, using dockers extensively. Okay, right. And then automated training. Automated training broadly, most of the time that we do anyway, there's a lot of search, parameter search that's involved. All of those things are automated and stuff, so that's fine. Do you use any tools in this, like anything for the different experiments or hyper parameters, such things that you are running or anything for? What do you use currently as a model you mentioned about AWS GCP, like is there a cloud preference or internally or is it because the clouds are different? Because sometimes people have installed on their own and they have their different clouds. So we are largely cloud diagnostic. We use cube when it is no. We don't use when it is cube flow. Not yet. We have not done distributor training. Okay? So our thing is we have GCP VMs for instance, we maintain our we use ML flow. So we have let's say we use for instance DBC for managing the data sets versioning and stuff like that for training. Again, something that is around ML flow wherein we launch instances in gcpm or AWS VMs directly. Okay, so that is this. And how does the process of allocation of infrastructure take place generally? Is there like the infrasteam allocates like okay, this is the total or the resource that will be allocated for the data science team? Or is it generally like how do you request for the sources or the data science team is pretty independent in that regard. We are fairly independent so we can spin up any instance that we want. Okay, that's nice. So we primarily try to use spot instances that's reasonably priced, right? And then use that for our training needs or inference needs. So basically Sivasa, is there anything in the ML pipeline where you feel so I'll give you maybe a two minutes brief of two hounds and I'd love to kind of I know we scheduled this call for a Gmail time, I'd love to do a follow up call. I'd also love to learn more about the monitoring and the automated retraining pipeline that you are building as to how you are going about that and maybe dive into more details of Proof Foundry as well and showcase you deep up the product as well. So what we are trying to do is something like what probably your insta platform team is doing internally for you, right? Like they have built an insta platform and they then make it easy for datta scientists and engineers in your team to be able to containerize things and easily deploy models as they are being built. So similarly. At True Foundry. The goal is really that once you Karen. Pin the model building process starting from the training like using instances to train models or turning training jobs at a certain schedule to doing different kinds of deployments. Which could be a batch deployment or a realtime model deployment. We kind of enable that in a very seamless way. Either in a UID driven way or a CLI driven way or even in a Python way. So as to enable the engineers and the developers in the team to automatically deploy things in different environments which could be a testing. Production or staging or any kind of environment that you want to have internally in the company. And then as your use cases scale, it kind of supports complex use cases. For example, if you want to kind of do traffic splitting or shadow testing for your models, it kind of supports that auto scaling and all is automatically enabled by default. We kind of help you with choice of the different kind of machines. If you want to kind of restrict a certain kind of machines or use certain kind of machines for different kinds of deployments that is enabled to the platform and then at the same time if you want to be able to kind of automatically monitor whatever you are doing then we kind of with a few lines of code we kind of automatically generate the monitoring dashboard for you. So the goal is really to give control on the hands of the ML team so that they can deploy models, they can do it in a more systematic with the best SRE practices like CI, CD pipeline, auto scaling and all of that set up and monitoring by default. So that's the goal. And doing all of this it kind of optimizes for some sort of cost because of the way that the system is structured. So that's at a very high level at this stage we are still early in the journey and what we are looking to do is work with companies which have mid level ML pipelines or advanced ML pipelines and try and see as to if we can add value to their system. We are working with a few companies very closely trying to understand any problem statement around the deployment that we can help solve for their team and then we kind of do like a POC and then try to prove the value and that in that sense we kind of help understand our system better. Pin terms of improving it and at the same time we try to add value to the team and we customize some of the things that are specific to the companies that we are working with. So that is the goal here to understand maybe more about Maggie, the infrastructure, if there is a core pin point that the system that we are building can solve and if there's a possibility to try it internally and see the value add from a productivity and other. Perspective sure makes sense. I'll be happy to have a word with you, go a step further and talk about what we do. Okay, awesome. So is Friday something that will work for you sense what's the coming Friday like this day after tomorrow for a slightly longer call where I can also have a vishake and we can go over a little bit more depth and also more depth of what we are building at Two Foundation. And what I would really encourage you to kind of think about if it's possible is any problem statement at your end that you feel that we Karen, an external platform or an external team could help as you are growing your machine learning team, and we love to discuss that and see if there's any way we can potentially work around that. Okay, sure. I'll definitely think about it. This Friday would be let's try to meet sometime next week. Next week. Okay. Next week. I think Thursday or Friday. I think I'm broadly available Thursday or Friday. Yeah. Okay. So next week Friday, I will send out an invite for 04:00. That will be great. And also what I'll do is I'll send you a follow up of this. I'll send you a small deck that already will give you some overview of 200 that will probably also help you think around maybe our use case. So we are like a very early stage, and we look forward to people like your experience and who can potentially give us an entry in this space by working closely. Cool. Thanks a lot. Have a good one. Bye.