Hey, how are you? Hey Nicole, how are you doing? Savik. Good, good. So yeah, Ben sent me this invite. I think you connected with Ben, I guess on the. Thanks. Hi, Ben. Good evening. Okay. He's just setting up his AirPods. So you work closely with Ben at Haiti? Yeah. Okay, nice. We can't hear you, Ben. No, can you hear me now? Yeah. Yes. Hello? Yes, I can hear you. Okay. Ben, you can hear me? Okay, we can't hear you again. Yeah, we just lost you. While Ben ben is able to fix this issue. Shane, you been in the Bay Area for long? Yeah, I've been in Bay Area for a few years now. How about you? You're also in the Bay area. Yes, I'm in San Francisco now. I've been around here in the Bay Area in general, maybe like close to ten years now. Yeah. Okay. Pretty long. Maybe it's around that close to ten years for me too. I see, okay. I see. Nice. And Shane, you always been working in the data and the engineering and the machine learning space or what has been your focus? Generally? I have been working in a lot of different areas. Machine learning is one of them. I work with like data engineering. We cannot hear you, Ben. By the way. Keep talking. I was just saying mostly I've been working with machine learning, working with Datta engineering kind of end to end from that perspective for a few years now. Okay, I see. Amazing. Maybe like topic. I'll give you a brief introduction about myself. I would love to learn a little bit about you as well. And then the purpose of the call, I think Ben might have shared already some context with you. So generally the purpose of the call was that in the context of the startup that you're building, I wanted to understand basically IBM spending a lot of time talking to practitioners, understanding their current workflows, how are the building and deploy models today and where do they see that there are currently some gaps that exist in the workflows right. That can be improved. So I'm kind of trying to understand that a little bit. So we'd love to spend some time doing that and post that if you have time. I would love to maybe even show you a demo of what we are building and get your feedback of that. So a bunch of stuff I will maybe briefly start with my introduction and then would love to learn about you. Okay. So I come from a machine learning background. I was at Facebook where I was leading one of the conversational AI efforts. Prior to that I worked quite extensively in the e commerce domain, building out personalization algorithms, recommended systems, etc. And also horizontal AI platform for the company. Because the startup that I was working with, reflection at some point grew to a point where we had like five different teams building machine learning models and we kind of built out a horizontal platform to make developers more efficient. Okay yeah. So that's kind of where my background lies. Before starting Truefoundry, I built one more startup in the HR tech space where we help companies hire better tech talent and that company was acquired by Info H. You might know like owner of Knockley.com basically back in India. So they acquired and the same three co founders and Raghavish and Die we are actually batchmates from It Krakpur. The three of us decided to then build Truefoundry together at Truefoundry You, we are fairly early. We have seat funded by Sequoia and now like we're working with a few enterprises to help scale up their machine learning operations but like nowhere close to being like generally available and stuff. So we are actually working in design partnerships with some of these companies that are closely working with right now. I see. Okay yeah. From our perspective also just to share it, what we are doing right now is that we're trying to build some machine learning ML models on specifically in the ad and ad tech area. I shared some of the data engineering. I wanted to tell two things I've shared with him about our Airflow and ETL or back infrastructure. You can tell him what we want need for our ML dev and ML production environment. Right. So that is relevant for him. That's what going to we now have some MLR algorithms we are developing, we are training it on AWS essentially on Sage Maker. But we are running into issues like we have a notebook running on Sage Maker and in some cases it takes a long time to execute and finish. The other thing is that we have to log into AWS and kind of get servers on AWS and run it there. That's the only training part on phone conference part. Also we're trying to see if we can write from our desktops, can interact with the ML models and do inference and test it out and then roll it out into AWS. So that's where we are kind of wondering if there is anything nat you company or your products can help. One of the problems we have is. That when you are on mute again. One of the problems I think. We. Want to run locally arun fast but from a software policy perspective we just have 16 GB, right? So if we can run locally but you can also interact with AWS resources in some way, that will be good because we tried running it locally and some of the things did not scale right. So one option could be having the David Manner pull in the cloud. So. That we can run our training or we can do the combination of local plus cloud and we just part the command so we can do Fast. And. Then definitely the production will also in the cloud. I sep. So whatever does it give you a good picture, what you're looking for? Yeah, it does give me some picture. I have a few follow up questions. Ben and so the other thing I. Shane to highlight, sorry, is that ours is all there is no real time, it's all bad, there's a lot of workflows. Basically typically we have the schedule. We are heavily invested in Snowflake, SDMR and all those things. So ideally all the remote pilots will be invoked through our airflow main scheduler. Right, got it. Okay, understood. Yeah, completely makes sense. Topic maybe one thing that I wanted to understand is like you mentioned that we are able to interact right from your desktop with the machine learning models. Right. I think Ben also hinted on something similar. I want to understand what kind of interactions are you referring to that you want to do directly from a desktop? Is it like being able to train the model or see the results of the model? Yeah. So what we're trying to do is what the ideal thing is just we build our training, let's say the program or the algorithm, write it out in let's say Vs code, right? And then fire it off and say run training and it goes up and we specify, let's say certain instance sizes. Like I want 90 CPU and whatever, ten GPUs to this model, right. And here's the input is maybe s three. Directory output is another s three. And say, okay, go run the model, create those artifacts and then push it into s three one seamless way. And then we can actually and then for productionizing this right. How do you productionize this type of interaction? Right. And also the second part of it is inference once you have the training done. Now let's run some inferences then also if we can use our local environment but can push it to use our local environment but also take advantage of the bigger cloud environment like the Amazon. Right, that's kind of the idea. And the other part is like Ben mentioned is that our laptops ben might shane 16 GB mins. I much scaler. I think mine is like eight GB. I cannot even run like very Qovery, simple logistic regression. I mean like very simple models with very small set of data. It hangs and it crashes for me. So essentially if there is a way we can take advantage of the bigger cloud environment, but we don't have to go in and like type in into Sage Maker. If we can manage all of them right from our local desktop, that would be the best. And Sage Maker also we are not really using Sage Maker, we're just using Sage Maker as a Jupiter notebook instance creator, right, just holds our Jupiter notebook. It's not really we are using anything on the Sage Maker end. And also the other thing is that some of the libraries are also not available in Sage Maker. But that might be like a secondary thing. But primarily we want to use the powerhouse of the AWS, but from Nimble, from our own machines. So that's kind of our idea, if that is possible somehow. Understood. Okay, very helpful. And one last question on this insurance side of thing. So we mentioned like you want to be able to also run the batch inference same way from your local machine, right, which is similar to training job, right? Like batch inference is basically no different than training job in one way, right? That you read data from one place, run your batch inference, save phone conference in another place, right. That's kind of what you mean to say. Okay, understood. All right. And then you talked about productionizing, this training job, what does productionizing mean to you? So productionizing is essentially we deploy the training job somewhere. Mostly for us, it will be probably AWS airflow, right, which will kind of continuously keep on training it. That secondary. Actually. That comes secondary. Initially what we want is also productionizing our inferences. So that every day right now, what we are planning is we'll take our model, package it into like a Rest API and put it into some AWS container, dockerize it and put it into AWS container and use airflow to invoke it, send it an input file and get an output from it. So that's kind of our current thought process. We are not yet automated the inference part. But yeah, if we can just have this productionization also a part of the ML Ops framework, that would be very helpful because that's kind of the ultimate goal. I see. Okay, understood. Sounds good. I think this is very, very helpful and I'm assuming topic that like this is early in the form of a conversation with Ben also that you are early enough in your journey right now that things like drift tracking or is like model performance monitoring and dollar not very useful right now. We actually that's a good point. Model performance is something we definitely would like to monitor when we reach that steady state where we have a model and we know that's kind of what we are going to use. But that is something that is also something is there in the back of a mount mind where we are going to while training. When the training is happening. It also kind of spits out those performance metrics automatically into our snowflake or some storage and then we kind of graph and chart it and see how the performance. How it is performing with newer new data sets that we're training with. Understood. And how about CICD of these models? Are you also intending to do that? Like you push the code? We would love CTO. So those things are not yet completely so we are not yet thought of. We have thought about it. We have not put a lot of thoughts into it understood. Yet we don't have any framework for it. We have to just come up with maybe thinking AWS code pipeline or something on those lines right. Which can be used for this. But those will be very good input if you have more information or if you can offer some of your thoughts on those. I see. Okay. And the other question is around the type of models. Sovic so are you primarily using classic ML types like Xg boost psychedelarn types or it's mostly TensorFlow Python types. We started off with LSDM chad and we are now doing a little bit more of like classical Gmail. We do like multiple of them go at the same time and see how each one is performing. So we'll use a mix of both essentially. Okay, I see. And I'm assuming that for your inferences right now you're not using anything like TF server or Torch server, any zoom conference servers? No, we are not. Okay, and do you foresee any use case of being able to use some of those servers to speed up your inferences and on? Sure, I mean, I don't know about that part, but if you can share more information, that would be awesome. Understood. Okay, cool. Let me actually show you a little bit of how some parts of our platform actually works. And I want to keep structure the call such that it's more of like a feedback session as opposed to a sales session. Okay. At this point, I think learning about the use cases is more important to me than selling the software to any specific individual, to be honest. Okay, so I'm going to share my screen. I will try to give the demo of the platform in the context of what you described, but there are additional features that I'm going to talk about which may come in handy at some point, although it may not be relevant right away. Is that okay? Alright, Ben, I don't know if you have your attention now, but I would love to get you feedback as well. But I'm going to start sharing my screen and keep gong. Okay. Can you guys see my screen? Okay, so the first thing, by the way, a lot of what we are doing is built on top of Kubernetes and the way we have designed the platform is it actually works with take a note of existing sep as well. So for example, if you have your own clusters, which in this case I am hearing that you don't actually have you own Kubernetes clusters because you're using Sage Maker. But like if you chad your own customers, you could actually connect with that and stuff. Like if you have a certain docker registry that you're using, I don't know if you're using like Docker Hub ECR, any custom docker registries would integrate with some of those. We are planning to use AWS ECR to publish our like inferencing code base essentially. So inferences will happen in the dockerized easier. Makes sense. Okay, so we would integrate with some of the existing docker registry so that we can actually pick up docker files from your registry and like, of course whatever your existing code set up. Do you guys use GitHub or big bucket? Yeah, we use GitHub. Okay, nice. So we integrate with GitHub as well. And like a couple of features that I'm going to show you is that like, you know your GitHub directly. If you push a code to GitHub, we will directly deploy from there, which really helps in setting up your CI CD pipelines eventually. Okay. So IBM going to show some of that and then obviously your secrets management, like if you're using AWS, secrets manager, parameter store, et cetera, we are going to integrate with some of that as well. Okay, so this is a little bit of the connection or the sep side of things, like if you wanted to leverage some of that. But very quickly, I'm now going to jump into the DevOps side of things. Okay, sorry, the developer side of things, which is how do you deploy your models like your training jobs or your batch inference jobs on our platform here. Okay. So each item that you're seeing here is basically one training or batching force job. A job to us is something that you go, you arun, you spin up a machine, run a piece of code there, and then the machine kills itself. Right. So it's like a short term thing that you run there basically, right? That's what a job is. Now each line item is a job here, soviet. And what you can do is once you've created a job, you can actually just create a button here, like click the button here and it's going to retriger that job. So you notice that the job triggered successfully and it will get into the running stage right now. Right? So that's one thing that you can do directly from here. You can actually take a look just. This is scheduler, right? Just like that it replaces airflow, is that correct? Right, yes. You can actually schedule jobs as well. That we can run Chris smaller training job every day at 09:00 P.m. Or whatever, you get more data ingested or you can do it like the one time thing that you notice that. Soumen data came in right now and you just wanted to re trigger that thing. So you can do that as well. Okay. The second thing that you can do is you can actually track the metrics of your jobs. In this case, it has not been running, so you're not seeing the empty graphs. I can probably try to pull up something that has graphs, but the point being that when the job is running, you can track your CPU memory, network usage, etc. Directly right here. Okay. And obviously you can also track your logs of the job as well. Okay. The third thing that you can do is like everything that we do is actually version. So like for example, like model training, right? You may have like two different types of use cases. One is that you have the exact same model every day. You're running the model with different data set because like, you know, you get new data coming in, so we're in is happening or batch inference is happening every single day or every single hour, whatever. So you will be able to track all those runs. Like these are the different runs of the same model. And you see that you are able to see which version of the particular job ran here, right? So you can see all of that here, and then obviously you can see the corresponding application logs if you wanted to. So very useful feature for your model retraining and your batch inferencing actually. But if you wanted to kind of like revert to a previous version. So like, in this case, we have one version deployed and you just wanted to redeploy this version. You can do that. If you have a version two and you want to redeploy version one, it's practically a click. And then you can just go ahead and redeploy a previous version as well. Now, the important thing that I'm going to show is what you described about you want to be able to control this. And by the way, from a DevOps standpoint or from an infrastant point, this is actually running on a remote Kubernetes cluster, right? Like you can see the pods and all from the Kubernetes cluster. But the important thing I'm going to show is your developer experience, which is probably very similar to what you just described, that you have your local machine where you want to run your job from your local machine, but you're able to leverage the power of the cloud, right? That's one of the questions that you had as well. So I'm going to first show you that part and then talk more about the details of the platform. Okay, so for example, the developer experience would look something like this, that you have a job that imagine that this is like a trained PY script that you have, okay? And this by the way, is a vanilla training script. I don't know if you can read the code here, but this is like a Villa training script. There is no truefoundry specific code here. Just sklearn. You're importing that and just training like an Chris model. Okay? And then after this, once you want to deploy this from you local machine, what would happen is you would basically just get a template of this code that we have. You copy this code, and then you provide us a couple of information that you create a job object. You provide the name of the job that you want to run. You tell us the command that okay. The name of my script is Train PY, and I want to run it by Python train PY. If you wanted to provide like a separate requirements TXT so that all the packages issue that you were talking about cervix you can actually provide the requirements and then simply hit a job dot deploy on a and basically once you hit the job deploy, what this will do is it will actually create a docker container from this training script, right? So it will create a docker container from this training script along with the requirements of taste that you have sent out and will run that job. On this you will start seeing like one extra line item coming up under the job staff here, which you have just ran from your local machine. And that local machine could be literally like your local whatever vs code environment that you wanted. It could be a jupyter notebook that is running on your local machine so you can spin it up from your Jupiter notebook as well. Or it could be your CI CD pipeline that is like your GitHub has soumen checked in code and from there it wants to trigger the job so you can trigger it from wherever you want. I'll take a pause here to see if you have any follow up questions. So will it run it on the proof in that Kubernetes cluster? Right, it will run it on the Kubernetes cluster that is linked on the proof on the dashboard. Yes. And that cluster could be like your own cluster. It could be like if you wanted us to manage the cluster, we could do that as well. So all those things can happen here. But if I want to look for specific, let's say I want to train a specific big data set, right, on a big data set and say I want to run this on AWS 96 CPU machine or ten GPU, those G machines, right? How do we specify some of those like infrastructure? Great question, let me actually explain that to you. So basically what happens is when we are creating the clusters, right, like you could like maybe like, you know, you already have let's say a Kubernetes cluster that's like one route, the other out is that you want to create like you know, those specific kind of machines and everything to you thing, right? So basically the way this works is that we can run on any cloud that you want and here you can for example, I'm gong to show this to you here. So for example, here we take a note of these instance families that are supported. So cluster is basically a group of machines, a Kubernetes cluster. Now you can tell me that, hey Quinn, in this cluster I want to specify these four different types of machines because I expect that these models are going to be large and I want GPUs or I want some specific kind of CPUs in here, right? So you can actually specify the type of machines that you care about. Okay? And then that's the type of machine that your cluster comprises of. And here you can specify whatever, 90 CPU, ten GPU, whatever that you wanted to get. Right. But you can do actually further than this on our platform, actually. So not only can you specify the type of machines at a cluster level, you can also do it at what we call as a workspace level. So, for example, sovic that maybe you have one Kubernetes cluster, but you're using it for your dev staging and pro environment. Now maybe for dev environment, you want to train the models with like limited data set. That okay, this is going to be a smaller thing because I'm just trying things out. But for production environment, maybe you want drain with a full data set. So the type of machines that you want to access in your dev so that you don't increase your cost too much, would be different from the type of machines that you do it for your production system. So that way you can actually create three different workspaces. Let's say a dev workspace, a staging workspace, and a prod workspace. Okay. And then you can actually allow something similar at the level of your workspace as well. For my dev, I only want to use like four CPU and eight GB memory, for example, right. But for my production, I maybe want to use like nine P CPU and 200 GB memory, for example. So you could do stuff like that when you want. And this also helps in some level of access control as well. That is, I don't know if that's a use case right now, but you may say that dev environment is open to everyone. So IBM going to add all my developers as an editor, but my production environment, I'm going to add people only as viewers, and also that it only gets updated from a proper GitHub CI CD job or something, and people can't invoke it from their local machine. So it really depends on the level of maturity that you want to get to with respect to your infrastructure here. Got it. So these machines, let's say I want to trigger a big job and then shut it down, right? Yeah. But Chris, are these machines always running. Or no, absolutely not. That would be very, very expensive. Right. So the way these things work, Sobic, is that whenever, for example, you have a bunch of jobs that are scheduled here, right? So the machines exist as part of your cluster. When this job has CTO run, it will actually spin up that machine from the pool that you initially configured. That okay. For this type of job, I care about like ten GPUs, it will spin up that type of machine, run the job, and as soon as the job is finished, it will kill the machines. So you only pay for the infra cost when your actual job is running. I see. Makes sense. And for inference, active, right. You have these inference jobs. So let's say I want to how do you access? Chris in itself is like an Airflow kind of environment where it's the scheduler, it runs zoom conference and depending on whatever is there, it will take something from the input and save it to an output, right. Something on those lines and it runs probably in its own scheduled whatever the schedule is, right? Right. So basically these are like voila Python scripts that you can point your Python script to read data from s three, arun the batch inference and write data back to s three or to whatever database. Right. So that's kind of how you have written your Python scripts. What we are going to do is allow you to run that Python script on a remote cluster and manage do a lot of good software engineering practices around it. That is, how is your version doing? What are the different types of runs, being able to access the logs and the metrics and all of that in one place? I see right now what we are doing right now. Post planning, meeting the data out of Snowflake inference data right. And putting into a three and then calling or we have not chad the process, or we are going to call an Http call to the docker ECR container to hey, can you run the inference on that? And then that will wake up, run the inference, put it into a string and again go back to its own sleeping stage. This whole infrastruct is handled by the Airflow. It does its own input, creation of the input and also when it gets the output, it also gets other datta sets and then writes it back to Snowflake. So I guess that the whole process can be done inside this one, right? That is right, yes. The entire process can be managed from here. And then a few other things that we have built out from a machine learning standpoint is for example, if you are running different versions of the jobs or retraining model for the batch inferences, we actually built out a way where you can track a lot of this neatly in one dashboard as well. So, for example, let's sam, you had like four different retraining of your machine learning model that happened, right? So each line item here is basically like one such retraining, right? And then you have like very quick overview of here are the metrics that happened that were generated in these retraining jobs and these were the hyper parameters. Like in blue, you are seeing all the hyper parameters of the job. For example, these were the hyper parameters that were used for this retraining. Right. So you have all that information available and then you can basically get inside a particular run and see a lot of details that typically you care about as a machine learning engineer. That is your config, your metrics, your training graphs. If. You have any, right? What type of data sep was used for training these models? Like the summary and distributions of that data set and all. So you can do a lot of those that kind of tracking on the dashboard, which typically people care about when they're building machine learning models. So we have kind of built out some of those things from a machine learning standpoint that I think Airflow is designed more from a data engineering type of thing. Here we have done a little bit more work from an ML standpoint. Yeah, I can see essentially it's very specialized for machine learning, right? That's what you have here, right? And also essentially what we are doing is we're looking at the distribution of the data in the Jupyter notebook and while you're building up the training algorithm, but since you have integrated that within this service, when we are retraining, we'll be able to see those distribution, but are those static set of distribution. So can you show me that distribution? That the charts that you are showing me? Yeah, absolutely. So let me actually show this to you. So you Shane a POS demo or something, right? Yeah. So for example, something like this. So basically what's happening here is that each row here is like one feature that was fed to your machine learning model, right? And basically like you're able to track these some statistics of your feature across your training data set and a testing data set, basically. So you can say the testing times maximum, training type maximum and then you compare the distribution of that feature across you testing and training with a lot of times people also want to make sure that their training distribution is looking similar. CTO you testing distribution, otherwise your model itself can't perform too well, right? So that's the distribution that you're able. To what is this standard deviation or what was that? What metrics is that? Is that like something that we define or any metric that we want to expose? Right? So this one, the graph that you're seeing is basically a histogram, right? And then after the histogram, that's what automatically gets plotted. But there's a lot of other things that you can do if you wanted to track like fancier metrics. I'm going to talk a little bit about that. You can actually define your own metrics and you will be able to see those distributions as well. But in there, I just showed you some of these specific. Like histogram is what people typically want to plot when they want to see a data distribution. So that's what we do by default there. And I will come and talk about these dashboards where you can define any of your metrics if you wanted to. Do that as call today histogram of which, what is the metric? I mean, is the histogram of the. What is the x and y axis? This is actually a feature distribution, right? So I'll explain you. What is the x and y axis here? So in here, the x axis is the range of values for a particular feature. So imagine that here you're talking about the air quality index. Let's say right now, air quality index could be whatever 00:42, could be 01120r, could be zero 331 right. Across your different data points. Right. And what it is telling me is that, okay, zero 82, I have 100 data points that have the quality index close to this one and maybe ten data points that has the air quality index close to 00:42. So it's actually taking your data like a feature, figuring out what are the mins i, which you have the maximum number of values. So the y axis is the count and the x axis is the range of the value in which you are plotting the distribution, basically. So this is like one of the maybe let's sam, you have ten independent variables, right. So this is the one independent variable. That is exactly right. Yes. So then you can do it for each of the ten other ten also, right? Exactly. So these are like literally like these are the all independent variables and you can obviously keep looking at more and more independent variables. Yes. You're looking at the distribution. Right, okay. That's right. The default since the frequency distribution. Right. It's a frequency distribution across different features. You have that, right. For each of those feature or each of the variables. Right. You have that graph that we can see. Okay, that is exactly right. And then you could also do that for your dependent variables. That is like your actuals and your predictions, like the model predictions as well. So you can do that as well. I see. Okay. And obviously, anytime your model, like your training job is generating any files or whatever, if you put it on, you can track it there or you can track it here as well. You can just quickly get just see all the files that were generated or used and every artifact that was associated, you can track them here as well. So basically this gives you one place where you can come track like your model performance, run your jobs remotely, all of that. And that's kind of how the system is built out here. Correct. One of the other things that I wanted to show here is I explained a little bit from our developer experience that is like you write this Python code and you are able to deploy your models directly from here. Right. I'm gong to show that also eventually when you have services right. Now you mentioned that you don't have services, right. But eventually you have API endpoints that you want to create out of this model. Correct. So that is what we call a service. The services to us is like a long running thing that an API endpoint always is up and running and it can take requests whenever, right? So every time you spin up a service from our platform, it basically goes and creates this is the demo actually, maybe I'll just show you the fast API endpoint so it actually goes and creates this API endpoint that you can actually just try it out in real time. Like you pass in whatever data that you want here and it will basically invoke the predict function of your model and give you the response of the model. So here are the probabilities across different classes that the model predicted basically. And this actually is like a real to give you a call request that you can integrate with your product. So you quickly have that API endpoint up and running. Right. And then the developer experience of spinning up an API is actually very similar to a developer experience of spinning up a job. So in this case, what would happen is you would write your vanilla Python code again that you implement your like phone conference function that is loading your model and just running the dot predict on it. Right. And then you basically again do something similar. But instead of a Job object, you create a service object and then you hit a service deploy same thing. You can just provide your requirements TXT and we will just create a line item on the services tab in that case. I see, so this is more for like real time use cases, right? Correct. You could do this for real time use cases, like getting batch use cases. Jobs itself is sufficient for real time use cases. You could do that. And again, you get all the niceties that you get with Job. That is you get to track your model performance, the metrics associated with your system, you get to track your logs. Everything is versioned, which is even more important in the case of an API because APIs you frequently might need to revert to something older maybe because model changed or code changed or whatever. So you can redeploy a previous version of the API if you wanted to do that. Got it. Thanks for giving the demo. So. IBM trying to bucket what sort of process it will help us. So, what I'm trying to say is that we have Sage Maker, we have ML Flow, right. So it's not replacing those functionalities, right? Or it is replacing those. So technically it could replace Sage Maker. And I think the use case that you had mentioned, Ben, right? Like that is you want to work off of your local machine and use the power of the cloud, right? That itself, I think the platform would empower it from the local. But is the power of the cloud exactly? Yes. So all this code you can literally be writing on your local jupyter notebook or you could be writing on your local BS code. Like this entire code that I'm showing. Is actually coming from the cloud line things, right. So you can put an input you can give input file right. And then run it in the cloud, right, and then get the output. But the output will be generated in the cloud and then you can just download it. That is exactly right. So you could do all of that. And then the second thing that can do is if you already have an instance of ML flow that's running, you could just use that instead. I showed you a place where you can track the model performance and the metrics in our dashboards if you wanted to continue using your Mmflow. Our system also, the reason I'm thinking. Is Chris, open Source maker, we're not very excited. We're not using it. But those open source or even Sales Maker, they are quite deep in functionality. They have all the best and we sell the UI and all those things. Again, I understand that you also have a UI where you can control the basic things. So I was trying to figure out at least so from our side what is kind of the MVP for our functionality. We should be able CTO because we are a very early know gentleman. We just need some small things, right? Yeah. We have a model which somebody is running. It may be coding it locally. And then I guess your input data is in sitting in S three, right. So we want to is it dragging it in the so then you have to see our input pass the input file, correct? Yeah. You are running it from the local. You pass the input file correct. And then that input file has to be local then. Correct Nikun from your environment. And this thing will work from your S three as a local? Almost. Okay, so you have the code in your local and you can pass the st location. Yes, exactly. You can pass a st location. Okay. So that's fine because you have the access. Okay, so then it will run in the cloud. So how will you take your code from local? It will run the cloud and then dump the output in S three, correct? That's exactly right. Yeah. So that takes care of our day environment. And you have all the Kubernetes and you have all the built services of the UI and all those things, right? That's right. Yeah. Okay, so that's good. And then for the production, it should. Do something similar for production as well, to be honest. Basically like these jobs in a certain cadence. Do you have any questions? I have one question. So if we want to use, let's say in the app, right? If we can just want to use, let's say, phone conference inference engine right now, actually we are just what we're trying to do is we're creating this docker container on ECR, right, on Amazon, right? And that becomes like another API. Is there like something similar that we can use on you in that platform? Or it has to be like the end to end flow, right? No, you could literally like the way the platform is built out. So because you could literally use any one part of the platform and it actually is designed to work with other tools if you wanted to continue using it. So for example, for phone Conference engine, if you wanted to have your own docker containers from whatever ECR and just deploy that as an end point here, you can actually do that very easily. Like you have your service, you just say that okay, I'm going to deploy the service here. Okay, sorry. This case, this is a dev environment. Let me just show Chris to you in this environment. So you have your service that you want to deploy. Okay? So you click on new deployment, you select whatever this thing, and then you can actually just specify the docker file here. So what you're doing is basically here's my docker file. You just provide me the path of your ECR URL, of the docker file and that's it. You provide that and everything else is basically more advanced configuration if you wanted to configure that. Okay, which port do you want to run it on? If you wanted to specify any environment variables from an access control perspective, you could do that if you wanted to control like how much CPU memory you want to allocate. If you also wanted to by the way, do stuff like, oh, this docker container I want to run on GPU, this docker container I want to run a CPU kind of thing. You can control those things and just hit submit. And this is basically just going to run that docker container and then give you an API endpoint right here, basically. Okay. And we can run it like on demand or it will be always on, right, this docker container. So if it's a service, if you want to use it as a rest API endpoint, you probably expect it to ben always on. If you arun that docker container as a job where it just comes up, runs a training job and then kills itself, or runs a badge of the job and kills itself, you could do it that way as well. Got it. Tell me one thing. What we are trying right now, it's mostly for us, everything is batched. Real time is not. I mean, we can see it in. The future, but you should deploy a job type in that case. You would not do a service type in this demo that I just showed you, right? In the batch environment, we want to run it even if it is like asynchronous let's say I want to sumit it, we need to have a way to send certain information that this is the input file, this is the model location. All of them are in S three, right? But somewhere we will want to send it, right? Say, okay, this is your input file. Use this to run this model, which is there in S Three, and this is the output location. So that is kind of like our use case. But I guess that might fall into your service use case, right? No, it will also fall in the batch in the job use case. Let me actually explain this to you. I think we might even have a demo of what we are doing, but let me just try to pull up that specific examples. But otherwise I think it becomes very, very clear if you just look at what's happening in the code here. Okay? So, for example, like, there's a train pi here, right? Now, in this case, I am reading the data load iris, right, using a function load iris, right? Now, this load Chris function could very well be implemented such that depth load iris read from S three, right? Bucket name, whatever. Like, this is my bucket name, right, and here's my file name. So you could literally read the data from here, right, and then return that thing. Correct. So in this case, you're reading data from S Three in your model training. The next thing that you could do is here's what you got as an inference of your model, right? That is some parameters, some model pipeline dot fit, right? So what you would have is you would have like, inferences is equal to pipeline predict, and then you want to dump it to S Three. So, for example, dump inference, and then you provide S Three path, and then you provide the inference object. So if you do that, basically what your code is doing is reading data from S Three, training a model, running phone conference, and dumping it in S Three, right. That's what your code is doing. And in our platform, what you would do is you would just go ahead and say that, okay, now go ahead and deploy this job. Deploy. So when this job runs, you're automatically reading things from S Three, training the model, and dumping it back in S Three. Does that make sense? Makes sense. Okay. Like vanilla Python code that you're writing. So you have all the flexibility that you need to read data from S Three or snowflake or wherever, and every time the job runs, that entire thing will get repetitive. Got it? Super. I think we're do you know how we can, like, try it out or like do a POC? Yeah, it's actually very, very straightforward to try out and do a POC. Like, for example, if you just wanted to try it out, you could directly we will have to create an account for you. And the way this would work is you basically just get an API key for yourself, okay. And then you install a client library on, like, a pip install, like, a client library that we have SDK that we have, right, you do that. So these are like a couple of libraries that you install and then directly you can start using it with that API key access. So that way it is running things on a Kubernetes cluster that is directly hosted on our cloud. Right. So that's for the trial part of it. Now later if you wanted to have this running on your own cloud right, for a longer term engagement then basically we work together and we kind of like spin up whatever customers that are needed on your cloud and manage it that way as well. So both options are trying it out. The setup is maybe like 15 minutes that it takes you and if we decide to move forward then the entire infrastructure set up takes maybe like two to 3 hours worth of it. Oh, got it. Okay, I think that makes sense. That would be helpful to try it out and see how we can how it works out for us. Generally. One of the things that I do to help people try out the experience of the platform is like you get whatever one model that you want, right for your model training and one for your batch influencing or whatever. Some people have like these real time use cases, CTO get that. So you basically get your own code in a meeting. We sit down for like 30 minutes. Even 30 minutes is actually sufficient to just get it EndToEnd and we actually get your model deployed or get your model like patch and print job running and save the data to s three in that 30 minutes and pretty much on the platform from sign up all the way to finishing that entire thing. So we kind of just do that and you're able to try it out and then we open up the platform for you that you can go and try it out on your own and maybe like in a couple of days if you like it, we move forward with the next sep essentially. So that's typically what we end up doing. Yeah, that definitely will work for us. But probably I'll reach out to Ben to see how do we try it out and see how it fits into. Sure. Is this something that kind of like at least you see that is adding value in your use case? What's your feeling like after you saw the platform around being able to run things from your local machine using the power of the cloud and stuff like that? Yeah, that definitely adds value. The thing is that we can try it out and see where exactly it fits because to be honest, the entire going through the demo it looked like useful. But once we try it out we'll know more how it fits into. We have air flow, so we have airflow, how do we have another airflow like environment but for a mail? How does this thing can work together? That is something we have to figure out and see it looked very interesting, especially with all the line ice charts and stuff that you can create on top of the ML models. Sure. Very helpful to know this. And by the way, one of the other things, harika had mentioned Chris briefly to Ben as well. That is our team generally comes from a machine learning background and we have built out a lot of these ML infrastructures that are simple to get started with and then all the problems that people encounter in the future. That is once you start using things like TF Serve, Todd Serve, what happens with that once you start using CICD, what happens? Once you want to monitor your models, what happens? We know that these problems, even though not relevant right now, eventually will come. Right? What happens when you have different versions of the model for different types of clients? All those things basically happen. So we are kind of accounted for a lot of that in the platform itself. Soviet of course not everything is relevant in the beginning. So we are also happy to sit with you and kind of build out integrations that work the best for your use sales and then help you scale as your use cases mature. That's the mode in which we are anyways working with folks right now. The goal for us, obviously we are a start up. So this is not like we are just doing some goodwill work kind of a thing. It helps us build out our product as well. Just like you said that we have Airflow and I want to see how it works with that. Every other company would have some other part of their stack that they want it to work with. It cannot remain in isolation. So this kind of working helps us build out a product with that level of maturity at the end. Customer you actually kind of get to set up your infrastructure and everything with whatever learnings that we have had so far working with other customers as well. So I quick chad with becomes like a win win for both people in that case. Makes sense. Yeah. We're also a start up. Start up with a start up essentially. Yes. Well, we help each other out if this looks good, but yes. Soviet please do talk to Ben as well. If you want to do like a quick POC or something, very happy to set that up together. Right? Sure I'll absolutely awesome. Thanks Shane. A good day. You too. Bye.