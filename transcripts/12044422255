Hello. Hey, how are you? I doubt actually. That'S what he also. Said in the message. Let's hear him out. Let's get to a second meeting where he includes someone high level introduction type. Okay. Hi, good morning. Hey. Hi. Hello everyone. Sorry, I'm not on camera, I'm at home. I'm not in a position to share my video at the moment. That is fine. It's fine like you are okay for the time of the call now we understand it's a Sunday, so we don't want to be bothering you if you are in middle of something. I just want to check that first. At home, I was just doing some work, but I'm kind of available right now. Okay, perfect. Awesome. So basically, first of all, thanks a lot for taking time for the call. What we wanted to do on this call was learn a little bit about the overall machine learning pipeline at Novartis and especially with focus on how the multi cloud comes into play and what are some of the challenges you see there before diving into that. It will be great if we can introduce ourselves. I can give up the background, have seen your profile, but would love to understand how you have moved across companies and the kind of challenges you have seen there. Does that sound good? Sounds good. Okay, cool. I can get started. So basically I'm graduate from IAT Karakpur 2013, batch did my bachelor's in electrical engineering from there and post that, worked with a hedge fund called Worldcoint, where I was working on building trading strategies across different global markets. So we used to kind of use a lot of different data sets and build algorithms for trading. Primarily focused on equities earlier. And then I spent like three and a half years in India, then moved to US, where I was kind of handling two roles. One was as a member in the CEO office looking after growth initiatives, and then also as a portfolio manager, where I was managing 600 million in assets for worldwide, including this, analytics and so on. During that time, also invested into startups a little bit and then wanted to build something myself. So I left my job, built our first startup in the talent space called Entire, which we sold in 2021 September. And then we started building True Foundry with the aim of making machine learning more accessible to companies and trying to provide a pipeline in which you can easily test and deploy models to production in a reliable and secure way. So that's like a brief about my journey. And I'll share a little bit about Vishek and Nikkwench, who are my co founders. So both of them like same background as myself. We went to ID together and then post that they were more on the engineering side. So Vishek was on software engineering side. He worked with Facebook for six years. Let the videos team there. Let the casino redesign team there. So brings a lot of experience on the distributed system side and leads tech for us at Truefoundry and Nickel was on the machine learning side. So he initially went to UCB after IIT correct, then worked with a company called Reflection where he was leading like recommendation systems like Effort for them in terms of providing the entire platform for companies like Ebay, et cetera. So build a system like 600 million users, scale there and then move to Facebook where he led the AI team for Portal, which is a voice system device, which is a voice system device that is the competitor to Alexa et cetera. So that's like basically the background. And one of the things I wanted to share is the reason we started to Foundry was in our first startup which was in the talent space. We were building models for matching candidates to jobs. And of the model Spot was actually pretty okay. But when we had to kind of deploy it in a reliable way into our website and integrate it, it took us almost like more than a month to kind of understand the cloud systems and then piece together open source. And then we were discussing about the FBI Learner system which is Facebook's internal ML platform. So like they were telling like it took like 15 minutes and the infrastructure was really awesome. So that's where that wide that does not exist outside. And that is pretty much the discussion point where we started to kind of think about Foundry. Got it. Just one more quick thing. Weaken like Chin, gmail leads the product side for us as a part of founder's office and Vivek is also part of the founder's office. Okay, cool. Really nice to meet you and get to know you tonight with it. Likewise. Also in a very impressive profile, must say your backgrounds that you described. A few words about myself. I graduated from Iitras MSU Electrical Engineering. I had focused on communications, signal processing. Then in 2006, then I moved to Zurich, did my masters at ETH and joined IBM Research Debt as an intern and then stayed around for the next about eleven years or so. And while I was there, I was doing my PhD at EPFL. My focus at that time was basically IBM had basically asked me to work on the problem of reliability. Fundamentally, very large data centers have failures, hardware and so on, but you don't want to lose data. So, given that I was working on information theory in my masters wanted to understand if there were ways to have some sort of coding over data to protect data in large systems and prevent data loss while at the same time reducing the overhead in terms of course, you could make ten copies of everything and distribute them around the world, but it's going to be expensive, so you need to have some trade up there. That's what I started off with. So I went into more modeling of failures, and rather that was my PhD topic. Then moved more to performance modeling of systems. This was again, the storage area. Idea was you have different types of storage devices, hardest drives, solid state drives, newer media tapes, and so on. Each have different characteristics in terms of performance and cost as well. So for large scale systems, if you can build the system in a way that you are always having the so called hot data on the fast devices and have them in there, maybe even predict when what is going to be accessed, so you move them in advance from tape, for example, then you could build very large systems at much lower cost. Again, here was more of a problem of performance versus cost earlier. It was reliability versus cost. Then we soon realized that my colleague and I were having coffee one afternoon. We were like, no matter how much you optimize for cost, for performance liability and all that, fundamentally what's happening is data is increasingly growing at the same exponential rate, but technology is starting to saturate, also is going exponentially, but starting to saturate. And so there's going to be a problem possibly middle of this decade or end of 2030, when organizations will struggle to actually balance the cost into data or data science. That's basically the promise of data, that there's some business value to it and the cost of actually the expenses. I mean, on the technology side, on maybe even the people side and so on in these systems. That's when it struck us that basically the problem is not maybe to optimize these systems, but more to optimize the data itself. Meaning not all data is equally relevant or equally valuable. So if we can, as data is collected, understand what data is relevant, what is not, you'll be able to build better systems, also know what data is actually adding value. So around this concept is what we were working on in IBM for a few years. The challenge was personally, internally within IBM, it was hard to convince the executives to make a product out of it because they were competing on the cloud market with I'm talking about 2015 or so. They were competing with AWS and Azure and so on for market share. And they were trying to grow by acquisition IBM, and they were not doing so well. I don't think they have any market share at this moment. I'm not sure. Yeah, it's almost like negligible. Yeah, maybe something on the GPU side, etc. Like, there's some things that I use, but from a cloud perspective almost it's very negligible from what I understand as well. Yeah. So the executives were basically willing to listen to ideas that would help them grow their market share. And I was trying to pitch them something that would help customers reduce their cloud spend by understanding what data is actually important. And so this was not fitting in with the business side of things anyway. At that time I felt I was too much in the technical side, wanted to get a little bit of a business view of things. So they've taken an opportunity in the automotive industry. Then I moved to Germany since 2018, where I was reporting to an executive board member of a 45 billion euro automotive company called Continental. And so it was more following this guy around, understanding the business and challenges and so forth. But what is also interesting was I got to interact with across this company is about 200,000 employees worldwide. Across the companies I could get a view of where AI or ML is being used primarily being used in the, in the advanced driver assistance systems or what is called ADAS market, all these radars camera systems and so on. But they also were trying to use AI in other places, also within functions like you said, Hitchharp, for example, talent match kind of thing in production and so forth. But it was happening in different pockets of the company, okay? And so everybody had built their own kind of everything was happening organically over time. So people realize, oh, I can do something with ML and with my data, so let me get hired a couple of data scientists, get something going. So you start off with issues, their environment and what they want to do. There's no centralized approach to this whole thing. And so what I was then doing for the next couple of years was sitting with all these people across the company and trying to understand what this big picture for AI look like for the company, what are the main strategic gaps, what do we want to address, what are the challenges and so on. The main learning for me was in the automotive industry again, coming back to I don't know how applicable again, by the way, just stop me if I'm talking about something irrelevant to you, but I'm not sure where I can help you and where I cannot. But the challenge there in deploying ML models, if that's the core problem that you're addressing. And for them, as I said, the core ML model or code set of ML models are important for their camera systems and so forth, okay? And the industry there's a certain way the traditional industry looks at it like the series and automotive industry and as a new disruptors in a space like Tesla, do it differently. So the important part is this safety side of things. So at the end you need to convince regulators that this model, if applied, let's say, on a billion miles of real world data. Thus at least as good as a human does. And you can only show this statistically. Got it? The traditional industry basically Continental is what's called a tier one supplier, automotive supplier. So they make all the parts that then the car manufacturers buy and then put together in a car, like brake systems, instrumentation, cluster tires, radar system, camera systems and so forth. So we know just one thing here in the Continental, the most of the use of AI, it was mostly in the edge devices like at the end card. So it would be using different sort of hardware, right? Like compared to generally what we are seeing in the cloud systems and the challenges I'm guessing, right? Yeah, cloud comes in more on the development side, so not on the deployment side. So deployment as you say, is specialized hardware. And hardware was an important aspect as well because you needed to have very energy efficient hardware. So they were looking into GPUs, were consuming a lot of energy. So I want to put like a supercomputer in the trunk. You want to be able to, I mean ultimately you want to do matrix vector multiplications very fast and very energy efficiency. So there are a bunch of startups that are looking at it, placing the memory and other elements in a way that maximizes the energy efficiency and so on. Yes, but on the attack end, all the training and capture and everything had to happen on their data center or cloud. Okay, so also all the testing and so forth. But the main difference between the traditional and the industry and Tesla of the world was that Tesla was kind of very vertically integrated. So they could kind of make a model, deploy it on their car over the air and then collect information that is relevant to improve this model, send it back to their servers, and then quarterly update this model. And the traditional automotive industry had like many players in this chain, continental had to get this data from Volkswagen, from Toyota or something. And Toyota has to get this data from the customer over the air updates were not the norm in automotive, so everything was decided in advance. So traditionally if you had to buy a brick system, engineers and this inter teams would sit together and kind of list on all the requirements, 50, 60,000 requirements, room for a break system, and then they agree on the R and D cost. So it takes two to three years to develop tests, build the product and kind of productionize it, that is kind of make the whole assembly line, optimize it to make millions of parts at very low cost. And then they agree on a piece of piece price. So they say a radar is going to cost â‚¬40 a piece and there's some expectation that so many million pieces will be stoled. So this is how the traditional automotive industry uses this does not fit in with this whole machine learning thing where you cannot list all the requirements in the beginning and you have a constantly evolving system and that needs to be updated over the air and so forth. So that was a major challenge there in deploying ML models. But again, it's not on the cloud but on the automotive hardware. Another problem was for these people. They were trying to build large data centers. So they thought because they are collecting all this data to develop this product, they believe all this has a lot of value. So let me build my own data center. So these guys were starting off with, I don't know, 1020 terabytes, but soon it was growing to hundreds of terabytes. But this was not manageable. I mean, they were not experts like Amazon's, AWS and so on to manage the costs. So that was another major problem the automotive industry was facing the scaling side of things. So they had a huge scale problem. Yeah, understood. Basically I'll just maybe also share a little bit about the direction in which we are kind of thinking and it will be then good to hear. Also from the perspective of nobody's, like for truefoundry, actually, we are trying to do the machine learning deployment infrastructure on cloud. So right now the actual focus is not on the edge devices. So I think edge devices, but there are a number of startups definitely doing that. But currently we are not trying to focus on that because that needs a different skill set in terms of it will need more like optimization of the ML algorithms so that the models are short and then there's things around the hardware. So that part we are not focusing on. It's more about how do you kind of run a seamless training infrastructure and then a deployment infrastructure over cloud. So I think the context of Novartis will be really useful. And we also had like a few questions we wanted to get through there. So I would love to kind of learn a little bit about the Novartis side of things as well. Sure. So novartis. Sorry to interrupt. I just wanted to check if we. Can record this session because it will be helpful for our team to go. Through it at a later stage. Yeah, thank you. Thank you for the permission. Sure, Nick. So I've been novartis since last January. I kind of understand a bit of their portfolios I was working with. So I was reporting the chief AI and Data Officer, but he has now, since June left the company. There's a lot of change going on in the company. The struggle here, again, has not been so much on the ML model development and deployment. So to say, it has more been about actually having an impact at scale. So in terms of the business problem you're describing, my understanding is that they don't have a pressing problem of, you know, I think it's fine if they can productionize it in two months or three months. But this is not the issue here. The issue is actually even getting to the production productionizing stage. So a lot of these projects so 2018 wasan Nasamin took over as CEO and he's been promising his shareholders, we're going to go big on data and digital. Okay. As part of that, he set up a chief CEO office and the central team was formed. There was already some AI ML that was happening in different parts of Novartis, like the research site and certain other areas. But this was supposed to be the group that was going to come up with new interesting stuff that nobody has thought of in Novartis and so on. But the problem that there's such a big reorganization happening since last year is that over the last three years since this organization was set up, they have not been able to show a very good return on investment in the sense that they have been showing proofs of concepts and showing oh, hey, look, I can apply ML models to come up with new, interesting molecules. That reduces the amount of effort that is needed to test molecules and so forth. Because this model understands or tries to predict different characteristics of the molecule, people usually look for things like solubility, stability of the molecule and so on. So that is one application. Another major application. Yeah. So just want to ask, you know, are the applications of data science currently in what is mostly around the research part? Or like, I understand you would also have like a supply chain and manufacturing. Yeah, so AI is across I'm just talking about this example was just about research site. Another major part for us is legal writing. Okay? So the whole big machinery of drug development is around getting all the paperwork and everything ready for all the different stages of the trials, animal testing, all the way from animal testing to final drug approval. And here we see a lot of opportunity to automate things because right now we're using a huge army of people to actually get this right things. Again, here the problem has been more in, let's say, not the model development, but actually applying this model in the different areas and countries and contexts. And so the problem is more with the data management side of things. Just trying to understand vino so you said that it's used across, et cetera. So usually what we've seen is that the data housing requirements and problem in this domain, you said that models are not being able to be deployed. Is it because there is a scarcity of data or like the models are not being able to like, prove their value in production, not being able to retail? Just trying to understand what's kind of the reason for that. So let me give you an example. One thing that was tried was they did something called a buying engine that was essentially supposed to be like an internal Amazon marketplace where we want some test tubes and other lab equipment. For example, we can search and we can find the cheapest or the best suppliers and so forth. So the simple application and somehow, some ML, I don't know, was used to kind of find similar items and make some recommendations and so on. That was the application. But the prototype never actually went into production because there were so many minute details that were important like the quantity or the concentration of some product like chemicals, some impurities and lot of different information that needs to be there that is not actually managed very well with the related metadata and all that that could actually be leveraged by this model. So this model is coming up with something that's cheap and so on, but that's probably not what people are looking for. They have a reason not to want it. Yeah. So you're saying data maturity itself and. The quality of data is exactly. I'm just trying to understand. So I think this is a specific scenario, but for some of the use cases that we've seen, like across, like yield increase or cost of poor quality reduction in manufacturing or like optimizing a supply chain, some of the relatively commoditized kind of models, if I might say, are those also not being able to prove value? Because is that also like a data issue or what? Do you see the reason for that? I think it is successful where there is good data maturity. So if data is being managed well and the data quality is good, then I think things are running smoothly. But this is again, to my understanding, since I've not into too many details on all of these different parts, it's happening in pockets, in certain pockets it's effective because the underlying data management is clean and you have good data quality and you can trust and trust it and so on. The problem is when you try to scale it, so you want this across all countries or all your regions and so on. And that is why right now there's a lot of focus on data management in the company. As a company we want to improve our overall data maturity. So like establishing data governance, having data quality processes defined, data owners defined, and so on. So a lot of this process aspect governance aspects that seem to become the foundation seems to be missing and that needs to be strengthened quite a bit. Got it. So a few things that I wanted to ask to understand the overall MLC for starting with the data pipeline, is there a tool that you use internally for the data management, like a snowflake, data bricks or anything? Sure. Okay. I would love to understand how is that structured and then would love to go into the ML side as well. So we have multiple so we use data bricks, we use glassbox. That's one of the things that we are trying to kind of centralize as part of the data management as much as possible. There are also legacy things like for example, drug development has used its own some sort of homemade solution because they have been doing this for many, many years. Okay. And we want to move some of those things to these newer platforms, but that's going to be a major undertaking in itself. The diversity is very high, and that is part of the challenge as well. I see. Is this primarily across data or even for the AIML? What is the overall structure? Is it like from what I understand, it's not a central team, but there are different business pods that are doing their AIML in different ways. I'm understanding there is no central infrastructure as such. Right? Right. There has been an attempt to go towards a central infrastructure by creating a marketplace where all data is cataloged and people can search for a data sense, can search for interesting data sets, and then work on them on AWS, centralized infrastructure and so on. But what we have so far seen is that the adoption of the central system has been quite low. So as part of the Central Enterprise Data Management Team, we're trying to drive reuse of the central infrastructure so that we are a little bit more consistent in how we are managing and using this data. Because a lot of this data has regulatory requirements, contractual obligations and so on. So there needs to be very careful thought on all this compliance related aspects as well. Okay, that needs to be thought about. And is the data housed centrally in AWS or you use clouds like DCP, AWS, everything. Finally, we're using AWS and Azure, the central thing mostly I think it's maybe 60 40 or 70 30 split. And any reason why you're using requirement? That's a good question. I actually don't know what the thought process behind that was. I think there was some partnership with Microsoft that has been pursued by Novartis. And I think part of the agreement was to also use their infrastructure. So that's probably the reason why Azure was also used most. As I said, maybe 60, 70% of our infrastructure is on AWS. The central infrastructure. Again, to be clear, there are like a bunch of other infrastructures that are owned by those different business divisions, like research, drug development and so on. Okay, understood. Another question we know is around like the overall machine learning pipelines are starting on the research side, it will be good to get an understanding of how many folks are actually doing data science work. And then for the research part, like how is the current infrastructure in terms of where do they run their models, like to run their training jobs, et cetera? Do you use some sort of a tool there? And then up to what level do data scientists go and from there, who picks it up? So if we can understand that type part a little bit, that will help. Sure. So in terms of team size and so on, centrally, under the chief AI data Officer, there were about 900 people in the central team. I don't have a very good idea of how many data scientists are there in each of those other divisions that you must never actually compile. But I'm thinking research has a good number of data scientists, and I would say probably, if you see it, could be in a few hundred. If you had to count across all the areas of the business, the size of the user base, what is your next question? The other question was for the research infrastructure. How is that managed? Like when these data scientists are running their models, like training jobs, etc. Who kind of kind of allocates them the research clusters and so on to do this? Do you have a view into that? Yeah, unfortunately I don't have a view into that. Okay. Novartis Biomedical research is our research division. It's a $9 billion organization, $9 million a year organization on its own. And it has own it. So we have a central It, but the research charms has a separate It kind of infrastructure. So I'm not into too much details there. Okay, pay for your team, like your central team, like there whatever research is happening even for this entire marketplace. Do you have a view into how the data scientists there are kind of building out models, et cetera? Yeah, I mean, for example, I can shed some light on some projects that I've been involved with or have some deeper insight into. One of the things we were working on was on customer data. The idea is basically that with all this COVID and so on, our representatives, field representatives, are not able to have enough opportunities to meet in person. So they are having to send emails or do better analysis of the doctors preferences to match their kind of salesforce effectiveness to the needs or preferences of the doctors. So this project, we call it Next Generation Engagement, parts of it was done by the central organization, what was called a decision engine. So it will decide basically for each representative among the doctors that representative see what would be the ideal course of action and what can be done when, and so on. Should you send an email and should it be about this and your conversation? Should it be more about giving evidence, or should it be more about talking about other doctors in their network who have had some good experience with other people sometimes convinced about literature or by their network and so on. So it gives some advice on what the best way to present the content. And here we were using glass box primarily, so the data is more about the doctors themselves. So we have a lot of information about doctors, like which university they went to, where do they live, what is their affluence, do they prefer sports or not? All sorts of things about doctors, okay. That the sales people have filled out based on their visits. And all of this information is then kind of then just modeled using some kind of a data pipeline, you. Just say just drag drop stuff to build the data pipeline, what you want, what data sets you want to combine and for a particular drug related information you want to bring it into this mix and combine it in different ways form this whole data pipeline. Okay, so most of it is done pretty much say the GUI and little bit of coding here and there. Okay, and do you know which tool we use here? Is it like an internal tool or some sort like data robot or anything like that? We are using like an AutoML tool here. I think glassbox itself has some glass box itself has some AutoML basic stuff like clustering it has some I mean I've not used anything very special. Okay, so basically glass box itself and. Then. Is this glass box like hosted within your infrastructure or like you send data out and then glassbox processes attention the output back. Not aware of that. Okay, got it. And we don't like one question around when this kind of data is logged on and finally there's some sort of models, does this models get handed over to some sort of infrastructure and it team to kind of host and deploy it and maintain it at scale. That should be the ideal situation, right? So that is where we are struggling. As I said, this team, the central team over the last three or three years they've been building all these proof of concepts so they have never actually been able to productionize something. And the challenge has been on the business side, adoption of these. For example, in this case, I mean I described to you the problem with that manufacturing buying engine, right? The problem with the labels and the specifics of the product and so on. And in this case there is some resistance from the sales people to adopt this kind of a tool because I see they want to see first actually helping anything. So we need to show that doing this and have like AB testing or something to show when we have the store and when we don't have the stool. What is the actual component, what is the difference? Okay. Why is showing this difficult today? Because you already have the data. So if there's past data with this tool, if something is being forecasted then using the past data you should be able to see the actual impact and then showcase it to the sales people past data. You can't do it. Right? So you have to have this recommendation tool deployed in some areas and then not deployed or continue with existing other areas and then run this over three to six months which was done I think sometime last year. And then show the results. Yes. Like the AB testing piece. That is right. That was done. I think it was not very convincing enough. I mean it was not a large enough scale and the difference that was shown was like a few percent or less than one or 2% or something like that. So it was like okay, I mean, is it even within like a margin of error? Of course they say this percentage is statistically significant and all that for this data. But again, the business side doesn't fully see the promise yet. So the challenges, again, as I said before, it is not more time to productionize. It's more about are we solving the right business problem and is this model actually able to add value at the end? And that is where the real challenges and most of it has to do with the data side of things. Either people don't trust the data or it's in too many different systems or there are too many constraints because of geographical boundaries, all that stuff, and different regulations in different places. But data management of site is what is the main problem here. Understood. Okay, we know one other question that I wanted to understand. So when you have like and one comment on this point, what we have seen is in a lot of these bigger organizations, the time to build out these models, business use case and then getting it to a production, it's still a huge time gap in terms of two to three months. And that basically slows down the speed of new model testing, right. In companies like at Facebook, the new model testing was so easy that today you deploy, you are able to EBITDA and within a few days you do new model. So do you think that one of the reasons why it's very difficult to see business impact is because the delta in terms of doing that model building to actual focusing impact even over EBS, it is like a huge time lag and is it limited in any case, like infrastructure challenges? Just wanting to understand that a little bit more. Yeah, not sure. Right. I think what you're asking is do we have problems that are relevant to what you are trying to solve, what your company is trying to solve, right? Yes. I don't see it yet. Okay, so our problems are completely different. It's not about the time it takes to productionize something. Okay. And the fundamental difference is Facebook and companies are basically, I mean, their whole business is on the internet. Everything is digital for them. Automotive, healthcare, these things have a lot of real physical things to be done. Research molecules. If something predicts you should test this molecule, somebody has to go and do all that stuff and physically test it out whether it makes sense or not. Like if it recommends you to do something, do sales pitch in a certain way, the sales representative actually has to execute it and then do it the same thing with the manufacturing or whatever. Right. Everything has a physical impact and that product is actually challenging because that is where you actually get the feedback for the model on the physical system is where you get the feedback. And that means that there is a more effort in actually getting this feedback and actually showing that there is value in doing it and then improving and iterating it over time to improve the performance takes more time. So that has been my observation being present in automotive, having been an IBM and then moving to more traditional industries like automotive and healthcare. That is where I think the main difference is. Okay. Got it. And other thing in terms of the implementation of this AI, like from a business perspective, what are the core KPIs ultimately that matter and also what are the constraints like say cost, et cetera. We'd love to understand, when you all discuss about this, how do you think of the AI implementation within, say, when a division within our system? Good question. I mean, I've been involved in strategy and was not fully clearly defined so broadly the strategy was okay, let us on one hand reduce the cost for data to access data, time to get access and so on, simplify the process, reduce the cost to insights. So this is more related to what you are saying, like have means to quickly test models and show value and then reducing the cost to decision making. So if you're making the decision based on ultimately these insights and all the data behind them and there's sort of a certain process right now and there's certain accuracy and so on related to those decision making, how are you bringing down the cost to do that and the effectiveness of doing that? Okay, so these were the higher levels, let's say strategy statement for what the former chief AI officer was calling a two pillar strategy. One pillar teaching cost. Another one was more increasing value of newer AI products. And it was formerly a little bit hazy what he was trying to go for there. Like for example, he was talking about more having healthcare specific inventions within AI and so on, molecular stuff and then try to commercialize that the two pillars were one was reducing cost, the other was increasing value. Got it. And the increasing value part was a little bit more fuzzier and reducing cost part, although it was defined at the strategy level, was not actually defined in terms of a practical KPI that is defined and then it's tracked and people are actually held accountable for it and there's a governance around it all that wasn't there. Got it. So the three years that this organization has been running and they have been more like a startup, they've been like they have the visibility of the executive committee and they kind of were doing their own thing, trying to show POCs and stuff. But it has not transitioned into a business as usual kind of model where you would have very factable KPIs and so on. So one of the things that we are doing right now as part of enterprise. Data management is to define these kind of metrics and then have them aligned across the enterprise because a small team in the center will not have the capability to have an effect. So we want to have a strong data governance set up across enterprise. So we have representatives from research, from procurement, HR, from finance customer and so on, and sit together and discuss and align on central topics like, for example, infrastructure or reuse of assets or even trainings and so on, company with trainings on data compliance and other things. So a lot of aspects we are trying to centralize the governance aspect so that there's more consistency and also improve the data quality on all the business relevant. So if there are certain things that are business critical, then we want to make sure that the quality of the data underneath it is satisfying all the requirements for that business use case. And we want to have a very practical way of measuring that effectiveness and how it's created creating value. So we are actually now defining values for data management. Okay, got it. But eventually it should also calculate to the AI stuff. And just for reference, is there a team which productionizes this model at this time because you use multiple clouds. Is there a team which is orchestrating these clouds and organizing the infrastructure team? Does that exist for a single business? Yeah, I mean, there's a central team that manages this marketplace that was telling you about 70% AWS and 30% Azure. They do most of this data engineering, ML ops kind of work. But as I said, it is currently there's not a lot of ML things that are happening. Maybe more of the data engineering side of things is happening, more of cataloging data and creating this marketplace is happening. But adoption is again less that is a challenge. Okay. People ask why you have everything in centrally. I mean, headshot will say, oh, this is all sensitive data, why should I make this available for something? They would even not want to have it searchable in the sense that they don't want people to come and ask for request, request access for data because then they would have to kind of go through them and accept or not costing overhead. So we need to be very clear about this. Strategy of this marketplace itself needs to be revisited. What are we trying to do? Are we trying to get all the data? Incentives are not going to be feasible anyway. This marketplace team is the one who handles the infosec compliances and all right. For this marketplace use case, they will handle the infrastructure. Yeah, okay, understood. Cool. I think this is super helpful. We also exceeded the time. So I just want to be mindful, but I also wanted to share a little bit in terms of where we are in the journey. So basically right now, earlier we're working with a few startups, but what we realized is the need for the scalability, the liability, the kind of a system that we are building is more at the enterprise level. So we started to kind of look at enterprises and there are a few companies we are working with actually one healthcare company as well that we are working with which is again 100 billion dollar plus company and then two other companies which are one of them multicultural rate and one of them on a chip design manufacturing site. So we are working with the business team and the corresponding lead of the platform team which is basically the team that kind of ensures that the platform for the developers and everything is available to ensure that ML runs seamlessly to kind of then make it faster for them to run these experiments. And not only running these experiments but as you said, also on the business side, it has to have value. So we had like a monitoring layer where you are able to track the value of these experiments and all so around that. Wanted to potentially have another call with you, Erin. Would love to kind of show you the value prop of the project and would love to understand who what this could be. A few people you think we could talk to to understand the side infrastructure and the deployment side infrastructure currently. And I totally understand that the time from a discussion perspective might not even be right, but just to kind of even get a view as to what is happening internally, how is the current infrastructure because we saw that everything is multicloud and there are a few things that you all are doing internally. Would love to get a picture of that and then if there's a possibility you'd love to kind of take your time to at least identify those people who we could potentially speak to, to understand the platform side perspective as well. That will be really nice. And also maybe view the product briefly and hear us if you have any thoughts on where this could potentially add value or how we could think about or if we have to kind of discuss about 15 companies where it could add value. Make sure. Okay, so what I would suggest is let's have another 30 minutes or something sometime during the week around the same time, and then I listen to some of what you just said a little bit more in detail. And then if you share some material me after this call, I will kind of send it back. Because I said to this reorganization, the central AI team doesn't exist. Infrastructure team. Yes, as I said, I'm part of this data management team. So I'll try to see who might be the right person because we're still in a sort of luck. Not all the oaks have been defined and lots of responsibilities and changing names. Changing, yeah. So I'll get back to you with some meaningful follow up after that. Okay, that sounds good. So we'll follow up with you to set up one time, and then we can have this discussion there. Does it sound good? It's sounds great on the door. Okay. I really appreciate thanks a lot for your time. Yeah. It's really helpful to understand. Thank you so much. Happy to chat. Thank you. Have a nice day. You, too. Bye. Have a great Sunday. Bye bye. Thanks. Bye.