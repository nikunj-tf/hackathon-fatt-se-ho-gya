Bye. Hi Bishop. Hello. Actually I didn't realize in my that I added you to the call. I think I added you by default. I didn't think we copied it. I haven't attended any upper calls yet. We have a few questions that we need to answer. There's a bunch of levers, technical standpoint questions, timeline. We'll send it out to them and accordingly we'll just prep like a quick thing from a pricing perspective as well. If you are also free after this call free, whatever. If you don't need to absolutely sleep after this, maybe we can just do it right now. If you want to sleep, that's fine. We can do it tomorrow morning. Whatever works. By the way, they are getting a demo from the other ones right now for 30 minutes, which will basically be indicated because I think core strength is app building based on voloka. When we stream it, we prefer we didn't go by the UI route because we think streamlit is more flexible, more easier for data centers. That is our stance. New York UI validator, they give a drag and drop building. We can just say that STREAMLET is way more flexible and easier. Hello. Oh, you're right. Link. Hello. Hello. Hi, I'm Nicole. How do you pronounce you name? Hi, I'm Saurav. Nice to meet you. Yeah, nice to meet you. Cloud. How are you doing? I'm doing fine. Sorry for running a little bit late. Hello, nice to meet you. Nice to meet you. Yeah, they're just waiting for Ashwin and Asif to join in. Okay. Yeah, asif swami yesterday, so I don't have a lot of contacts. Hey Ashram. Sorry for keeping you all waited and I think you haven't met the rest of the people before. Yes. I think I had called like we are doing a demo with two platforms on a very similar use case. So we got delayed from that one. I think I mentioned to you we added Law. So between Law, Ashwin and me, we are the three directors within them. So most of the customers for the product that you're pitching are here. So it's a great opportunity to do the demo and kind of learn about the pain points. Yeah. Cloud, feel free to give your introduction. Ashwin and I, we had already met. I don't know if you've already given. Introduction, but feel free to sure, yeah. Like Ashen mentioned, IBM mostly working on the science part. My team is like machine learning engineers, build models and then also India deployed the model to production to serve search use cases. Definitely will CTO be here and then understand the capability to help our developers life for development and also monitoring. Amazing. Awesome. Looking forward to it. Hopefully the demo that we are giving seems like it's adding value to the workflow for you and asif I think we are starting a few minutes later. So maybe we should just get started with the demo and then we keep some time in the end to answer any questions that you all might have right. And actually through the demo. Also, if you have any questions, please feel free to interrupt and ask instead of keeping actually the questions for the end, that might be better. Okay, awesome. So actually 1 second, is it okay record we can share with other people also? Yeah. Happy CTO. Happy to do the recording. Record and then share it later. Perfect. Awesome. So should we get started? Cool. So just a quick thing here a workflow of the demo. Basically today, like, the way I was thinking of gong the demo is like we will first show about the infrastructure element of it, which is how do you set up Truefoundry You? Like, what does getting started with Truefoundry You look like, okay, that's number one. Number two, I'm going to show the developer experience in the context of it. I know that the team is trying to use Sage Maker, so I will show the developer experience on top of Sage Maker notebooks on how you can deploy different things like different API, endpoints directly being able to deploy like a model pickle file or some of that element or even arbitrary functions deployment. Right. And post that, IBM going to show that how do you manage all your deployments applications, et cetera, from our platform, basically the platform UI, and I'll end it with showing use cases on model monitoring, data drift, side of things about how this entire thing gets covered. Okay. If you all have any questions, please feel free to ask in the middle. Jumping in right away. So you can now see my screen. Yeah, we see. Okay, awesome. So basically the beginning point of any deployment with Truefoundry You starts from the cluster setup. And in this case, it is a Kubernetes cluster that we are setting up. You can actually connect any of your existing clusters directly. So it's very straightforward. Like whatever cloud provider that you're working with, you provide that you can do your cluster level, like the role based access control, so you can add people as your admin member and viewer accounts in your cluster. We also allow you to set up your ingress URLs and everything in your cluster. If you already have some monitoring set up, like for example, if you have Profana. Loki Prometheus, you can attach that here. If you wanted to do something like a new director or datadog or something that you wanted to connect, we will support that too. Besides that, in a cluster, we allow for setting up different type of machines that this cluster is composed of. So for example, for some ML applications, maybe you want to have GPU based machines. And some other applications you may have CPU based machines. Even in CPU, different processors might have different performance. So we allow you to kind of let the cluster be composed of whatever machines that you have selected here. And then we are also working on the cost, just being able to configure the cost side of things as well and be able to visualize that. So that's how you kind of go ahead and add a cluster. Once you add a cluster, adding a cluster itself does not do anything. It just shows you as, okay, this cluster is added here. But to actually connect your Kubernetes cluster with truefoundry health IQ, you basically go ahead and do you basically just go ahead and add these commands on your run these commands on your cluster. So you basically install some of the helm charts and then the cluster basically becomes connected. And that's when you can start managing the cluster from the UI itself. Okay? As soon as the cluster is connected, you start seeing that all the namespaces that you have on Kubernetes basically start getting available here. You can actually create newer namespaces as well. Namespace, by the way, we call like the Kubernetes namespace equivalent. We call them workspace because we built out some other functionalities on top of namespace. But the idea being that think of a namespace or a workspace here as like a logical separation in how you want to distribute your cluster between your teams, right? So it could be like you want to have different workspaces for your dev staging abroad, or you may have different workspaces for different teams. So maybe Asham's team and Laws team are using different workspaces for running their experiments, essentially, right? And again, if you have dev namespaces, you can vivek, admin or editor access to everyone. If you have prod namespace, you can give access to different developers, right? One of the things that we do at the namespace level is we allow you to configure different types of size within the namespace so that developers are like, by design, they cannot end up spending a lot of money by overrunning a certain job or something, right? So in depth, maybe you want to constrain the size of the namespace itself, right? So that's one of the things that you can do. And again, you are allowed to add different types of machines at the workspace level as well. I'll take a pause here to see if there are any questions on the cluster connection workspaces so far. I think this part would be mostly from Ashwin's team. Ashram, feel free to ask in case you need any questions. So this workspace, for example, if there are different requirements. For example, currently what happens today is the Data Science team is launching a cluster similar to the workspace that you're and then they would be shortlived where they would run the training or any kind of processing that they need to do and shut down the cluster so it's at the time of use. So is that similar to this, what you're mentioning? Or this would be in this case, a long running where people kind of launch different jobs or is it like a shared cluster. It can be a shared cluster actually. So that's the advantage that we have that you don't need to like that advantage that we get from using Kubernetes. You don't really have to spin up clusters every time you can have a cluster. Let's say there is nobody using it. Like you can have just one machine, the cluster is up and running and then let's say one team starts using it. So the jobs will spin up, machines will be added to the cluster and when the jobs are running, at that point you are paying for those machines. But the moment the jobs shut down, the cluster will automatically scale back CTO the minimum size. So you don't even have to go and delete the cluster manually or anything like that. It will automatically auto scale. And here all we are doing is specifying any time that you have to call some service from these clusters, what should be the machine, but if you're not doing it, then those machines won't be spun up, right? Yes, exactly. In case of jobs, it's true. So jobs are like basically you want to run something and they shut down after it's done. But services like service, at least the way we call services, services are supposed to keep running. That's an API end point services you will have to but then the job will shut down on you. Yes, job will shut down on their own. Yeah, and a couple of other things we actually support. And by the right now I'm showing you an admin account so that's why you saw so many workspaces and everything. And then later I'm going to also show you how like a developer experience looks like where they see a lot less things. Shane, what you're seeing on this admin dashboard, but basically we also integrate with most of the common docker registries as well. So, like, Docker, Hub, ECR, et cetera. And similarly for the setup, you can actually go ahead and connect your GitHub repositories, bitbucket repositories, et cetera. That will allow you to directly deploy from these repositories as well. And I'm going to show that workflow as well. Besides that, we have done a lot of work around secrets management. That is they're going to be connect with a bunch of different parameters, store, secrets manager, hashicop, et cetera. Some of these are basically getting developed now and some of these are already built out and we can talk a little bit more in detail if that's interesting. And same thing across cloud platform like being able to connect Kubernetes clusters running on whatever cloud all in one place, et cetera. That's the other thing that we're building out here. So that's primarily what you need. And of course not all of these are essential. You can set it up and you get some additional functionality, but you may also decide to not set up certain things and that works too, but that's pretty much what is needed from the setup perspective. And of course, the India team can actually control a lot of things around what size of cluster, what size of machines, what environment, et cetera is available for people to play around with, basically. Right. So that's the India team set up and after that I'm going to get to the developer experience. Okay. Yeah. Okay, so this is kind of like the part sorry, go ahead. No questions. You can go ahead. Okay, so out of the four parts that I described, the intra setup, the developer experience, like, now we have done the part one. And now we are getting to the second part where what is it that the developers basically get to see? So now I'm kind of logging into a developer account and as you can see that this particular developer account is only allocated one workspace. So that's what they're seeing. So anything that they are doing will get deployed in this particular workspace. Right now, the most important tab in the entire platform that the developers care about is this one where they are able to manage all their applications. So these applications could be services, as we describe it's, like a longrunning service API endpoint or a model demo that they might have built out. The second thing could be jobs. That is like a model training job or something. And the third is like, if they have built out any models and they directly want to deploy that, they're able to manage that. Right? So let's start with the service here. So one of the things that you would have in any service is you basically have like a swagger endpoint that comes with a service. So in this case, I'm showing a demo of the Iris data set, like a fairly popular data set that all of us use for demos. But basically you can provide the inputs CTO your models in the swagger endpoint that you have. And then just invoke the model, it will basically call the predict method of your model and give you the response of the introductions. Which class are you predicting it? Right, so that's like a swagger endpoint for your model demo. And then the other thing is also you could be deploying any arbitrary function as well, similarly. So like in this case, we have deployed a few functions like normal distribution, uniform distributions, multiply, like basically you just go ahead and enter a couple of inputs and it basically just returns like a multiplication of these. Right? So that's basically the two things that you're able to see in a service. Now, how do you manage a service? Let's get into the details of that. So basically every service comes with its own metrics dashboard, so you're able to see some of this. Any long running service has its logs available to you so that you can check how is your service performing. We have some log search filters and everything that you can basically do log management. Again, we integrate with your existing setup if you have any here. And besides that, within a service, every service that gets deployed on the platform is actually version. So in this case, like, you can see that we have deployed two different versions of the services, right? You get to see that who has actually built out the service, who's owner of the service. One interesting thing in this case is it's very easy to basically roll back to a previous version. So if you have a service and you notice that the second version, you know, introduced a bug or something, just go ahead and roll back to previous version with one click it will allow you to redeploy this previous version very easily. So rollbacks become super easy. If the service itself is talking to netmeds, any environment variables, any secrets management that you can do also from the platform and what pods are running, you can check that. Any questions? No. I think the most important part is how does from a Python script, you deploy it as a service, what do you think? What do data scientists or anil engineer have to do to have it reflected over here? So in this UI, you can do a lot of controls and monitoring, but how do they bring it here? Sure, yeah, I will for sure go ahead. We also have two types of modules. Like one is the online inference model and then the other ones are batch offline modules, right? So right now things might change in the future, but as of now we have majority of the models or offline models. Correct. So they arun some service, push the data into S three and then data is stored in some database for lookups in the future. Makes sense. Yeah. So I shouldn't Shane CTO fall into the jobs category. So in this case, basically batch inference is practically like a job, right? Like you spin up, like you launch a job that reads the data from S three, runs the batch inference, stores the data in some database, and then or S three again and basically kills itself, right? So that's basically where you manage the jobs. And basically once you have deployed, like imagine that this is like a batch inference job. You have run this and you wanted to do like again, just run this batch inference again. It's literally a click of a button here that if you trigger this job, it will go ahead and do this batch inference and save the data wherever you want, basically. And similar things like, you know, you can always see that what jobs are currently running because I just spin up one job, which ones are complete, and you can again manage the same versions like you can do in the service itself and harshit. I'm going to come on how you do the job deployment or the service deployment from the Python port as well? I'm going to show the Sales Maker notebook in the third part of the remote. Okay, awesome. So the previous case where you were scheduling the job, can we also set it up as an automatic scheduler? Yes. So, for example, one of the things that, again, the way we do the deployment, I'm going to show the Python way of doing the deployment, but we also have doing, like, a job scheduling using the UI as well. So maybe I'll just quickly show that to answer this question, So imagine that you are trying to schedule a job on this workspace basically, right? We can do it in a couple of ways. Like if you already have a docker image, you can just give a link to your docker image. If you don't have a docker image and you have a code that you directly want to link to, so you can have a GitHub. Let me actually show this here, because this already has a GitHub account linked. So, for example, I'm doing this. I pick, let's say a workspace here. So what you would do is you would basically just like select the GitHub repository, the branch where the code is, et cetera. So, for example, you would say something like python train PY here. Okay. And here you can now say that either you can manually pick up the job once or you can even schedule the job. And here we support, like, any kind of crony of scheduling things. So you can actually specify, like it does some parsinger, whatever the standard crown expression, you provide that and it will run the job at that schedule. So you can say that. Okay. Every day at 09:00 a.m just run the job basically, right? And similar things you can control, like your resource allocation to this job, what kind of instances on which you want to run this job, et cetera. Those are the other things that you can control. And by the way, you could do with the UI. I'm going to show the equivalent version of it from the python code as well. Awesome. And the cluster and all these are basically because this is on the developer account. This is something that the infrared setup so that not everyone will have access to all the machines. That is exactly right. Yes. Okay, Perfect. I think that's something ashwin from the intra side you can control. It's not like everyone will get access to every machine that is available, right? And one last thing that I want to show here. I think you all Chad mentioned about the pickle file kind of model deployment. That you have a pickle file? So one of the things that we do is we make it super easy. Like, if that's your use case where you just have a model pickle file and you want to deploy that actually, it's as simple as you select. You want to deploy a model? Okay. And all you really need to do is provide a name of the service and the Uri of the model. Like, if, let's say, you have stored the model on S Three or some other model registry like ML Flow or something, you just provide us a UI of the model and hit the sumit. It will just deploy your model and create an end point out of it. I see. So you could use Sage Maker notebook to literally create that pickle file and then give the address of that as Three location, and you have that as a service right away. That is exactly right. Yeah. So that's kind of how the overall developer experience looks like. And again, same thing with models as well. You get to see the model endpoint directly that you Shane run here. In this case, I Shane to link it to the docs, and you basically get to see all the end points that you have Simon up for the model, essentially. Right. So this is like the part two of the demo is what a developer gets to see. Then I'm going to show you how the Sage Maker experience notebook experience looks like, unless anybody has any questions on this part so far. Yeah, maybe law. If you have any questions, feel free to ask. These parts are probably relevant for you and I more than ashwin rate. Yeah, I think I have a question, but can be maybe answered later. But my main question is, you mentioned about this API. Quickly build an API stuff, right? How does it compare with the experience with Shane Maker, the functionality that Shane Maker provide, what are the additional benefits? And also things that Stage Maker does not offer. Sure. So there's a few things here. Number one, that is, I think, the overall developer experience, being able to track different versions of it, being able to deploy like a previous version of it, et cetera. I think Sage Maker does not have all that support, like a clean support. And the most important thing is that a lot of Sage Maker APIs are designed keeping, quote, an ML engineer in mind, right, that you pretty much have control over the infra, you know exactly what's happening, et cetera. We have designed the APIs such that it's much easier for a data scientist to also be able to deploy that pendo to end, essentially. So that's one thing. And then the second thing is that you're just following Githubs at every point in time, starting from your notebook experience to your UI experience. I'll show you what I mean to say by that. For example, here, every time you deploy something using 200, right? So let's say if I go to, like, a service here 1 second. Yeah. So if I go to a service here, everything that we're doing gets converted to one application spec. So this is like the common way in which we have designed the system in which your UI based deployments, python based deployments YAML based deployment. Everything comes here and you're able to track that on get essentially. That's the other thing that you lose when you just work off of like Sage Maker notebooks and all. Because notebooks, it's very hard to maintain quality control in the notebooks itself. But the way we do it is we transform that to a GitHub principle still maintaining a data scientist workflow. So I think that that might be like the more stability related differentiation per se. Any other question? Yeah. Yes, got it. So my understanding is that all of those so it will be adding value on top of the Sage Maker notebook. It will be added, yes, for sure. Although I will admit that the goal of both the platform is similar. I think the user experience of how we have built it out is actually superior compared to Sage Maker. Basically what happens is take a note of people that we have spoken to, law like people adopt Sage Maker, but then people need a lot of time to train their engineers to become experts in Sage Maker. That's the part that we, Shane, heard from a lot of other Sage Maker users, which is something that we solve in our platform. I think the ML of part, mostly like Sage Maker has a lot of functionality, right. As a notebook, it's very easy to follow for a data scientist when you're trying to make from the notebook into an end point and then set up monitoring and all those things. That is the part which becomes hard. And I think that's where your platform comes into play. That's right. Yeah. And I'm going to show some of the experience here. Thanks a lot, by the way, for jumping in. Ashamed clarifying this. So now we'll move to the third part of the demo where we are going to see this agemaker notebook experience. And here IBM going to show practically all the things that we saw in the UI. That is, how do you deploy a job, a training job, or a batch inference job? How do you deploy a machine learning model? How do you deploy any arbitrary Python function that you may have? Remember Ashwin, and asif we had discussed in our last call about the pre processing, post processing functions, right? So that's the use case that we'll cover here and then being able to deploy like a fast API endpoint. And lastly, how do you deploy the pickle file models, basically, right? I'm going to cover all of those here via the facemaker notebook. Okay? So in this case, I am just running one arbitrary, very simple job, right? All this job is doing is just sleeping every second or something, right? Once you run this, it just sleeps for 30 seconds, basically. Okay, but the point being that you have written like a simple Python file and this could be as complicated as any train PY model, training batch inferencing, et cetera. The way you deploy that directly from the notebook is you basically start with the library. Clients said library that we provide you instantiate a job object where you would just tell us the command that here's the command that you need to run. And then you hit a job deploy API endpoint. And the way this links to your dashboard, to the UI is essentially that you provide the workspace Uri that okay, here's the workspace that this job will get deployed. And that's practically it like you do that, it already links you that okay. Now that you have deployed this, it links you to this particular dashboard, and it will directly take you CTO the job that you have deployed, essentially. So you are now on the counter page and you can run this. Any questions here on the job? So where do you define the resources needed for the job? Right, so in this case, I put like a very simple, basically settled for default resources. But here this API itself actually take a note, parameters that I showed you on the UI as well. So when you were doing the job deployment from the UI, remember, I showed you a bunch of different things that you're putting up in the form. Right. So all those things actually, the API itself accepts those parameters. You can define that from here. Okay, yeah. And just so we have some clarity here, all these things are actually mentioned in our docs as well, that when you're deploying a job, you can actually provide a lot of these parameters, basically. And you can go to the advanced options. There you'll find all the box for how to provide them. And then you could also model. So one thing that you could do is from here, you do it with the default parameter, but then you go to the UI and then edit the settings to set up. You could do that also, right? Okay. Yes, that workflow would also work. You just modify the settings and rerun the job, and that works. CTO, you. Okay, now, like, one thing to note here is, like, we are actually not interfering with your actual training code. So that's one point that I want to highlight. So your run PY actually has no troop on very specific code. In this case, you could write your any arbitrary training file, and the way through it as a deployment is like a separate piece of code. So it basically makes adoption very easy. If you want to just take one job and start using Truefoundry, you super easy. If you don't like Crew Foundry, just remove that block of code and you got rid of it, basically. So it's not like such a tight integration that adoption and like, leaving True Foundry becomes like a big challenge itself. So that's one of the things that we're trying to do in my design here. Okay. The second part that I wanted to show was basically how do you deploy. Like a model training itself. So I showed like a toy example. Honestly, model training is also exactly the same. Like, you write your train pi which has your more complicated data processing, model training, et cetera, et cetera. Okay. The deployment is fairly straightforward, by the way. And by the way, if you have more, you want to provide your own dependencies, for example, so you can actually provide like a requirements TXT file as well. You can tell which Python version you want to use. All these configuration that you want. You can do that. And then internally we will generate a docker container and basically run that job on Kubernetes, essentially. So then the question that comes up is in the training, if you go up a little bit from here training. So here you are specifying the X, like the features from some location of the file, right? So here we can definitely instead of this location of the file, we could just connect to a snowflake database and get the features from there. But during inference, if we had to specify that, how do we do it? Now we have a service which requires input, but Chris input every time the service is called, we'd want the service to know that it needs to call a different service which has all the features. How do we connect those two things? Or maybe a table which has all the features. How do we do that? I want to take it. Yeah, I mean, can you show sample service code? I mean, it's pure Python code asif so you can pretty much import any libraries that you want, like Snowflake libraries or whatever, or simple calls, make rest calls to your features and get all the features like the first line of the service. Nicole, have you opened a fast API service? Let me open up fast API service and then we answer that question. Yeah. Inside this predict function, you can pretty much make any API call that you want to any service Python code. You can import whatever you want. Okay. So this service can itself be defined in a way so that it can talk to any feature store, any sales or any place where we have the live features stored. It can talk to that right. Service account with respect to each service and workspaces here. Where do we specify the service account. Here in case of features? Not go by service accounts, but like let's say if you're talking to three buckets and all, then ideally what will happen is sep. There is one way where the infrastructure provides you access key and secret key and every developer starts putting access key and secret key. But that's usually not considered a good way of it's prone to security of access key, secret key links. That's a problem. And all those things happen. So the way it's ideally should be done is the inderjeet basically configures a service account and attaches it to the workspace that are providing it to you. So let's say Data Science team needs access to two s, three buckets and one postgres database or something. So what they can do is they can create a gift and attach it here. And once they do it, then data scientists can keep working autonomously without having a back and forth between infrared team and dev team. So infrared team also feels secure. That okay. There is no way security can break. There is no leaking of access key, secret key, and there is no need to further back and forth to happen. And that actually improves the developer productivity a lot. And by the way, that's exactly how you control how certain services that the developers are launching can only be accessed from certain other services. For example, if you had this Python model endpoint that you want to be invoked by, let's say, some back end server, right? You can only configure it so that it's open to that back pendo server, which has the credentials we'll be able to invoke, but nobody else can send out random sales. You can control the security also that way. Okay. That's it. Okay, the next thing that I want to show here is how do you deploy like, any arbitrary Python functions? Imagine that these could be a preprocessing post processing functions. Now I'm going to show the developer experience. So here we have implemented whatever, three random Python functions, right? And again like a pure vanilla Python code. Nothing 200 specific here. Okay? And the way you deploy these functions as endpoints themselves is the following that you basically just register whatever function that you want to deploy with our function service. And then you call service or deploy again with a workspace you are in. So like, the experience is fairly consistent when you go from job to services as well, right? And then you will start seeing like once you do the service or deploy, you basically start seeing your service getting launched here. So this is your function service. And this exactly has the three normal uniform and the multiplier functions hosted here, basically. Okay. Does that make sense? Yeah. And then the same way this service can be called by anyone, if you give the right parameters as input, you can get the output from there and it works. And that's exactly right. And then I'm going to skip over this very quickly because we have talked about this, that you can deploy similar thing like a fast API endpoint, like where you have a model predict function you want to deploy, that the experience again is very similar. You define a service object, you provide whatever parameters that you need, and then you call a service deploy. It will basically create your model endpoint. Okay? And in the predict function, you can write any custom Python code. Like if you want to take a feature, transform it again and do all those things, it can be done without. And then the last thing that I want to show is the model deployment. By the way, this feature is in beta. We are just releasing it now, but it's not available in applied yet, but probably will be there in next week. Right. But here what you're doing is all you want is like the access to the Uri of the model wherever your model is stored. So imagine that you ran your training script on Sage Maker. You stored that on Sage Maker. Sorry, s three, you have the Uri and you can see that for model deployment, all you're really providing at that point is the path to the model file, right? And the model deployment deploy and the model itself gets deployed. As an end point, I'm going to show something similar to you from the UI as well because I just copied the UI. So all you do is you go to the UI, go to models, provide whatever that you want to deploy this model to, this workspace demo, and then you provide this and then you just hit submit essentially. And it will basically just go ahead and do the deployment. At the moment, it takes a couple of minutes to create the Apple containers and everything, but that's pretty much the experience here, right? And could you show me the Fast API page for a model that has been deployed in this way? Like any of the examples where you took? Sure, I'm doing it right now, by the way. Just search is just showing that we did this thing. This is something that I need to check here. But basically this is a Fast API page for a model that has been deployed like this. So you have all the endpoints basically that you need so you're able to manage practically everything with respect to the model, the versions, the inference API, endpoint, all of that. I sep so you have CTO, get and the post, all of them. And if you click on it. Go. Ahead manish okay, so it shows the post and then in the get it also shows like it creates the command on its own that you can utilize. Right. And on this one, by the way, I do want CTO admit that given that this is a beta feature, there's some user experience cleaning that we are going to do. So probably not all of these will be relevant for everyone. So we will actually hide some of them under whatever advanced APIs or something. So there's some user experience that we are still cleaning up for this part of the system. And you can link to we have a similar sort of like swagger set up internally, which we call as Agora services. So you could literally like housing this as a tool. You could help us deploy it as a service within that agora services that we manthan internally. It should not be too hard for you to connect to that system. Right. Do you want to take that question? If we have an internal Swagger already enabled, how is it? I personally have not worked with the Agora thing but when you say internally Swagger is available there is one API hub kind of thing where Qovery single API is documented and people can go and look up the input and output. Exactly. So that is actually pretty easy. You can just open that link. I think then it should support like if the standard just open that link this one? Yeah. And go to docs test JSON. I think go up there is an open API JSON thing. Go up at the top. You see there is open just click on that. This JSON I think it should be importable in your Saurav service if it is just compatible with any open API thing. Okay, so then we just have to make sure that this JSON is included in the service management code. And then any service that we publish using True Foundry will automatically get reflected. Yes, exactly. So the formats are compatible regarding the exact method of time. We just have to look like how. Does I take it will help build those integrations. If something is needed, like something that makes it easier for you guys to link with and run a services will do that actually. Okay, you can help us with that. Okay. Awesome. The last part sorry, go ahead. I think you had a question. Another question. No, this is great. I think that's one of the things that we would want to make it extremely easy. This is where we run into ML Ops people, right? We build this model even though in some cases we know how to have it as a sage maker endpoint. But we don't know we have to do a lot of code management to then finally reflected in our own services management repository. But if you set it up once and then everything that we do through the UI or through you tool gets automatically reflected there. I quick chat with be a great win for us. Thanks. Awesome. The last part I'm going to show is a little bit about the monitoring component. So this is the fourth part of the demo. So here what we're talking about is that for example, we already saw that with respect to services, you have some highest level monitoring, right? Is the service up and running, what kind of CPU memory, etc. It's using, how many requests that you're getting, etc. So you have some of that monitoring set up here. But for machine learning you need more than that, right? That is what you use for you traditional software. This develop monitoring. So one of the other things that we build here is basically like our monitoring dashboard for a machine learning model. And here what you get is a hierarchical view of how you debug your ML models, right? So to begin with, you have some level of model summary that you are able to see that, okay, how many predictions is your model getting? How many actuals are getting reported? But you can also performance, you can also model the performance of the model itself here on Chris dashboard. Like, here we are tracking just one metric log loss, but technically you could be monitoring your accuracy, precision, recall, and DCG metric depending on whatever machine learning problem that you're tracking on this dashboard. It's very easy to configure. What metrics do you want? CTO track. Okay, so this gives you the view that, okay, how is my model performing? But let's say your model started performing like numbers in a certain period of time. Then how do you debug that? So one of the other things that we provide is basically an understanding of different slices of data in which how your model is performing and how is your data distribution looking like. So in this dashboard, let me explain this dashboard how this looks like. So you're seeing your prediction distributions, you're seeing your actual distributions, and you're seeing the distribution of every single feature. So each row here is a feature, and you have some stats about the feature, and you have the distribution trend as well. Okay, one thing I want to call out is you can see that the graph itself is color coded. So this is like a darker color and this is like a lighter color, right? So what that means is that the darker color itself, that segment is a high loss, whereas the lighter color is actually low loss. So it becomes super easy for you to understand that if a certain slice of your data set is not performing very well. So, like, frequent use cases come up like, oh, my model is performing really well in California, but not in New York, or maybe like cross country, that becomes like a very, very important use case, right, that will ben performing really well in the US. But not in Canada for whatever reason. So you can actually figure out those issues in your model. Now, let's say that is fine, but maybe you want to see that was this issue also happening during my training time or was this issue also happening last week or is it only happening now? So you can also add comparisons to your Asif dashboarding. So in this case, now you can compare your training data set against your inference data set or data set from two weeks ago, from the data set with one week ago, and just figure out that, is there any breadth that is happening in my data set at the actual level, prediction level, or the feature level, basically. So you can compare all of that in this one dashboard. And lastly, just one other thing that I want to call out that, okay, so you evaluated the performance at the model level at any slice level of the datta, and finally you actually want to see like real data set that okay. At a particular prediction level. What happened in that? Okay, my model predicted six and maybe the prediction set was six. Okay. These were the feature values that happened. So you can actually see all of that, what probabilities were predicted across different classes and stuff like that. So basically you can start from your service level, go to your model level, look at the datta distribution and debug it at a single row level. That's kind of how we are building out the monitoring system here. No, this is actually pretty amazing because we run into this issue all the time, right? Like our client or someone complained saying that, hey, why did I sep this ranked in this way? So you are saying that we can go back to the actual instance where that prediction was made that impacted the ranking and you could see exactly what were the values and everything. But the question is, how do you do it then? Are you talking right now? For us to do it, we'll have to write a bunch of data jobs to look at every request. What was the prediction, what was the actual output? So how do you set it up from your side? Sure. So let me explain that actually how the developer experience of that would look like. So, for example, imagine that you're writing this predict function, right? So you have this like fast API predict function that you're writing where you're actually doing the model prediction, right? So here you obviously have access to your features, that is the data frame and whatever prediction that your model made, right? So these are the two things that you have access to. So what we ask you is basically inject a log line where you log these features that you have and the prediction that you have. And I'm going to show the API here that you basically inject a log line in that code, your prediction code, essentially. So now we have already gotten access to your features and your introductions and after that sometimes like the actual values, the truth brown truth is actually reported, realized much later. So basically, just make a second API call where you log the actuals along with a unique identifier, basically. So it could be things like transaction ID, any prediction ID that you are tracking with that you log that thing and we internally do the join that okay, this prediction, this actual and these features are all part of the same model prediction. So we do that john internally store that in our database and render everything in the dashboard that we just showed. You, basically, and no additional work. So right now what we have to do is we have to work with and like each team has some ML ops engineer, we're building all these pipeline. In your case, we don't have to do anything. Just add Chad Blunt of all right, the tool has all this tracking in place? Yes. That's interesting. That's pretty much it from a high level, like a demo perspective. I wanted to see if you already had any other questions on this front. Yeah, Law, feel free to ask. I think it might be relevant for you also. Yeah, sorry I have to drop for another call, but I just want to echo on machines, like points that I think this monitoring, especially visualization everything, is really beneficial to us in terms of like debug and also understanding the shane over the time. So, yeah, it seems like it's easy to do, like all those slides and dice as long as we set up the data correctly. Cool. Thank you, Bob. Sorry, but I really appreciate your time. Of course, yeah. Thanks a lot. Ashwan, anything from your side? Any points that you would like to add? I don't know if you have a couple of minutes to go over. No, I think from a versioning point of view, how do you control the versions? You mean versions of services, jobs? Which version of models? Oh, I see. Okay. Actually great point that you brought it up as I have not shown that. Thanks for bringing it up. So basically every version that you have in you, in the system basically is actually stored properly. So each version of the model itself has a unique identifier and you can actually deploy whatever version of the model that you want. Right. And again, with each version you're able to track things like how do you invoke a particular version of the model in your Python code, if you have any. Like, you can obviously also download the files itself here, so practically all the metadata associated with the model itself is available. One other thing here is that if you wanted to do it, you can also track the training level parameters here as well, Ashwan. So for example, in this case, different versions of the model are being produced by different experiments and you can actually just compare all of that in this one dashboard as well. So, yeah, like we have done actually quite a bit of work on making sure that you're able to track everything related to a particular version of the model, like the metrics, the higher parameters of that version, what data set was used in this case? I don't have that. Let me just show you another bit of it, which is like, what data set was used to train that particular model. So we have a lot of that available here as well. I think you had mentioned about the entire lineage and everything that which version of the model was trained on, what type of data set, what distribution was there for that data set. So we have all of that information on the dashboard as call today. Got it. Okay, sure. Thank you. Yeah. Any other questions, Ashram, from your side? None from my side. I think it'll be good to kind of try it out on our workspace so that we get an understanding of the capabilities and the driver that will be more beneficial. Makes sense here. And a couple of things that I want to just call out from a setup perspective, Ashwin, which might be interesting to you, is generally like as I showed that the set up itself is actually fairly straightforward. That if you already have a Kubernetes cluster running, you just linked that on the dashboard and a couple of them charts and we get it going. The other option that some of our clients have done with us. Ashman. Is they have actually given additional call it at a sandbox account on their cloud to us and we do the entire for the POC. We do the management ourselves so that we make it fairly easy for you that we spin up a small cobbler disposal there. You try out the developer experience and all of that. So that becomes fairly straightforward. And because everything is getting deployed on your cloud, none of your data ever leaves. So that security, everything is managed by the design of the system. And that's the reason why currently we are actually working with a couple of really large companies like Synopsis, Reliant, etcetera. And this deployment method is something that they chad really appreciated. So I just wanted to call that out that we've gotten some good feedback from folks on this. Okay, sure. And one last thing that we have not yet built out. But I also want to call out maybe I wish this is where you can also jump in is the way we are building out this platform is doing things like shadow testing and a B testing and all is going to become like very very straightforward because we already have our way of being able to deploy the model. Deploying different versions of the model. Right. We are tracking all the logs and we are tracking all the model metrics. So basically if you wanted to deploy like a different version of the model and split traffic to those models become super easy and then you monitor the performance on our monitoring dashboard. Similarly, if you wanted to replay the logs of an existing in production model to a new version of the model that is not in production yet, you can do a lot of that. So that kind of increases you experimentation speed. That's the other thing that is in the pipeline. It's not yet built out. Of course I did not demo that, but I remember that was one of the use cases that we chad talked about. Okay, so in that case, what will be like whatever is the prediction and actual? You kind of say if you use a different model, Shane, what would be the prediction and actual? Everything is associated with the model version, right? So basically you know, that okay. Chris prediction is actually in fact the sales of the data set that I showed you. Actually, we have the model version of Chris as well. So then it becomes fairly straightforward at the comparison view. You can say, how is my model version one performing compared to model version two? And you can do that comparison. No, this is great. I guess Ashwin, you, me and Love, we should catch up this week or next week, discuss the pros and cons internally and yeah, we should definitely figure out a way to do a POC and then see how it is helping us in the long run. Sure, makes sense. Awesome then. Thank you so much, ashamed, Ashman, for taking the time. I hope this was useful and look forward to hearing back from you. All right, thank you. If you have any questions, like anything was not clear. If you just remember any questions, please feel free to drop whatever an email or message or something will be prompted. Response. Okay. Okay. Thank you so much, Nick and Janet. Yeah, thank you. Bye bye. Thank you. Bye.