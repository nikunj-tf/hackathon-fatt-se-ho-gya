I'm based in Santa Clara. And what time is it? Is it not too late for you? It's 1130. I mean, that's the only time I could found. Because you guys are in IST, right? Oh, you should have told us. There's a different calendar we use, so didn't realize that. That's okay, this time works. Usually I get free after talking with parents, and then I was still up, so it's fine. I'll give you a quick context. So, what I wanted to do on the call was two major parts. The first is wanted to understand the ML structure at Docusign. US too. What are the use cases you all solve for and where does ML come into the user workload? Which parts are critical? The second major part that I really wanted to cover is around the multi cloud because you all kind of have a lot of vendors sorry, a lot of companies. So their data privacy and always like, is there something that kind of asks you to be multi cloud for a particular reason? And then how do you manage that? Is there an intent to be is it in the roadmap? Has there been a discussion? What are your thoughts about that? So those two parts, and just to get started, maybe we can do a quick round of intros. I can go. First time graduate from It. Electrical engineering. Spend majority of my time working with a hedge fund called World Fund. So asset management, primary algorithmic trading, coffee data use, circuit trading strategies. One other thing, and then spend like three and a half years in India outfit after that move to US. So spend three years between US and Singapore. And I was doing portfolio management for them, including like risk optimization and all. Got a chance to manage 600 million assets. It was a regionized experience. At the same time, I was a member of the CEO office as well. So looked after various strategic initiatives. And then in 2020 left my job. Vishik and Nikon, who are my patchmates from Kgp, they were on the same boat like they wanted to build, and both of them were at Facebook. Vishek was on the software engineering side, he was offering engineer, and Nikunji was the lead ML engineer for one of the AI teams. So we made our first startup in the talent space, but that became very hard to scale because HR folks believe the tech adoption was very slow. I'm loading a platform manager standardized interviewing. And then we used to map candidates to companies based on the interview process. Interview that was conducted over a platform. Interviewer standardized from companies like Google, Facebook, Microsoft, or put a platform like standardization or other feedback. And then we'll match candidates to companies based on their profile as well as the actual interview spoon. So smablog models. When I said that part, like the starting part was good, like building out of the models and all was fine. But when we had to deploy it, it took us quite some time. And that's when we were discussing about the systems that existed internally at Facebook of the workflow for the data scientists and ML engineer. And then we kind of started to build two foundries to make it easy for models in a seamless way, when without the knowledge of the infrastructure. Just give me 1 second. Sure, take your time. Yes. So that is basically optical foundry now. We have been like working for like one and a half years. The product is built out. The goal is to create as many abstractions as we can for developers so that binary knowledge given infrastructure and knowledge, they are still able to kind of do everything that will take the models that they are training on local notebooks or wherever to actual real use case. So that is the thesis around which we are building and we have tried to build it on top of net is trying to be multicount right from day zero. Those care around the thesis that we wanted to validate the doc that was shared by that's like a very brief kind of we are right now like an 18 member team. 1818. Yeah. Okay. And why is that? Like all of you are in India? Have you guys moved or temporary people are visiting India right now. Natanjay is generally in US, so he is based out of US for a few weeks he's here, but he is in San Francisco. Most of the team is in India. There are one or two members in Europe. But because tech hiring is cheaper in India, obviously we are in India, but whenever we need like we keep on traveling to the US. Okay. We all keep on traveling. Okay. I mean that we can discuss later. I was just curious, how do you manage because I think your startup is based out of or registered in US. Right? I saw your profile. I think I thought you are also based out of US, but I guess you have probably moved back. Is that the case? Yeah, I moved back because I wanted to be primarily in India, but Nikkunji. Is in US and the person also moved back. He has not really moved back for some time. He's here because initial text team it's important to be closed, but he plans to be in the US. So he'll probably move sooner. Okay, let's go. To get to the important thing first time I was just curious because. Towards the end of this call we can discuss it. I'll start my introduction. I came to US in 2016 for my Ms. I did my Vtech from AAA. I was working at this startup called Ruto File. I don't know, one guy was from It and then I started working for VMware three years, mostly virtualization product. How do you manage clusters? How do you spawn clusters? How do you change cluster configuration? How do you handle drift, storage, networking, depending on new configuration infrastructure, specifically? Because they were very open to let me try. ML infrastructure. They were forming this group called their ML group. And then he made him set up this team. Initially, Pala goal for us was to build the ML inference platform. You are limited by certain frameworks scaling, pre processing, post processing, embeddings company. We are able to monitor what is happening, but actually, it's a very workflow based process, multiple levels of approval. There is a lot of workflow system, so document creation is the most time consuming part in this process. Now, there is a scope of recommendations. So let's bring in recommendation instead of user creating all this pickend drop. Let's just give him all these recommendations. Key their document. Basically framework. Let's decide on one cloud story and let's bring everything on Azure. Microsoft let's first invest on Azure and then let's keep AWS as our backup cloud. So basically what I understand is people were using multi cloud coat. Whatever teams felt comfortable, they were using it. And then like ML teams of form we that is when people started. Okay, now we are moving to Azure. Even the acquisition. But this is specific. What about the software stack? Gcp. Company wide. Everything has moved to a zero. Got it. Understood. I think we can move ahead then. I think just wanted to get this context. So you are saying about the MLA core group, bana Smith, you are the third engineer. You all started kind of so you all built the base layer on top of Cuban underneath. They can create their own custom and then they can. Custom values. Basically. In terms of where have you taken different choices in terms of love? To hear about that. You have to make sure your pods are going on to that node. Pool only your GPU open source is the most suitable for art scenario basically deployments actually as of today so you don't have to do your own container dependencies dependencies already inherited to support all these frameworks dynamic batching by default to support windows dynamic batching for all the popular ML frameworks run times configuration best configuration to have the maximum benefit. Perfect. Triton configuration to get the maximum in terms of performance. You can do that. Evaluation simulations run. Cartage model pay. And then it tells you keep this configuration of dynamic batching and this configuration of your vector size of Embeddings, then you'll probably get the best maximum performance integration. So if you want GPU metrics. To. Extract, GPU metrics multi model support software engineering site across these popular frameworks. Industry standardization plug out. Let's go with that. Or so far, it's been paying off. That bet is being paying off to shift to something else in terms sense. Understood. ML configuration files multiple times, performance evaluation. A good discussion and I think it will be good on the inference side. Even furthermore, more deep, especially similar kind of it can also be potentially used for micro services. Alaki main core focus was ML deployment. But to kind of suit the best of both worlds you can build custom and then so it'll be good to kind of discuss that and also on the triton side like add support. But that was one thing that we were planning to do even more details. It'll be great. Also in the call side, from a business use perspective, they mandate okay, you have to store our data in only AWS. We cannot allow our data to go into we don't allow our data to go into AWS and things like that. From a business use case perspective we have never seen there is a need to support multicount. There are security policies, there are privacy, but there is no client stuff. They use only AWS. Okay? Fundamentally there is no business reason. There was only an economic reason would be that kind of preventing the cloud is offering better discounts. From a security perspective as of now, nothing, no challenges have come, right? Like I said, sometimes we see, okay, retail companies don't want any of their data to lie on AWS. They'll always want data you have not seen that at least happening, because that has not happened. What about from a GPU side like GPUs cost across and availability, machine availability, restriction of GPU, availability of It, cost of GPU missions because CPU. To maximize the consumption, to maximize the consumption for this year. This is it for the same GPU. How do you end up utilizing it maximum from an inference perspective also GPUs okay, it's this is very helpful here. I do want to kind of schedule another time to go over this system. It seems like your system is already quite evolved and it will be great for us to learn even more. One of your founders is in India, but currently in India for a very long period. His wife also works in the US with LinkedIn. He left Facebook, but now he has moved to India. So right now he's employed with the Indian entity, but he has a plan of moving to US. So right now his things are in India. It's the same entity is an option that you can do and then once you have L one, then getting the H one B becomes easy, right? Machine learning Models most of the companies today are still not comfortable with that managed cluster thing where people still want okay, everything is on our cloud, including the control plan. So because metadata which is flow there, people are like little bit very data, but metadata is still flowing, right? Like a number of services consumer that metadata is still flowing which people are a little very off. Most of the companies will want systems internally while we have that offering managed cluster or managed infra offering but love mostly everyone like Hamari cluster mainly people want the managed infra thing when they do not have any person who has understanding of Kubernetes system failure to say they are okay to be dependent. But any company after certain size workloads they will not want like the management. That's at least what we have found so far. India market is quite behind. Like if you look at companies, even the adoption of Cuban it is very, very low. So people are like specifically without using the benefits. Then you have to do a lot of system key ultimately then you can deploy to any client like AWS user to be used and even on the ML in terms of the details that you have like platform image of Banadia taking care of everything. Very few companies in India are doing that. So the market is very small. Mostly us target. Okay, nice level. 3.6 million gas per day. To be honest. Understanding of the engineering side, but ML side my people but I think that will be really nice if we can take a little bit of it. No worries.