The trip. Except for synopsis and other things. There's a lot of things that is there on me. Hey everyone. Being a few minutes late here, no? Hi, Nick. How are you? Hey, I'm doing well. How are you guys? We are good, we are good. Thanks here for making time close to the New Year to talk. No problem at all. Happy to have a chat. You are in Banglaria? Yeah, I am in Bangalore. Okay, got it. I think first of all, always date connecting to someone from KGP. So always have mutual studies to share. So that's there. And Slice, like Rajan is a very close friend, like, he was one of my juniors, our juniors from the same hostel. So as you are building this, we thought it will be good to connect and talk to SliceIt. And Rajan mentioned that you are the person who leads everything. So we thought we'll reach out and then we can talk to you with regards to what we are building, how we can potentially what problems we are solving, and learn about the SliceIt infrastructure and all as well. Sure, yeah. Happy to connect with you guys. And I think Rajan gives me a lot of credit. But to be honest, my work is mostly limited to data science strategy, little bit, and analytics, while I work very closely with our data engineering team. And on the infrastructure side, honestly, that part is being led by Vinot Keisha. Okay, but let's have a chat and then I don't know how many, how much I can help, but if there are follow up questions to this, I'm happy to connect you to Winnows as well. And then he can provide more information or even Upendra, who has joined us maybe seven, eight months back. But before that, Upendra was the one who was leading all of the infrastructure projects and so on. So, yeah, I'm happy to have this conversation. But I just wanted to let you all know I may not be the right person to answer everything or tell you about everything on the infrastructure or what we do, but surely because my background is mostly around data science and analytics rather than data engineering or analog kind of thing, sure. Cool. So what we can do, Nathan, is we can maybe start with intros like me and Nick, and we can give quick intros. And then after that, love to understand about the use cases as to how you are thinking of the ML evolving at Slice, what is the current strategy in place, how is the team, how you are thinking of the evolution of the team and maybe more from the data science aspect we can touch. And at a high level, whatever understanding you have on the infra, we'll love to touch on that. Does this sound? Yeah, sounds good. So me, Nikunj and my other co founder, we are all batch mates from 2013, batch nine to 13 PGP. I was in electrical, nickenj was in electrical and Avishek was computer science. After that I went to work with a hedge fund called Worldcount. So was a portfolio manager there. I initially built trading strategies and then moved to US and also in Singapore. So spent three years there as a portfolio manager, was also a member of the CEO of looking after growth initiatives and then during that time used to invest a little bit into startups and always wanted to build something myself. So then Nikunjana Vishik were in the same boat and then we decided to kind of leave. Our job came back. We built our first startup in the talent space solely to inferge. We are facing constraints around operational scalability and then finally over the last year we are building two foundry. Where about in US were you? I was in Connecticut area, stanford. You were in San Diego, right? I was in San Diego for a while, yeah. Okay. Nice. I missed you guys almost two years. I graduated in 2007. Yes, nice. In the US. I was in the Bay Area Natan generally I actually moved to the US for my masters, the Berkeley Gather. Immediately Karatburg and Tubs have been working in the Bay Area pale. I worked for a startup in San Mateo area and then worked at Facebook. In the conversational AI team at the startup I was doing a lot of recommended systems, personalization algorithms for Facebook journey with Unraguna Vishak that he described, basically. And Vishek was at Facebook, so he spent his entire time at Facebook. He was reading a lot of initiatives around videos, theme, cash redesign, intra site. So he brings a lot of experience on the intra and he leads the tech for US and Vivek is actually helping us on the business side, helping us talk to folks that can be initial customers, early customers of our journey. That's a brief background. Nitin I'll maybe take two minutes to share at a high level what we are building before diving into. Before we do that, I don't know if you guys want I can do a quick intro of myself as well. So that you that's in the profile, but scale our is great too. Yeah, I mean if you see in the profile, this is only my second job. Before this I was with FICO all the way, graduated in 2007, joined FICO in Bangalore. Four years I was in Bangalore and then moved to San Diego. So until 2021 I was in San Diego. What I was basically done different roles related to data science. Started out in data science analytics research while in Bangalore. The problem was we would because I was working in healthcare insurance space, we wouldn't be allowed to get real data and all that. So research worked out very well for us and then after a while we moved to US and started working with the product team on making sure that the analytical data science research made it into the product. And then for five or six years, I was working across the globe with multiple companies solving new kind of problems, trying to figure out what were their problems and then data science or analytical solution for them. Last one year I was doing some sales support and that's not what I wanted to do essentially, which led me to looking for opportunities outside and also started thinking about moving back to India. So yeah, here at Slice we built a pretty decent team around data science and analytics. So the idea initiative was to build maybe a five, seven number team which was doing data science and machine learning on the side. But once actually we started growing, we felt the need for the analytics team and the data science to be more heavy and all that. So it's a team of 55 individuals right now, I think roughly 60 40 split between analytics and data science. But data engineering is again something that resides with our engineering team, but we work very closely with them to establish pipelines and ML ops and all that. So yeah, that's basically what it is. Got it. Understood. I have a number of questions there I just wanted to share, like at a high level the context. So I think basically we have been building the foundry for a little more than a year. Right now the team is around a 20 member team, mostly engineering heavy, and the goal has been to make it very easy for companies to get started and scale up on ML journey. So like when Nikkunjan Avishek were at Facebook they saw internal platform of that Facebook has built which enables data scientists or even folks who have very less engineering knowledge to actually take whatever models they are building, test it out, release it to test environments and then scale it up from there. Release it to production in a reliable way. So we wanted to kind of bring that platform to every company. That's where the goal was to kind of build something that enables very quick deployments along with monitoring for companies. That is what we are building now. Obviously in itself, like that platform has different facets, which we'll love to kind of discuss with you later, but wanted to kind of have a quick high level overview so that you have context around what we are building as well before diving into the slices use cases. Sure, sounds good. So a few questions in terms of like when you say 60 40 split between analytics and data science, the analytics is it more business analytics? Whether they are using tools like. Yeah, so we don't use tableau, but it's the similar idea, right? So 60% of the bandwidth is around descriptive and diagnostic analytics. So doing our and creating dashboards and stuff, we were a big periscope shop. We have recently moved to Databricks for both data science as well as analytics. Databricks has its native visualization and dashboarding capabilities, which is what we are using now. But yeah, 60% of the team. So they are more focused on building dashboards RCAs and then doing any financial projections. So things like projecting what our GTV come out of transacting users, all of that would be those are the kind of things they do. And then everything else which is around predictive modeling or creating logic alternate as simple as variables. Using alternative data also is something that gets involved with variables characteristic development. And then even if it is something like a logistic model, then also the data scientists are involved and then all the way from there to doing things like computer vision, NLP, all of that is also a part of the data science. Thank you. Could you also Nathan, help us understand the use cases from a perspective of the type of problems you are solving and whether that problem is being solved for an internal need or an external need. Just a quick question. Before that you mentioned about variables from alternative data. Could you help me understand a little bit more about what does that mean? So, as you know, alternative is a very relative term. So this is alternative to what is traditionally used in financial space. So if you go to any financial or credit organization, they would traditionally use a lot of bureau data, a lot of accounts financial data. That right. So alternative to that would be any kind of device data that you have. So for example, if you're using reading, let's say location, we are syncing location data of the user. And if we're using that to do anything or build characteristics that would be considered alternative data, it could be characteristics that if we do some sort of an NLP thing or even a reject matching and try to extract information from people's SMS data, that is also alternative for us. The spectrum of alternative data is makes. Sense and nitin on this one, people actually do like a lot of modeling, predictive modeling on top of this alternative data here. Yeah. Okay. And this falls within the scope of the analytics team or the Data Science Pod. Data science Pod. Okay, understood. Thanks Nathan. Go ahead and ROG. I wanted to understand the use cases in terms of what problem and then whether it's internal related or sometimes it's external. So which of these do you end up doing? Like a batch model deployment? Which of these you are using real time? Both those classifications. From an internal and external perspective. That's what you mean? The external facing versus internal consumption? Kind of, yes, internal. And also like the actual problem statements. That are everything that we build is for us. Right? I mean, everything at first SliceIt is consumption. So there is nothing that we build for outside. But having said that, so if you look at any financial life cycle of a customer, we have data models which are working across that particular lifecycle. Which means that right from the place where customer applies or onboards to our platform and let's say starts the application process for a credit product. So the first thing they will undergo is some sort of like an underwriting model, right? So that's one which is like a credit underwriting model, then let's say once using that and some credit policies that we have on top of it, they will get underwritten and then on a regular basis. So this is a real time model running external facing. So again, you might have seen that within 15 seconds, 30 seconds we actually do credit decisioning, right? So it's running in real time, it's making decisions in real time and it's basically spitting that decision out to the customer in real time. So that's why once the customer is onboarded, then we have a host of models which are running in real time again. So for example, every time a customer is doing a transaction, it's sort of like a fraud monitoring in the background. So I wouldn't go so far to say that we have a fraud detection model, but it's a bunch of different strategies that runs. So again, the strategies that run right now have not been collated into a machine learning model just yet, but they run as a bunch of different rules which consume characteristics built on alternative data. So that's fraud monitoring. That again, the data science team works on. But we are in the process of building multiple models over there which would monitor the activity and highlight any alerts that we have, right? The third aspect would be a portfolio monitoring from a credit risk point of view. So every month we score all of our customer base to see which ones are still credit, like if anything has shifted from their underwriting space, right? And then on the basis of that, we take decisions like should they be given a higher credit line, should they be given a lower credit line and you know, in case they go, they delay on a few days on their repayment of the bill. Should we allow them to continue? Should we allow them and they should continue to transact? Or should we put their account on hold if they are less risky? But maybe they missed it by one or two days or five days and it's the first time they've missed it. We would say that. Okay, fine. Let's allow them to continue because we'll eventually make money on this user, right? So that's the portfolio models. This is something that runs in the patch in background. The next step that happens after the transactions have happened, the bill is generated is collections. Collection. We have a collections model. So now within collections we have multiple models. So for example, one stage of the model could be where again, we want to balance the size of our collection scheme and the money that gets collected again from everybody who goes into our collection pool. So does not pay by, let's say we give three days of grace period. So who does not pay by 8th of the month, when six is the due date? Everybody gets caught through the collection model and we basically determine who are the highest risky customer, highest risk customers that we need to start calling right now. Sure. That basically will get assigned to the telecolling team. Whereas we'll say for any medium risk customers we'll give them five extra days. We'll just send them payment reminders and all that to IVR or SMS and we'll expect we'll see that they might resolve on their own. So we don't need to to waste human resources on a customer who may not be that risky, right? And then again, low risk customers, we might give them ten buffer days or something like that. So it's an allocated later model which decides who to allocate later and by how much. Then similarly, once people have gone, let's say we have waited for 30 days and people have gone into what we call second bucket of collections, right? And let's say they go into that bucket, at that point we say that okay, we have an internal telecolling team but there are collections agencies out there which will go and visit the customer and try to get money from them. We have models there to determine which customers are more likely to be resolved using teddy callers versus more likely to be resolved using agencies. And we will because agencies obviously charge more, right? So it is an obligation of cost there that's happening. That's the credit lifecycle where you have acquisition of portfolio, fraud monitoring and the collection. So we have models across the journey and then apart from that, we give offers. So there's offer recommendation models that we have in place. For example, we are building our UPI platform right now, right? We launched our UPI platform a few months so we are adding features there. Also we have recommendations as to you're sending money to somebody, but then your contact list is huge. So some sort of a recommendation as to if you are coming to that page right now, where you're sending money, who are you more likely to be sending money to right now? The first few suggestions that we are showing them and then obviously you can search for others. But we were trying to see if we can come up with something where we are recommending the amount that you might want to send right now and the person that you might want to send money right now from a UBI perspective, right? So those kind of things are going on. Apart from that, there are certain things which happen on the customer support side. So for example, every single day we get thousands of customer support tickets. So models around which one should get prioritized because this is, I mean, this is a genuine issue for the customer and it's blocking and it's a really bad experience versus some issues can be dealt within a 24 to 48 hours time window. The other is around customers send all sorts of emails, right? So again, some NLP models, on top of topic modeling NLP is to divide them and route the tickets into specific boards to say that okay, customer, this is related to, let's say repayments, this is related to acquisition and underwriting. Or this is relative technical issues with the app and so on so that routing happens and then there are that's like a huge set of and obviously the cost will increase every single day. This is very helpful already. This is super helpful. Nicholas, you had a question? Yeah, I have a follow up question. So Nathan, you mentioned about a couple of use cases which are user facing, right. One is Offer recommendations and the other is like Contactless suggestion. Are both of these models I'm assuming the contact list suggestion is real time, right? Like as the user is typing the thing, you're automatically searching in. That right. And is the Offer recommendation also real time? Or offers are stored in batch and then served. It's it's not not real time. Okay. Currently it's not real time. Again, it's a call that we have to take like how much of data engineering resources or ML Ops resources we can store. The underwriting models are our highest priority from a real time perspective. Okay, understood. And do you think from business perspective in this space? I know that in some other spaces making offers real time make a lot of difference to the business in this space. Do you know if making it real time makes a huge business impact or not so much? Not yet. Right. Again, because it's a credit product, which is like even well, our products have changed. But I'm going to talk more about our card, which used to be our main product the last month. So even on the card side, we usually show more than one or two offers. It's usually a 1011 offers and then it's based on what kind of partnerships we have with the vendors. And the recommendation engine is also predicting like what is right for this person. And usually offers are not flash offers where they are only available for ten minutes, 15 minutes at least. Not yet. Right. So then if offers are available for three days, we can have seven or eight offers available for the user over a week or over a three day window and it wouldn't change. The business impact is not that great. But when we, let's say, start doing offers or rewards on the UPI, at that point, if I just did a transaction somewhere and I know where that person is doing transactions, if I have any offers which are right, in the vicinity of it or something as simple as somebody is shopping in a mall and let's say they buy something and there is a really good offer right next door or something like that, and it is associated with the product they just bought. I would like to show it and it becomes something valuable for the customer as well. So once we get there on the UPI side, I think it will be much more beneficial. Understood. That makes a lot of sense. And one other follow up question on the suggestion model, is the suggestion model running on the app itself? Like is the model getting shipped with the app? Because I'm assuming like the server call on that will be very expensive. No, it's not. Again, as far as I know, it's not running on the app. It's a back end call. It's an API call that happens to load the sunscreen. Okay. Understood. All right. Thanks, Nathan. The other thing that I wanted to understand, when so many people are building these models, like overall, net, from your perspective where you are leading this effort, what becomes the most important paramount factor between in the real time, like latency and all comes into play? Does cost yet a factor overall from your perspective? So I want to understand as to how that resource allocation for the data science team is currently managed and how much of that is a thing that is like a challenge for you. I think until very recently, cost was not a factor. But obviously now that we've launched more products and given our hyper growth in the last one year right. Data storage, the amount of resources it takes to compute all of it has started. It's actually ballooned up quite a bit, even though since the last six months it's ballooned up quite a bit. Right. So we're starting to talk about it. Can we reduce our tech costs, especially costs around data storage and have better policies around archiving of folder data and all that? And then the other part is around with databricks also use of the computing resources and do we really need all the clusters that we spin up on a regular basis and all that. So those conversations have just started to come in, come in. And I think over the next six months or so we would want to have better policies in place because until now it was not a factor. Right. So until now it was like, okay, whatever you need, just spin up a cluster or just do something. And we obviously had some guardrails around it, but it wasn't that big of a concern. The primary concern so far had been the efficiency of the machine so that it doesn't hamper the productivity of the data scientists or the ML scientists. Right. So the idea was, if we can get our systems or infrastructure in place, such that queries or notebooks are not taking a lot of time to run, if the models I mean, model training is still okay, but obviously model runs can't take, or people can't be spending a lot of time every single day just waiting for the outputs to come in, right? So those kinds of things were of higher concern, and they still are. If we were to choose between the two, we would always are on the side of having our team being more productive rather than the cost, at this point at least. So one thing that happened over the last six months for us, one, we used to be on Redshift and Periscope, right? Redshift was our primary database, and anything that we were doing from an ML perspective, we were doing directly on AWS and Sage Maker and all of that, now that we have data bricks. So I think a lot of the time was spent in migrating the data from Redshift to BX and all that. I think the final state of where we would be from a cost and a productivity perspective is still not there because things are happening in real time, as you can imagine, with the whole digital lending guidelines and stuff, they're happening in real time. And I think three to six months from today, we would be in a better shape to tell you that. What are our primary problems? What are some of the good things about the infrastructure? What are some of the bad things we're still learning and still experimenting with it. Understood. And databricks is currently being used mainly for this cluster spinning and running of the training jobs and training things, or is it beyond that? Once the model is being built, does it go to a separate pipeline? So apart from the model training and so we do all of our ML work on database right now. Okay. Every single thing, the training of the models, the testing, the production. Right now, I don't think we're using a lot of it for production. That's more again, we spin up APIs for our models and it basically gets called from the back end systems, whichever service needs it and all that, right. But I think our MLS team is looking into what we can do from a database perspective to even productionalize our models and stuff like that. So that's something that is actively being worked upon. The third area where we're using databricks is, as I said before, the dashboarding tool that they have. And then the current productionization. How is it happening? Just 1 second. Actually, a couple of follow up questions here because you're using data bricks, Nathan. So there's this one part that is not very clear to me. Like databricks allows you to spin up your clusters where you can run your back end offline jobs, right? Like model training, data processing, et cetera. Is it always running on a spark cluster running on the back end? Or it can be a smaller non spark cluster as well. Do you know about that? Both for most of our ML projects, we do run a spark cluster, but I think we do have, like, again, our analysts, the clusters that they're using for SQL queries and stuff like that, that I don't think is a Spark cluster. I think we do use both. Got it. I see one thing. This is something so I've used Spark and Data Breaks, but not very extensively. So something that can only come from practice. If you were to notice this, or if you were to ask this amongst the team members that do they end up using a Spark cluster frequently enough where the job does not really need Spark? Right. Like the job itself, if it's not optimized for Spark, using a Spark cluster actually would not add any value. And I know that it increases the cost quite a bit. So one of the things that I've heard from some other users of Databricks is database ends up becoming very expensive for this particular reason. So it would be really helpful if you can check with your team and maybe sometime we can connect with you and ask this feedback. Sure. So I know that it used to be a problem, and it used to be a problem also because you're right. Before we moved to Data breaks, most of our notebooks and stuff were pure python. Like they were not PY, Spark or anything. So when we tried using the same notebooks or minor modifications of them and started running on Data breaks, they would take either the same or even sometimes longer. Right. And you're right that we would be spending a lot of money over there, but not actually getting any benefit. So I think for a large part, all of our models and all of our notebooks have now been converted to actually use Spark and Spark functionality and pipe bugs. But again, that's where I'm saying that this is largely on the machine learning and the data science side, right? Or our regular dashboards and the SQL, I think they don't use parts equal or anything like that for them. We use smaller clusters also. And that's more of literally using it as a dashboarding tool where we have data tables and then we are just running SQL queries on top of it and creating visualizations that can be shared with the business stakeholders. Understood? Okay. And while before we get into deployment, which is a longer conversation probably, are you guys using anything for model monitoring at all right now? Again, some things that we haven't internally built, so it's like it's not. So I think that to be honest, model monitoring and model governance are our Achilles heel right now. That's where we feel like we need to do the most and we need to either create processes and systems around it, or we need to again, leverage external tools which are available. When I was with FICO, I know we had we used to have this thing called Decision Central. It used to be earlier called Model Central, but then FICO is all about decisioning and not just. Models. So they converted to decision central which is all about monitoring and governance of models. Again, we still think that our repository of models is something that we can manage right now. So we have model monitoring in place largely, which has been built by the data scientists themselves. Like every time they build a model, it's important for them to set up the monitoring and all that. But in terms of model monitoring and governance, that's where we lack systems and processes quite a bit. Got you. Understood. Cool. So then basically we would love to hear a little bit on the deployment side within whatever context you have. And obviously we can then do a more detailed call with the ML engineering team later on. But in terms of the context, you have, like, Cassie processes there like, once the data scientists build the model, what is the overall flow generally. For anything which is bad? We basically deploy it within we create processes within data bricks so that every month it will just pick up the data that it needs and it will work. So there we don't usually work very like we don't need a lot of help with data engineers or ML objects because it's more like we can spin up a notebook, we can schedule it, all of that. Right? So that's something that is usually handled internally within the data science team. Anything which requires real time intervention or real time decisioning. That's where what we do is we work closely with our data engineering and MLS team and then they have, for example, on the risk side, right. The two things that are running are the underwriting model and within the underwriting model there are a lot of new answers and all that, right? So there's not one model, one underwriting model, there's like literally six or seven models which depending on which cohort and which profile type you are coming from, that model that is called. And then there is the fraud decision engine which is running in real time. So for both of them, they essentially have a risk service and a fraud service. Okay? And whatever we work, they basically put it up into that service and that service gets called at the appropriate events. This service is like you mentioned, AWS. But do you know if they use EKS? Is it like the raw machines? And you said these are multiple models. So within the same service, I'm guessing there are multiple models and depending on the cohort that is being loaded dynamically. Basically the parameters that get passed into the service accordingly, it will generate everything that it needs to generate in password path. Nicuji, I saw your question about ML flow. I think that's something that the MLS team is looking into. They are exploring ML flow. But honestly speaking, again, my hand is a little tight around model deployment and ML of functionality. I think that's pretty much it. I think get a good sense of the overall thing. Model deployment would definitely love to dive a little bit more deeper job best relevant person. If you can also kind of connect us to them and maybe if we can do a joint call that will be super helpful from our context. Actually humble part two after testing model train guidance, okay, I'll give you a high level context. We first deploy on your cloud whatever you are using like say AWS, et cetera. On top of that, just say upkeep data science team, use the different resources. So within data science team you can allocate resources very easily from the interface. So there is the control of resources limited, cost limitedness and never it will exceed that amount. And within that, like when they are training, they can easily in that provisioned workspace, they can do their training experiments and quick metrics, they can compare that or get CEO, they can then go and push it to deployment. So they can push it to like a test environment themselves. If they like it, they can then promote it to the deployment environment. If they want to directly roll it out to deployment, they can do it. And we are adding functionalities for rolling out to like a small percentage of users, cake percent users release Kirby and those kind of things. And then from an overall like the intra team or the engineering team, there's an entire view of all the models that is being deployed on the service may deploy, concert service, cost consumed, all of that view is there. And as the deployment happens, there is a monitoring that is there which is built as a separate layer. But like companies have been using both deployment and monitoring and there are some companies that are using just the monitoring wherein for the model that is being built, you can just get access to the monitoring, data drift and so on. May I love to kind of dive a little bit more into the monitoring case as well. And again deployment well, apart from Samajmaya. And then love to kind of see possibility of wherein if the system or based on your use case, if there is some way we can potentially even work with you and help in whatever problems you are facing, it will help us understand at the same time, potentially we can add value. So direction I wanted to love to kind of take it ahead in that direction. Sure it does. So give me some time. I think most of a lot of the engineering folks are out until we have next week. So give me some time. I mean the week of 8th January. Let me connect with Vinos and then I'll shoot you a message in terms of what would be a good time to connect, what would be the right team to connect, because my guess is that even within Vinod's team, it will be somebody specific who would be a better fit rather than Vinhos himself. Right? Who would have the details? Let me do that. So I'll get back to you by let's say 9th or 10 January with. Some more and monitoring discussion. Like you would be the best person. I involved, somebody from my team actually people who are having issues with monitoring. But again, I think it should be a joint conversation because last I spoke to our MLPs and data engineering team they are also going to put things like model monitoring and model governance frameworks within the heroic mass. So they will build out certain monitoring features and governance which will make it easier for us. Because I think the biggest gap right now is that sometimes because between the model and the deployment piece we usually have another step which is the policy piece. Because the decisions like model is one aspect of it. But then there is there are decisions of the model that are output of the model that has to be consumed by a credit policy and then credit policy is the one that eventually determines what decision is being taken. Right? So between the model getting billed and it getting deployed, the policy building part takes a little bit of time. So what happens? The biggest gap for us is that if the model was immediately getting deployed, setting up that monitoring was not that much of a problem. But monitoring then has to be set up when model plus policy is going live. And at that point usually the people who built the model, they are already in a different world working on something else and they don't want to go back and spend a lot of time thinking what basically there is a context that has gotten broken and all that, right? That's why even our ML team is ML Ops team and the data engineering team are thinking of building something around modern monitoring and modeling. Okay. So I think it might be a joined conversation between somebody. Fair enough, that sounds good. So basically I just wanted to love to take your help in whatever way is possible. That is the main thing. I can coordinate with you and if possible love to meet in person. I am in Banglor. I believe next week. You are not taking off, I hope. Today and tomorrow as sort of like a soft off simply because the fewer people in the office and we have some casualties that lapse at the end of the year anyways, the calendar may leave out of them. But if there's any meetings, I'll always ticket. Awesome. I've taken the numbers. So actually my wife works for Qualcomm Center. It's 10 km both ways for us. So no other place would have worked for us. We actually have an apartment in Banner Road because that would be like too far for her every day. She would be ready to go back to us. Whenever you are next week or so. And then yeah, we can connect with the remaining team. So I'm coming to banglad on Jan fourth. Normally I'm based out of the Bay Area. We are in the Bay area. It must be very. In India. That's why I'll be coming to Bangalore next week. But maybe I'll go back to the Bay Area. Nice. Happy week is in Bangalore. Great to meet you guys. Great to meet. Thanks for the time. And all the details as well. It's very helpful, actually, to understand the entire flow from a fintech perspective in detail. That was super useful. Yeah. Thanks so much. Absolutely. Anytime. Thank you. Awesome. Happy to meet. Bye.