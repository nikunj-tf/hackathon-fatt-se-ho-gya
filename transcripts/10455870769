That's the other option. Okay. Which is not funding it, but basically. Hello. Hey, shame. How are you doing? Good, how are you? Yeah, we are good. Where are you based? I'm currently in San Francisco. Oh, nice. I'm also in San Francisco. Then we should probably catch up in person sometime. Yeah. Where Pin? San Francisco. I'm on Market Street, market and 10th. Okay, I see. So I'm in Inner Mission around Harrison and 20th. Practically walking distance from each other then. Practically. It's a little bit longer than walking, but yeah, it's not far at all. Yeah, I guess it always depends. Yeah. I had talked to Abishaid before, but I believe I haven't met any one of you. So we can probably just get a little bit of introduction. Also done. Yeah, sure. Let me briefly introduce myself. Ashim. Actually, I work closely with Abishek. Different places. Like, we did our undergrad together, actually on Ragabishek and I. All three of us did our undergrad together. Both that I came to the US, I moved to the US for my masters here. I went to UC Berkeley and since then I've mostly been working in the machine learning field. I worked at an early stage startup called Reflection where we built out recommended systems for the ecommerce industry. And then I worked at Facebook. I don't think we overlapped at Facebook. Which years were you in? I was 13 to 16. Okay. So I don't think you overlap with Facebook. I think you would have overlapped quite a bit with Vichek though. Yeah, I did. And with Vichek I actually have a bunch. So you all are from the 2009 to 13 batch, right? Right, yeah. Were you all in the same department? No, different department. Bishop was computer science and rogue electrical engineering. And I'm from instrumentation. If you're from instrumentation, you might know my brother in law, Sayak. Yes, I know Sayak. He's one year junior to me. I think one or two years junior to me. Oh, no, he's a year junior, 2010 to 14. Yeah, he's also in the US. Right. He's doing his PhD. No. He finished his PhD. He did a postdoc at MIT and now he works at Johnson and Johnson. He's pin San Diego right now. Oh, nice. Awesome. Yeah. I think Fact was also pin our hall archive, probably, if I remember correctly. Yeah, he was in the same branch. So maybe they all are in the same call also based on the branch. Yeah, I don't think that the latter part is true, but it just so happens that I think Saik was also in the same hostel. Yeah. So that's what I was like when I spoke to Abishik. I remember I mentioned that they went to the same school. I think Aubishik also knew him from Buddha. They were not in the same branch. Got it. Makes sense. After that, I worked at Facebook. I worked from 2018 to 2019 time frame where I was looking conversational AI. So if you have heard of Portal, I basically serve two product services, messenger and Portal. Like Pin, the models that are built out that's a story at Facebook and after Facebook like the three of us and Raga Michigan and I got together and we did our first status together which got acquired by Info H and now this is our second gig together. So good thing that getting to build the startup with my other friends I guess. Yeah. And so you worked on Portal and messenger. Portal is now discontinued based on what. I've heard from it's continued in the sense that they are now consumer product discontinue, they are going to move it towards future of work. Towards what future of work? Future of work only basically like use cases. That's where I think the more application. Of Portal I see and messenger. So you said you work on recommendation systems system. My work was in the previous startup where I was serving the ecommerce industry with personalization algorithms and all at Facebook it was more conversational AI. So generally an Lu based model. I see. Okay. Yeah. That's very interesting. So you are very familiar with the challenges that ML very familiar with that. With the challenges that ML engineers face. Yes sir. I've experienced this from a status lens whenever the Reflection so there I got a chance to build out the horizontal platform for the company and honestly we were super proud of what we had built out at Reflection until I joined Facebook and I noticed all the capabilities that every learner and the predictor system have and I was like okay. Clearly the startup ecosystem is like way behind what these large companies are. I'm sure you would have seen similar thing at Pinterest. When I interviewed at Pinterest like I was also quite impressed by the internal system that Pinterest had also built out. Yeah, Pinterest is pretty advanced actually and that's all that they had done. Pinterest. Everything is ML. They haven't invested on anything else. Right, so the ML systems are pretty advanced compared to where I'm right now upper which is more of a start up. They are much more nascent and trying to build out very basic can. I think most of the status space is actually catching up with that kind of stuff. Cool background. Come from small data down. And then I spent again my bachelor at It correct electrical engineering. During that time started a startup in the education space. That didn't go really well, but it was a good learning experience. Both that spend majority of my time working with a hedge fund called World Fund. It's an asset management firm based out of US. I was initially on the quant side building trading strategies and then spend like three to four years between US and Singapore as a portfolio manager trying to actually trade a portfolio of a good size at Goldfind. And then I was also a member of the CEO office there working closely with the CEO on various strategic initiatives for the growth of Goals fund. During that time I also started investing into startups myself through various indicates in angel list and all. It was a good learning experience and then I wanted to build something myself and then continue on the journey with India. Yeah, very interesting. So are you based in the US or in India right now? I'm in India, actually assumed right now. How big is true from how many people? We can give like a high level overview of proof boundary. Right now we have a team of around 15 members. Most of it is engineering. The engineering and the business team is primarily based out of India. Nikon is Pin US and we have another spoke in the Paris area, but apart from that, most of us are in India. I see, okay. Yeah. And I'm happy to share my background. Also already seen it, but we'd love to hear a little bit in terms of. I'm not too senior to you guys. I graduated in 2012. I did my engineering from VIP University. Then I got into Facebook. Then I think I overlapped a little bit over there. I did not know him that well, but then I got connected through an Indian investment syndicate where like the Facebook alumni and he was also there and he saw that I'm working Pin, the machine learning space. So we chatted a little bit. I connected him to one of the engine at Pinterest to get some context on what individual ICS go through. So my journey has been mostly like data science for most of my career. At Pinterest though, we evolved it from data science to machine learning, which is where I had my first management experience. And now I've joined up work as director of English and Cognition learning parallel. I was also doing my executive MBA from Wharton which ended this summer. So I graduated in August. Yeah, so working in the ML space, I also do angel investing, but you should not consider me like a big annual investor. Right? I typically invest around 5200 and that is also spread across multiple small startups. But then I enjoy the space, I enjoy talking to status. I tried working on my startup also where we were trying to build like a video aggregator that could recommend videos based on your mood. So if you are looking for funny videos, you could get videos from YouTube, from TikTok and in one app you can see all the videos together. If you want something expiring videos. One of the challenges was Apple App Store would not approve our app because of content aggregation and they were like, who takes the responsibility of the content if it is from YouTube? If there is something like some brand comes and says that this is their personal and then they don't want to issue it responsible. But Android did accept so we ran it for a little bit on Android. But then we realized that this is going to be a very big uphill battle. So we reduce investment on that and started focusing on just the recommendation and thinking of like can we build the recommended as a service type of product. We did consulting with I don't know if you guys are aware of Trell experiences. Trell app they call it the brand brand like video pinterest for India. So they were one of our clients and there we had consulted them to like they would recommend videos based on like a human civic. They had a group of viewers who would sit down and say that for every name in Mumbai, this is the lead that they will see on that way. And like that they do for many different demography. So we consented them to get off that. And that we did pretty well. And then barely upwork reached out. Also working on the same recommendation system space. But here we are doing recommendation which is like much more high stake trying to recommend. And once you hire, what kind of work can you get done from there? Nice. And for the Gmail up to the recommendation system that you're building, was it like a product kind of a thing assume and this work like actually kind of acquire this? Or was it more like the transition. To upwork did not acquire recommendation as a service. They just hired me. We don't really have that kind of large scale product yet. This is something that we are continuing. So the startup does exist. We are incorporated and everything. So we keep building it. The thing is one of our co founders is having a baby and that's why we kind of reduced and we know that this is not a good time to go and get this refunding because the market is to be tied. So we thought just try to manage it for this year, next year maybe and then come back with a fresher and better product and then think about it. So yeah, that's where we are and that's kind of my background. Thank you for sitting here. Basically that's really nice to know. I'll tell you some more context. Like basically we karen trying to have a few folks who in the Data science space pin the ML space who actually could be part of the small engine that we are doing. That way we are able to leverage their health during the journey and we are able to take feedback from them as needed. And we karen also able to kind of learn new things from them based on whatever they are doing at the organizations and contribute in the right way. So that was the goal. I think we found that you were connected, so we thought we'll reach out. Yeah. Tell me, what are you trying to raise? You said this is more of an advisory type of role. Also what do you expect from someone to like how do you expect to work with someone who is investing also in your product? Basically, literally, this is not really a round round. So we are not doing like we had raised around September last year with sequoia and a few injuries. And then we were including like few of our friends, few other folks who could be useful to us. So just doing a small safe with a 15% discount to the next round and a cap. And the goal was really if we can have a network of CPOs and network of records of engineering and people who have been in the ML space, the idea was really key. As we build our product, one of the things we do expect is we are able to showcase the product to get critical feedback instantly when before launching it to other folks. That was 1 second is being able to leverage your network itself in terms of the data science, like heads of data science or heads of ML in other organizations that could be connected to kind of maybe provide an entry into these organizations. Similarly pin organization that you are working in, if there's a need like being able to be aware of that and be able to kind of see if we can contribute there and being an evangelist for us in this site. Yeah. So tell me a little bit more. Last time I spoke to Viet, he told me that they were trying to do the entire and then they realized that most of the work in hiring is actually not so much ML, it's more like people managing it. But then what they realized is that there is an ML problem to be solved generally across the company. So they faced a lot of challenges and they decided to build a company around giving ML as a solution. But what specific things that you Karen, providing that I think you're saying that we are in the stage of research and understanding the problem. Okay, this was maybe last year sometime. Yeah. And that's when I connected him to some of the engineers he could learn. So I don't know where your product is right now. What are the exact options? Yeah. So basically to say what we are trying to do, we are mainly focused on building a developer friendly layer for the deployment of machine learning models. Today, anyone is building ML models. While the build out has still been assisted by a lot of platforms and models available over the internet. Once you have built out the models, what we have seen is data scientists in themselves like this will set to productionize models or create entrypoints of the models. They have to depend on machine learning engineers and then they want people to be able to do that. In companies like hiring ML engineers has been tough. And every company that's trying to hire ML engineers takes a long time to hire them then the ML engineers works to kind of get these models into production. It's a slightly longer process. Our goals was can we make it very simple so that ML developers themselves can actually go to the route of actually creating entrypoints of the models and if at least sending out a deployment request which can then be approved by the DevOps. What are you exactly. So Sage Maker entrypoints is something that exists in AWS, right? Yeah. Are you building something on top of that or you utilizing that so that people don't have to manage these end points? Yes. So right now we have not built on top of Sage Maker, we have built on top of Cuban. It is for example if you look at the workload, the workflow is something like this someone from the team can easily connect to your Kubernetes cluster and you can provision like a resource allocation for a team or for individuals to work on. That part happens very easily to the platform. Once that is done, then you can actually deploy like a real time model or a batch model easily again either through the UI or through the CLI or through the Jupyter notebook. That is through one single command. The center of time is very small. You won't need to go through like weeks to learn what is happening. You can just do it in 30 minutes because it kind of automatically containerizes. It automatically creates an end point and along with the end point, once you have done that, you will get access to the system monitoring both that are automatically created over. You don't have to provision a Cafana instance, it automatically creates that for you and at the same time you get basic ML monitoring out of the box. So this is like roughly the workflow. I see right now. While we do not necessarily expect data scientists to productionize because in most companies they do not productionize models, but ML engineers spend the time to build the platform. Our goal is they don't build this platform but they adopt ours and then they are able to build on top. And we have tried to make it in a way that it does not feel like a closed black box. So you can actually just build on top of it and you get access to the Raw Kubernetes cluster as well. And you can actually just edit like make things on top of that and everything in the system is API driven. I see. So do you have any demo or anything that could be shown? Yes, we do have things. We can show the demo at the end of it. We can show you the demo. So we use all of these things. We use we use graphener for monitoring. But you're saying that you're integrating all of them together. I want to see how that works. Actually I was very curious to know about the upworks like ML. Yeah. While you're loading the demo, I can give you a high level overview. So right now upwork's main serving happens through Solar Cloud. I don't know how much you know about it. It's like a middle layer, like AWS lambda. So everything that we are serving is through that. Now Solar Cloud is calling ML Services and the way the ML services work is they are connected to something called a feature store which we are using AWS feature and we have a model there within that recommendation service or whatever service we Karen using currently these models are like these are like pickle files that you manually put into the API. We are trying to go away from it and maintain an end point for that. Sorry, just once again Ashim, you mentioned who puts these pickle files and where. So we are changing a little bit. So right now an ML engineer actually creates the optical file and then gives it to the back end engineer and then they put it on Solar Cloud so that you can use it. But we are moving away from it. We have realized that this is like an additional step. We want the MLS to actually manage the entrypointtype video in Solar Cloud, not to have any models of their own. Solar Cloud will just call API and the model should be managed through some sort of AWS endpoint. So it calls that endpoint, gives some input and gets output from there and then serves it through Solar Cloud. So you're changing it. But currently what happens is ML just gives a pickle file to the back end engine and they just manually put the people file. They convert it into a Java file actually and then we put it into Sobberg log and then that model file basically talks to a feature store to do like real time infrastructure. And this feature store is then managed through some sort of a Kubernetes cluster or something, where you are continuously computing for whatever. We are trying to predict on what does the future look like at this point in time. We are continuously computing them. So as soon as the file from Solar Cloud calls the feature store, it can give you the latest standing of the feature and then it gets it back. Then since you have currently you have the model file within Solar Cloud so it just computes and takes the conference link, then sells it on. But these are all things that we would want to change. We have a grafana monitoring of this service itself, like on the Solar Cloud, the service that we are using to manage these connections, we have a grafana monitoring for that. But that is also something that someone has to set up. It's not like automatically if you tell Ups that you'll have a solution where we just write a few lines of code and we get all of this for free. Yeah, so that's the model serving part and then model training part happens completely separately. That happens completely offline through database platform where we just like we have daily jobs, that is taking features from an offline store, not online store, that it is for serving from an engine store, and just computes the features and then train the model. We define the model once in a while to kind of find out the right parameter. But then once that is defined, it is like continuously trained using an offline set. And then whenever we see that this model is kind of ready for us to be sent out to the back end, we output the pickle file. Ashame, one question about this pickle file thing that you mentioned. Usually the transfer of pickle files works out very well if there aren't many pre processing, post processing steps, right? Like if everything is wrapped within your model's predict function, then if you just transfer the pickup file, it works. But sometimes it's like this entire logic, right, that you have to do some transformations to the input before it goes to the model, et cetera. How is that logic handled here? So the feature store basically does that, right? So let's take an example. Suppose you want to take an average action taken in the last 24 hours, so you get each action from the UI, but then the future store is computing it, taking an average of it, and then the feature store is then feeding it to the store. The feature store kind of manages the transformation, but I think that does not do the job. What you Karen saying is probably we need to figure out better ways to do it because there are still limitations to it with feature engineering rights. So sometimes after feature engineering, we might want to do something more. And then. Do you use any external features or have you guys built it up into. We have evaluated Tekton. Karen, you aware of Tektom? Yeah, for sure. So Tekton provides what we found out from our evaluation is that Tekton provides a lot of features and everything, and it integrates pretty well with AWS, whatever you want to use. But they have way too many features that we don't really need and their pricing is not based on your use. They kind of like price you for everything together. And then we kind of could not find out a good way to use them so that we get like a pricing advantage based on what we want to use. So then what we are now migrating towards is just leveraging AWS. I see. So the reason I asked this question. Ashim. I don't know if you already have this data off the top of your head. But generally. In my experience. What has happened is even companies who heavily end up using feature stores. It rarely happens that all the features that you. Karen. Passing through the model actually come from a feature store because developers very frequently need these two extra features that they did not have time to publish to feature store and use it via an ID. Like referred to in the model via an ID. Basically. So the coverage of features actually getting used in the model coming from feature store is almost never 100% basically. That has been my experience. I don't know if at upwork this challenge has been solved that I don't know if you have maybe you have a protocol or something that you don't allow using non feature stored features or something. Currently we don't allow directly for the model computation, but you can use certain things like simple filtering logic, right? So before you're going into the index so this feature computation and model scoring happens for, let's say at the ranking stage when you are doing the recommendation, right, but before you pass it to the ranking stage, you might want to do some simple filtering. You don't want to pass every possible content for a user, right? So there you may not use the features there like pin the right custom logic that I want to query my database based on these filters and then send it to the ranking. That happens. But in the ranking stage we have currently limited it. We said that don't use anything if you haven't published it in the future store. Got it. I see people generally haven't broken the rule, but I can totally imagine a world where engineers, if they want to do it and they feel like they're too motivated by it, they'll do it anyways. Yeah. I remember we used to do this very frequently at Facebook. Like Facebook obviously has like a recently mature feature store, but we were like, okay, we don't want to publish it right now, we'll just write some code and get a few additional features. Yeah, and that mostly happens because your infra team is not supporting you enough, right? So our features, the way it works is we build all this model offline and then we tell the infertility that, hey, these are the features that we want to use for online survey. And then they tell us like, this is my directory where you can publish it, okay. And then they only allow us to publish it in staging and they do the migration to fraud. So sometimes they take more time, sometimes they'll be like, oh, I need like two weeks. And then the Gmail engine is probably thinking that if I don't get this experiment started, then in my performance evaluation I can't write anything. So then they will figure out other ways to do it. Yes, that is true. That is true. One question Ashim, in the current approach that you described, like, how long does it take to I know that you're relatively new at upper also, but generally if you remember like launching a new model versus updating a model, like maybe updating features or something in a model, approximately how long do either of those. Two steps take fairly long time if it's like a fresh model update. Right. Because if the features change completely and you're transforming them, use them very differently because there you're not only building the model, they're also like transforming the features and making sure that everything works. So it turns out to be a quarter worth of project at least. So quarter to half a year. Okay. But if it is like one of the things that we are doing right now is there's one feature which was coding the skills that freelancers have on Upwork based on an algorithm that was built, pin 2017 2018, and they had updated recently and they want us to use it across all. So that is like we are using the same feature and it looks exactly the same, but then it is computed slightly differently. The scores are different. So that transition. We had to use that feature to retrain the model because it's a new data, but the model definition did not change too much because it's just a new feature. Like few different adjustments in the weights. That one took about two months to get the experiment started. And then for up work, we typically need slightly longer than usual companies. We have to run experiments for like three to five weeks at least. Statistically significant improvements because the traffic volume low volume, right? Like the nature of the business is low volume. Yeah, it's not like a content platform where you have everyone. So it takes a little longer. But yeah, the development, if I just take the time to put it in the experiment took about two to three days. So I will say and each pin is like two weeks. Got it? Got it. Understood. Okay. This is very, very helpful to know. Thanks. Thank you so much, Ashame. So, yeah, if you can do things that can change this, like you can do it in a day instead of three weeks, you know that this is a great feature to try out. Just click a button and it does everything for you. That is right. While you're saying that you're giving opportunity for people to also not think of it as a black box. If you want, you can do this one and a half month of project also with everything. Or if you don't want it, you can just give it in two days. Yeah, right. You replayed our pitch to us. Hashim, thank you so much. Yeah. I am actually curious. You have talked to a bunch of other companies too. What do they usually say? How long will it take them? Yeah, it's very similar to the timelines that you mentioned, Ashim. We have heard companies that have invested a lot of time in optimizing their infrared and everything ended up taking in the order of a week to a few weeks. And companies that are in that process typically end up taking multiple months, as you described as well. But hearing. Multiple months is very common to us now. It was surprising earlier, but now we hear this very frequently. Literally, this is the value prop that we go to those companies with that for your simple, like 80% of your use cases are relatively like of most companies, use cases are relatively straightforward where something kind of out of the box will work out with minor tweaks and all. And those use case, we can reduce the time to hours instead of weeks. Like literally hours instead of weeks. Yeah, like the use case that I just mentioned, it is the same model. You just update the feature to a new version of the feature. Exactly. That should not take like one and a half months to it should not stop it. So if that can be done within one week, that is amazing. Mostly that's what is happening most of the time actually. You're not like rethinking the modeling, completely changing the feature a little bit, or you suddenly heard of a new signal that is available. Right. Most of the machine learning development is incremental improvements, right? Like you try new features, try new parameters, different algorithms, and also it makes complete sense. And if you can optimize just that one part of the incremental updates to the model, you save so much bandwidth of the team that they can then actually start thinking of rethinking and building new models or serving new use cases basically. So completely makes sense. That's what we are building out the platform pretty much for. And then the second thing is, of course, when the 20% of the use cases which are like more complex, something out of the box is really hard to work. The way we have built out the platform is like we pretty much expose the bare bones of the platform to the user, to the end developer. There is like literally zero limit that we impose on the user. Like we actually expose entire Kubernetes cube catalog to the users. That if you wanted to do anything on top of Kubernetes, you can do that. But ideally, in most cases, our abstractions work out for you and you can move much faster than you can operating raw Kubernetes, essentially. So that's kind of how the design philosophy of the platform, particularly. Yeah. Now I'm very excited. I want to see the demo if you have it handy. Let me actually show you a little bit of the demo here. First, let me set up some context for the demo itself. And by the way, I just checked with our team and they mentioned that they are doing some updates to the platform itself by deleting some roles in the database and stuff. So it's going to be a little bit slow, but I think we'll still be able to see the demo overall. So let me set some context here. If you think about the overall workflow for a machine learning developer, right. They would spend some time doing your data and feature engineering, right? Then after that, they're like, here's the algorithm that I want to use. Here's the model that I want to build. Once they built out the model, they can do two things. They will likely do two things like. Either set up like a retraining loop of the model itself that okay, at certain cadence the model is going to retrain, or the second thing is like, okay, this model I want to deploy. Right? And the deployment itself can mean, again, two things to people that either they can deploy that as an API endpoint, that the model itself is a service expose as an API entrypoints, or like the model inferences are just dumped to a database and some other service will pick it up from there. Those are the two ways that people typically deploy them. And then after that you could set up monitoring systems and all that you want, right? That's the general workflow for people. So in this demo today, just give me 1 second. So in this demo today, generally, we try to cover as truefoundry, is we focus on the deployment, pieces of it, okay? Okay. So in terms of the deployment, when you think about it, the model itself is built out for you, okay? And then either you can take like a batch deployment where you run something once, dump the results to database, or you can do like a real time deployment where you quickly create the model as a docker container and expose that as an API endpoint. Those are the two things that you can do, right? And that's where the demo would typically start, where you can manage all of this in one place. That's the core problem that we are trying to solve parth truefoundry. In the context of our discussion, this would basically mean that if somebody updated a feature, right, they could launch that feature change in like 1520 minutes, so long as it's not like a completely crazy new data pipeline that they're writing, essentially, right? So that's that. And if they're building out a new model, they could typically launch that entire model without worrying about infrastructure. If they wanted to do it fast, if they wanted to worry about infrastructure and keep things super organized with CI CD pipes and all, it would not be like 15 minutes, but it would still be in the order of hours and not in the order of days or weeks, essentially. That's kind of the overall theme of the demo. Does that make sense? Yeah. Okay. And by the way, we actually don't have a lot of time in this call today. So what I'm going to do is I'm going to just briefly show you maybe in five minutes some of the capabilities of the platform, and then we can do a deep dive using like a 30 minutes demo specifically for some recommendation systems and all, which you mentioned as a use case. To do a different type of does that work for you? Yeah. So I'll just show you a few things here. Can you see my screen? Yes. Cool. So the way you start is I mentioned that the focus itself is on deployment. And deployment itself can be of a couple of different types. Like you could have like your jobs, which could be like your batch inference that you are doing, right. And you could have your services, which is like your API entrypoints that you might have exposed. Right. And you're able to track all of that in one place, essentially. Now, a job is like a one time thing, right? Like you run it, it runs, and then it can get over. So if you wanted to retrieger a certain job, that okay, I want to get new inferences of my model. We simply go and hit a trigger here and it will be triggered. This job once, so spin up a Kubernetes cluster, run this job once, and once it is finished, it will kill the cluster, essentially. Right. You don't end up incurring cost here. And then the second thing you can do is obviously you can set up a schedule that every day at 09:00 a.m. Run this job, it will spin up the machine, run the job and kill the machine after that. Right. And in terms of the API deployment, you can basically see all the entrypoints of the APIs are available here. So you can actually go track your hello world examples here. That's what you have set up. And as you can see that this is hosted. So this is like actually shareable with your engineering team. And everything that we do pretty much comes with your versioning and stuff. So you can actually see different versions of the deployment. So here it is, only one version, but if you wanted, you could have multiple versions listed and each of them comes with their own logs and stuff, essentially. Right. So that's kind of how the platform itself showcases you these things. And then there are a lot of other capabilities that right now I'm not going into. For example, how do you manage your infrastructure using workspaces? You have all these workspaces. How do you connect your own clusters and all? How do you do your data monitoring and stuff? Once you put a model pin production? So I'm not going to what kind of cloud info do you support? Can I be using AWS, GCP, or. What kind of we support all the three major clouds AWS, Azure, and GCP. And AWS is like our strongest suit, but we support GCP and Azure as well. And the only limitation right now is that we work off of Kubernetes. I see. And so any company that is using Kubernetes for containerizing their services, then you will have a solution for them, either. Using Kubernetes or wanting to use Kubernetes or not caring that underneath the platform is using Kubernetes so for example, if you were using AWS Serverless, you actually don't care like what is the underlying infrastructure that AWS Serverless is using. You care about your endpoint. So long as the company meets either of the three requirements, it works. Yeah. So we are kind of more into that serverless kind of environment, so we don't really care about where it is running. Okay, so if I say that we have many models, many services and really heavy duty models, like thousands of features, that kind of thing. So how long do you think a team like upper sanitation required to do? How long will it take for a migration? Like if we wanted to use your thing internally, we have a service called internally called Taxi, which does this kind of manages all the APIs, but it is not really built for machine learning. It manages all kinds of KPIs that we have. But if we want to just take the ML ones and put it in, how long do you think it will take? Right? So I don't know. Honestly, it's very hard for me to give a number right now because we really have not spent enough time understanding your underlying infrastructure models complexities. Can you hear me okay? Yeah, I can. So I think that part is very hard for me to give a concrete number right now. But I'll give you an anecdotal example from a couple of other customers that we are working with. So there is this large healthcare company that we are working with and they had like not a lot but a few different types of machine learning models in production. Like for example time series models in production, some computer vision models in production. Right. The way our system is designed is we let people write Python Code as is. Oh by the way, Python Code is like somebody would write like a training or infringe job in Python Code and then they write like a small layer on top of it and that allows them to deploy something on top of Kubernetes. So like to them it took approximately two working meetings of 1 hour each with us and then about three to 4 hours of work internally from their side to migrate and test out the models. Now at this point, so roughly you would say a model migration, the first model migration took them approximately a day and the second model took them a little less than a day, maybe like half a day worth of work approximately. Right. And I imagine that this becomes lower as we move forward. But then devil is not a detail to answer questions, so we cannot actually get it. And how large, how many clients do you have? How large are they? Clients in the sense that we're working with a bunch of design partners currently. So we actually call people design partners and not like customer customer right now because we karen also fairly early right now. And we want to work more in a partnership arrangement as opposed to vendor arrangement. So for example, like the Use case that you mentioned where how long will it take for migration? Well I don't know how long will it take for migration but because we are working as partners we will actually help you migrate. So that's like the advantage that our current customers are getting when they work with us and we build out also sometimes you need custom integration. So some people are like oh I do this thing on Sage Maker, like my data is already on Sage Maker. Can your monitoring system integrate with that? Yes, we can build that out actually. So that's the kind of arrangement that we're doing with these customers. We are working with six companies currently and a few of them are like really large, like very large enterprises as well. Like a couple of them are very large enterprises, a couple of them are relatively smaller companies, a couple of them are mid size startups so they are. Not like the Google and Facebook kind of companies who have large tech teams. They are more companies which are big but don't have too much tech. We have both the large enterprises. The two large enterprises we're working with would put fall in the segment that you mentioned that they are large but they don't have like a huge machine learning team size. The mid size startup that we are working with are fairly advanced I would say. They have PhDs pin their machine learning team who are churning out many models and they want to optimize their developer time and model productivity. So that's what they are using. Yeah, no this is interesting. And then you mentioned about funding also you're trying to raise a safe. What kind of funding are you looking at? Like how much do you expect and what do you expect from an angel at this stage? Pin terms of the amount. To be honest, there is no bar as in we have few folks who have done as small as like five k through this syndicate as well. That is not the main thing. Like money is not the main requirement is mainly to be able to help us in the journey. So for example with angels major things that we are looking at either introductions to their network data science teams where we can like a stronger introduction to kind of actually get a few early customers to work on. Like as I said. We have a few design partners but ideally we would like to have a few more customers that we would want to work with at this stage given this version of the product that they can sold at a very high level like this we just launched like a few weeks back and we want also apart from getting introductions. The second thing is if they are a part of the team. Like in your case for example. Work in itself. You have a good data science team and this could potentially be a use case. Obviously we'll have to dive deep into it, but then giving us a chance to kind of either do a POC or kind of actually test it out and see if it works for you, something like that. Those kind of things are majorly. What we are looking at from angels, not really a dollar amount or something that really matters at this point. Like one message that we tell every angel is if you help us convert one customer, you have served like the biggest use case that we want to partner partner with you. Essentially this recommendation as a service. Right. I have a couple of friends who started companies which are running these sort of things. One of them is actually Exfix. You can look up shape AI. Oh, I know. Shape AI. Yeah. Do you know them and have you reached out to them at all? We have not talked to them. We have not reached out to them. But generally I'm interested like they're vic company, right? They're a vic company. So are you looking for introduction of that sort, like who are working in their space as a service? Shape is just particularly interesting because I was just curious to speak with them about their go to market and all. But right now I think what an rogue mentioned is not an introduction of that sort. The kind of introduction that we're really looking for is our companies who can potentially benefit from using our platform. And I don't think Shape AI is one of those. Shape AI is actually building a similar platform but focus on recommendations. So for example, if we were speaking with you, ashamed, the best outcome from our perspective would be that let's say we work together with upwork, we help upwork optimize the internal ML of platform, the developer productivity and stuff like that. And in that journey we get a chance to improve our platform internal platform as well. So that's like the best outcome. And that's what a lot of our engineers are doing that they're like, at least my company, I'm going to get a thing going. So for example, if you know Rubric, like Rubric co founder is also angel investing and he's like, I'm going to try to get you Rubric as a customer, basically. So that's the thing that yeah, so. That I won't guarantee. Nobody guarantees that. Yes, because you cannot. Right. This is kind of a hiring process. But what I can definitely guarantee is a call with the director of infrastructure. Infrastructure and other things that I can also tell you is that developer productivity is one of the goals that we have for next year. And we said that we want to reduce the time. We want to go from number of experiments run and increase it by at least 15% to 20% by the end of next year. So this year we are running ten experiments a year. Then we want to make it like twelve or 13 by next year. Like ML experiments basically these are like online experiments? Yeah. After you have deployed a model every time we decide that this model is in production is through A B testing and one of the way we evaluate developer productivity is how many AB tests will be able to run. Interesting. So one of our goals, these are not yet finalized like we, Karen, doing like review with the CEO and everything we want to say this because we know that developers are spending a lot of time trying to figure out how to deploy it, how to work across multiple different systems that don't talk to each other and it usually takes a lot of time. So we do have a goal of that. So there is a priority but right now our biggest priority is to make models converge with the principles of CICD, right? So right now our models are all trained offline and not like continuously trained and serving. As I told you that we take this file and put it so what we would want to do is put these models as a Sage maker endpoint then that endpoint would be called in that serving layer. If you can help us figure out like a faster way to do it right, that might be a good thing, right? So ashamed this is exactly what you're doing right now is exactly what we would hope for from an agent. So I think an introduction in itself is useful, but not supremely useful. What is actually useful is kind of helping us explain the priority of the company because that's like the top priority that you. Karen. Anyway. Trying to solve the team is invested in that and then how can we help to bridge that gap. Basically so kind of being that liaison between us as a platform and the company's requirements. Essentially so that we can really make immediate impact essentially that's like the best thing that potentially can be done here yeah. So this is one of the goals. But this is a big goals the other one is developer productivity we also think improves if we have good ML ups in place so all the models that we have. We need to be able to know when to retrain them. When to think about do the features are being computed. Are correct or not? Because like last week when we had an incident where we realized that one of the feature was not updating for like two months and we saw some event on the platform and then we started investigating and we realized that so we want to enable analogs across all our models that does not involve like the deployment part as much. But more on the monitoring part but you mentioned that you do both the things. Deployment and monitoring right. But if you can offer one solution I think that also works. I think analogs might be easier for to do. We already have the models performing, we already have the features being completed and help us figure out the right way to track and trigger. That might be something that can be also done together. But yeah, all I can say is that I won't be the one who would make the ultimate decision, would be the person who made the argument. But I can tell you what the problems are and can set you up, set up some time with you so that you can have a fair shot at talking about your product and then see how they respond. Okay, I think that will be great. Ashley what will be good is as the next step because we are not able to do the demo today, we can actually suggest another time, maybe during this week or sometime 45 minutes and we can actually set up a demo along with like a few more things about that we wanted to know. So that will be great and once you actually see, once you actually think it's useful, then we can think of the next thing. Okay, that sounds good. I can also connect it. I don't know if you know, cheap, huge, I don't think they really have a product. They're just selling the vision. But she's pretty smart. She has written a book on system design. If you know her already then that's great. But if you don't have to connect you to her also. Right. We met at a conference and she was also talking about solving problems to do like real time. Right. There might be, I think their platform, like their company is called tPORT. From what I've heard from her, I don't think they have a product. She just like sold a vision and she said that we can work and then figure out what the product is going to look like. You guys at least have some product to show. But she has good design thinking. She's pretty smart. So you can spend some time brainstorming and getting some ideas if you want to talk to her. For sure. We'll certainly ask you for these introductions with Chip and the founders of Shape By and all as we move forward. I think these would be interesting conversations to have. I think to begin with. One of. The things we would love to do is pick your brain further on, kind of narrowing down on what are some of the use cases and after actually you see also the demo of our platform, I think you'll also be able to mind map that what are the areas where you actually see us being able to create an impact. That's one thing. And also once we understand a little bit more from your perspective, we can also think in terms of our roadmap that oh, this is a problem, like the biggest problem and we are going to build this out in the next six days. So might as well focus towards that essentially. So those conversations would be the most helpful. Ashimi. Yeah. And this week would be hard for another meeting, mostly because Friday I'm going to La and prior to that I need to finish out a bunch of things. But let's say for next week. How does next Tuesday look for you? Around the same time. So, a couple of things here, Ashim. Actually one of our other co founders, Abysheek, he is currently in, it might be a little bit better if we do either our Psy morning. So like PST morning even until early afternoon is okay. If we do until 01:00 P.m. Or so or can we do it a little bit later in the evening? Would that work for you? Maybe, I don't know, like post 08:00 P.m. PST or something? Yeah, we can do 08:00 P.m. PST. Early morning is hard for me mostly because my workforce is also globally distributed and we actually have all our meetings. My work is kind of done by 03:00 P.m.. We have most of our meetings starting from seven and they kind of end by twelve. And then we do some internal discussions on everything and done by three. So I'm actually more free between three to six then at eight. But one occasion we can do. If it's just one time, we can definitely do maybe 08:00 P.m.. Sounds good. Okay, so we'll send out an invite in that case for this coming Tuesday and just making sure that we don't have a clashing meeting that time. We don't. So we'll send out an invite for coming Tuesday at 08:00 P.m. PST. And the goal would be these two pool, like understanding a little bit more and showing you some demolition demo of the platform. Yeah. And currently the clients that you're working with, do you kind of work as like a business? Are you just doing POC? Are they paying you for using your services? We have a couple of paid POCs. We have one customer who's actually paying us for the product itself and the other ones we are like in early stages of engagement. Because it helps both the pros and you are also getting a product. The thing is that only like what you told of the other person, like they sold the vision. We have a vision and a product, but actually a lot of the product roadmap will come from the initial intelligence because no product is going to be perfect at this point. Like it's just maybe 60, 70%. They're the main four parts will come as we kind of engine with the industry. Yes. So if that is the case, then you could also work with friends, experiences. I know the head of AI Pin fact, they wanted me to join as the head of AI ML, but I sell it to be extremely risky, leaving anything in the ups and jumping into Indian company so I know the head of engineering very well. I can ask him. If you are just looking for data and to learn, like, what are the problems and just trying out different solutions, I can put you in touch with them also. Yeah, definitely. That will be good. Cool. Let's continue chatting. I want to look at the product and then we can discuss the funding situation. Okay, sounds good. Perfect. Thank you so much. Take care. Good chatting with both of you. And let's catch up. And maybe when Sayak is here, we should do a joint hangout. He's visiting or not every other week. Oh, nice. We are not connected on WhatsApp right. Maybe we should connect on WhatsApp and try to set something up. Yeah, definitely. Yeah. I can share my contact. Yeah, if you just share your contact, I'll create a group or something and we'll keep chatting. And if stock is coming anytime soon, that's great. Otherwise we should catch up for a copy anyways. Copy all of it, depending on your mood. Yeah. He's also doing his own start up on the side. You should ask him about. He's trying to build a platform for connecting people to sporting events. Nice. So they're trying to kind of set up a POC in cold category itself where like, suppose have you used just play Pin San Francisco. If you want to play like, a soccer game, they organize these games and you can just sign up and show up. And there's a game happening. So they're trying to do something in Gokata where if anyone wants to play any game, they just sign up with a platform and someone is conducting these games. Very interesting. No, I have not. But makes a lot of sense. Nice. I'm going to chat with yeah, definitely. He's also doing a lot of nonprofit that nonprofit is actually doing really well. They're doing a lot of small projects across India to bring awareness on various topics and they karen getting times of India and all the features that oh, wow. Yeah. You should definitely reach out to me if you know him already. Yes, for sure. Yes. Thank you, Nikon. Take care and good talking to you both. Thank you. Bye.