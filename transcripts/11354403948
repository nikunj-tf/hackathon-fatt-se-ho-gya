Hello. Hello. Hi. Hi. Do you hi, good morning. Yeah, I got my bill. Okay. Yeah, nice to meet you, Bill. How are you doing? Nice to meet you. I'm doing well. How about you? Doing very well. Thanks a lot for taking time. Where are you based? I'm in Seattle. Okay, nice. Where are you now? So I'm based in San Francisco. Right now I'm traveling to Asia, but generally based out of San Francisco. Okay, cool. Thanks. Sorry for the back and forth on the time. So great, we finally find a time. Yes, thank you. For sure. And actually my cofounder Abhishek is not able to join in today, so I'm kind of taking the call by myself. I think we might have one more team member join in a couple of minutes late, but yeah, otherwise it's going to be us talking. No problem. No problem. Yeah. Great. Okay. But do you want to wait for a minute for your teammate or should we start? No, I think we should start. Maybe we can do like a brief introduction. I can introduce myself first and I would love to learn a little bit about you. The agenda for this call, the way I would love to set it up is we basically spend initial parts trying to understand the problem that you are trying to solve. Currently from a machine learning standpoint, given that you are an architect, I'm kind of excited about this call that you probably have different views, like both the ML modeling views and the ML infrastructure point of view as well. So I would love to learn about what kind of problems are you working on and what are the challenges that you are currently trying to solve. And after that I'll give you a brief introduction of what we are building, a true foundry, and we can take any next steps from there. Is that okay? Yeah, sounds good. Cool. So a little bit about myself. I come from a machine learning background myself, Bill. I used to work at Facebook where I led one of their conversational AI teams. Have you heard of portal? The product called portal protocol. Portal? No. Portal no. So the portal is like alexa? Basically. It's a device that Facebook like Alexa, but it comes with a screen, basically screen and a video calling functionality. Cool. Hello. Yeah, I was driving one of the proactive assistant efforts there and prior to Facebook, I built a lot of recommended systems for a company called Reflection which used to serve to the ecommerce industry. I think you have also worked a lot in recommended systems at Amazon. So we might have work overlap to discuss about and between Facebook and starting truefoundry. I did one more startup called Entire that got acquired by the largest HR tech player in India called Info H. So this is like my second startup and with the same set of co founders. They're my undergrad friends. So I'm going to shake both of them. I attended undergrad with them basically. So that's a little bit about my story. I would love to learn more about you. Nice. Yeah. What a great story and an impressive background. Yeah, I'm impressed. But for me, I work at Amazon, I work on recommended systems. I built one of the systems taking advantage of basically my job there was to upgrade their recommended system. And I took ideas from Transformers that chose to upgrade their recommended system to the next generation. And then I got a call from Tiger Graph and I moved to Tiger Graph. Since then I've been here at Tiger Graph. I led the machine learning team. So it's a little bit different. So right now our focus is on building the tools. We're not really at the solution phase yet because when I joined there were no machine learning at all at Tiger Ground. So we started out building the tooling. We worked with the database team to add all the functions features to build a machine learning model. And now we're at a point where we just released the tools called ML workbench. And then with that tool, of course we still need to iterate and improve the tooling, but then we have the foundation so we can start to build on applications. So right now we are focusing on for example, fraud detection and of course I'm always interested in recommended system that's another application while building. Then I think that's where I am now. Thank you for reaching out to me. So I'd love to see how I can help with you. Sure, yeah. Thanks a lot for giving that background. A couple of quick questions. So you mentioned that when you joined Tiger Graph was not doing any machine learning and you and your team basically spent some time building out a machine learning workbench. Right. And now you are starting to focus on the application layer, starting from fraud detection. So when you talk about the ML workbench, what kind of tools and functionalities that you decided to build ahead of time before actually getting to the application layer of machine learning? And the reason I asked this question is because usually people like this is a long work, right? Sorry. I signed by the way. Hi, nice to meet you. I think Nicony is having some network issues. Looks like we totally lost him now. Okay. Anyway, I think what he was trying to understand is what kind of features were it that you enabled in this workbench and how were you able to enforce that before they actually got to use what kind of things were already acquired some of the core capability that you would have built in this workbench. He just joined the back. Great. Let me see where to start. The Tiger Graph is a database company, right? So you have your graph data stored in a database so the data source is taken care of. But in order to do machine learning or any data science tasks, you actually need to process the data in an efficient manner. It's not just storing or transactions. So you need to actually process data in some complex manner. So what we start with is to build efficient data pipeline from the database to your analytic machine. All right, but we're not just dumping data out. That's the purpose of the database, I guess. Fortunately, the database, the Tidy Graph database, is built on a distributed session, so it can do some graph traversal very efficiently. So that's very convenient for us. So we can do some like sampling and feature engineering actually inside the database. Like, for example, we can calculate page rank, we can calculate communities, all those things inside the database, because the database is actually very fast in terms of distributed or parallel computation. So after calculate those features and do some graph traversal to sample subgraphs, then we build a function to string the sampled subgraphs to a Kafka cluster first, and from Kafka cluster to customers, analytic machines or computing machines, then we built the tool. Sorry to interrupt. Based on what I'm hearing, your answer, I think I might be missing a little bit of a context in which the solution is offered to the end customer. I read about the company, right? I understood that it's a database company, right. But how is Tiger Graph when Tiger Graph is building these machine learning solutions, is it for internal usage or is it for the end customer? What's the application of machine learning? For example, when you mention fraud detection, who is using the fraud detection? I'm missing that context, actually. Got it. Yeah, that's a great question. Actually, I should probably start with our customer targets, right? So our current targets are data scientists who know the wisdoms themselves, encoding. We give them the tools so they don't need to know all the details I just talked about. They have a python library. They import the Python library, and then they just type a command. Then they can start streaming data, streaming subgraphs from the database to their machine. And then our package, also our library, also support converting our internal data format or graph format into a popular frameworks format like PyTorch TensorFlow or DGL Deep graph library. So what users get is a subgraph in a format they can easily digest. And then once they get the data, they can either fit it to their own machine learning model, whether it's specular XGBoost or Pythor, et cetera, or they can use our sort of package models, which is a wrapper around the things I just talked about. We mainly use Pytochri geometric, so we kind of build a wrapper on top of that. And then the user can just type another line of command and get their model trained. Right, but since the target are data scientists, once they get the data in the format they want, then they can really do their own stuff. So that's the first phase. We target data scientists, they can get data, they train their model, they know what they're doing. And then the second phase will be targeting more like business customers where we provide a solution and then the solution, for example, we provide a fraud detection library. So in that case, it will be another wrapper on top of all the tools I just mentioned. They don't need to know how to get the data, what model they want to train. They just call one command, basically do this fraud detection, right? This is my data, this is my fraud labels. And then you just train your model for us. We will sort of stitch all those components together and train a model for them. I see. Is it trending towards AutoML? Sorry, what's the last part. Is it trending towards AutoML? Like the package model solutions that you talked about? Yeah, AutoML. Yeah. That would be sort of the public space where we can actually I mean, it depends on what you mean by AutoML. So eventually we would like to be able to do a neural network search to get the best model for the customers. But in the simplest sense, AutoML is just we have a few packaged model. We will do some hyper parameter tuning that can be done very soon. Understood? Yeah, but that's for the training phase, right? And it's targeting kind of data science, data scientist kind of person. So we haven't really touched the deployment phase yet. I guess that's what you guys are good at. So after training the model, currently we don't provide an infrastructure to host their models so they can deploy because right now most of our customers are having their own ML infrastructure or ML production pipeline. So they can deploy model to their own infrastructures. But we're going on cloud. So once we are fully cloud native, then we can provide hosting as well. If you guys are interested, maybe I can connect you to our business leadership to see if we can cooperate. So ultimately we want just one click experience of training deploy. Right. They train the model, they deploy it, they deploy the solution to somewhere on the cloud. Then they can just talk to the API to get whatever their predictions. So I think that's the ultimate goal. Okay, just a few questions on that time, like you mentioned. So the training of this model and hosting it, do you envision this happening in your own cloud or your customers cloud? Oh, we want to be flexible. Right. So we will provide this for sure, because their customers, especially small business customers who don't really have a big infrastructure, and we can easily do this on those public clouds. But there are other big corporates, right, they have their own internal infrastructure. Then we want to be able to provide an integration with their governance infrastructure. Okay. And the vision that you have would be something like a workbench right. On which they have their own infrastructure. They have the Tiger Graph database. They can train it on their own infrastructure. But it's a one click experience, if I understand correctly. Okay, understood. And with regard to this, what kind of infrastructure have you guys been thinking of? Like, has there been any thought around this deployment? Are you guys doing a bit of it or it's just like you're planning for this at this moment? Yeah, with this exploration, I think our last experiment is on Cubeflow, right? Yeah, I'm sure you have heard of it. We try to see if we can leverage Kubeflow to build an ML platform. Okay. It's easy just to build a Kubeflow platform if you are playing it by yourself, but to make it scale for production use, there's some work there. So we're still exploring. Okay. And I understand shipping the entire Kubeflow pipeline to each of your customers, that would also require resolving a lot of dependencies and diagnosing. That what we've seen with other companies. That also actually becomes a problem eventually. But what kind of timelines are you looking at to deploy this? And I just want to understand, with regard to the SAML use cases that you mentioned, what is the current state of the team? Are you thinking of deploying these models soon? Where does this lie on your timeline? Timeline, yeah. There are a few phases. Right now we kind of build the Python Library to integrate with your current infrastructure for both training and deployment or training and production. You can say p zero. So basically, we provide you the library. You do everything on your own infrastructure. And then the next phase would be to provide a cloud infrastructure to train your model. That's relatively easy. And then by training, we have a lot of work to do because training is not just running some computation. We also need to build hyper parameter tuning, some model search functionality around it, and also we need to do things at scale. So we need to consider distributor training and parallel computing, all those things. So that will probably take, I think, if not a year, but at least a big half of a year. That's the timeline. So it's like next year we will be focusing on building solutions and solutions by which I mean mostly train the models. Okay, go ahead. A couple of quick questions here. In terms of the focus, do you mean more getting in the direction of a horizontal workbench kind of a thing that your end customers can train whatever model that they want, or more of different verticalized solutions? Like you mentioned a couple of examples like fraud detection and recommender systems, which are more particularly where you give the model that people can train. Right. Which direction are you guys thinking of trending in more? I see. I think we will do both in parallel. We now hired more people, so we will have probably one or two scientists or researchers who focus on those verticals, like what are the best models for architecture, for recommendation, what do research there and then in parallel where we're building the platform or the tooling to train those models. Right. And then of course the two can be done kind of loosely coupled or even independently. We don't need to build an infrastructure for fraud detection specifically. We will build a more general model training infrastructure and then fraud detection will be just one use case of it. One other question here is in terms of cloud, like are you guys focusing on one specific cloud or are you guys trying to be multicloud? We are trying to be cloud Agnostic. So right now the database is on the big three cloud providers and I think we want to do the same. But of course there will be priorities. We will see which cloud is easier to integrate first and then do that first. Probably GCP first. I don't know. Of course I know AWS the best. But I think we have very good collaboration with GCP. So we will start with understood. Understood. So when you deploy your database at your client, does it work like you spin up your entire platform at the database? Like the handle for your database, everything on your cloud? Let's say GCP account, like using Admin, access something, is that how it works right now? I see. So for database we now provide our own portfolio of the cloud platform. So you can even choose on which provider you want to set up your database, but then you don't really go directly to the provider and deploy things. So we take a note portal for that. Okay, understood. I would assume similar, you would want to do similarly for the data center. Just want to understand the first phase that you mentioned about libraries, how would that work? Like you said, the infrastructure would be of your clients. So is it like, let's say the training requires an instance, the setup of that instance provisioning everything the clients platform team would have to do. Right. In the first phase that you're thinking of the solution, right? Yeah, I don't know, are you guys working, are you guys cloud only or you also do like on Prem, bring your own infrastructure business? So the way our platform has been designed, it's cloud Agnostic as well as we are also building the capabilities for on Prem. I mean it's on our roadmap, probably be done in another month or so, but yeah, that's something that we definitely do and it's something that's in the pipeline. So currently I think we support AWS, GCP and Azure, the big three. But on Prem also is something that we are looking at because the kind of clients that we are working on right now, some of them in turn have clients who want like air cap environment. So we do plan on supporting that. I see. Okay, that's good. So maybe let's talk more about cloud, right? For Cloud, yeah, I think we want to just provide the same or similar web experience where you can launch your training infrastructure without going to the lower level management layer. So we provide maybe in the end we have the no code experience, but in the beginning it will be more like a Sage maker or Vertex AI kind of experience where you can spin up a notebook and then the notebook a Python Jupiter environment, which can be hosted on any of the Cloud platforms. And then you then just import our libraries there. Write your build your own model or you import our package model or solutions like fraud detection, and you type a few commands that are under the hood. Right. We will spin up the instance to do the calculation, and once the calculation is done, we shut down the machines. Understood. You don't need to worry about all those details. Okay, I understand. In training, this would be the set up. Are you also thinking of somehow deploying or surfing this model, helping your clients serve this model? Like creating a pair endpoints or maybe have a cron job which is associated with the model? Is that also in the pipeline? When do you see yourself doing that? Yeah, so that's what I was talking about. For the next phase, for the direct immediate next phase, we will be focusing on training, right? We will take care of all the parallel computing and stuff. I think that's still off work. After that, after we have training taken care of, then we will start worrying about the deployment space. Okay. I think the reason we played with Kubeflow is that Kubeflow offers CASER. So it's very easy to deploy a model once you have trained it. But as I said, Kubeflow is not a worry free hassle free solution. It has its own issues. So we're also looking at other solutions right now. Understood. Thanks a lot. Just the last question before I introduce to found it to you. What is roughly the size of the team that is working on creating this platform or workplace right now? And how many data centers do you have? ML engineers or DevOps? The engineering team has about ten people. No data scientists, no full time data scientist is on our team. The data scientists are more on the customer solution team. Okay. We are collaborating closely together because the data scientists right now work with customers to get their needs and solve their problems and we build the tools to support them. And yeah, we work closely. And these data centers, like the customer solution team, how many data centers they have, roughly? If you have an estimate? Yeah, that's a very good question. I don't really know. So I think it's kind of mixed with the sales. So I would say I don't know. In probably in US alone we have 1020 people, but yeah, I don't have the exact number. Okay. I know a few people I work closely with, that's why. Thanks a lot for that information. I think that's really helpful. I'll just give you a brief of what we're trying to do in terms of time. So truefoundry is trying to make a platform which makes like deploying ML job a one click experience as we mentioned, is something that you also want to do in the future. What an idea is basically to abstract a lot of complexities that a data scientist is usually not familiar with and to provide that in a simple to use solution so that they can, without the help of DevOps or maybe even a client which does not have the capabilities of a DevOps or infra person specifically for this job can directly go and train their models, deploy their models, provision infrastructure for the same and also does not have to go anywhere else to serve it. So we provide support for serving it through model servers or creating fast API endpoints for it or like creating Cron jobs or dual jobs for it. Everything happens to the same platform. So like the experience for a data scientist or a developer or a client per se remains very simplified. That is basically the ideology of the platform. And we are cloud agnostic by design. So we are based on Kubernetes right now and we support the three major cloud providers. It's very easy to set up the platform. We are working on making it a single command to set up the platform. So we just need like a Kubernetes cluster and the platform runs on top of that. It provisions, the resources, runs the containers, auto scales them, provides like testing facilities, everything that a mature like production level stack would need. So this is something that we are working on, we would love to hear. Is that something that interests you? How does that sound? Yeah, sounds like we're trying to do the same thing, but it's just our focus will be on the graph applications or on the graph side. Okay, great. So you are building this platform so you manage all the infrastructure for the users. That's good. Yes, infrastructure and then like orchestrating the infrastructure for facilitating model deployment or training or even provide associated facilities like monitoring of these models, upkeep of this model monitoring drift in data basically more or less everything post model training till deployment, everything is abstracted into this one platform so it can be solved. The whole pipeline. Yes. Okay. Yeah. Okay. I guess one pinpoint I had not right now, but in my previous job is doing model experiment online. Do you support that? Yes, we do support that. We do have our own model to see as well. We support logging data or logging metrics, doing multiple iterations versioning of these models. Everything cool. All right, so yeah, I think that's actually very hard to complex thing to do. Basically when you have multiple models, you want to do a B testing. You want your endpoint to be able to randomly switch to different models and get that. Since you are asking me what kind of pain point we have, I think that's a pain point I had previously at Amazon. But of course Amazon has its own way to do that. But in general, I think to build that from scratch, you will be a lot of work. Yeah. So currently we have model logging versioning, et cetera. A B testing can be testing, et cetera. Something that is in the pipeline has been currently built, but that should also be shown to split model traffic and try out models and roll them out. So I think that's something that we're doing. Nicoon, are you saying I just wanted. To. Let you know. Can you hear me okay? Yeah. Hello? Yes, Nicole. Okay. I think there is some network issue. Basically what we'd like to understand are you guys evaluating some kind of a solution and would you guys be interested in evaluating? I mean, currently how we're working on is more or less a design partner mode in which we try to understand what kind of problems you have and more or less like co work in a usual gen and build on the solution together. Since we are in a very early stage, I just want to understand, are you guys evaluating some solution for this part? Like the Emerald deployment training or something? And maybe that could be something that we could showcase you guys in case you have any particular problems and stuff like that. Yeah, sure. As I said, we were playing around with Kubeflow trying to see if we can build this pipeline ourselves. But as we find out, there's a lot of work which I know we will have. I guess it doesn't really hurt to see if there's any opportunity that we can collaborate on if you are interested as well. Understood. I think that sounds really interesting to us as well. So what we can do is maybe we can schedule another meeting in which we can discuss like so what we usually try to do. There are a lot of features and different parts of the platform that we have built. What you try to understand is the experience that you would expect out of your first or what's on top of your mind, which I understand is streaming. And then we can best facilitate that through a demo. Does that sound fine to you? Sounds good, yeah. I guess from a technical side, I can foresee how it's going to fit together, but I will bring this up to the business side and see. Of course, we can first do a demo together just without any business considerations. But I'm sure to go further, to really work together, I will need to bring the business people in and see how they think about us. Understood. Anything right now for us as a team, the financial aspect does not matter that much and we're ready to discuss any terms. It's more of like a coworking kind of a thing with which we also are able to improve on our platform. So definitely if there's any discussion on that, any way that we can help on that, you can definitely do that. But I think if it's possible for us to give you a demo and just to hear your feedback on the platform in general, whichever way it goes, I think that would be really helpful for us. Yeah, no problem. I can definitely arrange a demo to the tower team. That should be no problem. But it's just that okay. Yeah. For longer term collaboration, almost serious collaborations, we can't leave that to the business people to talk about. Okay, that sounds really great. So I think as a next time, what I'll do is maybe if you have any other slot in this week or whenever possible, maybe we can sit on a call for like, 1520 minutes and let's try to understand what kind of workflow would you want to present to the team or like, what would be ideal. And then we can schedule a call with a larger team to actually showcase that. Does that sound fine to you? Okay, yeah, sounds good to me. Okay, then I'll send you a mail. Maybe you can share a calendar or a few slots and we can work from there. Okay, sounds great. Yeah. Okay. Thanks a lot, Irene. Thanks for answering so many of our questions. This is really helpful. Looking forward to talking with you again. Yeah, likewise. Here. Yeah. Great meeting you all and look forward to your email. Thank you. Thanks. Take a note later. Bye.