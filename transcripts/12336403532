He found this crosscloud hypothesis very interesting. Take care. So let's talk to him about this. Hello. Hi, Frank. We're doing well. How are you doing? Good. Nice. You're based in Netherlands, you said. Yeah, correct. Nice. Awesome. Great to meet you. So Frank, first of all, thank you so much for taking the time to speak with us. I was actually quite excited about this call to share with you what we are thinking about building and also learn from your experience as well. I will start with a brief introduction. It would be good to know a little bit about you as well after that. So my name is Nick Ringaj. I am one of the co founders at Truefoundry. I come from machine learning background where I work with Facebook and also start up in the Bay Area called Reflection. And I did my masters at Berkeley, UC Berkeley and bachelor's at one of the Iit's. That's where I met Abhishek, who is one of the other co founders. Hi Abhishek. Yeah. You want to introduce? Yeah. So I basically Frank, I graduated from It 2013, computer Science and then worked in Facebook for around five and a half years. Worked on different teams like the Distributed Caching System team, was leading the Mobile Performance team and was eventually leading the bidders organization there. And then early 2020 quit my job, did a startup that was acquired by one of the companies in India and currently working on performing. Awesome. Would love to learn a little bit about your current role as well, Frank. And then we can. So I work at VMware. I assume you are familiar with VMware? We are. Okay, cool. So I'm what's called a chief technologist. Now this is a bit of a weird title. I'm not a CTO. But I report to a CTO. I have the task of doing outbound work. So I present at conferences, I talk to customers. I'm a trusted adviser for large environments, large customers. And what I learn from that, I typically take that and bring that into R and D and use that for strategies for patent applications and talk to engineers and principal engineers and see what we can do to help and basically create a better product or strategize in a better way. So the main focus areas that I have is resource management and machine learning. So resource management from a computer perspective. So we're talking about CPU architectures, memory architectures and GPUs. I'm more or less starting to focus on deep use, but I'm shying away from that because we are using deep use as of now mostly to accelerate networking. And although networking is a very dominant role or has a dominant role in machine learning within VMware, we have a different business unit for that. So I just glance at it a little bit long winded explanation of what I do, but that's basically my focus area. Understood. So Frank, now it seems like your role is actually also involves a lot of breadth and strategy work as well. And before this, when you were working more on the depth related things as an architect at VMware, that's where you were doing a lot of this resource management type of work. Is that the understanding here? Yeah. So I did a lot of work for the resource schedulers, like the Numa and the DRS schedulers, like CPU and memory schedulers. And now I'm focusing on how we can make GPU resources better available and scheduling those better for the workloads that are now machine learning workloads, because we're trying to get those workloads on prem. And that's what we see with a lot of our customers, that they're moving their workloads ML workloads from large hyperscales. So, like the Google and the azure back on premium. So the question list that you sent to me, which I'm trying to dig up because I had that on my screen somewhere, but I cannot find it. I hear this, there were some interesting questions that you have for me, which basically resonates very much. Yeah, for sure. Actually, just before we get into the questions, one other thing that I wanted to ask you is VMware as a company for the internal development and all, is it basically like what's the cloud software system and all that it is using basically for? I'm assuming a lot of it is probably internal, given that VMware works in the space. But do you have presence across other clouds like AWS, Gcp, and all? Yeah. So you mean like what we use internally to add tools? Yes. Okay, so the primary platform that we run actually is AWS, which is interesting. So we are a top ten customer of AWS. To give you an idea of how large of a customer we are. Then we have tools like every other large customer. So to give you an idea, we have 43,000 employees as a company worldwide. And so we use native cloud services from every large hyperscale. So we use Google services, we use services from Azure, and we use AWS. Those are the primary three cloud platforms that we use. And then of course, we have the standard developer tool sets. Think about Gitlab GitHub and all that stuff, but this is typically Microsoft anyways, so yes, got it. Okay, so basically right now you're saying that the workloads within VMware are running on all the three major cloud gcp, Azure, and AWS. So we have our own clouds internally, of course, but for example, we have an analytics platform that runs primarily in house, but we also have external services that we need to use to do some analytics that is run on a hyperscale. And I cannot unfortunately disclose which one that is. Got it. Okay, that's fine. Understood. And how about the machine learning workloads? Do you see that, like, a lot of machine learning workloads are also running across different cloud providers? Yes, and that typically depends on what type of native cloud service you use. So for some, if you're connected to a BigQuery system, you use that, or if you need to use particular analytics, you use those type of things. But we also connect to an IBM service. That can be the case, right? So it depends typically what type of service you need to connect to and then you run the hardware close to that native cloud service. I see, okay, understood. So, very interesting. So basically, wherever you're using, storing the data primarily on whichever cloud service, that's where you kind of end up running the workloads as well. Well, actually the data follows the service. So it is what you typically see. This is what you see with customers as well. There is a particular service that a customer likes, or what we like. So the developers in house and that becomes the focal point or the element that everything revolves around. So then we are bumping in data at a service that surrounds that particular service and the computer is run next to it. There is a sticky element and then the adjacent services are typically used from that hyperscaler. The organic growth is typically, let's use Bigquery. For example, if a developer wants to use particular service from that, you see that if that happens, the data is bumped into that system or an adjacent system, and then the compute will be run on that cloud as well. And all those services that is needed to run the application and run extra services are basically used from that hyperscaler. Typically what you don't see is that you use a different type of service from another cloud to use because it might be a little bit better. It's already complex. Getting a different cloud service involved makes it so much more complex from many different layers. Even a technical layer at an organizational layer, add a regulation layer, security layer, all those things, it's typically a nightmare. Understood. And Frank, across cloud systems that are currently running, like you mentioned, services saving data locally and stuff, do you end up building out a platform like on top of Kubernetes or something, which at least standardizes the developer interface across different clouds? Or what's the approach there? So the funny thing is, if you talk to data scientists themselves, they don't like to use Kubernetes, they just want to have a docker container and that's it. Kubernetes is typically pushed by software vendors, by the tooling sets, because of orchestration necessities, or by any other requirements. And so Kubernetes is actually introduced not by the requirements of the data scientists themselves. And so what you see is that I'm making that distinction because it's not an architectural vision and so it's basically being deployed somewhere and then the previous conversation occurs again, oh, we need just to run Kubernetes because we're attaching it to this service or whatever. So it's not a grand vision of oh, I want to abstract clouds or I want to do. I want to maintain a unique API front end that can abstract multiple clouds whatsoever. You typically don't see that. What I see with if I talk about customers is that there is still a lot of ad hoc development going on. So many different business units within companies are trying to come up with a solution for a particular use case that they have for that particular business unit. So creating a revenue critical service or reducing costs for a particular service in their business unit and so they're really trying to create a particular solution for that particular thing. And so they're really tunnel vision in that solution instead of envisioning at a corporate level. How can we do this to create a machine learning platform that can be used by many different business units and create a machine learning center of excellence? That's something that I hear some CTOs talk about. But it's a grand vision. It's not done at this particular point in time yet, or at least the customers that I talked to. Understood. Okay, sorry. Go ahead, Jimmy. Does it make sense what I'm saying? Yes, it does. Just asking. Frank, let's say if a single do you see a single do you also having use cases in which they want to make use of different hyperscalers at different use cases at different points in time? And do they face challenges in that? Do you typically see that happening or when is more or less satisfied with a single cloud? So what I've learned is that what we see is that customers switch because a particular hyperscaler is too constrained. How can I say this? Let's say they develop on a particular it's too opinionated. That's a better word. So they are developing something and then they figure out that, hey, this allows me to really start quickly and develop fast. But at one point they need to customize it at a particular point in time that is beyond the boundaries of what that service can deliver. And so they need to come up with something else. And instead of saying I'm going to build it by all by myself because that's a bridge too far, that's basically way that's too complex. They're going to shop around and they find something else. Typically a different hyperscaler, that's something what I've seen before. So they still want to have a service instead of building it out and creating everything from scratch. So that's what you see. I don't see, at least at my customers, I don't see some hybrid cloud that is using multiple different clouds for one particular business solution. I haven't come across that yet. It's really difficult to do so because of all the different front end problems that you see, like authentication, like use problems, like regulatory problems. What we see with a lot of financials is for every solution, you need to certify that solution in a particular stack. Now it takes time to certify certain things for a particular cloud stack. If you start to do that across two cloud stacks, that pretty much doubles the time to certify it by a regulatory body. So you want to do this in a single cloud to basically get your go to market momentum faster. Does that make sense? Okay, sorry. Go ahead, Frank. You use Tanzo internally? Yes. Okay. We do. But we also do other Kubernetes environments, by the way. I see. And that's the same platform, basically, BMW sells to all the customers also, right? Well, yes and no. So, yeah, we developed ansu and that's part of our strategy. But we are still an open platform at the hypervisor layer. So if a customer wants to use the red hat version, by all means. If they want to run Rancher on top of it, go ahead. It's a workload, right? So, yes, we have our own flavor. Yes, we develop a full stack on top of it, but we allow customers and we welcome other Kubernetes platforms on top of the hypervisor. Understood. Are companies perceiving the standard layer over multi cloud Kubernetes and things like that? Are they open to it? We see the need. I'm not privy to that information because I know a little bit about that, but not at that detail. What we are working on primarily, we have a distribution of Kubernetes, which we're trying to we have multiple distributions of Kubernetes, and they cater to particular use cases, but we're trying to merge them all into a single distribution that caters everybody in a larger plan. It's all to merge for what we call tap. That's our application platform. So that's basically an application platform for developers to develop their code on. Got it? Understood. Actually, Frank, there are two things that could be helpful for us to understand this a bit. So you mentioned that there's never a set up where there's, like, one business unit or one application that's running across different clouds, and that makes sense. You also mentioned that there are different applications, different business units that end up using different clouds because of whatever preference over a certain service where the data is deciding and stuff like that. Right. So generally, I guess there's two things that can help. Number one, if you can describe me, like, a little bit of the structure, do you have different bu? And then is there like, a central platform at all that works across? Buchu has their own engineering intra platform team that caters to their own needs. Like, if you can help us explain what are the different bureau the setup. Of the horizontal thing within VMware? Within VMware? Yeah. That'S difficult. Within VMware, we have a couple of business units. And these business units, they each have their own set of goals deliverables, and that means products and services, and that means we can do whatever we want to with whatever tools we need to use. And that's what we see. That's what you see. So for example, I work for the CIBG business unit which stands for the Cloud Infrastructure Business Group. We are responsible for the hypervisor, which is called vsphere and Vsan which is the storage layer. And also the cloud services like the VMC, like VMC on AWS, on Azure, on Google, on Alibaba, whatever. We develop all these services in our own tool sets and so we have our own budget and we have our own preferences for tools. Now, we have a networking, the NSBU and they have their own ideas on how to develop things. Yes, we have our own common language, of course, and we need to integrate, but they have their deliverables and we have a timeline and that's it. And how they do it, they do it. Understood. Okay. Similarly, like how many other views are there, like in this company, like networking CIBG? And I think there are four or five. I see. And by the way, for example, between networking and let's say CIBG, there is no common engineering layer at all that spans across the two be used basically. No, understood. Go ahead. There is an office of the CTO that should be able to communicate inter business units. So there is a CTO, the overall CTO, kid Goldberg, and he is responsible for the common strategy amongst all business units and there of course is understood. Got it. And by the way, within a PU do you see, there's some platform team whose job is to make the developers more productive by building out tools and stuff. Like for example, within CIP is there like a machine learning platform team? No, we don't have that. No. Go ahead. Is this because there aren't enough machine learning use cases within the company or machine learning cases are developed in a different way? I wonder why is this? So yeah, I'm thinking about this as well and I'm trying to figure out how we are actually doing this. So we have particular services that we are using, like the Wavefront service. Like that's one of the analytics services that is being run by data scientists, but that's run out of, for example, out of the office of the CTO that can be used by different business units. But that's not a mandatory service. That's something that can be used if a developer wants to use it. So that's a common framework that is available. But if you want to run something or if you want to create something in Aos or if you have budget, do your thing right? Yeah, that can be an enhancement for our optimization for our business. Look, the whole thing is you have to understand we are now 25 years old and this company has grown in tremendous steps like from a small startup in the first six years. Then we were acquired and then we acquired an insane amount of companies in a rapid in a really fast time. So we went from 6000 to 40,000 people in a relatively short time. So in order to create centralized development practices, there was simply no time. Like, I've worked for a startup for three years. When you have greenfield scenarios, when you can structurally build out something, that's a different thing. But when you are at one point when you're acquiring a company and that has 3000, 4000 people yeah, you have to incorporate that. And you have to deal with their structures and their It systems and their development practices and just figure it out and try to figure it out how to integrate their products within the old core systems. Right, because that's what customers expect. That's a different challenge. No, that makes a ton of sense. Understood. And by any chance, do you know if any of these be used? Like while I understand there is no platform team, do they end up using any out of the box machine learning operation solutions? Like, I don't know if you have heard of Sage Maker or Vertex AI? Yeah, okay, we use a couple. So we use like Sage Maker or the Azure one, the Google one, but we also use Domino Labs. That's a big one internally. Interesting. Yeah, well, we have a partnership with Domino Labs as well, so we offer that to customers next to our Nvidia partnership. We have a Domino Labs partnership as well. Actually, you mentioned this a few times that you do something for the customers. You offer this to customers. What does it mean by VMware offering Domino data lapse to the end? Customer yeah, so what we do is we integrate with Nvidia so we have that within our stack. So like the vGPU is completely integrated with the Vsphere hypervisor. So within the UI of Vsphere you can actually use Nvidia technology and it's fully integrated and you can use APIs that you can build out. But we are also working with Domino Labs to integrate that and to make the deployment of Domino Labs seamlessly. And this is what we are trying to incorporate. And every version of the platform we are trying to make that better. So this is the stuff that we are trying to do for the machine learning customers. So what we have for machine learning internally, we have three pillars. We want to incorporate machine learning in our own products. So we use machine learning for Vsan, for example, to determine cache sizing for particular workloads. We do that in monitoring to understand customer workloads and do better scheduling, for example. What we also do is we incorporate particular machine learning services to enhance our own services such as Bugs, triaging and all of that stuff. So that's the reason why we have things like Sage Maker or other types of products within the hyperscalers to use analytics to figure out our own software vendor services for our own customers. We want to enhance that and make that better. And then the third pillar that's the pillar that I am partially responsible for is we want to allow all of our customers to run AML platform on top of the Vsphere platform. So Vsphere is completely, I think, 99%, not fully, but 99% deployed on the Fortune thousand. So every large company in the world is running revenue critical and business critical on our platform. So what we see is the next step is of course, making the VTRE platform and machine learning platform. And in order to do that, we need not only to incorporate the newest, latest and greatest hardware, but we also need to allow platforms like Domino Labs to easily deploy so customers can run something like a full ML ops platform or a development platform for machine for the data scientist, as easy as possible. By the way, this is very new information for me and that's why some of the questions to you might sound more new because I did not find that this is how VMware is actually shipping the product. I'm learning a lot here. I'm just trying to visualize what you just described to me, that VMware is shipping a platform to the end customer. Let's say that end customer is qualcomm. Okay? And basically what's the product form in which VMware is shipping and what does it mean by Domino Data Lab is integrated in that product form? Like, yeah, okay, so I need to correct something. We don't ship that. So what we do is the partnership is we engage with Domino Labs on R and D level. And so we try to integrate some of the calls from Domino Labs in our platform and all that stuff. Now, Domino Labs is still an early partnership. So what you will see over the next year, you will see tighter integration. But the best example is the Nvidia partnership. Now, when we ship Vsphere, let's say eight or next week, we're going to release update one of Vsphere. We have new enhancements that actually allows customers to run more accelerators in a host inside Vs. You can allocate more resources, you can split them up, or you can use NV switches, all of that stuff. And so that's the way we ship it. Now, the customer still needs to deploy it the way they like it. Because one of the key differences between Vsphere and a Sage Maker is a Sage maker is an opinionated stack. And that's one of the things that we've learned over the years, is that customers shy away from Vertex, AI or Sage Maker or whatever is because, yes, you can easily start, but at one point you hit a particular boundary or you hit a particular constraint. And what we allow customers to do is build your own ML platform the way you like it, curate your own tool set by all the tools that we have, all the services that we have, and you can build your own ML platform the way you like it. That's the way we do it. So we're not shipping a fully stacked Domino Labs skew to customers because then we're opinionated again, that's not what we want to do. So what we do is we talk to Domino Labs and we engage with them and we see, okay, how can we make this better? How can we ensure that it works and it's aligned together? And of course, it takes time because we are not a startup and customers don't expect us to change our hypervisor platform every week because that will break a lot of their own lifecycle management processes. Right. So we run it in a different cycle than the average cloud native platforms. Understood. Okay. Or do I need to disclose more? No, I think I've understood a little bit of it much more than the first time I asked you this question. But I would not pretend that I've really understood everything. But yes, it's very helpful. Thank you so much, Frank. Okay, so Frank, there is one question that I wanted to ask you. So at this point, True Foundry, the problem that we are trying to solve is like companies who intend to build ML platforms, okay? And I'm specifically saying that companies who want to build ML platforms internally, not directly use off the shelf platforms like Sage Maker or Vertex basically. Right? Yes. That's the segment that we cater to. Okay, perfect. And for that, we are building out a platform on top of Kubernetes. So the goal for us is that a machine learning developer can leverage the full power of Kubernetes. So let's distributed training, being able to run jobs in parallel, like being able to do a B testing by distributing the traffic. So anything that they're interested in doing on Kubernetes, they can do that. But typically the knowledge gap of a machine learning developer and somebody who understands Kubernetes is huge. So our platform basically bridges that gap. Okay. And in this attempt, one of the things that we have realized is it's incredibly hard to build one platform that just fits the needs of every single company, especially the large ones. So the approach that we are taking is instead of us saying that, hey, use it as you see it, we actually have taken an approach that modify it as you need it, basically. Okay, so planning being that some of these companies can build more layers on top of what we provide, but hopefully they don't have to build everything from scratch. They use a lot of abstractions that we have built out and they continue to build on top of it. And similar to what approach that you all have taken, where we are not trying to be extremely opinionated, there are some places where we have to be opinionated. For example, choice of Kubernetes itself is an opinion. But that said, what type of internal tools that you want to use for your model registry, for your future stores, et cetera, is something that we don't guide you. Okay, perfect. Now, one approach that we can take is to because we are building more on the software layer, the infrastructure software layer. Basically, we can just continue reaching out to these companies by ourselves, and they can adopt our platform. The other new approach that I'm just learning from you is potentially partnering with hardware providers. Now that you mentioned this to me, I know that I had a similar conversation at some point with Nvidia as well, where Nvidia is trying to package more ML OPC libraries along with it as a software layer on top of its GPU. So it makes it easier for people to adopt their hardware, basically. So how should I think about that? What's the stage? What's the level of product maturity at which there could be a scope of such a partnership that could make it interesting mutually? Yeah. So I think somebody else needs to talk to you about that, of course, because I am typically not the one that does that conversation. But look, if you want to have a conversation with VMware about partnering, of course they're going to ask you like, can you support Tanzu? That's the first question you're going to can you support it? So that's the first question. But then the second question is, okay, because if we look at Domino Labs and I'm not comparing you to Domino Labs, but let's use that as an example domino Labs supports Dance, but it supports also other distributions. Now, the key is trying to provide key benefits or extra services on top of the VC platform, like, where's the integration? How can we make customers more successful with our partnership? Now, the interesting thing is if we look at hardware scheduling, we do the hardware scheduling, of course, but if you can accelerate the development of particular libraries or particular models, all of that stuff, perfect. Right. So let me see who I can reach. I'll ask around for you. I'll ask around and I'll come back to you with an answer on who you want, who you want to talk to, and what type of questions you can expect. How does that sound? That sounds amazing. Yeah. Because to be honest, at this point, by the way, I did not come to this call with this mindset. Of course, my mindset was just to learn because I didn't even know that something like this happens. Right. So my only goal from this conversation is, what does it take to, let's say, partner with someone like VMware at the level of ML Ops platform? Right? Yeah. There's always the possibilities of partnering. I don't know what the requirements are. I don't know how impactful it is for your development cycles, because I can imagine that there will be some impact. Whether that's acceptable for you or not, I don't know. But let's start with the conversation. So let me find out who that person is and how you can prepare for that conversation. That. Would be very helpful. Thank you so much. Frank. Frank, we have like five more minutes in the call. I want to also open up the floor to see if you have any questions for us because I think we have asked you a lot of questions and you have been already super helpful so far. Can you send me a deck of your proposal, like what the platform does? Sure, yes, we can do that. Chinmay, can you take down that action item, please? Yeah, sure. So we can certainly send that across to you, but any other questions that you had for us? Frank, do you have any running? Do you have any Pocs running right now? Do you have any customers? How far are you? As you said, you are startup. How far are you along the way? Which round are you? So we had raised the Safe Round, like about 15 months ago. And we have also raised another Safe Round recently, like three or four months ago. So that's where we are. We are already working with a couple of enterprises, like fairly large ones, like a global chip company, maybe $50 billion, another conglomerate, $200 billion. So we are working with a couple of enterprises and we are also running some production workloads already on our platform, basically. So it's not just a POC. We have gone to like a paying customer stage. That said, we don't have a lot of paying customers. We are still fairly early in our journey, but the product itself is mature enough that right now we can support all the three clouds. So you can actually run production loads on AWS, Gcp, Azure, basically, Kubernetes Festus running on all the three clouds. And we already support the most, like, the major ML workloads. Like, for example, if you want to run a training job, batch inference job, like a real time service for model as a service integration with most common model servers like Todd, Serve, Tfser, Triton, Nvidia, Triton, et cetera. So we have actually covered a lot of those things so far in the product. Do you also use a model optimization service? So when you say model optimization, do you mean like hyper parameter tuning? Yeah, yeah. So you could also use hyperparate tuning. Basically, you can integrate with Cattle on the platform. I don't know if you're familiar with categories like the one with Kubeflow and we also are currently pairing with Opportunity, basically. Okay, I think I just need to look at the deck and then of course, I will get some more questions for you. I can certainly send that across. Yeah, please go ahead. Sorry. No, that's it so perfect. Sounds good. Let me see. It's afternoon for me on Friday, so basically I'm going to call it quits after this meeting. It's been a long week already, so I'll shoot some people an email. I expect early next week to have an answer for you. So you know, a little bit about the timeline and then we can talk further. Is that a good idea? Yeah, that's also a great idea. I just want to make sure that this deck that we have has the relevant information that you think would be important for you. Okay, so I'll actually not walk to the full deck, but just show you the important things. So we talk a little bit about our team. We talk about our investors and advisers who are there in the journey along with us. And then this is like problem motivation. What is it that we do as a company? More details about the company itself, like which parts of the ML platform that we handle. And then basically we have a product demo video here as well, so people can check that out. Here we show the product demo video and then we talk about what are the actual pain points that we are solving for different stakeholders here. Of course, this is a customer specific PPD, so we have some of these slides here, and then we have some case studies that we have done with our existing clients. Like I mentioned, that large companies that we are currently working with. So these are a couple of case studies that we have. And then how do you get your infrastructure on running on our cloud, basically, and then some of the architectural elements about how do we do an on Prem support, how do we do privacy and security, et cetera, et cetera. Basically, does this feel like it has enough information that you need for the sake of this conversation? Yeah, it does. So when you say on Prem, does it mean for you a Kubernetes deployment on bare Metal? So we support both type of things. You can either have a Kubernetes cluster running on bare Metal and then you can install Truefoundry on that and run it, or you can have a Kubernetes cluster which is managed by one of the cloud providers, like AWS, like Eks for example. Okay. You have never installed it on Kubernetes on a V share hypervisor. You have never okay, that's interesting. Yeah. Okay, sorry, the question sorry, maybe I missed the question. What was the question that you asked? So when you say on Prem, you never installed it on V share? Oh, I don't think so. I don't think we have done it. And by the way, generally we ourselves don't have to install Kubernetes and manage Kubernetes. Our platform gets installed on a running Kubernetes cluster that the customer can be managing wherever they want, basically. Yeah, exactly. And you basically don't care whether it's running on bare Metal that Kubernetes platform, or on Vcr? We don't, yes. Okay, that's perfect to know. That is perfect. Okay, cool. Perfect. Thank you for your time. Thank you so much. Yeah, I'll ping some people and hopefully I'll have an answer for you next week. Amazing, thanks. We'll send out this PPD over email to you. Okay, perfect. Send it to my VMware address instead of LinkedIn. I monitor that every day, sir. I'm just making sure that this call has your VMware address. Yes, we do. Okay, perfect. Thank you. Perfect. Okay. Take a note. Weekend. Bye. Bye. Bye. You too. Bye.