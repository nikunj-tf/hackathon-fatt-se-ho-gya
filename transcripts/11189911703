Hi Sashantam. How are you? All good, how are you awake? Doing very well. Doing very well, actually. Take a note joined. This is actually one of the note takers, note taker apps that we use. Okay. Have you used my places? No, I have not. Is it good? It's very good. I mean, depends like so it kind of transcribes a meeting. So like once you forget, if you forget taking some notes, that's helpful. And then it has productized the transcription very well. So I don't think the summary is very well. It aspires to do the summaries as well. But at least the transcription productization is really good. Nice. Probably I'll try it and it gets well with Google meetings. Yeah, every all of them. Like Google meets teams, zoom, everything. So you don't even have to add or anything. It just automatically joins your calls and stuff. Understood. Okay. So yeah, thanks a lot for taking the time again. I think we had briefly discussed chatter a couple of days ago, right. Where we discussed today's agenda would be kind of for me to learn a little bit about your current data science work. Data science processes. Right. And I think you mentioned the context a little bit that we are building out. Truefoundry. We're fairly early stage startup and the stage that we have reached in that like we are working with a few design partners already. The product is out and usable but not generally available and continuing to learn more about the existing processes, existing workflows, existing problems. That's the primary focus for the call today. Okay. Right now I spent a lot of time with the BFSA industry, so with Citibank, HSBC and PayPal for some time. And then within the Indian businesses, I work for Mahindra Group. So there it was. Either marketing for Mahindra Auto or risk for Mahindra Finance. And then we were building a common customer database for cross selling across minor group companies. So all that I did. Right now I'm working for an Etiquette. Gmail lead school. Good number of students, good number of schools trying to scale in India. And here the scope is scope is basically anything related to data science and analytics. There are some core deep learning problems that we are trying to solve, which is proving to be really challenging. And then there's typical stuff around Propensity, different typical stuff around Churn which the use cases will find everywhere. Right. I don't know which one you would like me to talk about because there are plenty of use cases that we have for sure. Yeah. So I think the focus it would be great if we can focus on the current role at Lead School where you're working on a bunch of data science and analytics problems. Right. I would like to know a little bit, ideally something that is that you are in the team are taking up very seriously to try to drive towards production. Right. So it could be either the deep learning projects which whenever you think is reasonable, or even the proposal churned once, if that's already kind of already running in production today, that's getting used already. So I'll talk about, let's say a couple of them, how the product works. Is it's a SaaS based product and we sell it to the schools, not to the kids or the parents. So idea is the product is designed to work in schools where teachers are not good. Okay, okay. So it will not work in a tier one metro in India in a high end school because teachers are very good. So the product is designed to help a below average teacher in let's say Roper or Bhagalpur or Asan, Seoul. See what I mean? So smaller towns where teachers are not well paid and they are not good, they get a tab. If the school signs up, they get a tablet and the tablet has a preloaded content. And then we also fix the television in the classroom so that the teacher can cast the content, right? And then we have designed the plans so teacher can design their day plans. They can teach topic by topic mark that everything is done. And then there is an assessment and then they have to go. So every in a year there are about ten assessments. Assessments are like unit tests and then there is a middle of the year exam and end of the year exam. So this is how we propose to teach or help the school teach. Now what happens is, let's say a teacher has taught a particular topic, let's say Newton's first law of motion, different things within Newton's first law of motion, logic, motion. And then there is an ASM ASM as an assessment. And that assessment happens. And then the teacher has to evaluate. So basically it's a question paper. You give out the question paper, the student solves the question and then they have to calculate the marks. But the teacher has to know my class is weak in what topic right? Now, how do you do that? To do that I would need question level marks, right? For each question I need to capture, I need to ask the teacher that it was a 20 mark question paper. There were eight questions. For each question you tell me the marks. Each question is linked to a particular learning indicator. So we call it learning indicator. It's like a topic, larger topic. And within learning indicator there are multiple learning outcomes. And then you enter it for all 30 students, 40 students that are there in your class. And then I tell you this is the topic in which most of the students have not performed right now. It becomes a very rigorous exercise for the teacher to enter question level marks for each student. So one assessment typically, lets say, has ten questions into 40 students. So that means 400 entries, right? So teachers, they don't want to do it, which means they need to use my product, right? So how do you solve this problem, right? And if they don't want to use my product, then it means the school will not renew its subscription. So the schools, they take an annual subscription with me, like a typical sales platform. They will not renew the subscription because the teachers are not happy. And teachers are my primary customers. I'm not selling to the school. Students also have an app, but that's a separate product altogether. So now this was the problem that was thrown at Data Team to solve. And they said that we want to do away with marks entry, make life of teacher much easier. So what we decided was we short listed and here some of the schools want to do it, right? So it's not like all 3000 schools don't want to do it. Some of the schools which is being run by educationists. So schools are being run by three types of people. One is real educationists, who want to educate, one is businessmen who want to make profit out of it. And third one is people like politicians who do it for reputation, or industrialists, right? So those are the three typical things. Education is about 15, 20%. They really want to help the students and they are ready to go that far. So what we proposed was that we'll pick about seven to 8% sample schools. Only those schools will do a question level marks entry. So I did not understand this last part. So right now, let's say I have 3000 schools. All 3000 schools were doing question level mark entry, right? And they were not happy about it, right? So we said that we'll pick 7% schools and only those 7% schools will do question level marks entry. All others will do overall marks entry. So the teacher, right now, before buying my solution, just does an overall mark, right? This student, 18 out of 20, this students, 16 or 20, there are students, that's all she does. So she's okay doing it. She doesn't do what she's currently doing. They just don't want their workload to work. So the 93% of the students schools will continue to enter what they are doing overall marks. And only the 7% of the schools will enter questionable marks. And this 7% data will be used to predict the lowest performing questions, remedials, as we call it. For the other 93% of the schools, you lose typical features, right? I mean, typical features is what board the school is in, what subject is it, what topic is it, what subtopic is it, what kind of questions were there? So we have different criteria of questions. We have something like rigor. If the question is higher order, middle order thinking, then we have the length of the question, we have the mark of the question. So then we have the overall mean marks of the class. We have the spread of the class, you have different percentage. You can calculate at an overall mark level. Then you do a look alike and you regress, basically. So you can get question level marks. You predict question level marks and then you sort it. And then you say, okay, these are your bottom performing questions. If you are this kind of school, and if it is this kind of class, where the mean marks is this and spread is this, and if the type of question is this, these were the three major buckets, then you regress and you do question the marks prediction. And then you sort it and then say these are the five lowest performing questions in your class. And we also give the remedials and you use those remedials to reteach the class. Because when we sell our product, we promise that 70% of the schools will score above 70%. I see that 70% of the students will score about 70%. That's the promised. Okay. One question here. So all these 3000 schools are getting the same set of questions. Basically, if you are of the same board. Okay, yeah. So in this 7%, you would basically get data from all the three full boards, like all the different boards that you're dealing with. Basically, we will maintain a distribution so that it's representative of the population. Okay, understood. This is one of the reasons use cases is another use case, which is a little bit around deep learning, wherein we say that let's say these question papers are there, right? A lot of these question papers are fill in the blanks one word answers assessments. Now, can we do autocorrect wherein the teacher takes the picture and then you have a different RGB coloring around the area which captures the answer. The first president of India is Dash. He was born in Dash. He became the president in Dash. He remained presidential. That is the question before I give to the student. At each dash, we created a different RGB color that takes the picture. And once she takes the picture, you will highlight. Okay, this is what is writing. As I mentioned, there's no digital intervention here because I'm talking about Ropers and schools of India. So very limited digital intervention. So teacher tablet is the only digital intervention. So they take a picture and then I capture whatever is being entered. And then I also index it, saying basically find out the key values that this is for question A, this is for B, this is C, this is for D, blah, blah, blah. And then at the back end, I have the answer script answer sheet. And then I say, okay, prajend riboshat. Right? Then the challenges here are the two types of challenges. One is procedural and one is operational. So procedure is, if the teacher is not taking the picture in the right manner, it can be tilted up and down, left, right and center. If the gyroscope of the camera or the tablet. So we didn't provide high Gmail tablets, right? So we provide Lenovo, two GB tablets, which is like Rs7000. More than that, the school cannot afford, right? So you can imagine the quality of camera. You can imagine the quality of gyroscope. So they can take pictures in different formats. The paper can be crumpled. Then the spelling students will like, imagine one person and students, they can write things in multiple forms. Then how do you want to penalize spelling mistakes? Rajendra Prasad the spelling can be written in multiple ways, right? So do you look for exact, Rajen, or are you okay with R-A-G-N-D-R multiple combinations. So should you use sound X? We did a lot of experiments and we were not successful. So the first one we were successful in, I keep predicting for almost 90, 91% of the schools, the right? Remedials. This one, we were not successful. So we had to have the project. In fact, we had to stop the project, but did not rule it out. We did a pilot. It was not successful because teachers were not approved. In fact, we did another one, which was the teacher will just take a picture of the teachers. What they're doing is they had to enter the question level marks, right? So we said, you know what, I'll give you a sheet. You just enter the marks there, take a picture, and then it will be uploaded in the systems automatically. Even that failed. That failed because of procedural reasons. I have a question here. Do you know about cam scanner? Cam scanner? Do you think in a situation like this, cam scanner can be helpful because they have some algorithms to correct the tilt and the brightness and stuff of images, right? So if your tablets have cam scanner installed and they scan it with that instead of taking like a regular picture, would that help? That was not helping. A the teachers are not very digitally savvy, so few of them are. But most of the teachers, if you travel to semi Uber and rural India, you will realize that they can make payments using Google Pay, but they are not very conversant with a tablet or a phone. And even the students, right? So our products sell at schools where average fees is, let's say Rs1000 a month, 500 a month. So the students there are probably from the families where there is only one mobile phone in the house, right? So when father only for 1 hour, otherwise there is no mobile phone. So there is no smartphone. Even if the mother has a phone, maybe it's not a smartphone, it's a feature. So we wanted to do a lot of things at the student level also, but students didn't have access to phones. Similarly, teachers, they can do because, see, change is difficult to bring, right? When you change, when you say that you move from physical things to move to digital things, I'll capture things digitally that change is difficult. When do people change? When do people change? They change only in two situation. That is what I've seen in India. I'm talking about from Mahindra and this experience, maybe anywhere. One is either you incentivize the change and the second one is when it makes their existing work easier. And we were not doing any of these things. We were not incentivizing. I mean, we were just giving them the tablet and telling them, you teach. And if the teacher really feels that incentive is that students will do well, then she'll do well. Right? But let's be practical. Most of the teachers care about the salary, right? Like everyone making a lot the student easier is not what everyone is thinking. Right? So there's no incentive. And then I was making the job tougher. They were doing everything on physical paper. Correcting. And then I'm saying now enter it in the system so that I can give you remedials. That is where the disconnect was. And now the focus is and that is where we re Pivoted this year. And now we are saying that we'll make the lives of the teacher easier. So we are doing everything that will make the lives of the teacher easier, which includes don't enter question eleven marks. Don't do this, don't do that. Everything will be done by the system. Everything will be done automatically. These are the two major use cases. Other than this, we are trying to do school retention. How do I figure out the school is retained with me? If not, what are the reasons for non retention? And then the field teams, they take required action. So there are plenty of them here. And there one for finance forecasting, one for collections. So, contract enforcement is very weak in the urban and rural India. Even if you have a contract, even the big companies, I mean, even Mahindra, the payment cycles used to be very long with the vendors. Let's say you signed up ATLA and assume that all your payments will come on time. It doesn't happen that way. 90 days, 120 days of payments, I think. Okay, so that's the challenge. Okay, thanks a lot for setting this background. So, Shantam, now I have a decent picture of what kind of problems that you are trying to solve. Based on what I heard. Is it fair to say that most of the use cases that we have talked about are like batch inference type of use cases? And almost nothing is real time at this point? No, nothing is real time right now. I mean, the marks entry was supposed to be real time. Again, the problem is to make it real time given connection, I need to have edge deployment site. I need to deploy my models or products on the edge on the tablet there, I have space restrictions. Now, you could also do API calls. Now, if it's connected over Internet, it's. Not connected over internet, right? It's not connected over Internet. Over Internet. Once a day they do a sync because school doesn't have WiFi. Right? Okay. So teachers, there is a room, they go to that room where there is a WiFi. So we have set up that Nas device there device and then they do an update and then they capture whatever. These are the real problems. When you have a lot of things that you would like to just assume that, okay, that must be working, you're like, oh s***, like all these assumptions are invalidated. Okay, I see. So then like the process would be that every day your cloud system would get a data update or something and then models would run in the back end. Basically. That's probably how the system is configured in that case. Okay, and are you using which cloud are you using? AWS. AWS, okay. And do you end up using like Sage Maker or something for your model building training, et cetera? We use Sage maker. We use a lot of APIs for OCR that AWS has like the table API, very expensive, but still there is a table API. There is a text API, quite a few of them text and form. Do you use generally end up using like segment comes with some pre configured models. Right. You end up using primarily those. Or we run our own ones on an account or something. Okay, I see. And you end up containerizing all your models? Most of them, yes. I see, okay. Who does this containerization? So like how is the team container? Is this like separate data science and engineering team? Okay, so there is a separate data science analytics team and there is a separate data engineering team. The containerization process or the API build process. So what data science team does is they build a model, they create a pickle file and pass it on to the engineering team. And then the engineering team is the one that will do containerization and deployment or ask whatever they are using previous. One thing here that means the part of sediment that the data science team uses is the same notebooks. Assume in that case update the pipelines so that models run, schedule them and all that. So you use pipelines as well? Yes. Okay, I see. And then the pickle file is uploaded to a s three bucket or something. Yes. Which is then passed to the engineering team. But besides the pickle file, you would also need some inference code as well, right. Because a lot of times you might be doing some kind of preprocessing post processing on top of your model or some code as well, right? Yeah, I think that's a batch process. So it's a scheduled process if I remember correctly. There is a preprocessing to recalculate the model and whenever we get a new data, we pre process and recalculate the weights. Like in those weights are past passwords. I think that's a batch process, if I remember correctly, because we reprocess and rerun once a month. I see. Okay. So actually I think the question, for example, the example that you gave earlier, which was around predicting the question level marks, right? Let's say you have ten questions. Okay? So essentially these are like ten regression problems in some way per question you're trying to solve like one regression problem. Right? Now the bigger model is going to predict is it will just give you a vector of size ten with like ten numbers as an output. Right. But that vector itself is not interpretable to the end user because they probably don't even remember that which index corresponds to which question at that point. Right. So after the model does this prediction, you might need some structuring of the data set where you are like okay, question one was this and here's the and stuff like that, right? That's what I call kind of postpressing here. Now, your pickle file of the model itself would not typically capture this information because the file output inside that vector of ten numbers. Right. So when the data science team passes this pigment file, I'm assuming that they would also need to pass on that this extra function on top of it, which takes effect as an input and gives something more interpretable as an output. How does that process work out? So when we take an input before the model runs, we take an input, right. In that input we also take some indexes which says this is for this subject, this class, this ASM like I said, for each subject class, I have ten assessments, like the assessment number, and then I have the question ID, and then each ID is code and then my JSON output will actually be these four. Right, and the output for each question. Correct. And this JSON output that you're talking about does not directly come from a pickle file, right? Pickle file is generally will not give you just output, right? Right. So we go back and we link it to the question ID, which is unique to an assessment is there in the pickle file. We go link it to the assessment, the class subject. Okay, understood. Actually, how does the requirements of pass to the team? Because you would also need to know that which pickle file was trained using which version of a certain package and stuff like that, right? I'm not sure, honestly, I don't have that answer. I see, so you are saying which package of like exe boost for example. You mentioned about training an exe boost model, right. So there's a lot of fixed python whatever version is being used. Right? I think right now we use I don't exactly remember the version which we use, but yeah, that version is important. I don't know man. Probably someone from the engineering team would not understood. Actually this is a problem of a data science and engineering team interface, actually, because it's possible that a data scientist in their Jupyter notebook or whatever, like Sage Maker notebook, ended up installing a certain version of a package, right? And they got a pickle file output. Now, the engineering team may not always be able to read that if that version is not known to them, I guess. So the thing that is transferred from the team to the engineering team, this process typically gets broken in that transfer. Essentially. That's why I asked that question. Basically, last I remember, we asked them to upgrade the production server packages because we were building it on the latest packages and they got them at the deployment. That's a valid problem because I remember asking them to upgrade. I forgot which version did I ask them? Okay, understood that's okay. And so, Shantam, like in this overall process, right? Actually, like one quick question is around the team size and all. So how many people are there on the Data Science team size? And how many people on the engineering side who help the data Science team? Primarily? So see, Data Science Pure Play is only about four people. And there is one senior manager. Engineering side would be around the same number of people, three people and one Senior Manager. I see. And you manage both the science and engineering? I manage both. Science and analytics is bigger. No, I don't manage engineering. I manage analytics and data science. Okay, so analytics is bigger because it includes a set of people who do dashboarding and reporting and all that. So data science is about four people and one senior manager. Analytics is about six, seven people and one Senior Manager. Engineering overall is about seven, eight people. But engineering doesn't interact with engineering. Their KRAS are to maintain the data platform. So our data platform, the single source of truth, is on Redshift. That's where all the base tables, reporting tables, and aggregated tables, denorbalized tables, everything is there. And their second job is to deploy the models. Those are the two areas for data engineering. They report into Engineering, I report into product. I see, okay, understood. Got it. So in that case, this entire prediction type of problems and propensity type of problems, basically, that would fall under the analytics bucket assume. Got it. And the output, where is the output of that model displayed to the end use? I'm assuming that end user is like some business person, operation person, etcetera. Right? Product. Product. Okay, I see. And how is that output of the model delivered to the product? You mean the Churn prediction one? Yeah. So Churn prediction one right now is the UI is one of the reporting platforms. It can be a looker tableau or Google Data studio. I see. If you have to think about all these, like you mentioned about some Data Science problems, some analytics problems, like Core ML, like deep learning type problems. Right. And also there's an entire operationalization element of this entire thing, right? Where do you think the team is spending time on that they should not be spending much time on? Where are the core problems that you're, I guess, trying to solve right now? See, core problems are with the single source of truth, figuring out where the data is and what does it mean. It's a very typical startup problem where you cannot go to the ear diagram every time you don't understand the meaning of an attribute or variable, right. Trying to figure out how the developer has coded and what are the values capture. So data lineage, maintaining what attributes are, how they are calculated, blah, blah, blah. So this is a challenging task right now where people spend a lot of time, and the second biggest time is on deployment, wherein it takes a lot of time. We simplified a lot of things over last six months, but still, I would like the deployment timelines to be faster because we do a lot of experimentation, right? And every time I change something, I don't want to wait for another sprint of two weeks for that to be deployed. So that is what we are trying to solve through Sage Maker. If we can do auto deployment, it has those features. You build a model there, you train it, you do wipe parameter tuning, and once you're satisfied, you deploy directly from Sage Maker if your applications are on AWS. But yeah, so those are the two major areas. I see. So on the second one, what is the current timeline of deployment and where is the bottleneck? So, current guideline is you train a model, we create the pickle phi, we provide the specs to the engineering team. Then PM gets involved, whose product is going to consume it. If it is an internal product which is getting consumed through Tableau or Google Data Studio, then we don't have a PM. They are Data PMS, data product managers who manage it, which again, is a part of my team. But then if there is a student app or a teacher app involved, then the teacher PM will get involved. They will decide how the feature will work, what the teacher will see. A flow will be created where those connections will be made. Then Data engineering comes in and they say, okay, this is the input I need in this format. And then this process will run once a day, twice a day, six times a day, how it will be updated, and then it will be sent back to the this is the output format, and output format gets displayed. So this is how it works right now because three teams get involved in engineering, data science, and product. And everyone has their own set of restrictions. Like, I don't want to rerun my model if I get below x number of data points, because that doesn't have value to my model. So I'll say as well, keep waiting till. I get X number of schools enter the data only then the model will rerun. Then whether it should be a day process, it should be a batch process, every day it should run or it should remain as is. And then who picks it up? When they pick it up, when should it get updated? So bringing everyone together is a bottleneck. Creating those architectural diagrams is not a bottleneck. But yeah, making those connections and making them work. This versioning issue is a very real issue. And then we personally want to move to GCP BigQuery, so we feel that the ML stack is much better. But then how do you do that? When you move to different platforms, is it fungible enough to move? Then what are egress and ingress costs? Like we moved everything to Salesforce Cloud. So we moved from tableau server to tableau online. And when we did that, AWS, my base data is in AWS. My reports are being published on Tableau Cloud, which is on Salesforce Cloud. So it becomes very easy on Salesforce Cloud, by the way. I mean, it's very scalable. But then there is a cost involved, right? There is an ingress and egress cost. Whenever data moves, how often does it move, how much does it move? And given the current scenario, there's a lot of focus. So now we are whether we should bring it back to server where it's deployed in AWS. Those are the typical challenges. Very interesting. On this note, a couple of follow ups. Have you all considered using Streamlit for something like this? No, we haven't. What is that? So Streamlit is like a data science app building platform. Call it data. Science focused. So, like, data scientists without the knowledge of any front end code can just write some Python scripts and it will generate some applications, basically web apps, basically. Okay. And that you can share with your product team or something. So they are not writing any code, they just end up using web apps. Right. Streamlit, is it streamlit? Stream. So it's like a visualization tool in some way. And the good news with this is it could potentially solve the problem of the data ingress igress cost that you're talking about. Because now you can host your streamlined apps directly on your AWS servers and your Data Science data science team has complete control. Like if they want to update something, they can just quickly go change the two lines of Python code and the app itself is updated. So I guess that process will also get streamlined. Got it. This looks interesting. Yeah. Thanks for this, man. This will be useful. And then the second question here is that you mentioned about the entire workflow that involves multiple teams, right. The net time taking would be in the order of multiple weeks to release any changes. Yeah. Would it be like four weeks? More than that, it's about six to eight weeks. Yeah. Okay. So as you mentioned, seems like you are already familiar the state maker kind of supports like this auto updation version controls, et cetera, models. Right. Why do you think the team has not adopted the current auto upgrades of the model itself? I understand that there is a product integration element that cannot be automatically done ever. At least the model version upgrades without breaking the interfaces can probably be done in a more streamlined manner, right? We are trying to do that. So a lot of team members are not trained on AWS, honestly. And it's not as intuitive as GCP, so I have some experience with GCP when I was at Mahindra. Even Azure databricks is relatively better. But AWS is also good. But we are trying to get more people certified and trained so that they can move in that direction. Problem with AWS is if you realize there are so many products that you get confused between what can be used for what and that is what GCP is better at doing. Right. I mean, they have few but really good ones. Instead of creating a bunch of things and people get confused. I think you are hitting the same problem that I keep hearing over and over again, but I have not been able to put like a dollar value impact of this. Like when you say that you're having your data science team go through the certification at all, how long is the training and on boarding time according to you, on AWS, Sage maker, like how long do you think it will take each other scientists to kind of truly learn and start using? I would say at least six months. I see. Given the workload because everyone, they have their own set of workloads, everyone is running a tight ship. A smart guy will take at least six months. I mean, they will learn, they will practice and then they will start using. It dependency. Right. I mean, even if the data scientist learns everything, but the other team should also be aware of it. The data engineer should also be aware. The product should also be product is usually a length. I mean, they don't care about the back end that much as long as they get what they want. I see, and if that's the case, then from your perspective it's like as a leader, you're having a lot of your development team spend time on something that I guess is outside of the core value add to the business, right? Absolutely. See, ideally I would like my data to be as clean as possible. I mean, real world data cannot be very clean. So realize that, see, model building is only about 1015 percent of the time spent by data center is spent on understanding data, figuring out what is what. If I can reduce that, then I can do more module updates, I can keep on iterating, building better models. And then if deployments are faster, I would like to right now we don't have an experimentation platform. Every A B test has to go through the same deployment cycle. And it's a challenge, it's a big challenge. So we were trying to explore some product level A B testing platforms. So we have HubSpot and then there is this vendor, Light, forgot the name. We had very advanced discussions with them. What was the name? Very famous product level analytics vendor. They'll give you what features are used, when, how amplitude. Amplitude was one, but the closest competitor of Amplitude was the name Pindo. Maybe not with P then. Mixed panel. Mixed panel, yeah. So we were trying to do it, but the economics didn't work out. So we were trying to see if we can build it in house, something similar. But again, it's not real time like you mentioned. Like A PM should go there and should be able to view real time. The teachers are going there, then going here, there should be a funnel view where they are dropping and all that that we have not been able to build just because things have not been real time. Got it? Okay, understood. One other thing that I want to learn a little bit sushanta in this context. Only when you talk about economics, like how is the budgeting done at a startup this scale, right? What kind of tooling budgets are no brainers. What kind of are moderate, what kind of tooling budgets are like unacceptable. And I guess at what level are these budget calls typically made? I'll tell you, it really depends on the funding cycle. Yeah. Six months back you would have spoken to me. 1015 lakhs a year was not a big thing. Okay. Above 25, I could have approved. I mean, I'm two levels below the CEO and we are a decent we just turned a unicorn. 2000 employees, about 600 cores in revenue. So we are a decent sized startup. If you talk about in a steady state, a typical VP of product or a typical VP of Technology will be able to approve about 1015 lakhs a year. If you're going up to Cr from there, then a CTO or a CPU gets involved. Okay? Okay. And then beyond that, obviously CFO will also get involved. And then there will be a budgeting thing, typical budget for a data science team in a startup of my size, I'll tell you, including salaries and everything else will be about five SR. Okay? ICR, you support 1520 employees, you support the travel, you support the tooling, you support the infra, you support everything included. I see. But right now it's maybe about three C and a half year. But in a steady state, depending on the funding cycle, that is where it will be an ola, heavily funded and you don't care much, then it will be much higher. But here even the devop costs are included. The cloud costs are included. How much do you spend here? How many tableau licenses is how many you have. So there are quite a few costs, but in a good scenario, ten to 15 lakhs is not a big amount a year. Okay. But right now there is a lot of scrutiny. I can tell you lack or something beyond that. I mean, there is a lot of scrutiny by where and all that, but these are not usual times. I see. And one other question for any of the problems that you mentioned about deployment times, right. The package issues and stuff like that, are any of these burning enough problems that you are looking to solve right away or you feel like you're kind of comfortable with the status quo? So right now I'm not comfortable, but I'm pushed to be comfortable with whatever I have, honestly, to make things more simpler and free up the see, my aim or my objective as being the leader is to make sure that we so 100% of the time spent on core issues is not something that's going to happen. So that's not a reasonable expectation to have. But I want at least 60% to 70% of the time of a typical employee to be focused on the core things. 30% to 40%, they can do other things that be it in meetings, a lot of time is being spent on meetings. A lot of time is being spent on figuring out how to deploy churn is a big issue. When people leave, they take away the institutional knowledge. Documentation is an issue because you are not a big company. Documentation process is not structured. So if someone leaves, I mean, basically you have to start from zero. Right? So those are my typical problems. And that is where most of the people are spending more than 50% of their time instead of doing their core job of improving a model, building new products, data products, coming up with new improvements, recommendations, insights and all that. But yeah, right now I don't have an option. We don't have a lot of options. We did renew our table license very recently, two, three months back, because we realized that it's an important product to take a note of. People are using it, but we are not renewing or getting into a new subscription like a mix panel and an amplitude. I would have liked to purchase a decision tree or segmentation tool like Knowledge Studio but even that is not happening because that would have made a lot of things much simpler. So that's the state we are in currently. But obviously no one likes the status quo. So I'm not liking the status quo. But yeah, six months, probably maybe 9th. Okay, I see. Understood. Okay, so I'll give you a little bit of like I know that we are way over time at this point. I will give you just three, four minutes overview of where we are, what kind of things that we are trying to solve to shut them. So the whole idea of what we are building pretty much is to speed up the deployment process for people without having the Data Science Team incur a lot of learning cost, okay? That like it is super easy to get started with the platform and all the common things that people end up facing. For example, this package management issue, model versioning, rollbacks, logging, monitoring, documentation, right? Metric tracking, all these things that people anyway end up doing or not doing and suffering because of that. We try to build that as part of the platform. Okay, and the hand off from the Data Science team to the engineering team, that happens. Like actually a lot of teams today end up doing like a call it an incomplete handoff which might require more back and forth between the two teams, basically. Right. Or just delay your overall end up deployment process. We also solve for that problem that is like your hand of using dockers, for example, okay? That actually covers a lot of these things, like your code, your infrastructure, your configuration, a lot of these things it kind of covers already. So that's primarily the problem that we try to solve. So very Data Science friendly tool, but hopefully advancing your deployment state to some of the best companies in the world. Okay? A lot of what we're building right now is derived from our team, like Mesh, both of us. You are at Facebook and people actually built out this platform called Predictor, obviously, which is very advanced, but that level of advancement is not needed typically with most companies. So we kind of like take some of the best core practices from there, encode that to the use case of the mid market. And basically we have built out this. Product and this will be used by the Data Science team. So this will be used by the Data Science team? Yes. And how does the department work? Like, how do you guys integrate it with the current processes? Yeah, so it's actually very straightforward. So if people are using things like whatever, like Jupiter notebooks, you could actually directly integrate like a Pip install our library, okay? And from there you can deploy the model. Like, we have a couple of APIs that we expose. And from there you can directly deploy your model as an end point. That's number one that typically works out very well for your prototype use cases, experimentation use cases, etc. People don't like to do that for their production use cases because digital notebooks are kind of flaky for production. Right. So in that case, we also integrate like once you start maturing, right. We also integrate with your CI CD pipeline, for example, that we push your code and you immediately get your wallet deployed, basically. So from Jupyter notebook to CICD pipeline, anything in the middle, like if you're writing a Python script and you wanted to deploy using command line, we will work with that as well. Okay. Yeah. Interesting. So actually that's kind of how you design the platform. And one of the other things that you mentioned, which actually I don't hear a lot from startups but we solve for, is like the migration of one cloud to the other. Like you're currently using AWS. We wanted to explore with GCP but we were concerned that how is that going to work? The way we are building out the platform is like actually going from AWS to GCP or to Azure. It's practically just a couple of clicks at that point. Okay, that's good. So those are a few problems that we are trying to solve and ideally the way we are working with Sharon right now is kind of working with a few design partners. The good news about that is because our product is not generally available, we also work very closely with the teams that we are working with so that if there are certain things that are required for their workflows and are missing in the platform, we actually go ahead and build that. I'm sure you would have seen that journey for the school as well. So that's the state that we are in now. All of that said, if you feel like right now is the timing is not right, that's totally okay. I think what you have done today, like giving me this background is already super, super helpful. If you feel like this problem is something that you might be interested in solving and you're curious to learn more, we can even set up a follow up call where I can walk you through the platform, show you some of the data, science interfaces, etc. And if the cost itself is a concern, those things we can figure out. As a startup at this point, my major goal is not to like make a lot of early revenue, but for me it's actually serve a lot of early use cases. That's my goal as well. Okay, do you guys have a website where I can go and read about it? Yeah, so we have a website. I can also send you some resources as well. You can ask my team members because we are a little frustrated with us. Right. We try to push a lot of things very quickly and then the engineering team says that if it works out for everyone, probably we can give it a shot. Okay, cool. I will send out some resources to you as a followup to this call and have been the website anyways. And after that, whatever works out best for you, just let me know. I would love to stay connected to Shantham and if I have more questions in this domain I will I guess, helping you and ask. Absolutely man. All the best. All the best. I hope you guys are really successful here. Thank you so much, really enjoy chatting with you. Same here. Have a good one. Bye.