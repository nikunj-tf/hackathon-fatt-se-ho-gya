Hello. Hello. Hey. Hi, Sarath. How are you? I'm good. How about you? Yeah, I'm good as well. Thanks a lot for finally taking out time to kind of meet. How is your health now? Yeah, all good. I started working since yesterday. Still not 100%, but yeah, I would say I'm back. Okay, got it. Cool. Just give me 1 second. We had interacted earlier with a few team members at Stylumia. And that time I think we were discussing about the experimentation part of the platform as to what we can enable to support the experimentation needs for Stylumia. But then I think we were not able to take it further. I think in terms of, like, our platform was under development, I think the folks were also busy. So I wanted to kind of take this opportunity to know a little bit more about the ML initiatives that are going at Stylumia, what is the current state, and then maybe you'd love to get a picture of how you reached out. Was there like a major problem or anything that you would like to kind of think about and solve? And then later part of what I can do is I'll give you a high level overview of the platform. Currently, like, a lot of progress has been made, and we are also working with a few clients. Where are you based, by the way? We're based out of Bangalore. Okay. What about you guys? Yeah, I am in Bangalore. Most of the team is in Bangalore. Like, one of my Cofounders is based in US. But yeah, who generally based in Bangalore. Got it. So see, I will not take a lot of time at a high level, not much has changed. I would say in terms of the ML development and the workflows. Right. We're still building a lot of computer vision models. Maybe one additional thing is that we have actually started building a lot of NLP based models as well on text classification, intent classifications. There are some few experiments or pilots or POC that we're trying to run at a high level. Our ML experiments have not changed. So what has gotten better this time is we have a much more streamlined process of our ML workflows. Right, okay. So that is one of the reasons why I reconnected back saying that now that we know, what is it that we're looking for, not just from a platform perspective, but what is it that we want to measure, track all of that. So from all the experiments that we're doing. So I thought it's been a while. You guys were very new at that time. And I think in one of the communications sunwag, you told me that there's something that you are building to become better, faster. Right. So that's one of the reasons why I came here, because got it. I just give you an example. One of the comparison tools we're also looking at is called as Neptune. I don't know if you guys have heard of it. Yeah. Heard of Neptune? Yes. So that's another thing, another toolkit or platform that we are also considering. Okay. Got it. So, like, a few questions that I would like to ask. One is currently, like, when you look at the ML workloads, how does the final deployment occur? Do you use AWS or do you use Kubernetes? How is it kind of yeah. Our deployments happen on a self managed Kubernetes. Okay, right. So what we'll do is one differentiation, I would say, is that we generally don't deploy GPU based models. Okay. Right. What we've done is I'm not sure if you've heard this, but we actually worked very closely with intel for a considerable period of time, and we've essentially built some kind of technology to convert GPU based models to CPU based models without compromising on accuracy or speed. Okay. In some cases, actually making it significantly faster. I see. Right. So we actually convert all our GPU based models to CPU based models, and then we deploy them. Okay. The final deployment is still on a self managed given it is cluster, there's. Just one additional step in between where we convert it into the structure or the model type that we look for. Got it. And like, who does this deployments, sir, that like, is it done by the infra team, or is it like the ML engineers in the team themselves are able to run? I would say we have an ML Ops engineer in our team. Okay. Deployment. So the machine learning data scientists so the machine learning engineers are not expected to deploy the model. I see. Yes. Got it. And how big is the current team in terms of ML engineers, intra people and so on? So infra, I would say, is two. The data science team and the ML team is about six people. Six to seven people. Okay. Got it. Cool. And one other thing, sir. I'll give you a high level quick word. We have already built a platform that is actually on top of Kubernetes. I believe you would want the system to run on your cloud, and you would not want a data flowing out of your system. Is that like a requirement of the tool that you are evaluating? I wouldn't say it's a hard and fast requirement. Yes, it's better if it is that way for us. Okay. But if that is going to add a layer of complexity and say if you have to try this out and self hosted is like four weeks versus cloud taking two weeks, then I would go faster. For me, for me, speed is more paramount right now than I would say data controls. But yes, finally, we would like to see if we can have a self hosted system. But again, there'll be multiple considerations, like cost, time. Right. Got it. It's going to take more time for my engineer to maintain and keep the system running, then I would rather use the cloud one so I can't give you a concrete answer. Okay, got it. It's fine. The reason why I was asking is we have both the versions. One is a public cloud hosted version. The other is like what we deploy on your Kubernetes. But the way we have made it, like we made it very easy. Hello? Like your software engineering services and other things. Hello? Yeah, I can hear you. Yeah, I was telling do you deploy services, et cetera, also on top of Kubernetes currently? Yeah, most of all, I would say 90% of our deployments happen on Kubernetes. Very rare cases, unless it's like a really quick POC or a pilot kind of thing. Then we go to other sources like streamlit and we try other things very rarely. But most of our production systems are all in Kubernetes and dockers. Got it. Cool. So the way we have built it, like both the public cloud is supported and then you can actually deploy on your own infrastructure. And there we provide a layer. So it's very easy to orchestrate the Kubernetes deployment. So it will connect to whatever cluster you are running, and then it allows you to manage that cluster also automatically through the interface and then post that. Then there are three parts, majorly of the system. I think the one that will be more useful to you is probably the experimentation and the deployment. And I'll talk about that. So the experimentation side, like basically what basically in some way neptune or Weights and Biases and other companies do in terms of being able to track, log, monitor your experiments, track the metrics, look at the plots, compare between them, track hyperparameters, then log other artifacts and so on. So that functionality is there in the platform. So that is one library that is exposed. You can do it directly from the UI, you can do it from the Jupiter notebook. From there, what you can also do is if you are running experimentation over, say, remote machines, you can kind of spin up clusters, you can spin up like resources on top of cluster and then run your training, and you can compare your training runs, et cetera as well. And from there, like when you have to promote or you have to move it to production so that is also supported. So you can deploy it to a test environment, promote to a production environment, and do it through a complete CI CD with auto scaling and all enabled. So that is the second part of the platform. The third part is more like online monitoring, which as of now is still work in progress. So I think the major part so Neptune, what you are looking at is probably more on the experiment tracking. So that's the first part. But we have both the experiment tracking as well as the deployment piece. Pretty well built actually. So it's in a state where it can actually be very well used. We are currently deployed with three enterprises primarily. One is like Reliance, actually one is Synopsis, one is another healthcare company and then there are three other pilots going on with like b to b unicorns, like what fix is their housing is there and one more other company. You got it. That's the state right now. So still very early in the journey. So I wanted to kind of think of it more like a help from you as to what can we do to ensure that for your needs, we kind of follow the right part to ensure that you actually end up using us as a platform. And I'll show you the platform like maybe in five minutes. So that way you get a high level overview at least. What would be the best steps that we can follow so that we can get it evaluated well with the team as well and also kind of potentially get used internally. Yeah, so one thing is definitely like a small pilot or PC, like a week or two weeks. Okay, we can get access to your platform, but like I said, we don't need the deployed one right now. If it's going to add time to move faster this way with the cloud one, then I'll actually get my team to try it out right from experiment tracking, we'll also try out the deployment one. Okay. Of course my team will have more deeper technical questions than me because I'm not involved in the day to day execution of these things. But mainly my concern would be on the if we decide to use this, how much time and what kind of effort does it take to use this? Because we've tried a few experiment trackers tracking systems in the past and they're not easy to use, I would say, okay, just to track an experiment, you have to write so much code. Again, sending data to those platforms is not that easy. Okay. It basically just started adding 1015 20% of additional time to do some experiments. So that's something else I would like to minimize. It should be no core solution or something, but minimize the additional effort as much as possible. Got it? Understood. I think that is fine. So we can actually try and discuss with the team and showcase them a mode live demo in terms of what is needed. So generally everything is built as a very API driven and in Python library. So generally it should be very easy to use. So like logging of metrics and all, it will help save quite a bit of time, that's my experience. But I would let the team maybe see the demo and then maybe try it out as you said in a POC and then take the call from there and then both the deployment, whether you want later on public or you want on your in house, both of those we can do. And we can help there. So both of that functionalities will also be there from a timeline perspective. Sir, how are you thinking of it? How do you kind of think, for example, next step, setting up a call with the team to just deeper dive and then showcase the demo. Then we can set up an account with access to a few team members and then they can try it out and then the next step. So how are you thinking of it there? What my team told me was that last time there were quite a few gaps. You take the demo and then we actually have a sync on Friday. Okay, for the next steps, what do we decide as the next steps? I see, so maybe in that meeting I can bring up whatever. If you have a demo to show for ten to 15 minutes, then I can take that to my team and then we can decide. Because sometime mid next week or starting next to next week, we will start all the trials with the other vendors also. I think so. I think we want to take the decision by the end of this month on if we start, say around 10th or 13 February, if you assume two weeks, we are done with the pilot, and then a couple of days later, we'll have the decision. Then of course some pricing, but also we will have to discuss but mostly it's a technology decision for this. Got it, understood. Cool. Let me actually show you once the platform Sarah, so I'll show you starting from this part which is basically the Kubernetes cluster part. So the way the platform is built, like it works with whatever you are using internally. So for example, your docker registries you can integrate your git or any accounts that you are using, you can integrate. If you're using Secret store you can integrate and so on. And this first part is where you connect your Kubernetes cluster. So imagine if you are using a public cloud then this will be our cluster. If you are using on your system, then what we do is we provide like this helm chart. With that helm chart you kind of just run it on your system and within 2 hours the system is deployed on your subordinates cluster and your Kubernetes cluster will start to show here in the cluster. What kind of resources does it need? Just a side question. These are the applications that we generally install along with it, like Argo, Cdrgo workflow. Some of this we can disable or enable depending on the use case. Like for example if someone who needs just say the experiment running, then some of this might not be needed. So that way we kind of enable it. Right. Also once you have connected your given it is cluster, you can create new clusters here from the corresponding like whatever even multi cloud support is there. You can even select regions if you have a base domain URL, you can do that. If you are using something for monitoring, you can give those URLs for connection and passing the metrics. You can even control the instance families like basically CPU machines, GPU machines, et cetera. If you want it like in a certain cluster, you don't want people to have access to GPU machines, you can remove that and so on. So basically the entire cluster can be easily managed from here. What about GCP? Is that also yeah, GCP is also supported. So basically AWS GCP, anything like basically it's multi cloud. Got it. Now, once you connect the cluster, what happens is for visibility to the infrastructure personnel so that their job becomes easier. First of all, they see everything whether the cluster is connected or not. Then they can divide the cluster into smaller workspaces. Suppose you want one team to have access to certain CPU, g, memory, et cetera. Then you can create a workspace and you can allocate it to that team with basically admin, viewer or whatever access and you basically kind of create that. So then that user will start seeing this workspace along with these resources that is allocated to them. So you can create multiple workspaces as well. One way the companies operate is generally they will create like for smaller teams, a dev workspace, a test and a production workspace and dev workspace. They will make researchers or the data scientists admin and the production workspace. They will make the ML engineer the admin, right? So that once you have run the resources and you want to push, you can do that. So this kind of also gives you a visibility of what cost is going on and anything if you are deploying and so on, you also have entire visibility of who the collaborators are and all of this can be well managed. So this is the intra setup part. Like if it's our intra, then this will already be created for you. Otherwise it takes like 2 hours to kind of get it created. Then the next part is kind of the experiment tracking piece which is there. I just wanted to ask about access control. Can I control once something is deployed? Can I control within a cluster who sees what? Oh yeah, of course. One access control is at the cluster level which is basically here. When you kind of configure the cluster at that time. Just trying to see where just 1. Second I think there in edit cluster maybe after you create it. Yeah. Even I'm getting confused. But there is a club basically before the cluster connection you can basically edit. So one is at the cluster level, you can add a user as an admin member or viewer. Okay, got it. The next one is at the workspace level. So in a workspace if you wanted to kind of again create this workspace, only these people have access, then you can create that like as an admin viewer or editor and then at an individual service level also, you can basically. 1 second I'm just trying to see. Individual service level right now. It's not at the service level, it's at the workspace level. So either at the cluster level or the workspace level, once you have given access to workspace, then within that workspace, the person who has access will be able to see the corresponding deployment. You are asking whether the access control is there at the deployment level also. Yes. Okay. So this is something that should be easy for us to build because ideally this functionality is there from a team's perspective. Okay. These are the team members or even like individual members. So if you want like individual deployment access control that can be built, that should not be a problem. No, it's not like a breaking thing. So let's see. Not an issue. Yeah. Okay, so that is one thing. Now, on the experiment side, like the way you do is wherever you are running your experiments, you can continue to run whatever jupyter notebooks you are running and so on. All you need to do is here in the platform, you get like an API key. You kind of copy this API key and you enter that API token in your notebook and you connect it through that. And then there you have to run like a few commands, like log metrics, log data. So the APIs are exposed. I'll show you the we wanted to see the. Model. I generally don't give the demo. That's why I'm getting a little confused because generally a Vishak or someone gives. The demo 1 second log here. So when you are kind of running your metrics, like this is a simple API that we have for logging. So log params log metrics over time. Like it's again a simple run log metrics. You just do this and your metrics start getting logged. If you have to log artifacts, all the kind of artifacts are supported. You can just log an artifact and give the name. You can log models as well and with the model version and all. So once you do that, like, it's very simple. Like you can come to this dashboard. Suppose you ran different versions of a model and you logged different parameters. Then this is like the project that I stored. And within the project there are these runs. Then you can select either all the runs or whatever runs you want and you can compare them. When you compare, you get to see your hyperparameter comparison. If you have logged metrics, you get to see that as well. You can select which metrics you want to compare. You can also like these are again different metrics that are logged. You can compare across that. And if you wanted, like you can also log your plots. In this one, the plots are not logged, but the logging of the plots is also there. And then we can further go within a run and in that again you can see more details like what are the hyper parameters. If you wanted to see all the metrics that are logged, all of that is logged here, your data and feature metrics. For example, if you wanted to kind of here like there is one split, but you can do test train other splits as well. And you can then compare the data set level as well. And all the artifacts that you will log will show here in the files, whether you are logging the data set and so on. So this is like a sample, like an example. You can log any files here. So once that is done, you also have an option to log your models. You can log your model in the model registry. Yeah, actually this is one of the features I think which was not there last time. Which one? The model registry? Yeah, the model registry is there. I'm just trying to see okay, let's. See. Which model will have that. I'm just trying to see. Give me 1 second. Yeah, so like here the models are there. So you can log like the models easily in a model register, different versions you can see as well. You can do your schema, logging your metadata and all can be easily logged very easily. All your files are accessible here as well, including your model and then your metrics. And all this is just very simple example because no, I think we're just. Trying to showcase capability question on this thing that where will these models be stored? So you can store it basically if it's connected to our cluster and on our cloud, then it will connect to the corresponding s three and so on and then store but if it's on your cloud, then it will store at your corresponding cloud. Okay, so we can control where it's stored, right? Yeah, we store all our models in cloud buckets. Google yeah, that can be controlled. Yes. Awesome, that's perfect. And then the magic here is like once you kind of have a model registry, just to give an example, like you have say log their model in a model registry, okay. Then the deployment of that becomes very easy. So I'll show you the deployment workflow here. You can deploy services like your normal software engineering services or even preprocessing and post processing functions. These are jobs which could be a training job or it could be like even an inference job. And these are models. So the very simple way, suppose developers will generally use this UI and then they will use the corresponding CLI version or the Yamla version. But for them the deployment is as simple as like select its model. Then you select a model, you select the workspace you have access to, and then you can easily load whatever model you have logged. You just select the model and you give the model deployment name and you click Submit and the model will get deployed. So if this is running in our Kubernetes cluster with these controls it will. Automatically start yeah, it will just automatically host on the cluster and then it will create an endpoint as well. It will create this endpoint that you can call or you can look at. So this is like an example of one of the demo where you are hosting like an application like as an streamlit web app. So you'll get these endpoints and you can test it out as well there if you want. And then suppose you deploy it to a test environment and there you now you feel it's okay and you want to promote it to a production environment. You just select the corresponding thing, select the application next step and then it will promote you to the production environment or something. Got it. So this is how the models part will work as well. The models will also have an endpoint within models. What you can also see is like we kind of also kind of expose this open API. So if you want to actually test it out, you can actually test it out here as well in terms of the request and all within models, like if you have different versions this has one version. But let me take an example of maybe another service that has multiple versions. Yeah, so if you have multiple versions then it's very easy for you to redeploy like by just clicking there and then you click and redeploy. Got it. So that functionality is also there. I also wanted to quickly show you maybe just a service or a job deployment. So the way it works is you suppose have to deploy a service which could be a preprocessing function. You can deploy a model as a service as well. You can deploy a streamliet app, you just select the corresponding workspace. You do a next step and then here you can load directly from whatever GitHub repository you are loading, like whether it's a bit bucket or anything or if you have a docker image, you just give your docker image. You can also control the endpoints. Like if you want to expose it as a particular endpoint. If you want to kind of put custom security rule with particular users to have access, you can do that. You can configure your environment variables here if you want. If not, that's fine. This is to ensure that you don't write secrets and then you click Submit. Here we expose the YAML configuration so that can be used for your CI CD as well. Got it. Just from like within five minutes we are generally able to take anything like a service or a model and deploy it to production or deploy it to whatever environment you want. This is very interesting actually. At least I'll have to talk to my team but I don't think we have found a place where all three are together. So let's see. Okay, interesting. Yeah. So, I mean, I just wanted to show you a high level overview of the platform so that you get a sense of what it is. But we can do like a good detailed demo, sir, if you want, that can be set up as per whenever you say. Yeah, I think, like I said, this Friday, we're discussing about this particular case. So I'll give you a time slot maybe next week so that I'll get some of my team members here, both from the ML option architecture and maybe a couple of data scientists also. Okay. So and just to kind of be sure, like, in that demo, like, when we set that up, you would want us to give an overview of both the experiment tracking and deployment is both or just any one part, mainly. Focusing on experiment tracking and monitoring. Okay. Because like I said, deployment is something that we have already, I would say mostly solved from our side. Deployment is not that big a pain as the experiment tracking and monitoring is for us. Okay. So I think those two pieces can be focused and then the rest are you okay for another minute? Yeah, absolutely. I actually wanted to ask about pricing, because the moment we go to a tool, the finance team will get involved. Right. My CEO also will be like, what are you planning for an ROI on this rate? Yeah. How do you charge? Right now, we charge in terms of number of users. Only thing is when you deploy on your cab, like, we have a minimum of ten users that we will charge for. So the charges based on which part of the platform you use. Generally, we would prefer if you use even the deployment here, because that makes it easier for you. We can expose that to you. Generally, we charge around $150 per user per month. So with ten users, it becomes roughly like $1,500 per month. And then for a year, it will be like, whatever, $18,000 for like, ten developers. But we can discuss about it. I would love to hear what is it that you are thinking of, what is okay for you? And based on that, we can do something potentially got it. That I'll discuss. I'll discuss. But yes, what I would really love is some kind of a Lakat also saying that if we don't use deployment, what is it going to cost? Use experiment tracker. What is going to cost? Because I'll have to make I'll have to pitch this internally. Right. So if we can work together on that, then it will make both our lives easier. Yeah, we can work on that. The pricing I was telling you, actually, to be honest, with enterprises, we generally charge much higher. So I'll give you an example for synopsis. The pricing that we were able to kind of go ahead with was something. In the range of $400 per user per month. But I do understand that's an enterprise, they have different requirements so we can discuss and we can make the pricing that is very comfortable for you. So the goal is really to have more users. So I think we'll work that out, Sarah, so it's not a problem. Cool. Thanks a lot, Anura. I think I'm also done. Okay, what do you suggest as the next step here? So the next step is Friday, we are having an internal discussion. Can you send me the link to that docs you had showed me on how to add it to the code and everything? Oh yeah, if you can send that. To me, then I can bring that up on Friday and then immediately next what we'll do is I'll get a larger team for a demo kind of scenario. Yeah. So in that what we can do is we can give a demo, we can explain like overall any questions that are there and then post that. We can set up accounts for a few members to play with the platform as well. Yes, sure, no problem. That will not be a problem. But for the trial, I think we'll align on a date on a start and an end date. Yeah, of course. So that the trial, I would like it, we don't waste it. Yeah, exactly. So generally that is how we also do so what we do is we align on a start and end date and we define like few objectives of the trial. So that way it takes like 20 minutes to write it down so the team can tell you these are the main objectives. So a lot of them will be supported. There might be a few things that are not supported it but suppose it's not supported, then what we do is if it converts, we will build it out for you. So that kind of thing can also work. Got it. Chica sounds good. Sadat. Thanks for thank you for that. I look forward to hopefully potentially taking this. Thank you. You'll get a mail from me early next week. Sounds good. Perfect. Sounds thank you. Bye bye. Have a nice bye.