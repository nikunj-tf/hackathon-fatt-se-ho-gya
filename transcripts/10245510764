Hello. Hey guys. Hey, Alexa. How are you? I'm good, and you? Yeah, we're good. Which part of the world are you in? I'm currently in Indonesia. Okay, so you moved locations? Yeah. Okay. For another three weeks. Okay. And then I'm going to be back in South Africa. Okay, got it. Cool. So Alyssa, I think just very quickly, Uri is on the call. Uri is one of our ML engineers with us. I think we spoke last time about some of the requirements that you were looking forward to and I thought what might be good is if we can show you maybe a demo of the platform, live demo and take it from there. Does that make sense? Yeah, that works. Cool. So just as a reminder, I think we as a propounder platform focus more on the deployment side of things and we are also like early stage and a lot of the features and other things are still in department coming up. But we'll show you the demo, what we already have and we can give you a high level overview of what else is coming. Okay, cool. Will you call this meeting for. That'S? Fine. Actually, do you want to take this ahead? Maybe give an overview of what we are going to demonstrate? And. This is basically the UI, the platform, and this is a public cloud offering, basically Truefoundry. Either you can use the public version or in case you want to use it on your cloud, like, let's say you have your own cloud and you can bring your own Kubernetes cluster. The entire stack is installable on your cloud so that all of the data stays with you as well. So if you go to the integration stab, you can see that you can connect to clusters so that is underlying info that is there. So in case you bring your own cluster, you can connect to that as well, including this application itself, you can run the entire stack on your info as well. So basically we have a bunch of pages. So we have deployments. Basically you can deploy obligations, we can run jobs. We also have a model registry so it gives you training and models. You can also register that with Truefoundry and Akhil. From there you can sort of use that alongside in the deployment. And finally we also have monitoring where all the models that is sort of registered with Truefoundry. You sort of can see what are the actual values and what are the predicted values and make decisions about or make inferences about how the training data has changed, how the predictions have changed. All of that is available in the form of data monitoring as well. So I'll show you a quick demo of the platform. So we basically have two that we use to interact with the platform. They karen called ML foundering service truefoundry. So for the sake of demo, I'll be using a notebook, but you can basically run all of this command line as well. So let me just install this libraries and after that what I'm doing is I'm connecting with the two folder platform here. Just give me a second while it's installed. Sure, no problem. So while installed, let me just give you a brief intro to the problem. This is the first thing that we'll sort of deploy. This is like a job. In this case it's a very simple job where we are going from one to 300 to 30 and we're just printing that particular number. Right? This is a very simple hello world program or show how we can deploy job with the platform rate installations are done, some login configurations here. After that in this notebook there is couple of configuration, there's a couple of things you need to get from the platform. The first thing is the API key. This is to authenticate the platform so you can just follow the link that is printed there. So I get my API key. And the second thing is something called a workspace. Basically workspace are like separations pin the cluster where you can deploy different things. For example I wayve a cluster here that I'm connected to the Kubernetes cluster running on our AWS cloud. So I choose that and I can create a workspace called my workspace. And when the features include you can decide who amongst your team. So you can give different levels of permissions to different people in your team. For example admin permissions which includes they can entirely manage this particular workspace or give an editor permission. In this case they'll be able to use it for deployment but you can sort of manage the workstation or somebody can just be a viewer and it also comes with resource limits which means you can specify how much limit on the CPU and memory there should be. So the idea of this is like let's say you are the DevOps person or you Karen Amit of the company who is running the two truefoundry account. In that case you can allocate different workspaces to different teams or individuals and sort of like a cap on how much memory amit memory limit and CV limit they're allowed to use. You can also specify what are the supported families of instances that you need to allocate to this particular workspace, right? For example, somebody who's deploying simple applications you might not want to give the bigger machine or if there are GPUs in your cluster, you don't want to allocate GPUs to some of the workspaces that deploy simpler applications. But for applications that are using GP now you can specifically select only the GPU instances and once you create this workspace, this will become available to all the users that have been added here. And if an editor or an admin they can go ahead and deploy application pin that workspace. So for our case we need to provide a workspace Identifier here so we have this Identifier that I'm using. Now. We have completely configured the platform and we are ready for deployment. And the first one, first job that I want to deploy is a simple job that goes from zero to 30. So you have the Rundown Pi file here. As you can see, it's just a simple script. And to deploy it, we can do it multiple ways. One is, let's say I have it on GitHub. I can directly deploy from GitHub using the UI. I'll show you that number. Apart from that, if you have it on your local or on a notebook environment, then you can use a Pythonic environment here wherein you trigger the deployment using a Python library. Or you can use the CLI service. So in this case, let's do the Python deployment. So the configuration viewer, you import this job object and you give the name. So I just call this counterjob Twelve and 22 and you specify the build up build type. Right. So in this case, it's a Python program. There's no extensive build step. We just simply run Python Uri. So I can just run this. As you can see, this file is saved is run Pui. That's why we are able to run Python, run PY. So basically it's calling deploy on this job was it should start the deployment. So this way for that to happen, if we go here for the link, this is the deployment tab. That is the first app on the platform. You can see that the job is initializing. You have tabs like these karen deployments. This is just initialized. It's happening right now. I've already done this before, so I can just show you what eventually this will look like. So this is something that I deployed beforehand itself with a different workspace, but the same mode that I've run. Right. So you can see it's not a job. So with the job now, it's a job rate. So I've deployed it, but I've not really triggered it. So there are multiple ways you can't get a job. One is you can schedule it to happen every ten minutes, every two days or whatever it is, you can provide a schedule for the job. In this case, I decided to not provide a schedule so I can manually trigger it and you can sort of trigger it directly from the platform at this year. So you can see that I wayve triggered the job. And in a second it should start running here. So we'll just start taking a look at the logs to see if it works as expected. In the meanwhile. Beg your pardon. Did you have any questions? Not for the moment, no. Okay, cool. This one that I just applied because I think this one is an older one. So as you can see here, the status is going to be running here from suspended. That's because I took it here. So basically you can see the application locks from the standard or directly available on the platform itself. So that is a simple job actually, in which case we just simply printed the numbers. But there is even sort of configure any sort of job that you want to trigger on demand or on a schedule using the platform. You basically have to just write the code for the code for the job and provide things like the name, the run command. In case you have a schedule, we just provide the schedule. All of that is configurable using the service funnel library here in case you want to do it using the UI, I can quickly show you the interface, what it looks like. So you go to deployments and you decide you want to pay a new deployment. So there's an option for carrying a service and a job at this point in time. Later we plan to add more specific deployments as well. For example, you can just decide to deploy a notebook or you can just decide to deploy based on environment, so all that will be possible. Currently, we have, like, Genesis jobs and services. So let's say you choose one of these workspaces. In this case, what I can do is I can provide a name for my service, then I can decide, okay, I want to deploy a source code and deploy application from the source code itself in case I already have a docker image built that also can be deployed that they have this docker image that is already built and available in the registry. So you can configure the registry. So basically you can configure the registry integrations. You can kind of deprive the registries as well and you can deploy from that. But pin case, you don't have the image already built out. You can also specify how to build it out from the source code. So in this case I've already connected my GitHub repository so I can choose one of the repositories that I want to deploy. I can choose a branch, then I can specify how it's supposed to be built, right? So if it's a Python application, I can just simply provide the command that is required to start this application. So I'll just provide it like this. And here I can sort of provide the schedule. You saw that in the previous take a note, provide a schedule so it was automatically on demand. But let's say you said that I want schedule to run the schedule, run for the jobs on a schedule. Then I can just provide it in the prompt format. I want to get somebody with this format but you can specify when to run it. It could be every ten minutes or every day after week or every Sunday, whatever it is, you can make the configuration. Apart from that, we have configuration resource. You can specify how much our resource will get to this particular job itself and what are the limits on here. So within the works is you can make sure that a particular service do not take up more than a certain amit of the resource that are available. The other advanced fields that we provide in terms of configuration include like number of retries that you want to have if a job fails or in case you want you want to have specific permissions. Within your cloud account, you can provide service accounts. For example, in a place you want access three Pin. That case you can sort of attach a service account that you created on your a place and that'll make sure that this job has access to that s three bucket or whatever the policy is. Configuration. So this is your experience basically, right? Just to give you an introduction to services. Services are pretty much similar to this. We provide some very handy functions. For example, there's something called the function service. Let me show you what it does. So I have a file called Functions PY that I'm creating here and there are two functions here, one for the normal and one for the uniform. Basically I'll just add one more function here called F multiplier. I have two goals that I'm taking in and you just return the product. So I'm just writing this file here and what I can do is, again using the social library, I can import object function service and I'm going to create a function service. Here. What you can do with the function service basically register functions themselves. So you can just simply service the register function. Let's say in this case I had multiplied that from functions on top of that rate, which should start with group as well. You can see that this is initializing right now. I already plugged one of these in a different workspace before. So I can just show you what the end point and product will look like. So you have the functions that you just wrote here, simplify the functions available as for entrypointtype video, which means you can consume them in any of your applications. For example, this is the normal function and you can just provide the parameters here and execute it here. I'm not so what are the inputs to this? The normal functions are so it's two floors on a list, I guess. So let's say I provide you the distribution is generated and this is just an automatic swagger UI that is generated by fast AP. But basically all these end points are available for you to use in your different applications. So that is a function service. It basically takes any functions and converts it to APN pin automatically for you. I'll just ask you if you have any questions. I guess we'll get to it. But still trying to figure out. How. Do you take how do you make an inference service? Machine learning inference service. Yeah, okay, so I'm just getting to that. I just wanted to add the jobs and services to you initially now that you have fair editor for the department is done on the platform, I'll just show you how we can customize this for the machine learning use case. Right, so what I have here is a trained pui file which is basically a training job for Iris data set. I am not sure if you're familiar with IRS data set but basically it contains four parameters, the separate length, separate petal length, pedal, bit of a flower and you have to classify them to one of the three categories. I'll not go into the training code itself, but I just wanted to show one line that we valid here, which is the ML Foundry log model function, basically. So ML Foundry is again a library from Autofound. Really? And the ML Foundry basically provides you multiple features. One is external tracking. Let's say you Karen having multiple runs on the same project. You can track all the metrics that are associated with that run rate. You get to log the metrics like Apple, whatever it is, any custom metrics you want to log related to your training jobs you can log that using ML Truefoundry and Akhil the Uri. You get all sorts of comparisons. You can see what Karen the score for different runs, which one had the best runs, which is the worst runs. You can see all the images that you have logged against the run. So for the budget features I can show you the UI in a bit, but mainly here we are using it for the log model feature which means you can log model objects here. So you don't have to write custom codes to store your artifacts and SEO or whatever. You can just use the rundown log model and provide the framework you're training it with and that automatically log the model objects for you somewhere else on refunding platform basically and it will be available for you to use later on. So basically let me just show you how that looks. So I wayve created this train PY here, then I create a requirements file here and finally I deploy job again just like I did the counter job which printed zero to 30. I'm creating a training job here. I'm just calling a train Iris. The command that is required to start the application is file and train pure. We just have to run the training script requirements for this one tht and since we're using the ML Foundry library which is a Truefoundry provided library and enquire some configuration, we provide the CNV variables as well. So you can provide any sort of PMV you need to provide as part of the job or the service as well. In this case I'm just providing the host and the API key for 200 platform to log the model basically and we'll start the deployment here. While the deployment is happening, I can just show you one of the previously deployed jobs itself. So if you look at this this is called IRS. I already deployed this in a different workspace before. So you have the job already available here. You can see I already ran it once as well and I deployed it previously. And you can see the environment variables that I set, they're available here as well. Now I can just trigger it once again just for showing you how it runs. We figured the job, this is the job that we just triggered, we just wait for it to go to running state because it takes a second. Alyssa, one question for you. I think one of the use cases that you're looking for is for chat bot site. Like how do you kind of is it for models that you're using for building chatbot site? Like basically Transformers kind of models? No, not chat bots, trading bots. We wayve running some models. Okay, awesome. We also have something like an integration with hugging face. So when we share the documentation, if you're using some model, it's actually very easy to directly quote and use. From there it's going to be TensorFlow, I think. Okay, I see that is. You get the job again. And you can see there, Karen, some logs generated by Msunder library, which means you should have your experiment here. So this experiment tracking app wherein you can track all your different training experiments. Right? So you can see that for this particular demo, I've run it twice. Once the front ones right now and you can see the course Karen, comparable because I not change any of the parameters. But you basically have a lot of metadata already made available to you by the platform. You have the score which are manually logged. Then also you can see the other parameters that is captured by the regulator. For example, you can look at system metrics. And this is a quick job, but let's say there's a long running job. You can see that the memory utilization of the particular VM or your environment, wherever you're running, how it changes over time. With larger models, this might become relevant because if you see that things are going or things are going wrong in terms of memory utilization, you can debug that with the sort of interface and pin the overview. Basically, you just have the score which we lock. But you can also log other things like in case I change the hyperparameters between runs. For one case, I was using a certain hyperparameter for the next training set. For the next training parameters, I can log those as well and compare them and finally sort of make a decision on which hyperparameters go ahead with. Right, but the one thing that I did do is log the model which is available here. So you have b one and B two, the tour under, and this model is now available as an artifact to use in any sort of projects you wanted. So I can just copy this Identifier of the model. We call it FQN. Just copy this here and you should be able to use it in different projects. So you have the training job. We deploy that in Truefoundry, we run it and the model is available in the Truefoundry and Akhil Registry now. And the final step in terms of deployment would be to sort of use this model somewhere, right? So what you can do is just provide the model here. What we're going to do is we're going to just deploy this as a service. And by the way, you don't necessarily. Have to use the model that you. Can actually use or connect to your own register if you're using to do that. If you want to make it simple, you can use the ML Foundry library as the model. In this case, it's very composable. You can choose one piece of deployment and one piece for Model Registry. But we do plan to provide some more advanced features. Like for example, you can directly deploy a service from the model. In this case, what I'm doing is I'm writing a Fast AP service on top of that, right, I have this main PY here and you can see that it's a pretty function that I'm writing, which is a Fast AP end point. First, AP is a Python server framework that we're using and I'm just simply calling Model Predict Probability here inside this function, right? And to load the model, you can see that I'm again using the ML Truefoundry library, which is the library that we use for registering the model, is getting the model using the model EMV and model FQN. And that's basically the code. And I create my requirements file and then I deployed using the service funding command line, the tool library. In this case, we deploy the service, the name of the service to start the application. This is the command that you have to run. That is the store that we are running. And this will start the Fast AP application. And finally, I provide the requirements path as well. In this case I want to expose 8000 because that is where my web application will be running. So I can provide that as a configuration here as well. Port 8000 and I provide my environment variables. There's a host which is the host and APK for connecting to two Truefoundry and Akhil model. The model that I just copied, the model version that I just copied from here, I'm providing that is an environment variable as well. And you can see that is getting used here and to load the model, right? So these are the thing and once you finally run this, this should start the deployment while it's deploying. I can show you an already deployed version. It's called iris service. You open that, you automatically have the prediction point and you can just try it out as well. So these are the parameters. I can provide different values for these and the inferences here, you can see that the properties of each category is returned. So yeah, any questions so far? No, it looks so good. Maybe what will be good is we can show like the UI interface and just quickly someone at a GitHub code. I sense about startup times and response times, it's quite time sensitive. Right. Is there any cold start for the ML service? And what's the average response time for a standard size model? For the average size, obviously we'll do another model. In this case it's pretty fast because it's a simple scale on model. But let's say you're working with large models, then obviously you'll depend on that. About cold start, like, this application should be running. So you just load the model. We started the application and it's a long running application. Once you have the load model order into memory, I don't think it should be a problem after that. In terms of atul performance, there's a couple of things you're doing. One is you can choose your hardware, right? So basically when you create in the workspace, you can choose for the machines that you want to have. In this case we currently have these catalyst of machines, but like different libraries like by Dodge and the floor, these are customized for all of these have good performance hardware. So in case that is available in the cluster, you can choose that. I think that provides some level of performance enhancement. The second thing we are doing is when we are using a model server, right. In this case I wrote my own fast API, but in case I'm using something like a triton or something like that to deploy my application. In that case, the model server itself provides some sort of like performance enhancement as well. So yeah, the answer to your question would be depends on the model and the amount of compute and the type of computer. But there's a lot of things you can do on the platform that enhance the performance. We're not doing anything out of this box right now to enhance the performance automatically, but you should be able to consider any sort of contribution that you need. Okay, cool. And even in deployments, I think you can select like type of the machine we can go to the deployment and new deployment. You can see. That change might not be rolled out to this. Oh, that's not rolled out here. You'll be able to see that as well, the type of machine. So suppose the workspaces and different kind of machines, and for a particular service deployment, you want to use certain kind of machines. You can do that here. So just to quickly walk you through some of the other features. So you have services and jobs here. Inside services you can see the deployment that you run. So if you run this again, you create a new course package and then release that. So you can keep the plunged versions of the same application as and when you have improvements so your typical pipeline would look like you have a job that is already running your trainings and let's say you have like a regular training set I mean you have more data coming in and you want to run it again so you'll probably have a schedule or you'll sort of like trigger the job. The job will sort of publish your latest model version to the model registry and then you can trigger a deployment of the application which will fetch the latest model version and start the application right? So that is one way you can sort of handle your release cycle for models on the platform we also have an application spec. This is useful for DevOps basically so what you have to do in case you want to deploy something from our GitHub right in case you don't want to configure everything again and you have some port on GitHub with the same configuration with the same set of files and everything but you can just place this YAML file inside the GitHub repo and create a GitHub action basically the CI CD and this particular file will know all the configuration that is required to deploy that particular application. So this is useful pin case you want to set up CACD the number of steps are very minimum you just copy this added to your GitHub and you should have your repo ID to be deployed on like whenever there's a push or whenever there's a lease as you configure it and yeah you can obviously edit the deployment from here we sort of deploy from other sources which in this case are lab environment and let's say you want to provide more CPU you can edit and make another deployment as all of that is possible and some of the advance features include redness probe all of this you can configure as well. These are the features that is available inside the deployment service jobs is pretty much the same type except we have runs you can see the different runs of the particular job so in case I go to one that I run before. On a job how does the charge work for a job? Do you get charged like for just a run or the fact that you just have a job there no usually. Get charged for the run that's primarily the reason why you would want to use the job otherwise you could probably have had a service entrypointtype video for a job you basically have to leave for the time it runs and once the run is done it's automatically shut down the company that is generated for that cool just quickly show you the experience tracking in case that is useful to you like this is a comparison view where you can sort of compare different ones in this case they're not general a lot of parameters already have score and versus over the same. So you can't see the difference, but basically you can sort of do comparisons between your different runs using the syndrome. And finally I just want to show you the data one bit as well, but that I think I'm just unsuring for a second. I just want to log into a different account where I have something already logged. Sure, no problem. Any questions in the meantime? No, I don't think so far. Okay. I remember like you were mentioning, also playing around with Bent Premium. So was there any progress there? What was the experience? Bedtime is fine, you just have to manage your own Kubernetes stack, which is very annoying. It's fine. What you can do is you can use the public cloud. I think that's what we had discussed. So in that case, like the entire Kubernetes management and everything lies upon our site. So you don't wayve to worry about it. We can create workspaces initially for you to test it out, where you can go and start testing out a service or a batch. And then once you have like a full account, then you can create more workspaces. That's what you need as well. Is it possible to deploy your own image? By deploying your own image, do you mean like a docker image or do you even deploy on your cluster? No, I mean deploy on docker amagi. Yeah. So I'll just show you the UI. I don't have the permissions on this particular workspace. If you have the docker image, you can actually just load the docker image. If you don't have, then you can use the build facts. So both the options are there in the UI, maybe three months. Yeah, I can actually get back to that. I actually start with your account once again. Otherwise I'll just show you the monitoring UI for the provider, all the features, and then I'll just quickly show you the deployment of autographs toward the end of this monitoring or dashboard. Basically what you can do is you can log predictions and you can log actual and you can sort of collaborate between the access and predictions. Let's say you log a prediction that you generate on your system, you can target using it unique identifier and if later some errors, let's say in the system, you sort of figure out the actual value for that metric right, or for the prediction. Right. In case you can log that actual as well and provide the identifier, the same identifier. So you know that, okay, this is the actual corresponding to the prediction and that is how you can make comparisons to the predictions of the actual year. It's not necessary to log the actuals unless you have it, otherwise a lot of the features will become available. But if accesses are also for it, there's a lot more features that you have in terms of, okay, this is the Atul prediction, this is the action value. So we can do that comparison here. Here you can see how for this particular model so considering this is pretty straightforward, like if you use ML Foundry, the library that we used to deploy to log the model, here, using the same library, we can log all these. So you can see the number of predictions that are logged over time. And these are the number of access. How do you log in action? Sorry to interrupt. So access also you can just call it Log access and provide the value and you just provide and identify it. Let's say you have a unique Identifier you used to identify certain predictions. It could be a user ID or it could be user ID, cross event ID or something like that. And you can use the same Identifier to recognize the access corresponding with the prediction. But I'm asking them, it surely must be like an end point for the actuals, right? You don't call it encode, you call it encode. I mean, you can just call it using ML truefoundry library. Basically, the code will essentially use truefoundry logact. Okay. Where's the service going to get the actual from? If the Actual comes from external place. Wherever it may available, let's say you eventually sort of get it as part of your code as part of an application, can I say your use case? In which case, how do you get the Atul? Do you get the Atul eventually? Well, the Actual would come from an external service. Right. It's not necessarily applicable to me, but right. If someone's got a fraud classification service right, okay. And someone actually doing fraud, someone sitting and actually doing the Arnaud classification to make sure it's going to come from an external service because they're going to mark the documents like Arnaud or not. Yeah. So in that case, you'll have to log it somewhere where you sort of processing that information, where it comes from the external service, maybe the web book, maybe it's a CSP, whatever format you're getting that information in case you want to log the actuals as well for monitoring purposes, then you have to sort of parse that and sort of log pin using the library. Okay, where does the actual log take place? In this code base here. In this particular code base? Yeah. Actually it's not one of this. Okay, let's find that it's actually public repository. I can send you a quote sample after the call. It's part of this code, but I am not able to find it immediately. That's fine, I'll send a link to this. This has a bunch of examples including log pin, the actual. So it's basically just a function call, but I'll send you the code immediately after the call in email. Using that, you can sort of get this and you can also get the loss as well, in case you sort of provide the difference between the prediction and the actual value as well. And then you have the data distribution tab. This is useful for you regardless of if you love the actual. In this case it's actually a red AI transcription data set in which you have all these features like fixed acid, water, del sidd, these are features of wine and what you can see is a trend in terms of the distribution, right? You can see that it's distributed between 4.6 and 15.6 is this value and the highest value is this. And an interesting feature is one is like you can sort of get a sense of which part of the distribution is congregating more to the loss. So you can see the legend here which says that lowest lighter blue and highest darker blue and in this case it's very mild, the difference between colors, but you can see that this block is darker than this block. So this goes to represent that in this particular distribution, if the fixedacity is between five, seven and 6.8, it's more likely to produce a deviation from the expectation rate than this when it comes to your model. So the answers are slightly more different for this case, which means this part of the division is probably getting more to the loss. And you can see that for each feature, right? So you can sort of develop ideas about where things are going wrong in terms of the rate of distribution or the model. By taking a look at this, right, you can see which part of this rebin is passing. Essentially you have this distribution for all the different features and you can look at it for different versions, you can look at it for different periods of time, you can make comparisons between different timelines as well. So all of that features are available here and you can see this timeline feature is important and this useful because you can see how things the distribution of actual actually comparison between the prediction and the actual and you can see what are the conference link distribution between prediction and the Atul. Let's say you trained your models on a certain data set and there's a conference link distribution there. You can sort of see that here as well. So all this conference link about the distribution of the data set and the conference link raw between the actions and predictions can be made using this particular dashboard. And for each individual conference link also you can see the prediction and the actual. So this is the unique identifier that I was talking about, a way to identify a certain data input and you can log the actual value that is created. So in this case it's predicted this is a classified that predicts between zero to ten, I think. And the actual value is actually six as well. So in this case you can see that Atul is not available to us so that is left as blank, but the prediction was six and you can see all the different parameters and yeah, in case you want to log other information. Along with that you can also do that as part of Raw datta. So this is just simply a table containing all the different October and predictions and collaborated against each other. Right. And the other feature we have is alerts. In case you see there is a certain metric that you're measuring and it sort of goes out of hand or it's starting to deviate from your execution. You can set up alerts as well. So in that case you can just say okay, if the average of this particular one process this particular value then add an alert. The feature is still not fully fleshed out but we expect to have it soon available as well. That we are sort of making a rule very soon. So yeah, this is the dashboard soon. Like the drift part will also come in here as well. So yeah, I mean the marketing part is still right now comparatively basic but there are few things that will come on top of it to enable you to actually track drift etc. Yeah, I'm not sure about the new algorithms but we are working on some algorithms that will make the values available to you and I think that should give you a really good picture when things are starting to maybe not go well in the system. Okay, great. Yeah. Anyway about one thing, I can just quickly show you the documents deployment as well before we enter them. No. What might be best is we can actually create an account for you and give you the documentation links. We'll create like a workspace in which you can actually start going and deploying your models. So it will give you a feel of the system. You can try it out at a basic level and see if this is something that works for you. Pin the same time we'll create like a Slack channel. Are you active on slack? Relatively, but I think this is personal capacity stuff for me at the moment. So probably better to send it to my normal email. Okay, we'll send it to the email but I was saying that there's a personal email or something you use for site that helps because then if there is any queries or doubts you wayve while trying to platform we can solve it real time. Yeah. So fully wanted to show you the docker image deployment as well. Just deploy a test container that's available on docker hub. It's called test containers. So I just choose this option, provide the name, provide this, I set all the other parameters to the custom default parameters and the status deployment creation and pin finish placing it right now. Let's just put it with them available. Yeah. So you have the application running here, right? So it took literally like 10 seconds for the application we need to be deployed and that is because we are just sending it off kubernetes and there's no build stuff involved but let's say you have code, then it takes the build step. That's why it's taking a little more time and this is going to come back to it. The integration is like we have our cluster that is running on a device currently, but in case you want to bring your own cluster, you can do that as well. Or you Karen, free to use our cluster as part of the public cloud. So that is it. And let's say in this case I deployed it from Docker registry, docker Hub which is a public registry and that is a public image. But let's say you have your own registry and you can add that as well. So it would be like you can add a registry which is either Docker Hub. Your private Docker hub. GitHub account GitHub or in case of cloud you can provide either AW CCR from GCP or this is customizable and you can be able to deploy images from these registries as well in case you wayve those in terms of integration. You can deploy from GitHub and Bitbucket currently and GitHub is coming soon as well. And finally we also have a secret store feature. This is extremely useful in certain cases. For example, let's say you want to connect to a database from your application, right? So to take you back to the deployment script, if you remember here you are able to provide env. So let's say you want to connect to a database and in that case you want to provide the BB password. So you have a DB password field. And how do you provide this here you don't want to have your BD password available in the code. In that case, what you can do is you can use the secret store here, you can go to the Secret store or add a secret something called DB password here. Ups. The Identifier is available here, you can just use the Identifier inside here and the system will automatically passes from the back end ordering department. So in your code you wouldn't have to sort of specify any of the secret values including passwords or APA cash. And that is provided by the secret feature we have you have seen all the tabs wayve seen the workspaces tab. The second has a lot more workspaces, but you basically can create workspaces and allocate certain CPU resource memory needs as well in case you are the admin of the workspace and the plumber itself. Okay, cool. Just before I go, can you talk a little bit more about that? Get integration, get up integration. Yeah, the wayve it works is you can just simply set link here and I said I just want to provide all repositories. Select like your own repositories, whichever you want rather than all right. That way when you are applying from the UI, it will automatically show you, okay, you want to kind of load of a particular repository from which you want to deploy it will automatically show you that. So that allows you to easily load from the GitHub and track your deploy. So the way you use it here is in the form if you deploy from the ups. So you can specify Brian to get the quote from. So in this case I got from IELTS. From GitHub. So I'll say this is the report that I want to deploy and I branches main and then it'll basically verify if the build step is sort of banned. So in this case this particular birthday. So for example, I have a docker file. That's why it shows it here because it's able to find the docker file inside that repository. But in case it's not there, it just tells you that the file that you look for is not available. So you can verify that your bill configuration is correct as well. So you provide the repo, you provide the bill config. I'm leaving the rest of it to be default here. The source is the number of replicas. In case you want more availability. You can increase the number of replicas and code is 88. So just start with deployment and then it should start deploying from the source, right? So you can see the sources, this particular repository and you can see that this particular commit is getting deployed. And this should give you an idea of the application. It basically has a docker file and it runs an application by running Python main PY. And inside the main file you have a gradient application. Pradip provides a UI for your machine learning models. In this case it's a simple hello world application. But you can basically replace any sort of you can probably any sort of a function here. Right? So let's say you have a POS tag. So you can take the send, pin the input and the POS tag sentences as the output and make that available as a gradient interface. And let me just show you how this will look deployed. You can see that the gradient is available. So if I just type my name basically that'll be deployed from GitHub. You can do that from Debugger as well. And. My last question, which is a quick one, is when you made that. Service. Is there a limitation on the number of models you can load in. Or no, there is not. I mean, as far as the deployment is concerned, just compute and this up to you, is going to be compute, pin any B trade. So as long as you think you have provisioned enough resources for multiple months to be loaded, then that should not be a problem. Okay, cool. Sounds good. So Lisa, what would you suggest as the next step? Is this something that kind of works for your use case? So you wanted to kind of deploy a service and you want to try it out at your end and see how it goes. And make a decision. Sorry. I kind of just want to play around with the platform a little bit and make a decision from there. Sure, sounds good. If I can just sign up right now then I'll just create a workspace for him so he can try it out. Can you just sign up here? Using a Google account would be fine but if you can sign up then immediately right now I can create a workspace for you and add you as an admin so you can do whatever you want inside that video workspace. 1. Second I will send out a few links or videos so that you can use that. And you can show the notebook as well. The notebook is ready to run. You can just run it from the top and I think that will work as well. It will be good if you can make a collection of few resources that we can share for Amish that will be so that it does not confuse to get started. I'll create like a channel where if there's anything like you can put in that and we'll try to provide looks. Like my internet is stupendously slow. I keep getting unknown error. I'll try again. Hang on. Are you trying to find out with Google? Is it GitHub? But it's on. I'll just take a note as a normal email address. Is it bothering with Google? Actually I'm not sure if there's an issue with GitHub. I know Google works when I'm on. Okay, just give me a second. Save this password. Sorry, almost done. Alright. Got a verification email now. There it is. I thought it was there to go. What the f***? There it is. Could you check your spam one? No, I found it. Maybe we can add to the workspace offline. I think that will be better and we can send all the resources at once. Sorry, if you can share the username you can do that or if you're almost done we can just do it right now. Sir? Yeah, I'm literally almost done. I'm about to sign in. Okay. All right. Do you know your username? If you're already it's elijah. Rebin. R-O-U. Got it. Workspace for you. And if you now log in and just go to the workspace page I really should see your workspace. Yeah. So just be careful like this. Oh my God. Trying to log Pin but it says. Once it's time you should be able to use it and we'll sort of study all the resource that you need to get started very soon. Cool, sounds good. Cool. So I think we will send out an email and after that if you're facing any issues we Karen also happy to kind of you know if you have a model you want to kind of come live and work together one on one where someone from the team can help you getting the first model deployed so that you are not fixing an issue that can also be done. Okay, cool. Thanks a lot. Really appreciate you taking up the time. And once we end up like, you can keep it for trial for a few weeks, so that way you have enough time to test out. Okay. Thank you. Bye.