Hi, Vivek. Hi. Quick question. The message was sent. Yeah. Okay. If you are able to do it, can you send me a screenshot on. WhatsApp I'll do that you mean LinkedIn message, right? I'm doing that. Hello? Hi, Saurav. How are you? Hey, I'm good. You guys can hear me? Yes, we can hear you. Okay, great. There was a weird aug. There was a weird bug with Google meet right now that as soon as you joined in, it showed my duplicate photo in your screen as well. And then like 3 seconds later it brought back your video. Yeah, it got confused. I think we have the same website. At least you're trending towards the same website. Yes, we are. Yes. I think might not be able to join, but I have the context. He updated me for that. Nickoj is co founders sync the CTO of the True Foundry. So the context here today is we just wanted to understand your asif process, the pain points that you may face in your ML model pipeline and getting to know each other. That's the broad objective that we have today on the call. What do you consider? Sorry? Of your smiling. You have something to say? No, I thought that getting to know a second after that, that's. Like where are you based? Where were you at? Like, which holler are you in at? Correct. Porto. I was in La. You? Electrical engineering. So I am right now in Bangalore. I Shane, been here for 1213 years now, so Bangalore is home. Nice. Good. Actually, all three of us on Rock. A bishop. And I are from 2013. Arc hall. Oh, wow. Same call today. Yeah, same hall, actually. And I did share, like, wing with a Vivek for the first two years, I guess. Oh, nice. I was in Stru and Rock was in electrical and Obese was in CS. Yeah. And all of you are in the Bay Area now? No, Abi. I am in the Bay area. Abhishek and Anuraag are both in India. Abhishek used to be in New York and Rag used to be in Singapore. Earlier, before we also relocated to India. That's now kind of thing. Yes. Initially they relocated because of these are reasons. Once after we quit our jobs, it's hard to just stay here and then later they're just continuing there, I guess. We put out a team in India, in Bangalore, so it's just easier to manage the team in. Nice. Okay. What team? We have a small office in one of the upwork things that we have taken up right now, we have a few people in the team who work from Bombay, Delhi, and one person in Europe as well. But primarily, like most of the team is in Singapore. Okay, nice. Cool. Nice. Actually, I'll give you a little bit of a background, sort of I think you have some context from the messages and all, but basically the idea was that we started to build the startup. Proof truefoundry. We're kind of building out platform for machine learning teams and we are still fairly early in our journey. It's been a year that we're building out the startup. So just kind of talking to people in our network understanding a bunch of things about the space and kind of getting feedback on where we are. So those are a couple of things that's happening right now. In terms of stage itself, we raise a seed round of funding. We are working with a few customers or design partners right now. A smallish team maybe reach 15 full time members. The product is still pre launched so we are in private beta right now. So that's kind of like the context of the company. I will tell you more on what we're building but actually it would be helpful if you can also share a little bit about your journey or what parts of the ML problem that you are kind of trying to solve at Freshwater as well. Sure, the products are kind of internally divided into that. So for one of the product lines I leave the machine learning engineering charter. It's not just machine learning engineering, we are verticalized. So it's basically the whole gamut of one particular thing that we do. So large part of it is machine learning engineering, a little bit of regular application type engineering as well. And in the ML side everything that we do is text. There is a little bit of annually detection type series type upwork which is there but otherwise most of it is a text and of course it's a sales firm. So whatever we do, we sort of do it in a way that all of our customers can just subscribe and use. We don't have any on boarding or any what do you call UT UT UT UT dallas presentation mechanism. Everything has to be through the product. There is no implementation or installation kind of system. Right? So that's broadly what I do. The main features that I build out are there's one virtual agent application that we have basically in It service management. How do you reduce the load of, let's say the It teams? By creating a virtual agent which responds either reactively or proactively to some of the things that employees need. Whether it be provisioning of software, backup of their laptop, replacement of some parts, requesting for a new hardware, software, whatever it takes, those kinds of applications plus general help kind of things. You would have seen the culture of companies having a sort of global health challenge, right? And then people say hey, my guess is not working, that is not happening and so on and so the virtual agent can figure out that hey, there's a problem which has been resolved before or there's a solution article describing this problem and so on. So you can check out what can sort of chime in and say hey, you have this problem before the actual agent goes and tries to do something, would you like to try out these few things? So that's one kind of feature, right, which is on the virtual agent. The second and there are a lot of peripheral things which are around that, that is how do you know which? So things like search and classification and intent detection and these are of course part of the main feature itself. These are exposed to the customer. But in the back end you have a whole bunch of things like relevance and those kinds of things. And then how do a better solution article? How do you recognize that duplicates in the article those kinds of improvement initiatives. You can go and tell the by the way, you understand what Arsh service does, right? Generally? Yes, I do. So that's one class of one class of features. The second class of feature is basically only MLS applied to business process, sort of engineering automation. So for example, how do I route a ticket? A ticket lads into the help test and then which is the right agent, which is the right broken, what is the right priority and so on those kinds of things. Then there are some advancements to this idea, like for example, when a new employee comes in and joins let's MIT, then there's a bundle of software, they typically start off right, they would need access to ID, they would need access to certain databases, they would need licenses for certain software that we use. So today it is zoom conference. Soumen which start applying like 20% of the time phone conference dock is out of a quick reality. Yes, that is true, absolutely. But the fresh service record is not out, right? The record of what the employee has asked for is true. And then you can sort of figure that out and say hey, everybody which has this deepak usually requires request for these five things, create a bundle out of it and send it to the employee proactively. And Sam, you know, your colleagues typically use this 15 things. Would you like to create a one shot request for all these 15 things together? One thing here I do not understand, sort of like what do you mean by fresh service record is not out of sync. What would be an example of this record, maybe? So every time I ask for a software, ask for them, it's a lot of that, right? Understood. Yeah, makes sense. Okay. And we internally use the service as the car own product that makes CTO the system record, right? Second class, there's also a use case which is around size application management, It operations management. So things like report around the quality of service across a cloud based application, like how many service they have, how are they performing and is there a potential for some failure. So annually detection kind of use cases that's not very fleshed out as a concrete problem. We have certain things which we are doing there, but it's not a very it's not an area that we have sort of gone a lot ahead. If it's the first two use case, first two use cases are more complicated that's broadly what we do. Understood. Okay. Very helpful to know sorrow. I think this product context itself is very helpful. I'll tell a little bit more about which kind of areas that we are focusing on and I'll tell you what kind of help I'm seeking primarily. So in terms of our platform, we are focusing a little bit on the our main focus is on the model deployment, right? How do we enable teams to be able to deploy and monitor their machine learning models fast? And here when I say deployment, I mean both batch deployment and real time deployments as well. Right? And in this context, I think what's generally one of the things that I would love to understand from you is key. How are you organizing your team currently in terms of people who are building models, people who are deploying models? Are these the same people? Are these different people? Right. What kind of tooling infrastructure that your team is using? And is it different from like does Freshworks have a separate infrastructure per team basis or is it one platform that's used across different business units? How are those things structured basically? So for the help shitta to understand, I think those parts because one thing that I think at this point we are struggling with is key. Larger companies are basically poor platform level decisions. Where would a product like ours fit in? We are in that discovery journey, to be honest. So I think just getting this context of how these entire things are organized would be very helpful as. Some part. Of the conversation, I guess. Okay, let me give you a little bit of a sense. Some of it is probably not directly shareable, but I'll give you a little sort of wrapper. The way we operate is the largest application engineering team. Works out of a monolithic code base and the related services are sometimes maintained at a different level. And we have a dedicated team to maintain the infra of the company. So that two, three scores which are responsible for deployment, monitoring, knock activities, on call support, all of those kinds of things at an entry level. And everything is on AWS. We typically don't use anything as well as I know anything which we buy stuff from AWS, right. Almost everything is our own, like west clusters other than RDS and things like that. Everything else is like we spin our own stuff and do things. Right? We don't buy, let's say Kubernetes service, we don't buy plastic service or we don't buy anything aspect. Everything is this is on the application. On the machine learning side, I think the things change a little bit. So machine learning as a theme came a little after the rest of the thing. So the way we are structured is we maintain our own infra everything. The infra is not very complicated, there isn't too much to it, but we maintain it. The way we have done it is we have a layer which abstracts the application from the ML service. So this layer basically acts as an orchestrator for any ML service. Every time a product needs a prediction, right? It tells us which account, which customer and what kind of prediction with some header information about what we are trying to predict. So it sends that request to us and one of our machine learning services picks that up and responds back. And this layer basically acts as a record keeper for all of this. So for example, if you want to know what happened to that customer's prediction, at what point of time this layer will keep track that we chad a request to give a classification for this text. We did it in this particular way. And if there is, for example if there is sometimes your record clicks and things like that, if there's a click on that document then you would have feedback back into this. The ML services by themselves are spinning completely without any state. So they're just simple endpoints which you can hit one at a time. That's how they design it. So the ML services typically don't care about which account, which company, which one text, et cetera. And they don't even take there is no state which is maintained for any of these services, for any of these things. Everything is controlled by Chris layer. That's the first design choice that we have. These ML services, however, are specific for each account. So for example, a classifier for account A would be different from classifier for account B and each of these accounts would have their own models, right? Typically we want to maintain this model extremely lightweight because otherwise you can't have thousands of customers, right? So you have extremely lightweight models, but different models. So our surfing infrared here is a custom designed app with Caching etcetera in built which ensures that whenever there's a call come in for particular inference from a particular account, it is able to guarantee certain sets of SLA parameters and respond back to that. So the service is basically the main thing about the service is to ensure that service rate. The easiest solution is to put all models into memory and keep running it. That's difficult. So we have a mechanism of our engineers CTO figure out ways to ensure that we are guaranteeing performance for as many models as possible. So that's where the trick comes in. And that app is completely ours. We have created that app. It runs on a scalable system. Almost everything is on Cuban which basically it appears like any other sales application. It's a Saturnite. Each model is like that's how it's designed. I see. Interesting. You mentioned that the machine learning system is built out stateless. So does that mean that a lot of actual model inferencing is like done in batches and stored in the database and then these services or they actually. Load the model from no, almost everything is online. There is hard, there's like one application which is bad, but everything else is online. Oh, interesting. So like the model is actually loaded from some model registry like an S three or something, a specific model registry or S three is the model registry at this point? Yeah, most of the time it is S three. Okay, I see. So basically the model gets loaded in real time from S three and the number of models in this case I assume would be in thousands because it's like number of actual applications times the number of customers. So probably in the range of 10,000 even. Yes. Oh, very interesting. So this is one type, there is another class of model where we use where we have one model which is for all customers. Like think of something like a vectorization service, right? We don't have customer specific vectors, all Texas vectors using the same area. And then you have one service that's easier, you have one horizontally scalable system which you just together the loads on the system are high because the number of customers who can use it. So you just have to ensure that makes sense. Understood. So like in this case maker itself or you're not using Sage Maker, you build out everything on other kubernetes internally? Basically yes, inferencing on the training, et cetera. Everything is database. I see, okay, understood. So the pipeline is basically get data, run the jobs in database, get data from our database, data like itself. So full datta in run those things, exit out by saving those modifies and whatever meta is required into an SD location. I spoke about this middle layer right, which is there, that actually also acts as a store of things like when was it updated, when does it need to be updated? When a new customer comes in. So it basically signals to the consumer it signals to the consumer whether or not pipeline training pipeline has been completed, if it has failed and so on. So then the product knows what to do with those kinds of information. Understood. Okay so actually this is very interesting because I know that Sage Maker itself had one specific like they have a feature where you can have multiple models being served from the same endpoint. The use case that you're describing, I'm assuming that you have built out something similar, right? Not each model itself is basically an end point, correct? No, not each model itself is an end point. The service that we have built creates as a single endpoint for the product. The parameters are the account name and certain other things and that basically tells us which model to use in which context. Okay, so each model application is an endpoint and the account name becomes like a parameter to the end point, basically. Correct? Yes, understood. That makes sense. Also, like one other question. You mentioned that databricks you're using for training, but database also comes with ML flow, right. So are you all using the ML flow hosted ML flow that database provides. And the model register or no, not their model register. We use ML flow to track experiments. So any run that we do in fact, for example, if somebody says, what was the accuracy of this versus that? Or internal experiments? Like, for example, we made some changes to our algorithm, can we see if it has improved search? Can we see if it has improved our classification? All those kinds of metrics are tracked in ML Flow, but we don't use the model registry. I think we can use the ML flow metrics to track things like drifter or issues, et cetera. But we haven't done it yet. Even in ML flow, even in the database about a year old that we started using ML Flow I sep. So again, same thing. Was there any specific design choice that you decided to not use MFL model Registry? Or was it like it's more legacy that S Three was already being used, so it just continued. I think it has to do with Sumit. And these apps are complex as a company. Right. Some of these are also decisions around how much do you want to get tied up to one particular service, then how much control do we need, how much is the product? So there was some POCs which were done around MLS, both phone conference service and the model registry thing. And after some CTO info, we decided not CTO go down that path. I see. Understood. Okay. I honestly don't remember the exact decision behind what Rubric did we use to decide on, but there were some PC. I see, understood. And for the Drip tracking, you're saying it's just that currently you're not using anything with respect to Drip tracking simply because it's not too much of a use case? Currently? No, I don't think so. I think we haven't contacted okay. But it's somewhere on the roadmap to. Work with we used to do it earlier. So the first couple of years I was here, we had that Google thing, what is it called? Based on Kubernetes. Yeah. So we use some version of Kubeflow and then Kubeflow chad some chatting tool. Now IBM getting very poundy about that something which you can use CTO, like the same ML flow kind of thing. But does it come CTO come with TensorBoard? TensorBoard, okay. Or something? Looker, maybe no, those are Bi tools. But chad a way of okay, I see. Anyway, that's okay. Yeah. So they chad something we tried using internal product. Right. That took a lot of time to manage and maintain, and the reason why we started looking so we also had a cloud Datta instance that was also self hosted. And in both of the cases, we decided that we don't want to spend a lot of time in managing all this. We'd rather buy this from somebody and manage our machine learning workload and spend like 50% of our time just debugging kubeflow and debugging, which is what ends. Up happening every time people use kubeflow. I hear that so frequently about kubeflow. Yeah, it's not a prime time kind of thing, at least at this point of time. Got it. Sort of whenever you get to the actual again, reviving the project around like drift and all. I'm curious about these things. Do you expect mostly to do these things internally? Do you expect to evaluate external solutions like Arise or Fiddler? There are a few companies that are working. Just focus on this particular piece of the puzzle. Right? Yeah, I think Fiddler also reached out to me a few maybe a year ago, six, eight months ago. I think I spoke to at this point. Right. I don't think I have the bandwidth my other counterparts might have some I don't think I have the bandwidth to actually work very seriously on this topic. So what is happening right now as a company? I think there are a whole bunch of things that don't build out. A lot of energy is going out to getting these things up. We are accumulating the amount of debt in that process. But at this point, that is of primary importance. We will have to relook at this. Once things stabilize a little bit more, we will eventually I think we can't go beyond something without quoting into this, but at this point, it just isn't on the priority list for anybody. Another thing which has happened is like any other company, we don't do it systematically, but engineers have figured out things by themselves as to what CTO do. They have their own internal dashboards and they have their own internal things. Our product teams have a small analytics product analysis team who actually keep looking at these. And the A team, I noticed that these queries are not working well, or the customer has given bad feedback about these five, six things. So there is an add home. It's not that we are going rudderless, but it's added everybody is doing something at some point. Understood. Okay. Sort of like one thing that in this context describes every time we talk to a company that is a fast company. Right. We hear this multi tenant design that you just described. Because I think the reason for this is like every customer has its own data and this one model fitting all customers just does not work. Seemingly, you end up having to build a separate model per customer. And from a lot of other companies I have heard like many challenges in building and managing this multi tenant system because of how do you manage these hundreds and thousands of models depending on like a number of customers. Basically what you described is actually. I. Would not say necessarily like a unique design. I've heard similar design before, but it seems like the way you all have built this out, it's working very stable for you. The way, the way you're talking about this is a kind of sorted problem in some way. You're not looking for this anymore, right? I'll give you my take on this. If I were to start today, I would not build it this way. I would look at Fiddler or maybe two even for that matter. I would use what Databrick has because getting these things right is expensive process. And to be honest, I'll give a very simple thought here. When we were trying to build out a vector search system, right? And that was decent, right? So we tried to make so we thought we could go to one of those Pine phones or one of these companies who actually provide this as a service. And we tried out and for various reasons, some customer privacy, et cetera, kind of reasons, we didn't use it. But before we took that decision, we looked at seriously why we do want to maintain a vector database at our end. Why don't we go and take something from outside and use it? Because creating that database, creating the service, maintaining that inside is a headache. So today I think IBM a little bit more smarter in this way. I would rather take something from you and then trying to invent it myself. But when we started out, let's say four or four years or so ago, many of these things aren't there. Like point number one. Point number two was because it was a young person start up, different teams, started pulling different datta. Somebody said, oh, we are going to go sage maker. Somebody said, we are going to go Q flow. Somebody said we are going to build our own things. Then I think we decided that, hey, this is not working out well. Let's all come together and create a strong one structure and we stabilize on ML floor. If we have mature companies who do all of these architecture, which I described for us, I would actually buy that. Like if I were to start Chris today. I see. This is very good to hear. I'll tell you why. Because as I mentioned, there are few areas in terms of our go to market that we are currently struggling with, right? And one, we are kind of trying to figure out what's a good niche to focus on for ourselves. And turns out that this SaaS market where helping companies build out these multitenant systems is actually one of the top niches that we are considering because SaaS itself is growing. Usage of machine learning in SaaS is growing. It's kind of a stable industry ecosystem overall. And this is a common problem that every sales company apparently is trying to solve essentially. So we thought that this could from a business standpoint, this could be a good niche to focus on. But I think from a product standpoint we haven't truly nailed down that what are the things that can be built on top of our platform. CTO kind of serve for this use case. Like for example, you mentioned that they built out a very strong caching there on top of our platform because you can't possibly keep 10,000 models in memory every time and then you can't suffer on latency every time that you're going to load model, et cetera. Right? So that's one example. But I'm sure that there are probably 100 other things in this context that enable building out this product faster, better I guess. So what I would love to do is maybe like spend some time with you again only time permitting for you sort of key show you what we are building out, discuss a little bit from a product strategy standpoint about how could we potentially tweak the product so that we can serve to this domain much better. That's one of the things that I'm really interested in doing and it would be very helpful to me sort of and turns out by the way, I'm coming to India actually in November, September 20, I will be in Singapore only. So if you're around, maybe we can even catch up in person and discuss this together. I love that. Please, whenever you hit me up we'll go and grab a drink somewhere and we'll talk about this. It will be great if you can give me a double of the system. I'd love to see what you guys are building as far as we as a company is concerned, I think. And anyway you have free sort of private beta so it's not good. The one point which I want to mention is when we discuss we can talk a little bit more about it is specifically if it is to be satisfied. There is a huge concern around how we used it. So we weren't so bothered about these condos till we were pre IPO. But now obviously the spotlight is on the company. So see the challenge why we would even consider building it ourselves. One major issue was that sending data to any third party for an emergency and you guys are aware of all of Chris I know that it's not permitted by a larger syntax. So you need to be if Fresh Works is sending any data to you then like you are building a service for us. Whatever the services then we have to get into a mechanism of mechanism of disclosing that and then do a whole bunch of customers hate that? If you sep in an agreement which says there's two countries involved. So what I like there is the way the topics are built. So IBM, sure you guys are thinking about it, but I think for ML systems that. Becomes a very mode and legally speaking it is extremely difficult to differentiate. But you can't go to a lawyer and say hey, this is a model file, like a binary model file. It doesn't have pi yet. And that's a very tough conversation. How do I know that? How can you prove that there's nothing private there. Completely makes sense. And actually sort of like that's one part. We actually totally taken database model like our current customers September that we're working with proof founders sync actually deployed on their cloud and we actually manage our infrastructure there itself. But we totally heard it and so much so that we were using in our system one total technology that was sending some metadata outside of our customer thing and we already got pushed back for that and now we have to change that. Basically. If you don't disclose then that's one point but if you don't disclose then you are liable for any problems. And if you do disclose and customers they don't want to sign up on that exactly makes sense. Can I ping you on like phone or something and look at meeting up in November? Vivek. Th thank you, Eli. Appreciate you taking this time. Take care. Bye bye. Thank you so much. Bye. You're bivic. Thanks again. I think this is fine. Let's not do a lot more things with this guy. This conversation that we are trying to have with him, I think he will be a really good person to give us feedback on this entire thing because we have actually this thing top of mind. This could be an interesting niche to focus on. Take care. And obviously I kind of switched to more like help seeking vulnerab mode with this with this guy. Probably this Saturday if you're free then Yejuj architecture explained kiana how he is using three for all the layer that is building. Probably one platform that he has built. Understand it from you once more to validate understanding the. Shane. Even better. Take care. Okay.