We end up losing clients by that way we want to be able to get to a problem actually how it is hurting them and third is they have tried to solve it somewhat from that perspective only the capabilities had without going into a very depth of the product overview and then the policy contacts ten minutes will kind of give you an overview because we asked generally we prefer key. Once we are able to understand and four things then we kind of do but don't think of this as a demo we'll do a more personalized demo based on your use case will spend some time understanding their system further time they feel is fine to go time better automated system will stop and they are able to build more models. Would that be something that is important to them? Next call may we will show the demo but before the demo we'll send you a document this ma'am actually put up a setup so you can agree to it. This is something that you agree and then we will showcase the demo as well. We will accordingly change and focus. That is the level of pretty much that we should try to time Nicolas if we know already from the beginning he needed directions without repeating things that will be good. One of the things that I realized is deployment time. I don't understand the stack well yet to be able to give anything at least the question I would want to ask. It will not convert not thing will convert without that experiment tracking and testing Uber so we should not necessarily say most people know ML Flow program if they have used it or they know it. Then it is just an additional thing we are adding and it will not lead to an impact anything. But basically for anything if they call out something interesting ultimately we end up saying overall demo and then a quote then people will not relate. We have to make them almost on the same page and we already know your objective care of seeing that we should try to get to that level. Say what any membership per transfer document. Is there anyway trying to adopt? Should we try to enter or no experiment tracking is an alternative to ML Flow at that point, should we go or should we not say. Why will they use you over ML Flow? We are better than ML flow. The product is better, that's what I know. What is on the user mind is not good enough. No user has validated key so there is a reason people have heard of ML Flow thousand times they know key community. Is that something under that level we have to ask we cannot just push. The main thing that I want to of course we have to spend some time trying to understand a little bit better. But that said, we already have some information about them, right? And we should have at least roughly in mind. What do we think we can add value on top of the existing tools that they are using? Either that or the part where they are trying to solve and they're not already solving it. They haven't already solved it. When we introduce our platform right, I want to introduce with that as a focus for us, they're already hooked into that description a little bit. I don't want to give like a generic description of the platform. Essentially solved monitoring and data driven is something that they are not doing. If this is a problem that they are trying to solve, that can be one value proposition at least that we can propose. Take care. The second thing is existing tools. Sage maker ML flow or fees outside ML flow combination. Where do we still think Jahapar soham that we can call out as like our core value prop? I'm guessing they're using for experiment tracking. End to end project lifecycle. ML project lifecycle management. Usually when people say that the only experiment tracking plus model registry at the very minimum use. Some people also use the projectpro component of MSLO in there where you package your code, your model, everything together into a project and then you push to a repository. Some people also use that, I would imagine first. Why don't we go and actually ask these things rather than making the assumptions wrong so you would already lose the attention of that person. This is not a replacement of asking questions. This is just tentatively preparing at least like a couple of ideas. What are the things that we think we can add value instead of complete generic pitch? The part that we have information. We should get that clarity first. With half information. If we make assumptions, it might turn out to be wrong that you. Have half information. But they are going to ask us this question. So either you can answer that question with absolute zero information generic deployment monitoring solution, or you can answer that question with at least what's your best guess based on the information that you have so far basis. So we can at least prepare before the call. That's what we're trying to do. Okay. Don't think of this as a replacement of asking questions. We are still going to go and ask the questions to them. But think of this as just call. Let that discussion move forward a little bit. Questions prepare because that is the most important thing. If you think if we ask the right questions, we'll get to something. Take care. So let's write down some questions that we want to understand from them. First of all, is deployment time a problem? Like are you trying to optimize that? Yes. I should never ask this question. Mom test Capella is this a problem for you should not ask this question directly. Okay. Are they actually deploying models in lambda or 10 million to 10 million rows? How much time does it take? How would that matter to us? Request comes fetch from data from Athena make prediction put fetch data and modern feature store that's what we wanted to ask. Generally, like a new model, you will go through the same process even when you're updating the thing. But the thing is from the call the time that they are giving us for a new model, but we should also ask. Would they be building more models? What are you using Flow for? Okay, if you are not using monitoring like currently, why are you not using it? But I read a little bit about the LinkedIn descriptions for the truck. Basically that's what I gathered based on what this person has written. By the way, one of the other things that I wanted to ask them was that's the other question that I wanted to ask they are doing it pretty frequently. They're doing it every 1 hour. Now, once you have reached every 1 hour, that's a decently frequent update of your predictions, right? Why not in that case, like build like a near real time system actually is that like infrastructure limitation? So that's the other question that I want to ask them. Just organizing features like features we both benefit all features in one place. You can query everything using IDs. You can track the performance of features running or there's a lot of benefits that you get out of using a feature store. Batch influence data. Set up at certain frequency. They just go and update and then some other kids just go and run it manually. By the way, that Chron tabala is like the more common way that people deal with model retraining housing because I want to save training cost. A lot of times your data is small. Your training cost is small. It doesn't even matter if my model is learning something. I'll just keep retraining it every once in a day. It doesn't matter to me. The more they take that approach, it seems like that's the approach that they. Are taking is that leading to any business impact? Do they want to do it more faster? Do they want to they are okay with it. So that will get captured here. Would it matter if they were able to retrain every seven days? Is that something? I think they have a Cron DAP Sep so it's probably training more frequently. If people do it every two months, they usually don't set up chron tabs at all. If somebody setting up a Cron tab, usually as I work, they are training more frequently than required mostly. But that's just a guess based on what this guy has told. Cost side understanding cost approximately on the training side, on the deployment side, cost latency and all of those things to. Be there because it's all offline. So it doesn't matter. Basically requirement they compute the results and dump it to a database. So latency is fine. Cost we can ask is cost effective for you? Cost is directly related to memory, right? Memory. Memory impacts. But it could be like just kidney. Frequently have jobs, trained career, right? Which framework and which type of models are you building? Is there a problem that can be identified using this one? If we get good answers, like answers that we want to hear. For example, if they say we want to do real time deployment, but we tried it, we could not do it. Right. So that's one way is deployment time. Maybe they are trying to do it very frequently updates to the model or number of models and every time they are taking one to two weeks of time frame. So how much time does the batch inference take? Maybe it takes like 14 minutes right now. And they're like as soon as you scale this will basically go out of the window depending on what answers they give out. Deployment frequency. There are other business problems that you are willing to solve that is getting blocked. Is your bandwidth being constrained? Then it means ideally they will benefit, but they will not realize it. Are there more business problems that are this the only problem that are there? Or do you have any other things in the roadmap that you want to solve? Are you looking to hire more data scientists as well? So questions that you can infer see whether there are more business problems or whether they are being crunched on time. If that either of this is true, then the first part it is not necessarily a problem they are pointing out. But then you can try to double down and try to tell more time. Then you can start doing that. But based on the two calls, this is how we have understood the system so far. Based on one based on two notes. Simon, this is a fair understanding and then we have questions further to understand more. I think that will be good and then end may understood. Give us some time to go back and just revisit this conversation. We'll try to set up a live demo for you and we'll try to showcase a few things that might matter. But give us some time to think about demo. Mostly people don't relate to it then no matter what you said, they don't. Okay, ticket nine to eight minutes. Or let's actually think a little bit about how are we going to position ourselves a little bit when they are asking. Because. Already you can contextualize, for example, developers. At some point as you grow, you might need an ML engineer platform instead of hiring. You can contextualize based on whatever you have heard from the first call. Maybe, but that's what I'm trying to discuss. Good too. We are already building an internal ML platform based on these two things and I'm trying to understand what is it that we are going to tell them. This is an area where we shine compared to all these Sage maker and this thing, a platform pitch would not work. In that case, if somebody is already using a Sage maker, all these high level things will not work. We have to tell them something specific. Like this is an area where we think we are much better than these existing things. That's one thing we need to be prepared for. This is the first question they are going to ask us. Let me discuss this. Let's focus on this one question. Because we are constantly going around and other things I'm discussing, I really don't know how am I going to answer that question. How ML flow will add benefit to them in the current stack? Is there something about Kubernetes that we can discuss with them? Software team Kubernetes use can we talk about some benefits that come with Kubernetes over Sage maker. Pushing like without anybody in the team using Kubernetes currently. Tell. Me like one or two major benefits of Kubernetes over Sage maker. There's no way to change it. You can deploy any container that you want. No change maker. But usually people don't use it for that. This person calls extra data for doing the same thing. It's optimized for model deployment. It's not really meant for general deployment. That's why people go. Is there anything that we can call out that Sage maker does not do so well that we are doing anything at all? Like a small part. Don't even think of the full platform. Cost is one thing. 30% costs. Any time useful for you? But it's fine. That is not an immediate problem. That's not good. If you are happy with it, if you really have no problem, then sure. We are actually not adding a whole lot of value after that. On top of that, in fact they will actually probably listen to us a little bit more. If you are about to disqualify. And last was feedback. So keep them in the loop that. We can keep sending them updates. And if a platform try, then we can check. We should build this community. Basically add them to the community of data scientists for feedback. Think something also. Yeah.