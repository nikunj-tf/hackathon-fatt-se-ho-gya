Hi, Doctor Anise. How are you? I'm good, thank you. Thanks you so much Do Anish for joining for the call and I again apologize for calling you the other day without appointment. No problem. All right, let's go ahead. Amazing. So Doctor Anisha, I think as I mentioned over the message that I've exchanged with you that the purpose of the call I've already talked a little bit about. But maybe let me start with a brief introduction about myself and then we can dive into why I was trying to reach out to you. Nick Pinch Bharat Co Founder and CEO here at Truefoundry, Dr. Anish come from a machine learning background, used to be at Facebook where I built out a lot of conversational AI models for them. So basically we're building their virtual assistant product called Portal. Prior to that, I was at a startup called Reflection based in the Bay Area where I was leading the machine learning team, building out recommended systems, personalization algorithms, and also got a chance to build out a horizontal machine learning platform that catered to the analytics teams as well. And I did my masters at UC Berkeley undergrad at it. That's where I met my now co founders on Rag and Abhishek as well. Besides this, between quitting Facebook and starting Truefoundry, the three of us did one more startup in the talent space that got acquired by Info Age. So that's a little bit of my background. Do you have any questions on that? Otherwise, I can get into something about truefoundry Dr. Anish. Well, no, that's helpful. Thank you. No questions. So about truefoundry Dr. Anish. Anish we are a fairly early stage startup, like 15 months into our journey, currently funded by Sequoia, and we are building out a platform to help analytics and machine learning data science teams to operationalize models and their data pipelines and stuff basically. And in this context while I'm traveling to India, I'm trying to meet with some of the data leaders in the country and spend some time understanding what are the practical challenges that you feel like this space is getting towards and what are some of the problems that we as startups could potentially address for enterprises. Essentially, we have a few thesis. I would love to also work with you and get your insights into those thesis as well. But before we get into that, I would love to open up the floor to understand a little bit about your work here at Doctor Edison, generally your interest and background, and after that, what's your take on the space and then I can ask more specific questions. Okay, sure. Yeah. Do you want to go get started? Yeah. So I can give you a bit of context in terms of what we are trying to do currently and some challenges that we can see. So specifically from a data science standpoint, we already have close to 30 plus ML models which are in action currently and specifically these models are mix of what we are trying to achieve in terms of data science for our manufacturing segment within Dr. Eddie's and research and development. That's the first bit. The second is one of the challenges that we've been facing is specifically around availability of clean data which has been some bit of barrier for us to deploy a few of these models as well. The third is a governance with regards to reviewing the efficacy of the models and improving the models over time. That's another area that we are really struggling with broadly. This is where we are right now. If I was to give you sort of a flavor in terms of what specifically we are trying to do. So we've got models under supervised and unsupervised category. Few are NLP models, some are computer vision, few are specifically on automation and optimization as well. That's where we are to give you to share a few examples, we've got one of the models under supervised machine learning where we use linear regression and F test which is for reviewing or quantifying the composition of drug components in specific tablet. That's one. And then we've got a model which is particularly around performing virtual dissolution trials which is for in vitro dissolution process. For this we are using random forest regression and then few are in the quality side where we are looking at maximizing the yield by taking the optimal combination of fraction sample which is a mix mixing all cycles, production cycles together so that the weighted purity and weighted impurities are within the specification limits of the product. And then more on the market side, we've got we currently are working on a model for predicting the tender bid price which should have the most optimum winning value for us to win the bid. Some of the models are around the finding root cause analysis of nonperforming business units. For this we're using gradient boosting method. And the last one I can talk about more from a marketing standpoint, again, using gradient boosting method is measuring the campaign effectiveness into sales. So if we have a marketing campaign, what's the efficacy of that campaign and what should be the idle platform means of marketing a particular product? That's where we are currently. There's one question that I have for you, which is how different is your platform when compared to data robot? Because we already have data robot as a platform. Understood. Yeah, I think there's a couple of things. Actually to answer that question, I need just to list two or three more things from you and then I can answer the question more specifically. Ranish so, number one in terms of cloud provider, is there like one specific cloud provider that you'll use? Are you on multiple cloud or hybrid cloud setup? No, we have a single cloud strategy and we use Google Cloud. Okay, understood. And in terms of the optimization functions for you, in terms of ML like, is the cost around ML, either the infra or the resources, a concern at all? Or that's something that's not a concern for us. No, it's not a concern for us at all. We have really scaled up our data science capability. What matters most to us is the efficacy of the model that's that's one. And how what best can we do to retrain the model to improve the efficacy? I'm sure you would agree that not every model hits an accuracy of 100% 1st time. There are several models that we run which are at 50, 60, 70%, which is fine, but how can we actually ensure that we are able to improvise the accuracy over the course of time as it learns with the basis of trends? Understood. And the last question is around your team structure. So, is there a separate improve team that manages things like Kubernetes and all your data scientists? Are they come more from a science background or they come from mixed like science and engineering background? What's the team structure like? So we have a standard team for data engineering and standalone team for data science. Understood? Got it. So I'll tell you how we are positioning ourselves in the market, why we are even solving the problem despite data reward and data IQ existing. Right? So there are a couple of reasons here, Doctor Niche. Number one is from an intra angle, okay? Where we work on top of Kubernetes. We are using Kubernetes as a platform, as a service approach, where we can help companies go across cloud intermittent deployments, which is something that I think you mentioned is not necessarily a requirement for you. That's one area. The second area is given where we are and given the backgrounds that we come with, we provide customizations in terms of your ML models, improving your ML models. Basically, and I'll explain what I mean by that. So, basically, we have built out a tool where, let's say one of your models is 70% accurate, okay? So it's failing on 30% of your data set, basically, right now, if you try to directly optimize the entire 30% can you still hear me, Doctor Niche? Yes, I can. Yeah. So the approaches that your data science team would have is try out different models, try out different hyperparameter combinations, which people typically anyways end up doing. And that's how you get from whatever 65 to 68% to 70%. But marginally is very hard to improve beyond that. Okay? Now, what we have done is we have created a tool where you can take this 30% of your data and segment it. What are the most common failure areas in this study? Basically, there will be almost think of this as a clustering that is happening for the 30% data with some root cause analysis of why models are failing. So we have built out an automated tool, and we call this, by the way, hotspot detection tool that in your failure areas? Where are your models, which are the lowest hanging fruits that you can basically optimize for? And sometimes the answer that comes out to be that you don't have enough training data set for this particular class. And if you just add like 100, 200,000 more rows from this class, you will suddenly get 6% extra accuracy. Sometimes the data is not standardized for a specific segment of the data, for a specific segment of the failure cases, and you optimize for that. In some cases, your model is not complex enough that it's able to capture because the behavior of the data is more nonlinear for that particular of the segment, which is not true for your other parts of the segment. So that's how we recommend different problem areas and corresponding fixes that helps you improve the model. Some part of this is already automated in the tool that data scientists can do by themselves. And given the early stage of the startup, we also work very closely with the team to actually deliver this business impact by improving the models. These are the things that you typically get, basically. I don't think somebody like a data robot or data at this stage will be able to provide to any customers given the scale at which they are operating. Okay. All right, so I'm keen that we set up a POC with Truefoundry to understand the effectiveness and suitability from our business needs perspective. What is the best way to take it forward and maybe set up one POC? Yeah. So generally what would happen here is we need to understand typically when we set up a POC, we try to try to figure out one specific area, specific model, or one or two models, or specific like two or three members in the team that we will work very closely to define the scope of the POC, essentially. Right? What's the impact that we are looking to deliver? And after that, there's two different directions we can take up where we just define a scope. We take an intake meeting. We understand the text track. We explain you our architecture. Now, the POC can happen in two ways. Where you want to do a lightweight POC, where you can do directly start deploying things and building models of our cloud and that's much lighter weight, very easy to evaluate the platform. The second is where is a more integral POC with a company where their DevOps team gets involved. They would have a part of our platform deployed on your own cloud. So let's say GCP, and then you would run there. So that adds up maybe like five to seven more days in running the POC, essentially. Okay, which is fine. And what is the cost involved to run this POC? Yeah, so usually for the POC, we don't charge a lot. We charge in the range of $5,000. And the POC typically last around two to four weeks. But if the problem is more complicated where we are actually working closely with the team to solve for data science problem. We could extend it to six weeks. Okay. Which is fine. So from a commercial standpoint, this is going to be a huge value which we wouldn't be really comfortable to spend as a part of POC because POC is more about trying to understand the efficacy of a platform for us. Is there any alternate way we can maybe organize this? Even if it's a basic or simple problem, it is more for my data scientists to test and try this platform versus what we have in Data robot. Sure, understood. So we can figure something out. What do you think would be so I'll tell you Dr. Hanish, I'll be honest here. To be honest, like as a startup, it matters less to us that we get the $5,000. The check itself does not matter to us. I'll be honest, why we do this completely transparent with you is because we never do completely free POCs. Because when we're doing free POCs, we have seen that a lot of times. We don't have the commitment basically from the team. So even if it's like a very small check, it doesn't matter. Even if it's like $1,000 check, I don't care. Even if it's a $500 check, that's fine too. But so long as we have one check associated with the POC, we are happy to do a POC with a very small check as well. Do you think that's something that you can make it happen? Yes, that's definitely the question. So basically if you can figure out something very small as well, that's fine. And then we can define the scope of the problem. For us, the main thing is can we figure out a problem set or a team where we can truly add value? Because that's how we are going to grow our startup. By adding more value to our first few customers that we are working with. So long that we can figure out an impact area, we are generally good to go. Even with a small check size for the POC, it doesn't matter. Fair enough, that's fine. So let me do one thing. Do you have sort of any capability back or which provides an overview of the platform? Because I would definitely like to speak to my team and bring it on the agenda of my next leadership meeting. And as the next step, I can set up a meeting between you and my data scientist to have a quick 30 minutes conversation and agree on the logistics related to POC. Hello? Can you hear me? Yeah, I can hear you. So yeah, this makes sense. And I can send out some material to you. Just one question. Where are you based out of? Anish. We are based out of Hyderabad. Hyderabad, okay. So next week I'm generally based as I mentioned, I'm based out of Samples, currently traveling to India and right now I'm in Bangalore, but next week I plan to come to Hedrabad. Do you think I can come by and meet you while I'm in the city? Because once I have gone back, it just becomes very hard to meet in person. Yes, sure. We can organize something, no problem. Understood. I actually don't remember if I have, like, what's the best way to communicate with you? What's? Like, do you prefer emails? Do you prefer WhatsApp? Do you have a preferred way of communicating and setting this up? Yeah, you've got my official email address. That's the best way to reach out to me. I am fairly happy with my email, so let's do that. Perfect. So there's two action items on me, but I will send out some offline material to you. And number two, I will try to set up a meeting with you in person while I'm in Hyderabad. And one action item on you, which is you will discuss the scope of this with your data science team. Who will be the right person? Maybe, like, in a scope of one project that we can discuss in detail. But I'm visiting you in the office about the POC and then bring it up to the leadership team to figure out the logistics. Yeah, let's do that. Awesome. Okay. Thank you. This is great. All right. Thank you. Thank you so much. Have a good one. Bye.