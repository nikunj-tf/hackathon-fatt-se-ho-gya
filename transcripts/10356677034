Hello. Hi, Nicko. Hi, Pedro. How are you? Good, how are you doing? Good. Where are you taking the call from? I'm calling from Brazil. You're calling from Brazil? Okay. What morning is it's? Like exactly? Noon. Yeah. Okay. What about you? Where are you calling from? I am in San Francisco. Okay. Nice. Lovely. Thanks a lot for taking time to speak with me today. Really appreciate nice. And I guess maybe we should do like a brief introduction about teachers to get started. Okay, sounds good. Yeah. I can go first and then we would love to learn a little bit about you. Sure. Pedro. I come from a machine learning background myself. I was at Facebook, where I led one of their conversational AI teams. Have you heard of the product called Portal? No, I have not. Only the game or Google home. Yeah. So Facebook basically launched a similar device called Portal, and I was leading the conversational airport in that device, basically. Okay. I think I do know it now. Okay. Yeah. Okay, I see. Nice. And prior to Facebook, I was at a startup in the Bay Area called Reflection where I was leading their machine learning team both in the US and in India as well. And here I got a chance to build out a horizontal AI platform for the company that was used by multiple teams within the company. So that's where I got the flavor of what we are building a True funding. Now, the first time okay, got you. I did that. I moved to the US for my masters. I came here to UC Berkeley, and before that I was in India during my undergrad. And that's where I basically during my undergrad is where I found my now co founders and ragnabishik. So they're like my close friends for last 15 years. Okay, that's good. Yeah. And before starting True Foundry, the three of us did one startup together in the HR tech space called Enquire, which got acquired by the largest HR tech player in the world. Pretty much. It's called in voyage. Okay. Yeah. That's a little bit about us. True. For still relatively early days, it's like one year we announced our seed funding. So that's nice. We got like Sico and all the parts of it and continue to work with some customers, continue to build out the product. Exciting, exciting times for the company. Yeah, that's awesome. That's cool. When you're out, that's really cool. That's going to be really nice. I don't know. It is an interesting time. All right, we'd love to know a little bit about you. Yeah. So I guess my name is Pedro. Before being a data scientist, I was a chemical engineer. So I was a process engineer, so I worked in different factories. But what I really liked was the analysis and modeling part of it. So I started kind of trying to make my work more like data science. And then at a certain point, I just decided to kind of go all in and move into the data science field as a generalist. Right. But then, yeah, I did my undergrad in Minnesota. I did math in chemical engineering, and then I went to my grad school education at UIC in Chirag. And then I worked a little bit for the Chicago Public Schools doing, like, kind of helping their data strategy. And then afterwards a little bit at DHL doing some machine vision kind of product for their warehouse. And then price lapse. A little bit of freelance stuff. There's some, like I did a little software for a company that provided services for Fulfilled by Amazon kind of market research kind of thing. So I did a little bit of freelance work for them. And then price labs. So I've been with Press Labs since I've been with Price Labs almost four years now. When I started Press Labs, working full time, it was just Anuraag, who was one of the founders. And then now we're getting to, like, I don't know, 70, 80 people now, I think, so it's pretty cool. Yeah, it's been, I don't know, kind of interesting to see the company grow. Yeah. I mean, there are other founders, right? But at the time, the only person working full time was on a dog. Oh, my God. Wow. So you're like, the veteran in the company. Yeah. As an employee. Yeah. Are you traveling to Brazil or generally you're based out of Brazil currently. Yeah, but I'm originally from Brazil, and I lived in the US. For, I don't even know, 1112 years, something like that. But then, yeah, I was living in the US. When I joined Price. But then I wanted to move back because of family. So I had my kid recently. I wanted him to grow up in Brazil, that kind of thing. Yeah. He is turning seven months in two days. Yeah. What's that what's his name atu, like, Arthur? Yeah, kind of. Okay, nice. Yeah. So that's it, I guess. Do you know a little bit about Price Labs? I read up a little bit before coming to the call, but would love to understand from you directly and also, like, what kind of machine learning problems you are solving and what are some of the challenges that you're facing right now, what makes it all interesting for you, I guess. Yeah, I guess Price Labs kind of the main product in Price Labs is dynamic pricing, right? So we scrape data to get an idea of what the market looks like for a short term rental site. So we scrape data from airbnb VRBO and booking.com. And using this data, we kind of power a model that does some price recommendations. It's a decomposed model, so it has, like, many different models that come into play. So there's some seasonality models that look at, like, past data to come up with broad trends. There's, like, event detection model that looks at future data to figure out, like, hey, this date is kind of trending up, so we should price it higher, that kind of thing. So. Yeah, that's what Press Labs does. There are a couple of other products in Press Labs that are kind of related but not exactly the same. So there's like, I don't know, we're working on a revenue estimator, for example, kind of a widget, right, where property managers can plug into their website to show prospective clients, like, hey, this is how much you should expect to earn if you sign up with us, essentially. But yeah, and the other things are we have a bi tool as well, which we get the same data that we consume for a model. Then we kind of put it in a format that the users can use to explore their markets. So that's price Labs in terms of the challenges. And what makes this interesting is we're growing company, right? So we recently got funding as well this year, and the goal of funding was to hire more. It looks like there's a big opportunity in this kind of market, and there aren't a lot of players, and we're like a significant player in this market. And so we wanted to hire more people to execute all the ideas that we had. So we've been hiring, I guess, like a year ago, I think we were like, fewer than 20 people, and now the goal is getting 200 by the end of the year. So quite in terms of, like, headcount. So with hiring come some challenges, right? Like, our Data Science team is much bigger now, and it's becoming more important for us to be able to, I don't know, manage data science at scale. We're still a small company, and we don't want to change things radically, like, at once, right? But we want to keep an eye in the future for what should we be thinking about as we grow in terms of the data Science team and how it functions? We're pretty like, engineering heavy on the Data Science team. So there is a separate engineering team, which is larger than the Data Science team, actually, but we do some of the engineering and the data Science team of those. We will, for example, scope whatever the infrastructure that we need, right? We use Digital Ocean as our cloud provider, but we will figure out the scope of the infrastructure that we need, and then we'll kind of collaborate with engineering to make it happen, right? So I don't know, sometimes it means, like, we're going to, I don't know, try some different data on the data science team itself. We're going to try some different databases, and we're going to see what works for our type of workload. And then we'll partner Engineering to get an introduction. And then I don't know, it also means that we own some of the reliability and some of the products, right? So, for example, the cut off point between us and engineering is usually APIs. Right. So engineering will consume our APIs, but our team is going to put the APIs in production and we're going to put their models in production and make sure the models run reliably and all that kind of stuff. Yeah, I mean, depending on I don't know, I think different companies have different strategies right about this, but we're kind of leaning towards a little bit more the engineering side and the data science team. Yeah, I think that's it. And I think what makes it interesting is, I don't know, this idea of keeping an eye on the future. I don't think there's like an immediate need to like, maybe there is, but there might not be an immediate need to implement something like brand new Pedro Pricelabs Day, but I don't know. We want to know what's out there to be able to make those choices when the time comes. Thanks. Awesome. I was just taking some notes while you were talking, so yeah. Thank you so much for caring. This background super helpful. I have a couple of follow up questions. Sure. One, about the business that you mentioned that you scrape data from Airbnb, et cetera, and create like, pricing estimates about the future and the current market. The end consumer of this pricing estimate, I assume the property renters, as I read online, like, people who own the property is it it's a mix. So there's essentially two types of customers. They're the property owners and then they're property managers. There's like a whole new business of people who manage properties for other people. So there are big companies in the US. That manage like 1000 listings, for example, for other people. And so I think these days it's kind of half and half. But yeah, we do have a lot of customers on both of those ends, like small customers that own the property directly and big customers that we use to offer to manage properties for other people. I see. Okay. And the value prop for them is like if they have a better pricing, the understanding of the pricing, they will probably make more revenue out of their profit of their property and they will share a part of that revenue to you all, basically to price that. Yeah. There are a couple of ways of looking at it. For the smaller users, the main draw is this increased revenue rate. But for the bigger users, there's increased revenues one, but the other one is automation. Right. So you might have a revenue manager on staff that could do maybe as well as price levels. Right. They know the market really well. They're like really experienced, but it just doesn't scale that well. Right. So they need something to automate their setup and price lapse can work for that. I see. Okay, understood. So that's very helpful. The second thing is you mentioned about the models so that the data science team itself builds and puts the model as an API endpoint, which is then integrated by the engineering team and the core platform of the product, basically. Exactly. So how big is the data science team, and how big is the engineering team currently? Sure. Data science team around eight people right now. It's a mix of mostly data scientists, but there are a couple of Python engineers in the team, too, because of this kind of engineering heavy load that we have. And we also have one developer that's kind of in between the two teams that helps us with the reliability projects. And then on the engineering side, I don't even know these days, but I think we're probably close to 15 to 20 people. Got it. Okay. I see. And as the team scales, Pedro, do you imagine this thing being always maintained by the data science team? Like your APIs model APIs. And do you all use docker containers? Currently? For our deployments, we don't. Yeah. Our deployments are, like, very manual. We're about to start using Ansible, but we don't even do docker containers now. Ansible is just going to be like an automation of what our manual work looks like. It's not going to be a docker container. We looked into it, but then for the complexity that we need right now, we couldn't justify it. We tried to keep our stack. We're kind of conservative with how we expand our stack because we're not an engineering team. Right. So we want to know, well, the few stuff and the things that we use. So we try to keep the scope kind of narrow. Got it. Okay. And is it like the API endpoint, like, for the web server layer? Do you end up using something like a fast API or a flask? Yeah, we use, like, flask. Then we deploy it behind, like, Junicorn engineer. Got it. Okay, understood. And one other question is around the training jobs. Right. So this is the inference APIs, I assume that you're talking about. Sure. How about the training jobs? Where do the training jobs run currently? And how our models retrained and stuff? Yeah. So historically, our models were based on Heuristics. So it was kind of like an expertly tuned Heuristic model, essentially. Right. One of our following underdog comes from the airline industry, where he did revenue management. He did the pricing models for the airlines, for United Airlines. So that's where the idea comes from. He developed kind of a Heuristic model that's hand tuned, so it wasn't retrained. It was tuned for I don't even know, for a certain period of time, mainly. And then after that point, it kind of became like a fixed thing. We're kind of reinventing the pricing model as we speak to incorporate some now that we have more people in Saffron to incorporate more data, it would be more like a machine learning model, like you're mentioning. There is still going to be some heuristics involved because of the nature of the problem. It's been helpful, but yeah, then we'll retrain it. There are some models that we have that are kind of run more offline that aren't. So I guess to kind of come back a little bit, there are essentially two types of models and price levels. There's models that work mostly on historical data, and then there are models that work mostly on future data. If you're working mostly on historical data, we have a few years of data that we can use and then from one day to the other, this data doesn't change significantly that we should rerun the model. Right. The model is essentially the same. Like, you add one day of data, it doesn't matter if you rerun this model once a month, that's probably good enough. So that would be something like a seasonality model, right. Broad trends for markets don't really change that often. But if you have event detection, for example, is much more online, right? You look forward in your data and then you see what are the dates that are getting picked up in the market. And that's going to translate into some pricing factor. So these models are run online, so whenever we run the pricing for a user, we get the best available data and run it through the model and then give them the factor on the fly. So those we could think about. We're probably going to have some trained parameters that go into this model, but I'm not sure exactly how often we're going to retrain them. But I guess to kind of come back to it, our retraining model, it's on a monthly schedule today because we only retrain like really long running models. I don't know if that was too convoluted. Sorry about that. Got it. Okay, so in general, what I'm hearing is that model retraining is not most important use case right now. That's what I'm getting. Kind of yeah, not right now. I mean, it might become important eventually if we change the way that we do models, which we want to, but today it's not that critical. Got it. Okay, understood. And one other question is around. Given that your team is doing your API endpoints yourself, some of these could be some of these would be like real time responses, right? Yeah. So how do you think about things like latency and scalability? I don't know if your traffic is very uniform pattern or you could have burst of traffic, so you might need like some other scalability and stuff like that. I don't know. Yeah, we're working on it right now. We ran into this issue. Our regular traffic is very uniform. It's not literally uniform because people log in at certain times of the day. Right. You can see like there's some time zone things that happen. Like people log in in the US. And they log in in Europe, they log in. Professionals log in during the week. Non professionals log in during the weekend. We can see those patterns, but in general, our traffic is pretty uniform. We did run into some kind of load spike in this issue recently when we were doing, like, a webinar, for example. We had a webinar where, like, I don't think 400 people joined everyone. Richie, one of the co founders, was leading people to it, and then he was leading people to use the exact same functionality at the same time. So they all clicked on the same button, 400 people. And it just kind of shut down our infrastructure. So we are working on some load balancing for our APIs, and that's an active project that we have, like, some load balancing with some automatic scalability that we can deal with. It doesn't scale like, I don't know, ten X, but it can scale like, 24 x. That one we can set up easily. Got it. I see that's very helpful to know. And Pedro, you have been working with the company for the last four years, pretty much from the beginning. Right. And now you have seen a lot of evolution of your models, from static models to something more dynamic currently or in the next three months. What are some of the biggest pain points or challenges that you want to solve? Like you said, you want to safeguard against the future. Right. I'm sure you have something in mind in terms of what kind of challenges are you looking forward to solving? Yeah. As the team grows, we want to make things more uniform and more stable. Right. We don't want to rely on some tribal knowledge to have things working. Right. So I think yeah, make things easier. It goes with onboarding as well. We want our systems to be easy to work on because we don't want to spend too much time on boarding people for, like, scaling up the team. Right. That's one. And it has to do with, I don't know, making data easier to find, like improving documentation, making our systems easier to work on, be it on the code structure or infrastructure. So that kind of thing. The other one, like you were saying, scalability is becoming more important. Like, we've always kind of set up our system. Our system is pretty nicely set up in terms of scalability. A lot of our jobs are best jobs, and we have kind of a pattern that we use that's been working fine for this. It scales horizontally pretty well. But for the stuff that's online, like the thing that you were touching on, we haven't put a lot of time into figuring out how we're going to scale it. So that's what we're doing today. You touched on a really good point, which is how do you scale horizontally when you need things, like, done on the spot, like with low, latency and all that stuff. So we're working on that today. Got it. Okay. Very nice to know and just a couple of other small questions around this. Like, number one, do you do anything for your model monitoring right now? So, like, you mentioned that. Yeah, that's another one. We don't monitor the model very much. We monitor a little bit. Right. We have some indicators that we collect every time we run the model, and we keep track of those to a certain degree. But because our models are also based on heuristics, it's a lot harder to monitor if a heuristic is wrong. I don't know. It's a lot harder to detect. Right. There isn't like an objective variable that we're trying to get to. It's a holistic, but we're moving or developing new models that have more well defined kind of objective functions. And so those we're planning on adding some monitoring and yeah, like you're saying, that's another good point. We are planning on adding some monitoring to those. We monitor the data source a lot more than we monitor the models, which is probably not the way to go. I don't know. We should be monitoring both. Got it. Okay. And for these new models, like, what kind of libraries or frameworks are you thinking? Are you trying to use, like, psychic learning types? Library xg Boost. Yes. There is some stuff that we're doing that's in psychic learn, like some boosted forest libraries from there. There is some bespoke stuff as well that we built. It's nothing fancy, but it's some time serious stuff that we work on that we kind of building it from the ground up in some ways. Yeah. I don't know. What's the name of the library? There's a timeshare library that we use. It's a port from R. I forgot the name. Yeah, I forgot the name. It's some time series library ported from R to Python. We use everything in Python, by the way. That's kind of our language of choice and data science team. Yeah, that's one of the most common. Are you talking about darts? It's not darts. I don't know if it's tslearn. They have a bunch of them, like TS fresh Tslern. Yeah, that's okay. No problem. Cool. And by the way, Python is like, the most common language of choice. Like, it's a common language choice for most companies as well. Yeah, cool. So I think we have five more minutes in our call today. Let me give you a little bit of an overview of what we are doing at Truefoundry All Hands. A lot for answering all my questions here. Of course, this really gives me an understanding of the problems that you are currently trying to solve. I'll tell you a little bit about truefoundry all hands. Then if you find it relevant, we can set up like a demo call or something the coming week or so. I'll tell you first why we started building through Truefoundry. There are companies like if you think about the machine learning space, there are people who are model developers there are people who do a lot of engineering. There are people who do a lot of DevOps, like things like scalability and engineering and stuff. Right now, companies have different stance at solving this problem. Some companies are like, I'm going to take a model developer and empower them with tools so that they can deploy their models or scale their models and stuff. There are some other companies that are like, I'm going to take an engineer and provide them with tools that they can build their own models. Right, I got you. Okay. For example, the second type of company is what they call as AutoML. You just bring your own data. You don't worry about machine learning at all. You don't worry about anything. And we will build and deploy models for you. Right? So that's like one type of company. We are the first type of company where we take model builders and empower them with tools to kind of help them with the engineering stuff. And the reason why we started building out this company was when I was at Facebook. So I told you when I was a Reflection, I built out a horizontal AI platform for the company that everyone to do. Like this kind of stuff that we are talking about right now. And I was like super proud of what we have built out at Reflection because it was like we were suddenly like two X more productive as a team. Yeah, when I joined Facebook, I was like, oh s***, like 50 X more productive because you don't believe Facebook. Really what I saw was as a machine learning developer, I go build out a model and then I literally click a button and the model deployed as an end point model will auto scale model is monitored. Like you see all your model graphs and all ready for you. And all the good things like your security, your data privacy, all those things are handled by default. Right? And I was like amazed at the kind of system that Facebook had built out. And then when I started this company, like the HR tech company myself, I was building models internally as well. And when I was trying to deploy those models without all that amazing infrastructure that I had, it was actually a lot of pain. It ended up taking a lot of time. I would like the same common issues that I know how to solve. But the question is, why would I solve them? That's not increasing my business, right? So when I ran into this problem myself, I went and talked to a lot of companies that what's the state of the art today? Like, what is it that people are doing? So as a team, interviewed more than like 150 people and we realized that people end up spending time in doing this thing over and over again. Like for example, you mentioned that you have one Python developer in your team and then one kind of handles the interfaces between the data science and engineering team. Right? Yeah. Now, right now it works because you're kind of not thinking about scaling the system tomorrow, when you think about scaling the system, you'll probably have to add another developer who helps with this stuff, right? Yes. Now, technically, you can imagine that for an eight person team, you have almost like a two member team who is handling these services, essentially, right. Of resources are going here. When you scale up, it's not like it will not scale up. So the question is that why can people not have similar tools that help them move faster and not, like, spend on things that are not core to their business? It's not improving your business per se. Right. Your business will be improved by your models that you're building out, the better pricing estimates that you're building out, but not, like, by ensuring two people maintaining the system. Right. So I think that was the reason why we started building out truefoundry health IQ. Right. Now, after having spent one year building this thing. We are both honestly, even price lapse at this point is not a super large company. It's a small company. I hear this problem from giants. I go to these companies who have like 100 machine learning developers and also have similar problems. Like, developers, they have like 25, 30 people who are just managing their platform. And from those 30 people, you could go to like 300, 400 machine learning developers and not need to scale that if you're using the right tools. Essentially, that's the reason why we have built out IBM. Happy to answer any follow up questions that you have for me. Yeah, I mean, I've been kind of talking to people in this kind of space not your space exactly, but in this kind of like more kind of like, value added machine, whatever, deployment tools, like, I don't know, not nearest bits, but like, snowflake or some other people, like a layer on top of whatever. And I always find it hard because it's been my whole career right. Doing this more engineering heavy thing. So it's a little bit harder to imagine what would it be without it. But I guess from my point of view, because sometimes we run into this issue, isn't it a little scary sometimes outsource, like, I don't know, your hardware or outsource, I don't know your load balancer, because how much access do we have to these settings? Honestly, I resolved that so much. Like you never outsource the entire thing. And that's why even in my examples when I mentioned, I never said that you don't need those engineers. What I said was you will not need to scale those engineers. So much got you. Even with a 25 team example, I did not say you could reduce the 25 team down to zero. What I said was that you can scale your data science team. From 100 to 400 basically. Yeah, generally. I also believe that without your internal engineering effort you cannot just outsource and forget about it. It doesn't that way. The point is that you don't need to spend the equivalent amount of resources that companies typically end up spending today. That's the most important thing that I'm trying to highlight by the way. That's a fundamental with which we built out the company as well. So in fact, if you would notice that at some point when we do the demo, you would notice that we call it out in our pitch. We are not a black box. Our goal is to empower the developers and not replace them. That's the design philosophy that we are building out with. And you would notice that literally all the code that we have, everything that we need is based on solid open source technologies and we expose the full technology to the end developer. That is if proof on it did not exist like you are still sorted. It's not like you are blocked on proof, honey. You could just move with that state of the art open source technology and just keep running your systems. And one other thing that we have done in this context is we don't interfere with your code also. So it's not even locked into True concrete. We actually put a layer aside your code that you use for the value add and if you don't like it, you just remove that layer of code and that's it. Like we are free with your own code essentially. So a lot of things that we are doing in this context is because we already come from developer background ourselves and we know that these things we would not accept if somebody came and pitched to us. So we are basically solving for those problems from the beginning, I guess. Okay, so Triffander is just software or is there like hardware? Do you run code? Do you have servers that you run stuff or do you like, I don't know, do you work on AWS or whatever some other cloud provider? Yeah, so we don't have our own service, but we integrate with all the cloud providers. You could either use machines that cloud providers are providing on our system and directly use them. So kind of after the service model, they also use us in the following format where they install Truefoundry on their own servers and basically just use their own servers, essentially. So we have both the formats available that way. Okay, that's cool. Yeah, I mean, that's interesting. Okay, considering what I told you about Price Labs, where would that layer of True foundry go? Did I give you enough I don't know if I give you enough information to talk about this, but like ditch flask KPIs, ditch internet servers, I don't know, ditch queue setups, what is it? I don't know. Yeah, so I guess you don't ditch anything per se. Petrol, what you do is you kind of optimize your setup and make it a little bit faster. So, for example, you are using faster. You continue to use Flask to deploy your models, right, to deploy your APIs, even if it's not a model, honestly, even if it's like a simple Python function that you want to deploy, you continue to do that. But then you could do things like how do you dockerize things? Because eventually you will run into now that you're starting to engage with libraries, you are going to run onto things like package management, version conflict. Oh, it works on my local, but doesn't work on production. Right. So you add that layer. How do you make sure that you go through CICD that as soon as the code is pushed, maybe you can deploy it faster. How do you make sure that you don't run into this auto scaling issue that tomorrow? If you have a spike of load, how can that be managed then? If you wanted to monitor your data better, how do you make sure that you're not spending, like, five additional days of effort after you have put the model production to monitor your data, essentially? So all those kind of come together that make your system slowly and gradually more mature. You don't adopt all of them together. It's almost like you have a baseline system. You take, like, one other feature, improve the system a little bit, and then you keep on incrementally building on top of it. So that's kind of how the future evolves, I would say. Okay, that's cool. Yeah, that sounds interesting. And by the way, I also don't know at this point I talk to you like, 15 minutes trying to understand some of the current setup. So I also don't know at this point if there is a good fit here. And to be quite honest, Pedro, even from our standpoint, we are also a small team, and we only are taking up projects that are like that. We also feel like we can add events value number one and number two, that we are not stretching ourselves too thin. So essentially, we are only on boarding, like, literally one customer a month right now. All right, so that's what we're doing. So we don't know if there's a good fit right away, but it doesn't matter. I love connecting, and probably in some point in the future, if you have a need, and hopefully by the time our team would have scaled up as well. So happy to connect then. Or like a burning use case that you feel like, oh, yeah, if we solve this one problem now, that could be great. We potentially try to work together, no problem. Okay. So happy to do, like, a demo or something if you're interested. If you want to do it a little bit later, that's fine too. Whatever works best for you. Yeah. Is there online, like, on your website? Is there something that I can play with. Absolutely. Again, given we haven't actually exposed the platform for people to sign up and play around right away, it's an invite only system right now. Okay, sounds good. Maybe we can do a training. Can we do so often? Can we do a training in, like, a few weeks from now or not training? Like a demo? Can we do a demo a few weeks from now? Would that work? Sure. Yeah. I would like to, because I think I get what you're saying. I think a demo would kind of paint a clear picture for me, and I think the software looks interesting. Absolutely. Yeah. Should we do mid next month? Maybe like, 13 October, which is the Thursday. Can we do the week after? I'm going to be off that week. Okay. Yeah. So I have some stuff 18th, 19th, and 20th. So those three days I'm actually outside, but I could either do 17 or the 21st. I could do 21st then. Yeah, you could do that. Awesome. So 21st, I'll send out an invite to you. What time? What time works for you? 21St. Sometime after 03:00 p.m.. My time. Let me check. What time is it for you? My time. Okay. Would that work for you? Yeah, that works. It can be sometime after that, too. It doesn't have to be exactly that time. Sounds good. Yeah, no problem. I'll send out an invite to you, and then we'll get it going. Okay, sounds good. All right, cool. Very nice meeting you, Pedro. Yeah, thanks, Nikon, for taking the time. It looks the platform looks really interesting. Thank you. Have a good one. Bye bye bye. You too. Bye.