Hey, guys. Hi, Michael. How are you? I'm good. How are you? I am doing good. I have children in background, so there will be some noise. No problem. I have a three month old at home, and already I feel like he's starting to keep us busy for that. Where are you based on chick? Pardon? Where are you based? IBM. In La. Yeah. La. Nice. Amazing. Where are you? I saw your area code. 405. San Francisco? Yeah. Oh. Are you in San Francisco based? Okay. Yeah. Do you come to the Bay Area frequently? I've been meaning to for a long time, but I haven't actually. I have a lot of friends and all. Even today somebody's calling me for a housewarming party, but I haven't been there. Okay. Just a drive away. Rather long one, but yeah. Yeah. Actually I prefer driving. When I moved from Pittsburgh to California, I drove. Oh, wow. With my car like that, only 500 miles. You really prefer driving? Yes. Well, I chose to drive because sending the car would have taken like, a few thousand dollars. But yeah, I've driven a lot. I like driving. Amazing. Yeah. I was so intrigued by your career transitions, like from astrophysics. Is it astronomical? Yeah. Astronomy. I hear that a lot and thank you. I take it as a compliment. But it also really surprises me because if you look at the fundamental level, astronomy is one of the oldest sciences, and astronomy is the data science just came out of nowhere 20 years ago. We have been doing data science for like, 300 years, since Kepler and Newton. All they did was take measurements and use that data to formulate planetary paths. Astronomy is, at heart, data science is astronomy, like, not differentiable. We should probably try to give a talk on this sometime. Astronomy is data science and data science and astronomy is a good title. Nice. Awesome. So, Abishik, again, thanks a lot for taking time. The goal for this call for me was primarily twofold. Tell you a little bit about what we are building at roofounding and more importantly, understand a little bit about what you guys are doing at Health IQ. What kind of data science problems are you trying to solve and how is the team structured? What are the problems that you're facing? Basically? Right. Spend some time on that. Maybe I can start with a brief introduction about myself and then we'll spend some time talking a little bit about his IQ. And after that I'll give you some background. About 200. Yeah, give me a moment. Let me find some entertainment for my kids that they don't keep screaming. Sounds good. IBM. Sorry about that. No problem. Before we go forward, just know that most of the stuff that we do, we use in house, build solutions, and we are just going to hit AP in, like, three days. What? In three days? AP annual enrollment period. Okay, I see. From now to December, company will be really shutting down any extra work or code development or anything and we just focus on selling insurance policies towards which we have been building all through. Got it. Okay. So it's almost like a code phase that happens during AP. Code phase? Yeah. Nothing happens during that period that can possibly break anything or slow down the business. Very interesting. I did not know this concept of AP code fees during the AP times. It's because all the insurance companies, they do 70% to 80% of their annual business during AP. It's because during AP you're allowed to switch plans. Everybody is allowed to switch plans and purchase plan for next year. Right. And once the AP period ends, then you are only allowed with exceptions, you have to give a good reason for you to change your insurance plan and all. So that's the 55 day period, I believe. Yeah, 55 day period where you can just change your plan or purchase new plan and do whatever you want. Yeah, that makes sense. Cool, I'll keep that in mind. Thanks Sabichek for clarifying that. All right, so a very brief background about myself come from a machine learning background. At Facebook I was working in conversational AI and at Reflection I was working on building recommended systems for the ecommerce industry. And in that space we also used to have our own version of Code freeze but that was due to Black Friday and Thanksgiving and Christmas sale because again, ecommerce businesses do a lot of their business during. Right. So that was my experience there. Before starting True Foundry, me and my co founders Sync, Rag and Abyshek, we built out another company called Entire and that company got acquired by India's largest player called In Voyage which is the owner of Property.com. Basically this is like our second gig together. We are relatively an early stages like maybe about a year that we started building out the company. And we are right now being a little bit selective about some companies that we are working with in terms of like we're making sure that the problem that they are facing is very relevant to what we are solving. They do have an intent and appetite to work as design partners and we are essentially onboarding like one company a month now because most of our engineering team is now busy with our already the existing design partners and stuff. So that's why this intro call is very helpful for me to understand what kind of challenges you are facing and what are generally the processes of model deployments and stuff like that. Would love to understand that a little bit. So if you can maybe start with an overview about the work that you do at Health IQ or what kind of ML problems are you solving, I can then dive deeper into some specific areas. Yeah. So overall over the years I have built and deployed several models and basically we use AWS ecosystem for that. The. Whole business give me 1 minute to explain how the business flows, right? So we are like the middleman between the insurance carrier and the customer and the way customer would qualify for a discounted rate and we'll negotiate on their behalf is a healthy lifestyle. That's why we called Health IQ. How much aware you are about your health, our data has shown, which is like no big surprise, that when you live a healthy lifestyle, don't smoke, don't drink much, do exercise, eat healthy, typically a disease, workout, yoga, whatever. In general, if you live a healthy lifestyle, you tend to live healthier, you use your insurance less. An insurance company like that so they are willing to provide discounted rate to people who tend to use the insurance less. Right? So that's where the whole thing we do is we basically generate leads from our different partners and we call them and we engage with them and try to sell them our policies. So if you look at the whole thing, the business can be broadly sumit into three, four broad areas. One is like marketing generating leads, right? When leads are generated then we goes to our system and then the sales team starts dialing them and talking to them and then most importantly is the bank back end where if the agent makes a sale then we have to get the paperwork together. Send those paperwork to carriers and then once carrier approves. Then the policy is in effect and people start paying their premium and all that. Right? So some of the models that we have worked with that I have developed is like how to generate better. Leads. How to generate better leads. So we look at what features a lead come with by default, right? And then what was the journey of the lead and basically have identified features, important features and build models on those to first of all identify the best features, right? That these features are critical and you can always just focus your marketing lead generation on those features that I only want this kind of lead you can tell to your partners, right? The second thing is once the lead comes through we kind of do lead scoring like what is the chances of this lead buying our policy and being converted and the higher the likelihood, the faster we want to dial them and engage with them. That's where it comes. The second part of the process, which is like dialing with the leads typically we have much more number of leads than the dials we make. Okay, so who do we dial? How do we select those? Once the lead starts getting dialed by our system we also generate features and create features on those leads and once that happens. Then I have built another model which basically has additional set of features in addition to what the lead already came with the features that we generated within our system and then determine minor lead score and then the higher the lead score the higher priority it gets in dialing. So that's the same kind of model. The third model kind of problem that we handled is there's plenty of vendors in the market who can augment your data, right? You are a person. A lead isn't intent, it's not a person but if you look at physical world at the end there's a person behind that lead, right? And that person basically will have a bunch of other attributes that we don't get by default, right, address work, which area you live in is high end area and what's your profession, right, those kind of things. So we also worked and tried to buy data from third party vendors who basically augment our data set and then build better models like for example if somebody is a mover like relocates or willing to relocate then medicare supplement plan works best for them, right? So that sort of model we built on that. Currently the most important thing that I've been leading is the strategic shift of entire company towards what we call Precision Medicare. So initially we used to sell all kinds of policies which is life insurance. Senior life insurance. Medicare and auto and this year we have decided that we are only going to sell Medicare and in Medicare we want to innovate and create something that we call Precision Medicare where we are looking at we are pulling all the possible health data that we can get from a person and then based on their current need and future needs. We recommend them the best plans. For example, we'll pull their electronic health record, we'll pull their lab test, we'll pull which doctors they are going to, which pharmacy they are going to, what are the drugs they are taking and based on that we build models that if you have a certain disease like what is the probability of transitioning into next phase, right? If you take an example of diabetes and diabetes many people don't use insulin but over the years as diabetes progresses then you start taking insulin, right? If you start taking insulin and that's a big thing, your insurance cost will significantly differ. So we would make a prediction if there's a diabetic person hey, what is this like? Currently these are your needs and this plan meets your need but it's very likely that you're going to switch to using insulin during this year and then based on those needs it seems this other plan is better for you. So that also has to be done by entire data. Science and predictive modeling has to work together, use all this data and then present the best plan for individual customers. And that's what I built from scratch and it's called Precision Medicare. You can go on health IQ website and get an experience first hand yourself. The challenges there are less with the deployment and all but just the magnitude of work that needs to be done. Like you have over, I think, 3000 counties. There are 70,000 plans. If you assume that county is what. Did you say after that? I missed you. There could be 70,000 possible Medicare plans. Active, inactive, 70,000, $70,000. But depending upon the county, the active plans are usually only $7,000. Right. But there is a process that all the insurance carriers have to submit their bid to CMS before they come into play. And one carrier can submit multiple bids. So first thing is you have to know what are the active plans? Like actual plans. Those are the challenges that we face. And we have like, again, third party vendors who provide information. The biggest challenge I believe right now is we have like 200 diseases and within each disease you can have. Multiple. Conditions and multiple transitions. Right. If you have diabetes, then insulin transition is not the only thing that can happen to you. Diabetes, people suffering with diabetes also tend to have dental erosion and all right, so it's like a multi level, multi classification problem for each disease. And we have like 150 diseases. That. We have selected which will cover most of the population, but list of goes on and on. But right now we are just trying to focus on those. Understood. Very nice. Very helpful background. Thanks a lot for giving me the background of the business. I think it really helps me put your work in perspective of the business. That actually helps quite a bit. So one thing is that you mentioned about three things, right? One is about lead scoring that you are working on. The second thing you mentioned is about some additional features. And the third thing was some kind of demographic information for which you work with third party vendors. What was the second additional feature that you mentioned that you're talking about? When we start dialing people that not everybody will immediately pick up the call and talk to us. Right. Sometimes we dial them two times. I think our SLA is two times a day at max. So once we have dialed and the lead stay in our system for a number of days, sometimes years, then we have built features like how many dials have been on this lead, how many connect have been on this lead, how many operator intercept, which means operator intercept could be because it's a bad phone number. There is a technical problem. So all those features also determine how likely you are going to have a contact with your lead and be able to establish communication. Understood? Okay. Typically the leads which are very old in our system, like year, two years, the chances are very less, but we have got some success. But even among those, when we rank them, we have got some success. We have sold them, which otherwise we wouldn't be because we are not even dialing them. Got it? Okay, understood. And by nature of this problem that you described? I assume that this is kind of done using some kind of batch inference where your lead scores are put in some database or something and people would. Pick up from there. Got it. Okay, I see. So what does this overall, like, the model building, like, this model inferencing process, look like a bishop? Like, what kind of tools do you end up using for this? Do you use notebook frequently? You said you use AWS, or do you use Sage Maker of AWS? Sage maker and sage maker studio. That's where the models are deployed, and we do batch inference. But initially, when I build these models, I just did it on my laptop with a smaller data center in the CDR phase. What phase? CDR phase conceptual design. I see. Okay, understood. And Sage Maker, so you use batch inferencing do you do it via Sage Maker containers? Yes. Like, what is that thing called? Dockers? Yes. What is that term for Kubernetes engine. Right? Yeah. External ECF elastic container service. Container service. Yes. Okay, I see. And who builds out these docker containers? Actually, maybe how about this? That you tell me a little bit about the team structure. Is there like a separate data science. Team compared from the engineering team? Compared from yeah, we have entire engineering team and data engineers who take care of at least the infrastructure part. So we develop our website and all is done internally. So there's whole team technology with many engineers working on different aspects of health. I see. And then how about the people who work on machine learning problems? Is there like a separate data science versus MLE machine learning engineering team? No, I do not differentiate between data scientists and machine learning engineers. I believe it's the same thing. I see. Okay, got it. We do have some data engineers. Okay, I see. So then how does the process look like? Basically, if you think about the entire workflow, there could be data engineering, there could be feature engineering, there could be actual model building. There could be containerizing the model, there could be deployment of the models. There could be monitoring of the model. How is the sumit between different teams, and also how is the machines resources allocated and stuff. So all of that decision I make on my own, like data engineering part is only getting the raw input data to me, which is like, we have our DBT and all SQL redshift databases from where I can get the data. And then from there till the end, till deployment, I or my team will do everything. It's like cleaning the data, doing feature engineering, building model, testing model, deploying model, and then doing batch inferences and monitoring for data leakage or performance degradation when we need to retrain the model. And whenever we do entrance, we simply make it pass through the ETL pipeline and attach with the new leads. Oh, I see. Okay, understood. Nice. Got it. Okay. And then in here you mentioned that one of the problems is kind of like 3000 county, seven k active plants. Is that like yeah, but that's not. Really a modeling or problem. It's just the problem of getting the data and identifying accurately what needs to be used. I see. Okay, got it. So in this entire stack that you mentioned that once you get like your raw data from redshift or DBT and stuff, your team handles a lot of components, right? Yeah. Are there any specific challenges in those components that you're trying to address a bishop? Like something that is top of mind for you? Well, it's just like the volume of work right now, especially the last bit of peace that I said, where we have to do 400, 200 diseases, multi level, multiclassification models. So we have done some initial work and it seems like the neural nets will be the way to go. And neural networks are not light. Having a good model is necessary, but having an agile and model is also important. I see. So currently the team is not invested heavily in neural networks. Currently the models are more like random forest type models. The previous models have been random forest. Now the disease models we are building are neural networks. I see. And being able to like, as you mentioned, the neural networks can be very heavy. In this context, are you facing any, like are you trying to solve any specific problems in this context? We generally build using cameras. We try to go like having two layer, three layer only, not like very heavy neural network. But as the problem gets more complex and challenging, then maybe we will need to add more layers. All right. I see. Okay. I see. So you mentioned that your team is doing a lot of things right now, right? Like you're kind of in some way resource constraint. Now, if in such a scenario, like if you're doing, let's say a team is working on five things and you're like, okay, five is too many, if I could, I would focus all my team's attention on these three things and drop the other two things. What would those two things that you would ideally like to be dropped? Like somebody else can take care of those two things while you focus on the core stuff? Well, I think we're partially already doing it because now that I know that everybody can do deployment, I basically I want to make sure that my data scientists don't lose their skills. They have the skills to do stuff. And if any load has to be taken off, then we can do later. The deployment part is now moving towards more like engineering. Engineering deploys those. So we build the model, we test the model and do some forward testing. And once that is done, the deployment part, we give it to engineers to do. And that is also partially because the output the model netmeds to be integrated with the web interface and mobile app and all. I see. When you transfer this model to the engineering team, I guess for deployments, what's the interface until what point is the data science team taking and what do you hand over to the engineering team? That would be the preprocessing script. So once they get the data, what pre processing needs to be done for the future? Data to be deployed on it to do phone conference in future. And then there'll be like model in pickle format or whatever that they can read. And then what format needs to be the output and how the output needs to be integrated back to the data. Interesting. I sep got it. Okay. Our own code reviews and all the code is uploaded on GitHub where everybody can see. So that way they can understand better. I see, understood. Got it. Okay. One other thing here is around like you also mentioned that you also have some kind of monitoring built in. Are you using an external tools? Are you using no. So how do you do model monitoring today? That part I'll have to discuss with one of the data scientists. I'm not sure since I was at a higher level of leadership right now, I don't have all the details of all the processes. I see. Got it. Okay. Understood. And in this entire thing about the neural network deployments, do you know if the team has suggested that they're going to use TensorFlow or PyTorch? Are they exploring some things? Like, I think right now they are. Using Keras. And with Keras. So the way you would hand over a keras model to the engineering team generally would be, let's say, a little bit different than handing over, let's say, randomized model to the engineering team. Right? Because randomized model would be like a pickle file. Keras would be like it's own the saved model format of TensorFlow. There are a lot of other complications because TensorFlow does not allow you to package your pre processing scripts with the model that becomes separate. So how are you handling that transition? Abysheek. So the preprocessing is usually done by python script, right? You have you bring your data using your SQL script, and then all the pre processing has to be done by python script. The python script will give you an output and that you can then pass to the model and the model output will again netmeds to be the process to bring it back to the original data where it needs to be integrated. I see. Okay. So then if that's the case that your python script is separate from your modeling script in this case, then if, let's say, data scientist has to update something in the model, right? Maybe like add a new feature or change like some hyper parameter in the model. How long does that part take to execute today in production. Well, that would just require the model to be retrained because you are adding features. Right. That would require the whole you already have the code ready. You just need to add one thing and retrain the model. So I would assume less than a day. I see. But I guess that's like your offline model training part. Right? But now that your model is expecting like N plus One features even in your model inference site, which is something that the engineering team is handling, you would have to add that N plus One S feature. Right. So like this code cannot be out of sync. When you're training the model with N plus One features, your inferencing should also happen with N plus One features. Right. So there has to be some communication between the data scientists and the engineering team so that this happens in production. Yeah. So once the model works so N Plus One feature will first go through all the usual process of testing and training and whether the feature is producing the effect that we are looking for. And then the engineers will be updated that we need to replace this model with a new model. And all the scripts are then updated in GitHub. All they need to do is to engineers don't really need to do anything. Once you push your code in GitHub and it gets approved, then it automatically the process of automation. It flows through the production. I see. Okay, got it, understood. So in terms of any major changes to the pipeline, anything that you want, any problems that you specifically want to solve for, nothing that you have top of mind for you, it seems like a vish, is that right? No, got it. Understood. Okay. I think this is very helpful. Thank you so much appreciate for giving me this context. What I would like to do is maybe two things as a follow up call. I know we are a little bit over time. Also, number one that you talked about monitoring that you don't know currently, like what kind of monitoring systems that people are using. Like are they using Sage Maker monitoring or are they using anything external? I assume it's a Sage Maker because we don't have any external thing on any of the model, the entire data part of it. So everything is done in the sage makers environment. Sage maker itself does not provide visualizations on top of your model. It allows you to log your predictions and actuals in S three. But it's like visualizations are actually like it does not have almost any coverage on terms of the proper visualization. So is there something that the team has built out internally? Do you know? IBM not aware of that. Sorry. Okay, got it. Okay, so maybe what I would like to do is in our next call, discuss about this a little bit more. Hopefully you get a chance to like if I sync up with someone internally. Maybe you can check on this. And then the second thing is I would love to get some candid feedback on what we are building out. I think a lot of stack that you described is similar to what we use. So I think your feedback will become very valid. And we also cover this transition from building out these random forest models to going to Tensorflows and fighters and stuff like that. And some level of allowing this thing that you mentioned that you push the code to GitHub and the model automatically gets deployed and stuff like that. So we do also some of that. So those things it seems like your experience, because you have actually already built and your system like this. I think your feedback will be very helpful. So maybe we can set up like a follow up call where we do a demo of this thing. Sure. Got it. Okay, cool. So what date, time, et cetera that would work best for you, Michael? Next week? Wednesday should work Wednesday afternoon. Okay, so same time next week, Wednesday afternoon? Yeah. Okay, sounds good. I'll send out an invite then. Cool. Awesome. Thank you so much, Abyssaic. Have a good meeting you. Bye. Thanks to me too. Bye.