Hello. Hello. Hey guys. Sorry, running two minutes late. How are you doing? Hi Peter. I'm good, how are you doing? IBM good. Putting baby's sleep? Yes. He started crying. I just came to the meeting and then he's was sleeping and he started trying, so I just got him. Yeah. How old is he? Four months. Four months? Wow. Do you have kids? I have three. Oh nice. I have three kids. They are the youngest one is five. Okay. How CTO baby. Make sense? Your first one? First one? Yeah. Oh cool. Yeah. That suddenly keeps us busy. Well, Qovery, nice to meet you. I feel like today I was actually going over the truce on to the doc you sent over. I think today could be a good introduction call. The next time I'll bring my head of machine learning on the call. His name is Archie. I think that'll be good because I just want to compare notes with him on his take. We definitely have challenges like every company that is investing in ML, but I wanted to hear from you guys directly. Absolutely. Yeah. Happy to share some context. And thanks a lot Peter, for opening up the floor for conversation. Maybe I can give you a very brief introduction about myself and true Hungary to begin with. I would learn more about you and like you mentioned that you're having some challenges and I would love to get your perspective on that. I know you might have more details that you would probably share about Chris. Let's kick this off. So speaking a little bit about myself, Peter, I come from a machine learning background myself. Have you heard of the Facebook product called Portal? Yeah. Okay. So I actually was leading one of the conversation AI efforts at Poland. By any chance do you use the product? I have not. I thought about buying one and I've seen people on the receiving end when calls tracking. What do you think of the product? Do you think it's worth buying? Are you using you're not using that right now? I'm not using it right now. I have it in the other room, one of the photos. I like the video calling feature quite a bit. I think it's really nice even in kitchen when you're calling family and stuff, that is very helpful. I don't think we did a good job at the virtual assistant bit of it. I think it's just too shallow a product in the virtual assistant domain. Honestly, the part of the team that I was working on, I feel like that's the part that has not done a good job overall. But the team that I was not a supervisor at all, that is like video calling. I think that's quite nice. I was driving one of the conversation I heard there at Facebook and prior to that I worked in recommendation system space where I built out personalization algorithms for the ecommerce industry as part of a startup called Reflection. And at Reflection I spent first two years building out these models and the last year and half building out a horizontal platform for the company, like machine learning platform for the company because we got like multiple teams that were building machine learning models and we realized that we're duplicating a lot of efforts so centralizing that through a platform made sense. And that's part of the story of what we are building at Proof only as well. Like, you know, my learnings from Reflection, what worked, what did not work, and then what stark difference that I saw when I joined Facebook in terms of the platform, that's part of what we are trying to capture in our efforts at what we are building at Truefoundry. You I see. When did you start truf? Andre it's been over a year now that we're running through Foundry. We are fairly early stage, like maybe 15 people in the team and seed funded, working with few enterprises currently as design partners. So that's where we are in terms of the journey and realistically one of the goals of what we're trying to build peter at Truefoundry is we believe that it's really hard for any external machine learning platform to just take away the ML platform work within any company whatsoever. Companies will always have custom use sales gong, tail of use cases of machine learning that's really hard to just kind of automate with a bump size, fit all type of product. So our Gmail is that we build out a platform which makes it really easy for internal platform teams, internal intra teams to build on top of essentially so a lot of common things that everyone tries to build out. We support that out of the box, but we provide enough flexibility that people could build whatever they want on top of it and kind of scale up their platform. So like, you know, if you're thinking in terms of like the optimization of cost within the company, if let's say we are two infrared developers are able to serve a team of like six to ten people in machine learning. Ideally those same two developers should be able to scale to whatever like 25, 30, 40 people in your machine learning team. That's kind of the goal that we're looking for. You can scale up your machine learning team, but you don't necessarily have to put in enough resources to continue building internal dev tooling and scale with them essentially. That makes sense. That makes a lot of sense. We have a relatively small, fairly small machine learning team, like data scientists, handful engineers, and we've also found that and we just started a team, I would say less than two years ago. Yeah. So it's not a long running team. And our use cases are I would say there's definitely a cluster of use cases around recommendation for sure and recommendation could be on the lines of stories, shopping products and also specific types like quizzes as example, because mostly quizzes on the key offerings that we have and it feels like a long journey for us, for sure. So, for example, even it wasn't until this year we started offline testing, there's so many things that happened even to get CTO this point, even the speed, which the number of models I think I remember speeding Chris number doc chris is very common challenges, right? The speed of integration, the monitoring piece, the observability piece itself. Some of it is more people, soumen definitely but some of it definitely is tooling as well. Which product service is ready for a test, whether it's the app home fees, for example, whether it's a module for shopping. Anyway, there's quite a few actually just even talking to you remind me of a slew of challenges. Yeah. And there's some other initiatives that we're working on that are not just about model specifically, but more leveraging, but more building audience segments for advertising. That is one of the newer initiatives that we have. But they're not new use cases in the space, right? Look like audiences based on what? Coercion data, optimizing performance during campaign and so on. I see. So I guess like one question here is like, you talked about a few challenges in the space, which I think you and I both understand that these are fairly common challenges for teams trying to build their machine learning applications, build and scale right now. And of course you have done a bunch of work, your team has done a bunch of work to get to where you are already solved a few of these challenges along the way. What are some of the new set of improvements that you think that the team is going to be investing on for the next six months to one year? What are some of the top of my challenges currently? I think definitely speed of testing models. Okay, one. So how many models can we test? I feel like the rate of iteration on the models can be faster. We have a set of them. I'm actually looking at a list. We have a list here. Different versions of them start out with a very near use case and when expanding the use cases, now we have X number of use cases. And then there's iterations v zero v, right? And so on so forth. How quickly can we get to that duration without adding people? Linearly one, challenge second, there's a few IBM just thinking about one of the key ones. There's some issues not too related that IBM thinking about right now, but IBM sentilink them anyway. Like in the space, a lot of failures are expected, right? And us getting comfortable with the failures, understand why moving on and also explaining to our own teams and external teams why we Gmail what's next and not feeling like we go from iteration CTO duration, the anticipation of like we should continue seeing positive results is just not realistic in the space. Yes. And actually that's hard for sometimes for outside of tech to understanding the space. Oh, you run as many models, you spend as much money, but net from zero to b two, you only got 43. Modeling doesn't work in this space. And IBM shameless because also we're budgeting for next year. So I'm actually figure out the best way to maximize iteration speed and quality while minimizing the cost across product service areas. Because we have six different brands, even the brands are now less relevant in the space as they use case themselves. But even the underlying data, we do have some work to transform, say, complex data. We're still transforming into a common datta model that Buzzy has because we just acquired complex a year ago. So there's an underlying work needs to happen before we can even provide service to them. Right? Yeah. I think you can relate to a lot of these Qovery similar problems that we have. Another one will be, I think, the amount of tests we can do, which is both human as well as technical. I'll give you example. We're testing different audience segments both on scale. How many audience segments can we create relevant to business? Which ones can run? Because it depends on the campaigns that are available to us, how many feedback loops. So if you like right now feel kind of slow, get them like, hey, this on a segment and get tested compared to our current baseline, get results back after couple of weeks, what's the next one? So it's kind of very not as if we have just a massive model here. All the tests, we could just run. Them. And then compare and then say, what's the next sep? It's not production. No, it's not productized yet. So it's very slow adoption problem right now to get it, this is more along the audience segmentation initiative. Right? What else? Is it helpful so far? Absolutely. Yeah, totally, totally helpful. And as you mentioned, there are a couple of I think number one and number three are primarily tooling challenges. And number two, as you mentioned, is not necessarily a tooling challenge, but in a way, Peter, it is. And by the way, it's not a problem that we necessarily solve. But I feel like I have heard similar problem from some other leaders. And a lot of it, Peter, boils down to visibility. Right? Like, for example, the team is conducting multiple different experiments. Some are bound to fail. Right? But if there was a good way of creating visibility into what kind of experiments got conducted, why did we take certain design decisions and what was the output of that? Even if it's not like positive output of every experiment, I think that itself helps people narrate teams, narrate a story that, hey, these are the things that we did. This worked and this did not work. And this is the learning that we have. But with machine learning, a lot of times what happens is that the experiments that we conduct are very siloed. Like, let's say a data scientist or an ML developer is kind of training a few models. Learnings don't get transferred from one person or one team to the other and stuff like that. I think that's the thing that kind of creates this feeling of like, we did a lot of work, but nothing came out of it kind of thing. So that's one of the things that I've heard some companies trying to adopt to solve this problem. And to be honest, I think there is some level of truth to it as well. That I think we can be a little bit more efficient with our ML experimentation if we were able to capture learnings and kind of apply from one project to the other, which does get dropped because of like lack of tracking. Tracking. I don't know. By the way, I might be overstepping it here. I don't know, maybe you all have built out a lot of visibility related tools or maybe you haven't. I don't know what's the current state, but that's just like a common problem that I've heard. I would say we have some, but I think the way we I mean, I think within I would say a small subset of us within the team understands where we are, has high visibility and understanding of where and why, then the majority second like two degrees away. They don't understand it. And it's actually how you tell the story needs to be humanized in a way that isn't just about zero. I mean, we have great documentation on things, which I think is very helpful. But even as I portrayed, I say if I'm trying to finance it's just an example, right? What they care about is time to impact and cost to impact. I chad to explain from that point of view, say, okay, so if you've done all this, when do you think we can improve the conversion rate for this in a meaningful fashion? That's really the army for. And what's interesting is we're not quite there yet, but as you know, it could happen. It's not like we can plan for it. But you've got CTO keep going until it happens, right? Is that kind of consistency. Of course, part of what we're trying to do is break shorten the cycle. Right? And I think that's a very true thing. How do we shorten the cycle? There's a few things actually even design is one piece of it, which I did mention before. Design meaning actually because we have many recommendation services and we are introducing algorithm into it that's different versus curated. We shane to change them when they coexist. What's the pattern that we feel we can coexist and then right across multiple. So it's not like we're rethinking this again every time it adds more time CTO it. So there's almost like a predesigned process of saying we're going to do recommendation for say, content. We have X number of modules that all do very similar things but in different contexts. Apply the test to this specific set subset. Okay, let's make sure the design patterns are similar enough we can just deploy to them. Right. So I get a plan ahead, way ahead. And engineers and designers talk about designing a picture is good. Then you how do we integrate, agree on that too. How do we monitor, how do we value the results, how we can be exposed to and so on and so forth. It just takes a lot of coordination. Today I feel we have a timeline cycle. We feel it's way too long. We have different phases from thinking about the problem itself, about the ML development itself, the integration itself. Together it's I would say maybe four months per site per space for problem space. I see. And integration takes the most right now and integration piece. If it's a new space, I understand first time new integration, but really hoping iterations are exponentially shorter. Initial ramp up is so long and sometimes the roadmaps don't match. Right. Because they're working across different teams. The amount team is working with apps team, the app team is not ready yet. So there's misalignment issue that's more people than tools. Anyway, I'm just sharing being transparent here because I do think that we can do a lot better. Yeah, for sure. Thanks a lot for sharing this Peter. Honestly, I would love CTO do a deep dive on this with you and also kind of showcase a little bit of what we are working on and just exploring that. If there is any way that we can potentially help to kind of reduce this cost of experimentation and hopefully increase the speed, that's something that we are very actively focused on. So maybe that's something that we can do a deeper dive into in our next call or something when we meet together with Archie. Yeah, let me leave Archie on this and share the doc that you have. This Shane a discovery call together with him, right? Yeah. And meanwhile, Peter, in today's call I wanted to touch base on one more thing with you, which is I noticed from your LinkedIn that you have been like mentors, mentor, advisor, investor in a lot of companies as well. And I was doing it based out of New York area. Right. Actually I moved recently to Southern California, but I do go there very often, every couple of weeks. I'm in New York three weeks ago. Interesting. By the way, around there. Do you know Mike Boufford? Mike Buffer, michael Buffett? Like the CTO of Greenhouse. I don't. I've seen him. IBM aware of who he is, but I don't know each other. I understood in this journey we are getting a few key leaders in the space on board with us as angel investors and all different capacities basically. Right. So of course at this point you don't have enough visibility into what we are doing, who we are, what our team is, et cetera. Happy to dive into that a little bit, but at a high level. I wanted to check if this is something you're still doing, like working with startups on parallel personal sense. Yeah, I'm still doing that. Okay. I think it's actually very helpful. Personally I find that pretty important because my world is a bubble and the bubble has a lot of blind spots and I have constraints within the bubble. So I think that's why the ventures, the startups advisory, it's actually my way of understanding a what a new approach is to existing problems, which I see a lot too. Are there new problems? Actually different we're looking problems because that's actually a lot of times where innovation happens refrain and that's when if I'm not out there, it's not going to happen to me. I have to go out there and talk to people and work with them. Then I see what's going on. Like first person on a first person basis. So I really do appreciate those. And you're in an interesting spot for sure. This is the space. I've seen so many challenges. Right? Yeah, I like to believe that. Right. So far we are on the right side of the history. I think there's going to be a big change that's going to come in the space in terms of machine learning, adoption. And I feel like adoption really is contingent on how easy you can make it. And clearly right now it's not easy. It is actually meant for very savvy teams who are able to build and scale ML platforms and we need to make it like hundred x easier, not ten x easier to really take you to mass adoption. And it's gong to happen. Like that is for sure. Question is, are we playing an important part in that journey or not? That's the main question. Yeah, exactly. And when you were at Facebook, what did you see that you felt like, okay, that was a good approach. The way they approached it is something that other people can't actually much broader, applicable to a much broader community versus just within Facebook. What are some of the lessons? Curious for sure. Yeah. So at Facebook, basically what we had was they were almost like if you think about the overall ML development lifecycle, there were different tools that are built out for each part of the lifecycle. Right. I think that's more understandable that you have a feature store, you have a distributed model training component and then you have a deployment component. Right. You had all of those three. And as you would imagine now some of the interesting things that I noticed that Facebook was that once a model is deployed right. Once the model is trained sorry, not deployed, trained. Right. After that deploying that model literally used to be like one click. Like how you get that model trained model to be deployed as an API endpoint, right? And then deployment does not necessarily just mean you have a fast API around the model itself. There's so many things, interesting things that happen when you go deeper in it. For example, is your model getting deployed, that is, getting deployed, scaling automatically in different regions that you might be serving traffic with that particular model, right? When you deploy that, are you logging every prediction that the model is making and then are you able to connect it with the connected dots later with the actuals that you might receive? So for example, in recommendations, maybe you want to track your clicks that people are doing, right? Can you do that automatically? Can you track the performance of the model right after that? Right. And then whatever things that you received, can you quickly feed that in a retraining loop? So these things were sorted out so seamlessly at Facebook that as a developer, I could actually build and deploy probably five models in a day and like five really high quality models, right? When I talk to other people frequently the timelines that I hear are in the range of like weeks to months for a single model that people are doing. And even then I feel like that the level of maturity that we saw is not available. And Peter, one of the learning that I feel like sucks the most after I tal to people is most people actually don't realize what they are missing. And that actually is by the way, is a risk to our startup as well, to be quite honest. But when I talk to people and they are like, okay, we take like three weeks to deploy a model and they seem to be comfortable with it, they think that's the status quo and that can be done. But realistically they're not able to expand to other use cases because their developers are busy training and retraining and maintaining the existing models, as opposed to being able to expand to new use sales or run new experiments, essentially. So that's one of the other things that I had, and by the way, including myself when I built out the platform at Reflection, I thought, oh, we have achieved a lot in terms of from where we were, but from where we could be was the visibility that I was completely missing. So that's one part of it. The second part of it was around the training component of it itself, where I felt like Facebook itself did not do a very good job at. And that's the other learning that I took. For example, frequently Facebook would train models and like the resource allocation was extremely sequential. So I have myself waited for my models to come back, train like for 8 hours, 24 hours, 48 hours, which I feel like for a company like Facebook should be, you should be able to solve that problem much faster. Ideally, like put parallel computation and stuff like that, which I don't get a very good job at. And that's something that I'm taking a lesson learning that okay. When IBM building the platform, you're going to solve for that as well. One from each angle. I guess one of the other things was that no dependency of machine learning developers on the India folks until a certain limit, and they did it with context and not controls. I'll explain what I mean to say frequently, like, companies like machine learning is compute intensive, right? So you want to get more and more machines to be able to train and deploy your models. Now, machine learning developers are frequently broke on intra folks to either provision the clusters for them or install certain new libraries or do a lot of develop work around that. They automated all of that. Now, once you automate all of that and you empower the developer to do whatever they want to do, they can actually increase a lot of compute cost. So they did a very simple tweak to this automation where every time you train a model or deploy a model, it shows your cost number, like a dollar value, that this training run that you are going to spin up will roughly cost us $4,500. That's it. They're not stopping you from doing it. They're just giving that context to you. And that itself made us as developers, like a lot more sensitive that maybe I can run with a smaller amount of dataset when I'm doing the experiments, because just to realize the typo in my code, I don't want to run a $4,500 job. Basically few key learnings that we got from there that we're trying to incorporate one by one in our platform. And I feel like the people that we are working with really appreciate some of some of those features that we. Are building in that's key, actually. Yeah, those are like little fine. Exactly that's kind of developer experience. Yeah. That's awesome. Thank you. Let me bring in terms of scheduling. Did you have a calendar link? Is that how we got connected? Yes, I can share my calendar link with you. Pardon me? I will make that easy because oftentimes Chris is when things break down, no different than Chris integration point. Calendar is one of the worst integration points between people. Totally. I pinged it to you on Chat here, but I'm going to also ping you on LinkedIn or email or wherever we are connected. Just email me. Yeah. Okay. I'll just email to you and then maybe we can set up a call with RC together. Yes. Okay, sounds good. Awesome. Thank you so much. Have a good one, Peter. Thanks.