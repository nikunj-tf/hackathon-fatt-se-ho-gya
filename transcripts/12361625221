Angel, how are you? Yeah, good. So angel I think what we wanted to do is based on yesterday's understanding, we wanted to kind of maybe quickly just summarize overall one as to what is our understanding. And then we'll walk you through what are the major components that we feel is needed from the perspective of the solution? What are the things that we feel from an architecture perspective, like as to how the platform has been designed? And then we'll walk you through the platform and in that we'll see which API is like this at least you can use when you integrate that will allow you to kind of expose to the customers of this company. Okay. Just I'll summarize once and maybe you can correct me if it's wrong anywhere I'll just present once. Do you want to summarize this? Basically, I think this is what we understood. Resolveai basically is using kubernetes based system and they want to have AI that is integrated into the platform they provide to their customers. So one question I have Angela is what is the platform that they provide? Because actually I was trying to look at Resolve there, but I could not find I don't know if it's at O-L-V-E. Okay. It was also known as actionable science in Us. So Resolveai rezold this one, right? Yeah. Okay, so what they're providing is something like a support in their platform that they provide to their SaaS companies. Like where exactly is they are going to be used? If you can illustrate that once, for. Example, there is a client called Modi, for example, what will happen is all the Modi employees will be on boarded in their system and Modi will upload their knowledge bases, they will upload their specific information related to their policies and their policies and ticketing system everything. Now this will go into right now it is going into cognitive search which is kind of last search. So from there they develop this Q and A base model and whenever employee ask questions from chat, let's say or anyone else who is coming to moody's customer desk they are asking some questions. So what will happen is the AI will respond to that query and also if that is not responsible, then it goes to an agent. Okay. And then when they upload their data, like you were saying that they will have to retrain their model, right? The model to retrain a new version should be created based on their data. Yeah. So it was getting very complex. I was talking to the founder again today and what he said is to simplify because it was getting very complex, we allow everyone to upload their models. It was becoming very large implementation. So what he is saying now is like let's have one model per use case. One use case can be let's say ticket analyzer. So we integrate with lot of itm tools like service desk tools. So we can basically have one model for each use case, and same model can be used to serve all the tenants. Okay, so right now, strategy like he's thinking because it is not possible to put all these multiple models in the same process and all right, so he's also thinking in that direction. How much are we talking about for each model? Memory? Yeah. How much Ram are we talking about for each model? You mentioned that it is not possible to do. No, we haven't tried yet, actually. It's not even in production. No, we just started. There is a docker based image that we have created using flask, but it's very difficult to productionize that kind of deployment like flask for every model deployment. That is not correct. Okay. A lot of companies, like some of the companies that we work with, angel, they have like five mb, ten mb models. So even if they have 100 models now, they load all of them because it's very small. Yeah, but how do I calculate that actually size? Where can I see what the size of the model? Like the pkl file, right? Yeah. If you even have the pkl file size, that will also be good. Okay, 1 second. These are tensorflow or Python. Python. Python models. Okay. But Python serve has a way to load multiple models. And you require only gpu or do you require gq during inferencing? We require gpu. Yeah, during inferencing also. This is our understanding that we might need it right now, but we are not sure whether with you will cause problem or not. These models are very small tvs only right now. Okay, then it should be a problem to load then. Hopefully if you're loading a lot also. Maybe we can go through some of these questions further, like angel, as we go through and understand yesterday's understanding was at least key. The platform will host a model which is probably created internally, and then if a customer's data is uploaded, then the platform should be able to retrain it. And that's retrained model should then be exposed as the endpoint to this, folks. So that model is now being called and across different customers, like, the same thing will happen. Like different customers data will be there, but different versions of the model should not interface with each other because each customer's model will be different. Right, so that was one thing here one thing that I wanted to understand yesterday when we talked about loading multiple models into a single service. Would you be okay? Is it loading multiple models of the same customer or is it loading multiple models across customers as well? We can do across customers because we have a middle layer, right, where we can take tenant ID or something or some identifier and then query the back end with that specific ID. Okay. And one thing was that gpus are used for retraining. So fractional gpus should be there to be used as a shared resource and then ideally you'd want that the cost is optimal as the scale as it scales from say 100 to 1000 customers and so on. And model serving should be supported for different frameworks. Like I think you were trying to use nvidia but that is one of the use case also that we understood in this one question that I still have is access control also came up like ml Four does not have it. So you need the access control for internal developers or do you need it also because for your clients your systems would have access control that allows them to access only a particularly we are. Not exposing ml flow directly to anyone. So we will have, as I said, like a middle layer where we will do the authentication authorization part and manage it. So we have key clock for that. Actually we will do that. We have the existing system. Okay. We just need to get the talent ID and store the model and all that in a specific talent ID path and route it properly. So I don't think that exactly is required right now. Okay. Because we are not allowing end user to basically directly query. Okay, got it. Cool. I think do you want to take it from here? Yeah, sure. Share my screen. And feel free to kind of correct our understanding, angel, if you kind of have any questions and this is just basis whatever we understood so far. I think that one more development happened because yesterday I didn't draw in the office. But what these guys are now doing also is for Q and a based model they are using Haystack. Are you aware of it? Haystack? No. They are basically trying and asking me to have this haystack also there. Okay, we see it's an open source model for the nlp. Like it's a transformer kind of model. Okay, so you would want to also host Haystack on this. Yeah, we are doing it. So I'm writing that picture for this right now. This is for one use case which is Qnne. Okay. So Haystack part also it can actually be done. So we'll also kind of potentially talk about how that can be done. Understood. Cool. So basically I'll give you an overview. Angel. First, we can think of it as a platform on top of kubernetes. Okay? So we have a concept of a control plane and a data plane. This will be important. I just think you'll understand this really fast because we are working with kubernetes day in and day out. So we have a concept of a control plane and then you can connect like multiple covenant clusters to it. When I show the demo, I'm just giving you this overview so that demo comes a little bit clear. So this can be a kubernetes cluster, this can be a cluster one, cluster two, like that. And this control plan basically has your model registry here okay, so this is where you can see, this is very similar to what ml Flow provides. It has a model register. You can also go and deploy things in multiple clusters. Like if you want to deploy a training job, a model deployment, all those things, it can coordinate across multiple clusters. Okay, so according to your use case, basically you have your own app, right? And you will probably just need to save the models to some model. So you can continue using ml Flow or you can save it here. It's up to you, like whatever you want to use. So you can either use ml for this pretty much ours one is very ml Flow compatible. It's the same API that ml Flow uses. You can also bring models from there to one. Like, it's fully compatible, basically. So this is using onyx or something like that? No, we save the models in the mls for a serialization format. Okay, so then you can convert those models into an X format from there. Okay. Tensorflow will save in the tensorflow Save format, python has its own format, sql format. ml Flow follows that sql license thing internally in build. So we just kind of piggyback on that. So model registry is there? And from the model registry, then if you want to launch a training job, we also allow you to run a training job. And you can run this training job as cron jobs or something or based on some data has come or not from your clients, you can trigger this training block which has Python API. So everything is driven by apis. There's a Python sdk also that we provide. Okay, using which you can trigger training job. So basically I think what you will probably need is log the model API. Then trigger training job and training job can be a Python script that you provide. Trigger training job. Look at the from the training job, you can push the model to model registry and then deploy a model pretty much these are the apis that might be mostly will be relevant to you. Okay, I have one question. This cluster one and cluster two is basically customers cluster. Yes. So your control plane, your two founder, is basically a SaaS platform and providing that control plane. If both options are there, angel, you can use our hosted control plane, or you can also deploy the entire control pin in your own cloud also. Okay. So I will move on to like do you want to see the code side first or should I just show you the ui side first? Let me show you the ui side and then I think it will become easier. So these are basically the clusters that are attached to this control bin. Okay. And behind that we use argos. You need to deploy argo workflows for Jobs istio for service mesh, kata for auto scaling, just for metrics. And then you can always put your own charts, like new link, whatever you want. At the end of the day, cluster is owned by you, so you can install whatever charts you want on them. So we can bring our existing cluster also? You can bring your existing cluster also. Okay, so the way you bring your existing cluster is like, you don't have to actually give us the credentials to a new cluster. We just ask you to install one handshad that you provide. And that hemshot basically connects to this control panel. And the cluster starts showing up here. Okay? So once that is there, now I will show you quickly the model registry. Because the first step you'll probably do is probably want to log all the models that you're producing. So I will just quickly show you the model registry that we have. And this view is very similar to ml Fly. I think you've already used ml flow. So you probably just understand this. Really. Can you tell me in a way, like, let's say how to start from as a data scientist, let's say I am not there and the data scientist comes to your platform who doesn't have any idea about kubernetes or anything, and then he wants to use this platform. Understood. Data scientist entry. I'll just quickly explain to you. So data scientist will probably write their training job like this. This is a standard training Irish training code, Python. So data scientists will write the training code. Now, they want to run this training code, like maybe every day, or they just want to run it on the cloud because there are gpus available and things like that. Right? Then all they need to do is they will have the requirements or text to this code. And then they need to write this Python code to deploy this job that they have written. So they import our library called Service Boundary. They tell us how to build the image. And then they create a job with the name image equal to image resources, how much they need, and then they click Job Deploy. Okay? And when they do this Job deploy, what is happening underneath? Angela, explain to this part is so this is the only feature where they need this infrastructure. If you see this workspace fqn, okay, it's like workspace ID. So where is this Workspace ID coming from? So this workspace. ID is coming from So you have added this clusters. Each cluster has multiple namespaces, right? We call each namespaces Workspace okay. And each workspace across. If you have five clusters, each of them has 20 namespaces. We'll have 100 workspaces. So one data scientist, you will allot a certain number of you can allot one or two namespace, two data scientists, it can be either dev staging, production namespace, or you can have one namespace per team. Okay, so this data scientist, let's say like in my data scientist on our team, and he has been allotted this Workspace okay, so then he will just go and copy this workspace fq and from here and they will substitute that here. Okay. And when they click on job or deploy this basically runs the job on the cluster. And inside the job they can do every logging. So they can do something like this. So they can do log parameters. This is similar to the ml slow log. Exactly. This is very similar to ml flagging. So they can log plots, they can do logic, data set and all those things they can log which they can then come and see on the ui right here. So this is basically all the projects that they have. They can go ahead and compare different runs that they have done. So this is the ml flow bit basically. Except comes with rbac and everything. So every project you can do like a permission control which they don't have access to. What project? You can also have multiple projects per customer. Probably you want to organize it like that. Okay. One more thing is let's say if we are bringing our kubernetes cluster and let's say right now we don't have any gpu based load. So in this case if there is a need like data scientists are asking for a gpu. Are you adding that node pool or something with a gpu or is it something like a manual thing? Cluster administrator has to first do it. Your job will run. No, we will prepare that. So in aws we use carpenter that automatically adds and removes node pools on gke. Also there is this dynamic node auto provisioning. So both aws and gke have very good support on this. azure there is nothing yet. Okay, so in azure we have to pre add that node pool with gpu. But then the minimum size will put it at zero. So if there is nobody using it, then the gpu will be zero. No gpu will be used. But the moment a request for gpu comes, that note pool will scale up to one. Okay. And these are all the models. The data scientist is saving the different versions of the models that you can track. So this is pretty much like you can track that schema is not available for these curries and rug. Do you know if any model has a schema or anything? Yeah, red wine has just go to red wine. Red wine is not a discount. Just go to the experiments and then. It will just show the schema of this thing reduce. I don't think it's just there was. Actually one you go back first. Red wine politic. Is it not searching like. Anyway, I think we should go ahead. I'll pull that up. But let's go to the main way the thing will work. We check like from the workflow perspective which is around the deployment multi model and everything. Basically you can store for each customer. You can probably create probably a model registry here. This was what I was showing you. So these models are basically saved here with different versions. And then if you want to deploy these models, like, let's say you've already saved the models, then when you want to deploy, it's up to you based on the use case, Angela, you might want to deploy it as a real time service or you might want to deploy it as a batch inference, right? Okay, so on a real time service, also there are multiple ways to deploy. One is you might want to put it in your own fast API layer of last layer, right? In some cases, when you have pre processing or post processing, things like that, then you might want to put it like that. Or you can directly deploy a model file that has been saved on the system to a micro service. Okay, like one model in one microservice. Or you can also put ten models in one microservice. So it's up to you. So basically what will happen is I'll just show you all the three approaches. One is when the model is already saved on the system and you want to deploy. Okay? So I will just choose this namespace where I want to deploy. And these are the list of all the models that I have. All the. Models that have been saved. And then you can name them according to the customer and all. And this is currently we only allow one model for microservice, but we are changing this to allow multiple. So you'll be able to select as many models as you want to fit in this micro service. And then all these models will be deployed. Basically, just select the models that you want to deploy, you select like if you want to target gpu, you can also select that. Or if you want to select a certain type of cp machine, you can select that and then you can click on deploy. And this will basically what it will do is it will create the service that I'll go to show. And if you want to deploy from hugging face like model directly, that also you can just give us the hugging face link and we'll automatically deploy it. And this is where the model gets deployed. You can see the parts running. Like you can also do new versions of the model that redeploying. Regarding the reloading part that you mentioned yesterday, Angela, that part will also be possible here. I will go there shortly. And this is the open API. I just want to show the open API part first. So this is the API that is generated that the product team can call to get the inference of the model. Okay? And regarding the live reloading thing, so currently how this has been powered, I'll explain and that part will become clear. So when we do this based on this framework of the model so let's say this model is trained in tensorflow. So this will automatically choose tensorflow server. If this model is trained in Python, it will choose pytor Serve. If you are doing pytorch model with gpu, we'll select Python. So whichever model framework is the best for the framework on which the model is trained, we'll basically choose the corresponding model server framework to deploy the model. Okay, so models can come from different framework, but this will automatically choose the best way to deploy it. So this is this one. We are showing the ui Angel. But like data scientists can actually do it via through the Python scripts or anything. So all the apis are exposed and these apis can then be integrated into the layer for the customer. So you don't have to expose this uis to the customer basically. Okay, so everything is driven by API. At the end of the day, everything is a rest API and we also provide our Python sdk. So the same thing that I did by ui, you can do it by Python code also. Okay. And then the batch inference is standard, like you want to launch a job. So in that case also it's like pretty much the same thing. So this training job, one thing will kind of in some way trigger the retaining in case you wanted that use case also. But suppose there's a model deployed already for different customers and now the data of a particular customer comes, then you want to retain it for that customer. Then for that customer, suppose the model is lying in the model registry. Anytime the customer uploads the data, basically your product will call the training job API and it will run. And then it will register and write to that registry and then expose the new version of the model at the end of the customer. So that will be the workflow. So that way you can also do continuously training every time a new data is fetched, if you want. Okay. Yeah. So if you want to deploy a job, what I showed you earlier was doing it directly from Python script. If you want to do it from a git repository, you can also do that and you just select if you have a docker file, we'll take the docker file. If you don't have a docker file, we'll generate the docker file also. If you are doing gpu, you can select the kuda version that you want to do. Again, the same concepts apply here. The same if you want to select a gpu type, you can select gpu type also. And it depends on which cluster you are deploying. Like some clusters gpu support is there and some cluster gpu support might not be there. So based on that and then you can trigger the job and then the job will run and you can actually track all the runs that have happened of the job. So this is basically where you can see every single time the training run has happened. Like if it's doing every day. You can track every single day what happened in the run and what metrics did it produce, what models did it produce. All of that can be tracked basically. So this one question inferencing server also runs in the customer's cluster, right? Everything runs in the customer's cluster. Okay. All of this is jobs inferencing all of them run in the customer cluster basically. Then you can go ahead and compare the standard ml flow functionality that the data centers will need. Okay. So this is there. So your jobs have been deployed. The training job will run and once the training job is there, you can deploy the models and the live reloading. Once you integrate the triton support is coming. Actually angel, we don't have it yet. Try to support will come and then your live reloading will work by default. Okay. And services is basically the same thing. If you want to deploy a fast API service, something that has an API service is that service. You can just go ahead and deploy and choose the repository you want to deploy. You can choose your docker registry and the end point if you want. Like what is the end point you are going to expose the service at? It is basically the kubernetes deployment. Yes, kubernetes deployment plus service plus virtual service plus Istio files. We kind of create a small abstraction over it and just we are able to do this dynamic node provisioning. As I mentioned, like in case of aws and bgp, this part is really good in the sense like your node pools are zero at the start. You don't have to attach any pre configured note pools. But if you select here automatically, node Node will come, it will run your service and then it'll shut down. In azure, you will have to pre create the note pools that you want to expose to developers. But that part also what will happen is this guy will automatically whatever note pools you add. So this is where you configure. So for every cluster you configure, which base domain is there and node pool will also come here. So if you have added whatever note pools you have added, you can select here and only those node pools will be available for developers to deploy on. Okay, so on one cluster you have given gpu, they will see the gpu and what cluster you haven't given gpu, then they will not see the gpu. Okay, got it. Can you present once the architecture actually how it will work? Like model store the data store and then different versions of the model. That way, like in general, you have models stores, different model versions are stored there. You can have a data set different one and then in each namespace you can have a service that uses model one, uses the data set one. I mean, your training job will probably use the data set one, output model one to the model registry and your service will probably run the model one and the model in one service. You can load multiple versions of the model or different models together, basically. Okay? And one thing about scalability, so let's say a lot of these models are coming and our cluster is not appropriately sized. So in that case, does your software raises issue or alarms that we are running out of capacity or something? Firstly, we kind of configure the cluster in auto scaling mode. So if the resources are going low, the cluster should auto scale. So we put both hpa, like the service auto scaler is there and then there's a cluster auto scaler also. Okay? But in general you should put limits like that. It's not like the cluster auto scale is certainly scaling up to 100 nodes. So we don't directly do those alerts. We also have quotas, right? So gpu quota is there. We also have, let's say spot quotas. So we often see that we are hitting the ceiling because I see that you have a lot of these promises, integrations and all that. So are you giving that kind of observability out of the box? We don't give cluster observability as of now out of the box. Angel we do give some grafana dashboard that are populated with some of the cluster metrics, but not we don't go too much into that. Like those metrics. Like we can give you graphana dashboard that will show you everything that is happening. Okay. That is what we give, like reconfigured, but not like our platform by default does not do like observability details observability, basically. Okay, I think I got the idea, like how it is working regarding pricing. How does that work. Pricing? Angel generally the way we do is we charge customers on the basis of like if they're installing on their cloud, which is generally what we have seen so far. We charge them on the basis of number of users, like their developers or whoever is basically using the platform. So generally our pricing has been like $300 per developer per month. That is what we have been doing for now. But in your case, I would love to understand this is because we are working with mostly a lot of these enterprise companies as well. So there the pricing is with a minimum of ten developers and then per developer pricing and then there is a price to this support. But for your case, you will be working or integrating it into the start, which is likely set up. So I want to understand, like we can think of maybe a blanket pricing. Also here kind of the startup is very small. They don't even have there is one data scientist right now. Yeah. And even the cloud and devote part is mostly done by the cto. He engaged me recently to do certain things in this space, but he was managing by himself. So. We need to understand that if. It is very highly priced, then I don't think he will take it. Yeah, and also user base pricing, I'm not sure whether but we don't have a lot of users right now. Total number of users in the account itself is including developer, everyone around 50. But they are not like data scientists, only one person. Okay, got it. So who are these 50 when you. Say these 50 users are who are like 50 developers? Okay, engineering team. Okay, angel, we can think of a simple blanket thing that can work for you all because for now our goal is also to kind of work with companies and understand the use case. And actually your use case that you mentioned is really good. That is one of the use cases we are trying to think of for generally any saas company would want. So if we are able to kind of get usage and provide value then the pricing will not be a concern. Let me put it this way. Okay, so that part we can work out as to your convenience. Few things that were not covered, I think. I wish it like yesterday there was also talk about fractional gpus being able to kind of support that thing that. I got it actually. So we let it one load machine at least we need to put it right and then our jobs can share that resource. Yeah, that's my understanding. The one node will be added and then each job you can specify how much if you want. So you can specify 2.30.4. Yeah, that's like Tubenitive resource manager. Exactly. Okay, so that part is clear. Anything else? That one thing that we did at least for these data centers, one thing that is there is in the platform itself, angel, there is basic logs and when it is events and all so that if there's any problem that will be good to service for debug ability, that helps quite a lot. And these logs and metrics are stored in our cluster or in the central cluster. Same cluster, your local cluster. So we don't ship anything. Actually this logs and metrics are metrics on the same cluster. Okay, so you will deploy loki. You will deploy loki is not compulsory. Okay, can you give me like list of items which will be deployed on the customers cluster? Because that is going to cost us whatever we are deploying. So I'll tell you, I'll give you a quick sense of that. That is usually not a problem, but I'll tell you so this is basically what the logs and metrics experience is. And kubernetes events we also show here. There's no event here now, but you got a sense, right? This is just from each metric, cpu, message count and all those things. So I'll quickly explain to you that part in this case, if you see so first of all, tell me angel, like which model will you prefer? Like our posted control plane and your. Data plane for compliance perspective, I think you are compliant with GDP. We don't have GDP compliance yet, but we did some vip testing with B six and all. Does that count? We have a vip report and everything. No, I think for gdpr and Sock two, we need the vendors sock to report and GDP report. That is what we have asked our existing vendors to provide. So I'm also doing sock to compliance for them. We have engaged vanta. Okay? I have seen that kind of requirement coming from the underlying platform or whoever is providing services to this client. Okay, that is required. Let's say it is in our control plane in our kubernetes cluster. So right now we don't put promises. Actually we use log analytics. I'll tell you quickly, control plane what happens and workload cluster will get workload cluster will get argo CD argo work close istio. And then if you need auto scaling, we need Prometheus. Basically, hp metrics are powered by prometheas. So prometheus is not compulsory. Only if you need auto scaling, you need from each the above one is a strong heart constraint. Everything is an optional thing. So if you want that feature, you can add that. Otherwise you don't need it. But in communities we do with the metrics auto scaling. This may request count based normal of a cpu will work cpu based auto scaling. But we can also do request based auto scaling. Okay, so request count it will not support basically hp does not support request count by default. Okay. Let me write it more properly. Request conference. But Chromeich is also like we have found it to be pretty light. I'll tell you what configuration low key is if you need long term persistence logs, if you need participants log or if you just need logging, basically log aggregation. If you don't need if you're already using azure, you can continue to use it. We have flint based approach, so we have Elastic search. We are pushing this log. So then you don't need this. You should not you basically don't have to install this thing. And you can see in the log center log portal only. So this one comes out to be around four cq Civic, four cpu, two to three gb Ram Lactizing. Now we are deploying are you deploying all the components? What are the components you are deploying? Only the ingress gateway. Only the ingress gateway sidecars we don't import basically. Okay? We don't put sidecars, only the ingress gateway goes. This part is very light. Control plane is a bit more heavy, but this is also not like super heavy. Mother yaqa. Two C, five X large Machine So that is around like eight to ten cpu laga and probably 1516 to 20 gb configuration. And this one we can make more lightweight. It can fit in like two to three cp. This is the configuration that is needed. Can you give an idea of the cost it takes roughly for a month. Maybe that will be helpful. I think we know that cost, angela, I don't know pricing. I can give you the AWS. Usually we have multiple clients, especially small startups, just two C, five x large machines, one C, five x dot large cost you around $35 a month. Around $70 is no machine. Costa $70 and then Eks cost, eks management cost is around $73 total of. 90. Front end, back end or model inference service. So that three services are running and that is like $70 a month for them. Okay, got it. But they use our hosted control printed later you can migrate the control print migration be easier because hosting cost is not a lot. $100 a month. $100 a month divided among all clients. Okay, so hosting one apartment, we can give it out for fuse. Okay, got it. Okay. I'm used to this technology and I have worked with them, so I'm aware of it, how it is working. Any specific thing that you have delayed which you want to add? Not for discuss, but in general for another customer where we want to build a platform to sell. So we want to understand from the market perspective who want to use what kind of platforms. And recently we learned from sivo. sivo started civo, which is kubernetes, sorry, kubeflow as a service. So sivo was also a londonbased data center and then they started selling this as a service. Now one of my client is also at this data center there. So he's thinking to have this out of platform where he has already got the client. And now we want to build that platform in such a way that first we try to solve one problem for one customer and then let's say Flow works, then fine. Otherwise we cannot try other popular platforms like Gray is also popular. We will provide basically this platform as a service will take one of them and this production is it. Yeah. Okay. Got it. Angel, how do you think we should do the next steps in terms of we'll love to kind of take your help and at least a customer if there's something we can do there in a shared vision that will be really. Good what your price problem is providing is a good use case for resolveai. Okay, resolveai want to solve this kind of problem like they want to do. One moment. So basically the multi model serving hot reloading, once that is available, I think it fits into their solution. And this was also one requirement that ml Flow equivalent is there, which is all the version control of models and record of each model's performance. Okay. That is one thing. Or we can then try and move ahead. What I'll do is I will have a quick call with him and give some idea he understand all these things. So it will be very okay. Yeah. Then if he wants, I can connect both of you to him and have one more round of call. Okay, it will be good call manual. Let's do it. Initially I was suggesting kubeflow because a lot of things you are doing is also in kubeflow. Like pipeline jobs, automatic inferencing also what else? Only the ml flow part is missing. Model machine is not possible I think. And auto scaling all that it takes natively from kubernetes. So istio also comes there. Captain Argo, a lot of things are there, but that was kind of scaring this personally. He was saying there's lot of infrastructure I need to put in my kubernetes cluster, which I don't know, because let's say Harbor CD, it takes at least two cpu for that harbor CD server. Your total number of nodes we are at let's say five, six nodes production. We are having like ten to twelve nodes. Usually ten to twelve cpus. And he has like running this shop for five, six years. But he has not introduced a lot of components. In their cuban cluster. They kept it very like it even they haven't installed promises, which usually all people do it right. They have very basic fluent lock forwarding to elastic service cluster elastics as a cluster they're hosting themselves that's running on a single vm. So that's like no very basic minimum stuff they have done. Even auto scaling was not there. I did it recently. We didn't add spot pool load pools and auto scaling, note pools, hpa, all these kubernetes reliability stuff. It was not there. I came here just for that. Like upgrading their systems and putting all this. We also started with initially until we realized it comes with too much things at once. And also the apis were not very intuitive. Like the api is to write that job and all deployment. They have just taken the yaml python convertible. They have made the apis. So it doesn't sound very intuitive and python. So that's why we had to do it from scratch. So that we don't introduce a lot of stuff, because that's what we are seeing. My problem. My recommendation is going with Argocidal is better. Like I've used that bloomberg previously. We did everything like to customize centralized repository and then we used to trigger to all of our cluster. It was the best approach. Like github is usually everyone is doing it, but this company is not matured in the DevOps or cloud native practitions. Okay, I guess we can topic by vasquez and then see if you think it's useful. Then we can also have the chat once. Yeah, but I got very good understanding. So even any other places if I have this need, I can recommend your product. That is good. Generally people are okay. If you explore. One has automated a lot of stuff because oneta has created monitors automatic tests for everything. Like what will happen, they will connect with your sso single sign on provider or directory computer pay one time that is basically doing all this endpoint collection for your employees. And then they have for each cloud they have cspn type of solution where they will figure out whether you are compliant or not. And they will trigger test. And until you fix that compliance, it will kind of continue to trigger it. Okay, so I can show you quickly. How much end to end kit. But at least two months. One to two months or two months with a soft to the certification pending proper certificate. The compliance, you see something like this, you can see my screen, right? They will deploy one which is basically OS query agent which will appear integrations of that. So, first thing is to basically connect all the services that you use, okay. Your scm, your cloud providers, your task management system, google workspace user, that all will come here. They have like a lot of integrations. All things you will put here. Then what they will do is they will start collecting all this. So this talk to is very it's not an exact thing that you will understand. It's an accounting terminology. The document will ask for some policy, like say my policy Kaibar could channel document hotels. You need to provide, okay, like risk analysis. Let's say vendors agreement for example, that is an automated test. So let's say public assistant. Then it will test it and they will show you how to fix it. And once you fix it, it will re trigger the test and make it compliant. But there are a lot of things which are policy driven. So, for example, GDP are EU represented. This is some HR related activity whether you have it or not. Data inventory map, no, depends on your customer, right? You have anyone in GDP requirements? That is inclusion detection system. We tried with lot of cloud based cloud. I think you are in AWS, AWS guard duty for example, you need that kind of protection. Yes, you have to add all those things. So we found that was very expensive for that. Specifically, this is your firewall. It was like in one zone you are deploying and it's charging us $1250 plus data processing charges per month. That's very expensive. There are other system, even AWS is very expensive. Like I have worked for Anuta networks. Let's say we gave them suggestion to deploy guard duty and all that. Later on they also accepted one of our solution was wazu. wazu is hids system. You can go with this also open source security platform. It's kind of a xdr simpl plus Hids. What it will do is there will be one agent on each node you are running. Like in kubernetes, you can deploy a demon set and it will then find out if any malicious activities are happening on your node. And there is elasticsearch cluster within wazoo where it will put all these things. And inside this there is a compliance queries automatically there. So for all gdpr sock to iso 27,001. So all those are readymade dashboards like here, pci, dss, hipaa. It's easy to fix it from this. Got you. And this first. They also have lexus offering. You can also do it by your own PC. I can connect with their founderis, Pro Yellow. But they say, like, cloud may help. That's where you need some person from Cloud Engineering. DevOps Engineering. Okay. But risk pro actually recommended vanta to my client. And then vanta is now onboarded. But ultimately what they are doing, they are doing this platform which is providing a systematic way to do it. I know about iso 27,000 1000 that we did. That was like, I think five to six lakh, something like that. Okay, got it. Understood. Tig angel, thank you so much. It would be good to kind of try. We'll be happy to kind of work closely up to kind of get this done. Sure. Yeah. Thank you so much. bye.