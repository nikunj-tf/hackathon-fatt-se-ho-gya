You doing? I'm doing good because finally we are able to meet. Finally we meet. It's been quite right. In three or four weeks, I guess. Or maybe more even. Probably. Yeah, probably more, I think. We've been trying to get in touch since December. I'm based out of the way, but right now I'm in India just for a week. I'll fly back on Tuesday. Got it. Where in India are you based? My home is in Lucknow, so right now I'm in Lucknow. Right, okay, nice. So working remotely for a week. Got it. Very nice. Thanks a lot for taking the time. And do you live in proper Dubai or like, areas around it? No, I live proper near my office. I family with Dubai a little bit. Yeah. My sister lives there, so I kind of bother. I've been there a few times. You must be knowing then. Charity. So I live in charity. Okay. It's near Marina JB. It's a very QST place. It's a very fancy neighborhood. It's a lot of fancy. It's just building and concrete walls. That's it. Well, good buildings and good concrete, if you want to put it down like that. Not sure, though. It's a matter of taste, I guess. Yeah. Awesome. Good. And you have been there for a long time now? Four or five years. Yeah, it's almost, I think, four and a half years I've been in Dubai. Nice. How do you like it, Dubai? Oh, it's good, it's nice. It's a little bit of a counter intuitive choice. Right. I don't see a lot of people who work in tech decide to move to Dubai. Right. So how is that experience? I'm very curious to learn. Yeah, same for me. I didn't want to go to Dubai. It just happened by accident. And I happen to accidentally find a very good team and very good company, and everything happened by accident. I actually worked for Zamato for three and a half years, and I was working in a very hyper growth stage at that time. Zomato was expanding like crazy. I learned a ton of things, and then I wanted to move out, wanted to go to actually Europe, and this company had an office in Europe, but they wanted me in Dubai for some reason, and I just went to the Bay and liked it. So it happened by accident. I didn't want it to land, but yeah, it's growing actually. I'm a techie myself, and I feel that there's a lot of people started coming to the Bay, like tech culture started to walk four years back, there was nothing. Right. So it started to come now. Nice. This is amazing. Yeah. And do you see a lot of also generally, like, you're saying that you see a lot of good talent coming in there. Yeah, exactly. Talent is very good now, especially around Kareem. Has a good talent in general. Okay, nice. Nice. And how is the salary seen? Like, is this like the Dubai tech salary, that typically higher than India. Like, if you think about India, Europe and US, like, where would it fall in that range? I think it's comparable to I mean, because there is no tax, right? So it's comparable or maybe a little bit less than US, I might say, because I have friends in US. Wow, that's amazing. Yeah. And it's close to India, so that's also one benefit that I feel living in the way then. It sounds like the best of both worlds. Right? So that's why I stick around. Right? That's why I'm not you have the luxuries of India. You can still hire people and get your chores done by people, and you earn the salaries of the US. So that's like the best of all worlds. Yeah, exactly. It's mini India. Right? So I don't miss Indian food. Exactly. The people who could come by Dubai have Indian cook, which is great. I still love chicken, actually. I can't afford this. And maybe us or somewhere. Absolutely. It's nice. Where are you based out of, by the way? Like are you in us? Right? Yeah, san Francisco. Wow, I beat the San Francisco. It's a nice city. When did you go there? I was there in 2018, I guess, 2019. So I went for a conference and then I went there to SF as well. I wanted to see my friends, so I went to a lot of cities. My conference was in New York, but then I went west coast. I went west coast. I went to Seattle as well. That's awesome. That's really cool. Yeah. Awesome. Nice. Awesome. Thanks a lot for taking the time. So, as I mentioned on the chat that I was very interested in seeing, like, learning from you about the platform that you all have built out at Kareem. We have been building this for some time now, and, like, learning from different people's experiences is something that is very important. If you want, maybe I can give you a quick introduction about myself as well and would also love to learn a little bit about your area of focus in terms of work. And after that we can dive deeper. Is that okay? Yeah, sounds good. Cool. So just a brief background. I did my undergrad at Iad Karate Poor, and then I immediately moved to the US from my masters there. I spent some time at UC Berkeley. Primarily. After my undergrad, I shifted to Barry, and I've been there since then. So, postmasters, I worked at a startup called Reflection, and we built out a lot of recommended systems for the ecommerce industry there. I got a chance to spend the first couple of years just building out these algorithms and deploying these models. But the last year and a half, I spent time building out a platform, an ML platform, because by the time we had like five or six different teams using machine learning. After my student at Reflection, I joined Facebook, where I led one of their conversational AI divisions and I use their platform, but primarily focused on building models, not so much building the platform itself. After Facebook, I started my entrepreneurial journey. I did one startup in the talent space called Entire and that got acquired by Info Age, and since then I've been working on Truefoundry. That's nice. That's a little bit about my journey so far. This space of building ML platform is motivating for me for a couple of reasons. One, the fact that I built it out at a collection, and the second I saw how 100 X better platforms can be built when I was at Facebook. Those are two, I guess, trigger points. The last one was that Entire, when we built out a lot of these models and fresh out of Facebook, I realized I did not have access to all those amazing tools that I had there and building those out was not fun. So that was the third trigger point I guess. Would love to learn about your journey. Cool. So I graduated in 2015. So I'm from LNmi Jetpo, it's a college, it's a semi pirate college in India. Then I basically shifted to Bangladesh, stayed there for ten months and then I joined Zomato. It was kind of like career defining company for me in some sense because I was working straight with DJ Di, Pinder, Gunjan and all that. So I was in the mix of the code team, so I was able to do a lot in those three years. In fact, I wrote one of the first quotes for online ordering in Zomato. So yeah, it was a steep learning curve for me and then decided to move out, try something new and then Kareem happened and Kareem basically not changed. Basically I worked in a lot of different teams, but it was all data always. So initially, I worked with machine learning platform. So before coming here, I never knew that there was something called as Machine Learning Platform. But we had it quite a while. In 2017 we started off with this machine learning platform and that time I was looked into this serving stuff and training stuff. Then I actually built for two years I was building experimentation platform. So now we run around 400 experiments or something at a given point of time. So that was quite a learning experience. And then I moved back to Experimentation platform and right now I'm building an experimentation platform. So basically because we thought we got to know that because the one we had before was actually quite old and we wanted to revamp it. And then we started working and building a new platform, which we did. And now, yeah, we are starting a lot of the journey of model deployment, from continuing to deployment has come down quite a lot because of this platform, and we have a pretty fancy UI to do all that. So we have a few UI engineers. I work as a back end engineer, but we have a good UI engineers, back end engineers and data scientists working in Cohesion to actually build this thing. So it's quite nice. That's amazing. So, I do have a bunch of follow up questions actually here, just a couple of things about the tech stack. So, do you all use a specific cloud? Yeah, how we build a platform is basically cloud. First of all, it's cloud agnostic. Everything is on Kubernetes, so everything and if I say everything, it's everything. So even like today, if you say to me like, we want to move to maybe KSA or something, because that's a plan to move to KSA, some of some of the places. So we can actually with the clicker button, we can actually get all that infrastructure. So we are not tied down to any basically cloud provider because it's all Kubernetes. Kubernetes could be anywhere. And tech stack, the primary, everything is written Go and Python is just for basically where Python is needed. Performance stuff is everything in Go serving is super fast, is actually in Go. And we use open source serving frameworks as well, TensorFlow serving. And now we're using ML server as well, right? And yeah, the language is only to like Python and ColaNG and for react we use for front end. Understood. So when you say that everything is built on top of Kubernetes, do you actually end up building models across different clouds? Or while the platform is cloud agnostic, you end up using one cloud primary, like EKS, GKE, whatever. Yeah, so we right now use EKS. So we are on AWS, we use EKS, but like for example, deployment happens using YAMLs, right? I mean, we not use Yammer. We have wrote our own cr things like it's called custom resource definitions. So we have our own controllers, so we have our own Kind. So for kind example, machine learning serving, and that does the serving for us. We have all kinds training, right? So we have our own operators that we have written and that does all the stuff for us. Okay, understood. Awesome. So basically, primarily it's on AWS and you use the managed Kubernetes here. What do you use for your service mesh? Do you use SD or something? We use Linkerd. Okay, understood. And ingress isn't Cong, so we have Cong for ingress and then we have service mesh. We just choose Linkerd and deploy spots here. Understood? Got it. Okay, then the other question is about the number of people in the platform team, right? So you mentioned that people from back end data science and UI backgrounds work on this, right. So what's the size of the team that's building this? Honestly, like I said, we are a bunch of I think I don't give it to keep track because we keep shuffling around. So I think eight to nine people, right? And then what we have is like a bunch of projects coming from data. So it's machine learning platform, it's experimentation platform, which is actually a big component as well. We have feature store as well. We have injection platform as well, which takes like a million requests, something like per minute. So it's a high throughput system as well. So we have Pitoria of systems and these systems are maintained by a group of backend engineers. The team is expected to actually kind of know each of the systems. So right now for example, like for building machine learning platform, it was two or three people who actually bid everything up. So it was me and a few other people and we got some input from data scientists. Didn't do a lot of work because we ingenious have done it, but they use it for the inputs of how the expedia should look like for them. But yeah, in generally people things move around. So if we need some UI work so we can actually get it from some other place like this team as I was talking about, so that's how it works. All right, so you're saying two to three people build out the platform, then what is the eight to nine people in the team that you mentioned? So we have different parts, like as I said, experimentation platform, we have features to as well. So I was actually talking about all combined, to be honest. Basically all of these components combined are eight to nine people. Have been working on this for about three years now. Actually like three or four people have joined us like last year, but most of it we build with five people. Yes, so it was a lot of programming we did. Amazing. This is very amazing to hear that five to six people build this entire functionality out. I'm very curious to do a deep dive into it, but this already sounds really good. Awesome. Then the next part that I wanted to understand was like you mentioned about the kind of different components of the platform feature stores. So are you using like Feast or something that you're building on top of or it's like custom, really custom built? No, we did it custom build because as I said, we wanted to have control and Feast is actually on Java and stuff like that. The thing is the velocity increases if the team is very accustomed to a language and team knows the things quite well what they're working on. So that's what I observed at my past company as well in Zomato. So I intentionally made sure that because building something, if you know something, building something is faster than actually maintaining and leveraging something in my opinion. So we built a pretty basic feature store. Feature store is like for us it's like serving and we have different components to actually put data into. Feature store get data from feature store and that is pretty fast as well. Like we use gRPC and stuff and make it performant. And the thing is, we control it, right? It's not Java, something that we need to do and it's not that complex as well. Feast is a very open source platform and they have like a lot of use cases. What we have, we know what we have and that's what we build it. That makes sense. You build a completely custom so you're basically a Go expert. I love go. Yeah, actually yesterday it got at least one point. I made sure all my reports are in one point. So yeah, Go is a pretty good language and if you're not using Irish. I would highly recommend you to use Go now. Very good. Okay, so that's that. And then you talked about the experiment tracking bit of it, right? Like experiment. So again, experiment tracking means different things to different people. Like some people think ML Flow as experiment tracking. Some people are more like the entire prototyping layer. Like where do you run your jobs? And hyper parameter tuning false. And experiment tracking, what's the definition of experiment tracking to you? No. So what I was talking about experimentation is not neither of them what was talking about the experimentation of basically you have project A and B and you wanted to experiment on AB test. Yes, that's experimentation that I was talking about, we build that project as well. The other tracking we have, we use ML flow tracking for that. So we have a custom layer on top of ML flow tracking and that's how we track model versions and stuff like that. Parameters have a parameter to be tuning as well. I see. Okay, got it. Okay, understood. So use ML Flow like you mentioned, you mentioned like building out a custom layer. What did you have to change in ML Flow to suit your internal needs? No, nothing much. So as I said, this platform, honestly, the next version right now, not even we are serving any request with that platform. We just build it off. And the thing is, we are using ML Flow. So for tracking we are using ML Flow and the custom is nothing but just some few wrappers on top of it so that we are not bound to any libraries as such. Oh, I see. This is one of the things that we did we did want to do. ML Flow is all abstracted to data centers. They don't even know that we use ML Flow, it's just that we know we use ML Flow and it's all abstracted out. Got it. That makes sense. That makes sense. Now, what's the interface of other data scientists who are building out these models to deploy it on Kubernetes? Have you built out any further interfaces on top of that? Do they do it using a UI? Using a Python SDK that you have built out? What's that? We have Python SDK as well. We have a feature called auto deployment. We have pipelines. We use Argo short. So we use Argo for that. It's a gain basically from CNCF. Argo, it's one of the frameworks. So we use Argo, and then our Gmail is basically for pipelines. And then we have deployments. Deployments. It's from the SDK as well. So from SDK you can deploy stuff, but also we have it from the UI, so people can the model is changed with the click of a button. They can say, okay, so I want to deploy this model for X cities or FX fleet of cars or for Y number of users, so they can define which where you want to go out, and they can kind of click of a button. So that's how we build it. And as I said, we use Kubernetes. And Kubernetes is abstracted, and they don't even know we use Kubernetes, so they know that they need to click on a UI to do this. But we use Kubernetes. Lovely. Awesome. This is about the deployments here where you use both Python and this thing. Does this work for both, like, for example, model training and model? Like, I'm assuming you have a lot of real time use cases as well. Like real time models as well. So with real time models, you mean right. So basically the data keeps coming in and we feed the data to the model in real time. Or maybe we retain the model frequently. Into the I meant like synchronous API calls to a model as a service so you can't batch predict the model, save it in a database and serve. I think most of it is this is something that I want to improve on, but most of it is actually real time serving. That's how it is. Most of it we built it in such a way that we have kind of an ingress that we built. So if there's an ingress on top of our all the ML model ecosystem, we deploy it on Kubernetes. So it's kind of like thousands of spots running on Kubernetes, but then we have an ingress layer which actually forwards the traffic to these spots. This is a real time that you're talking about. And this is here we do all the kind of enrichment features too, and stuff like that. Got it. And do you also end up dealing with GPUs for training or inferencing? Yeah, so we have few notes. Few notes which actually run GPU. So you could actually leverage GPU to train your models. Not much of a use case right now. I'm not sure if data scientists are not really familiar or not, but we have it on the platform, but people don't use it. Not that much. I see. Understood. How would a data scientist who's trying to deploy, let's say, a model as an API endpoint configure the resource requirements? Some models may be more resource intensive, some may not be. You may need auto scaling enabled in some cases. Some cases like traffic pattern is different, basically, how is that infrastructure overall managed? That's one question. And the second question is how is the permission control managed? Like who has access to what type of machines, what type of data sets, et cetera. These two things I would love to understand. Yeah, we managed it quite beautifully. Regarding serving, basically when data center is deployed, we have the configuration they can put in, but the default we have put in such a way that it applies to most of the cases. And we actually push them not to change that, let's say, because our scaling policies are almost common across all our projects, right? So we know that, okay, like for given CPU and given memory how we want to scale up. So data scientists, they don't usually understand these terms, so we try to educate them, but the baselines are basically divorced such that most of the cases are actually dealt with. And then we have like in Kubernetes, we have horizontal port scalar. Right. So when we create a resource data serving, we actually create a horizontal port scaler as well. And people can change that, but most of the time we actually advise them to just stick with the defaults. And the defaults we make sure that defaults are very good and things are scaling off and scaling in. I see. And by the way, you mentioned Kfser that you are using for your model serving. Do you also end up using Kubeflow pipelines? Any chance. For basically training? We use Kubeflow pipelines. So what we do is people like their Python job and the queue flow is abstracted out. So we compile it into Kubeflow pipelines and run it on. So Argo supports QFlow pipelines and that's how we run it training jobs. I see. Okay. Interesting. This is design. Have you buy it? Do you all also consider something like a Metaflow? I actually was reading Metaflow when I was building this up. I was kind of find it good. But yeah, I mean we were actually in quite rush to actually build this up, so we didn't get time to prototype that much. ML flow was something that I actually worked with, so I knew how the APIs looks like. But Metaflow I know of. I read a book from the founder. I finished the book, but I found ML flow quite intuitive, to be very honest. Understood. Okay, got it. And what do you do for your like the permission control bit, right? Or is it everything is completely open to everyone? No, it should not be. So we have authorization and authentication on our back end in the UI. As I said, I talked about UI and the back end. Basically people, data scientists go to a UI and they have authentication. First of all, they can't just go log in. So we have some kind of OIDC, I would say Open ID connect. So we use some Open ID connect and then people can log in, right? Then they log in that's authentication and then when they log in, we gave them a guest role. So we create the guest role for them, and then we can elevate their permissions based on, like, if they say that, okay, I'm part of this project. We assigned to this project. And fewer people there are few people that actually do this, like Ops people. We don't do this, but we give them a framework to actually do that. So data set also, if you are the owner, you could only see the data sets. For example, if you are the owner, you can only deploy it, not someone else. There's authentication authorization already pre built in the system. Understood, I see. How about the entire area of the model monitoring? Have you all already built out something there? Are you currently building out? So, to be very honest, we haven't done much on this and because as I said, we just started off with this new platform, we have a lot of bunch of ideas. We are trying to reduce paper and new things that are coming into the foray and we'll see but right now we don't have anything on the modern monitoring. We have something like our monitoring of the infrastructure is very fine, gmail, so we could tell you, okay, where your latency is our way off and it's very controllable because we control a lot of things but modern monitoring and such, it's something we haven't done. But we would want to do that after the adoptability of the newer platform. But in the older platform we had something like something like trip detection as well on the older platform. But yeah, in the new year we don't have it right now. I see. And you plan to build that entirely in house? Yeah, everything we build it in house and this also we plan to do it in house as well. Got it. I'm curious what's the rationale behind, let's say, building out the entire drug detection system in house, but let's say adopting an external vendor like Arise or Fiddler or True Error. There's a few people that solve this problem, right? Or even open source solutions like evidently or While apps or whatever. Yeah. First of all, I should not say we would see. So when we will do that, at that point we'll actually make a call even if you want to do by own or maybe we use something else from somewhere else, right? Because the thing is it's a very small not a small problem. It's a problem, it's a big problem. But it's still like on bio system of machine learning, it's still a small problem. So we would see that how much we want to invest into actually get this done and we need to make a call. I'm still not very sure like how we would go about making that call. But for now I feel like we could do it on our own. We will see, but not sure. Got it. Can you give me an estimate of number of data scientists or ML developers generally using the platform? Yeah, they are in hundreds. That's what I can say. Wow. Basically less than ten of you is. Building out of yeah, I mean, less than 100, actually. Less than 100? Yeah. So it's actually data scientists and ingenious both. Right. So our platform, the Shelani platform, is just not for data scientists. So ingenious could work as well on the work in there as well. So we have a lot of utilities that people could leverage. For example, you must be knowing of Jupiter notebook on the cloud sites or built in the UI itself. So people could be the click of button. They could start a notebook. Right. So that's also one of the things that we have. Then we have like, the streaming application streamlit. I'm not sure if you know of streamlining application as well. People could create, just write a job and get a good UI, and that's also one of a click of a button. So that's also good. I think our team is quite resourceful. I must say it's, sir, your team is quite resourceful. I think they're good. It does sound like it, to be honest. Of course, as you know that in this business, I get a chance to talk to a lot of people who are building our demo platform. But really, do I get such extensive back end and UI and different types of features that you're talking about coming all together? So very good to hear this. What about the scale, roughly, if you think about Kubernetes cluster, do you manage one very large Kubernetes cluster, or do you have many different Kubernetes clusters that you end up managing typically, like the pods and RPS that you're dealing with? QPS that you're dealing with would be great to get that sense as well. Okay, yeah. First of all, Kubernetes is something that we don't manage. We have an infrastructure that manages the Coventry operations point of view. They all do all that. So they do EKS upgrades and they do maintainability of the platform. What they do give us is basically the control of the cluster itself. So we have the rules to actually play around with the clusters and production. We don't have a lot of permissions to actually play around, so we don't have so they give EKS cluster as a service to us, and we build the platform on top of it. And that's how it should be. Like, infrastructure should be concerned with all this, and we should be just building projects on top of it. That makes sense. Okay. Got it. If you had to think about some of the core problems that you're intending to solve now that is not already solved, what would that be? To be very honest, right now, my biggest problem is to make sure the adoptibility of the. New platform and make sure the transitions so that's my inner basically problem that we have not problem but the things that I'm working on right now. But I haven't thought of the next problem because I want to make sure the company is solved for the current problems and we are very very smooth in AI transcription and we have what we need and then we think about all these add ons that we are thinking like trip detection and what else we have. Maybe we could define hyper parameter tuning as well. So you think of using Gray for that like three clusters and I'm not sure if you know so we are thinking of using gray as well to do this hyper parameter tuning as well. So there are a lot of fancy ideas, like a lot of good ideas we have but we haven't thought of exactly specific what we want to do right now. The current problem, the current thing that we are looking forward is adoptibility of the platform. Yeah. And that's what we are thinking of. Understood. Okay, this has been very helpful. One of the things that I would love to do with you is, I think, by the way, a couple of disclaimers that the stack that we used to build out our platform is practically the same as the stack that you just described to me. Kubernetes Argo CD and then build out this entire streamlining integration, jupiter notebook integration. One click deploy. Like all those things that you mentioned. We also intend to we are also building out similar stuff. So I think we can it would be great to show you what we are building and get some feedback. That's one thing that I'm really interested in doing. But what I can do is in fact invite my co founder and CTO Bisha in one of the calls where we give you a demo of what we are building and just get some insights from you based on your experience of building out what works, what does not work. That can be a really good value add that we can get from this conversation. If that's something that you're open to. It should take like the demo itself roughly takes like 20 to 25 minutes and then we can do a feedback in 25 minutes. If we block a 45 minutes call, I think we should be able to cover most of it. Yeah, it's fine. I can learn as well. I would love to see you guys, what you guys are building as well. Yeah, so I think what we can do is if we can set up do you prefer weekdays, weekends, what works best for you? Yeah, generally Fridays are a little Friday. We can do something similar like next Friday, similar time or would you be traveling that you would have already traveled by the time no, okay, yeah, I. Will be traveling on Tuesday, so that's okay. So maybe like next Friday, like 05:00 p.m. IST would that work for you? Should be I think 05:00 p.m.. 05:00 p.m. IST? 05:00. P.m., right? Yeah. Let'S do it like 04:00 p.m.. DXP. Because generally I have 535 30 IST. Yeah, it's 530 IST. Okay, understood. I can send out an invite to you that time and I will invite Abby Sheikh to join the call as well. So that would be a great next step to take up. I'll send out an invite for 45 minutes for now. Yeah, I actually didn't get asked any questions to you exactly. Sorry. So by the way, I do have some time if you want to also have time to go over if you do as well. No, it's fine. We can do it later as well. I wanted to just know about what you guys like what is your vision of Truefoundry and sorry, I didn't get time to read a lot about I. Can give you a quick overview now itself and then we can do a deeper dive in the next call. Like we can do more technical understanding in the next call. Let me just give you a little bit of our vision of what we're building. So True Foundry is I already explained to the context where we are coming from, why we decided to build out what we are building because some of the past experiences and where we are right after this, we spent roughly three to four months just talking to a lot of companies, understanding their ML operationalization problems and stuff. Right. And we realized that the platform, the kind of platform that you're building right. There's only probably like a dozen companies in the world that can afford to build a platform like what you are building basically. Okay, doesn't might be an exaggeration maybe like a couple of dozen companies, but it's in that order. I thought everyone is doing it. Everyone is doing it, but nobody is. Able to do it. Okay, you're right that everybody is trying to do it. Nobody is able to build it. Like, we have talked to a lot of companies and the problems that people are facing are very basic. The kind of problems that you mentioned that you are solving are way far out in their roadmap, basically. Oh wow. Pretty much what you have built out literally puts you in the league of whatever like the Airbnb, Uber, Facebook of the world in terms of the ML platform that you're building out. Right. There's only a very few companies in the world that build this out. Now while people are not able to build this out, everyone needs this to be actually able to adopt MLS. Otherwise the state of the world is that people build out machine learning models. They are not able to deploy it, they're not able to maintain it. They have to roll back and fall back to rule based systems. That's what keeps happening with most machine learning today. Wow. Now one approach that people can take to solve this problem is, hey, here's an ML platform that we have built out now. It solves all your problems. Don't worry about building out something in house. Go use this platform and you're good to go. That's one approach to solving this problem. After we talk to these couple hundred companies, we realize that this approach does not work. It's incredibly hard to serve the long tail use case of machine learning sitting outside as an external vendor. Like, for example, you mentioned that, hey, I know what kind of features I need, what kind of feature stores I need. I will just go optimize for that instead of building out a full fledged feature store. Right. That makes sense. That's a smart choice to make, and we cannot make that choice for you as an external vendor. So that was one realization that we had. The second realization that we had was more business oriented, that let's say if I, as Nikkon, come to you as and tell you that, hey, what you're building is not needed. Just go use my platform. You're like, dude, what am I going to do next? Right? Basically, the point is that people need to have their own jobs. And you can't go and tell them that, hey, my platform already solves all your problems. So that was the second realization that we had when we spoke to a lot of people, right. And given those two things a technical reason and a personality reason, we realized that building out a platform for the platform team makes a lot more sense. So now let's say, imagine someone like your team, instead of trying to build every single thing from scratch, like making deployment of TensorFlow models and Pi dot models simple and a unified API for the developers, right? Building those things out from scratch. Instead of doing that, you build out on top of what we have built out and put a thin layer that optimizes for your own use cases on top of it. Okay, yeah, we are doing that. We are doing that, by the way. Like, we have PyTorch and TensorFlow serving animal server, and we have a single API. After I heard what you're building, I'm confident that you're already doing that. So I completely get it. And the point is that not everyone needs to do that because essentially what you will end up doing is probably going to be very similar. It's going to look very similar to what we are doing. Right. And the same thing for the remaining other 100 companies in the world, right. So that layer, I think there is less scope for innovation. There is a lot more scope for innovation about how do you optimize your own internal pipelines. I guess that's the pitch that we have, essentially. So that's one part of it. And obviously the things get more complicated when you start dealing with different types of data sets, like how do you do model monitoring on vision data sets versus how do you deploy large GPT type of models versus how do you do like data processing for a tabular data set? Right. So this complications keep coming with different types of ML problems and that's why we want to build out a base layer on top of which people can build out. That's our positioning in the market and we are completely going cross cloud. So that means that if people are dealing with deploying and building models on different clouds they can manage all of that from one UI that we are building as opposed to actually using whatever sage makers and Vertex AIS of the world. So that's where we are. We are already working with a few enterprises so like in India we are working with Reliance. In the US we are working with Synopsis. There are a couple of very large pharma companies that we are working with so that's our target segment. Large enterprises and a smaller team. We are 15 member team right now including all our business and tech folks et cetera. How big is it? Tech? Sorry I'm cutting you there. Yeah we have three full time people on the business side and we are three co founders. So remaining everyone is on tech. Yeah. Nice. And it's all self paced or you have somewhere else? No, we have a large Bangalore presence. In fact our center of like basically our primary team is in Bangalore. Nice. Good to hear that. That's where we are. We are funded by Sequoia. We have had some large basically pretty much builders of ML platforms at Google, Facebook, Snowplay, Uber, et cetera all advisors to us. So that helps as well in taking good design decisions. So that's also nice. Basically they decided to invest in our company as Angels and some of them also took up advisory roles so that's generally nice. That's very nice. Good to hear that. Yeah, I'm looking forward to seeing your platform. Yeah, for sure. I'm also looking forward to catching up with you and let's see how things go from that call. Yeah, sure. Are you guys also use Go or is it some of the language? We are starting to use Go. We are starting to use Go. The platform is not coming. So far we have focused primarily on Python. Realistically you can be building out applications in Go and deploying on our platform. Right? Because we operate of course on top of docker and stuff but a lot of user facing API is like the developer facing SDK that we have built out is in Python because ML developer. That needs to be we also have the same we have in Python that needs to be. So yeah, that's where we are man. Good talking to you Nickish. Thanks a lot for taking the timeline. Thank you. Of course. I really enjoyed the conversation and looking forward to our chat next week. Yeah, thanks a lot. Same here. Bye. Bye. Have a good week. Bye.