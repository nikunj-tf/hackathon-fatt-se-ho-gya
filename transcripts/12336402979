Hello. Hello. Okay. Hear me. Highly Lida. Yes, I can hear you. Can hear me, okay? I can hear you fine. Nice awesome. Thanks a lot for for taking the time again. Good to connect again with you. How have you been all this while Oh yeah, I'm doing well and that's been quite a while, right? It's been, it's been a very long Yeah. time. Yeah. Yeah. Where are you? How'd everything. Things are good. Things are good. Lida I am spending some time in India, now. Just like, you know, visiting visiting India. They gonna spending time with my team, so that's very good. Last time I had a discussion with you. I think I learned a lot through some of your experience at Kota, Sorry at Pinterest at that point. I think you were at at Pinterest and I learned a lot from that. That from that experience. And now now we are starting to take up a different hypothesis. I thought that Laguna will reach out to you again and learn learn a little bit more in that in that domain. But Sure. Besides that where things have been well with you. Where, where are you based lately? I'm serious in Seattle. Okay, nice. Nice nice. And you joined I noticed that you joined Kora. Kora was actually it's been a while that you joined code actually. Okay. They have been there for a while and let's see. Yeah. Nice awesome. So, and by the way, we have Chinmay on the call Chinmay. If you want to briefly introduce yourself, You know hi nice to meet you. So lida this is Chinmay. I work in the Founders office at True Foundry. I joined the team about six months back and mostly looking at product sales and customer development before this. I was working with McKinsey and company as a management consultant. Nice to meet you. Hi, nice to meet you. Nice to meet you Chinmay. Well. like, Lida like basically a chance to read the document that I sent out. Yeah. Nice. So be the way of a single run This call. Today is I have a few specific questions that I wanted to ask. Maybe like you know, say your experience at Kora and then also Mmm. they're going to be an open up the floor and brainstorm a bit more open-ended if that's okay. That sounds good. Sounds good. So about quora itself, like, what kind of cloud setup? Do you all have? You have, like, one cloud, different, multiple clouds? What's the set of the today? I think we're mostly almost all on AWS. There is a very small footprint on GCP. Understood. Do you know which part of the platform is on GCP? And why? There's actually two part. One is third is one particular model. We Are training on tpu. So we're using TCP. And also we using some of the Google ML API for some imaging and text processing. I see. But it so, If that is the case, then  on AWS, do you all end up using something like a basically? Like Are you in Sagemaker or you know, ML infrastructure running on top of like, Kubernetes and stuff. We have our in-house platform build on Kubernetes, for all the MLS we don't use Sagemaker. We don't use data break. Okay, got it. Got it. Good. And like do you use qflow at all? No. I see. So this is like completely built from scratch where like you know, dockerization and like containerization sorry, drization deployment on Kubernetes management. All of that, from scratch. Yeah, it has been beautiful a number of years. I think probably since three to four years has been that it's keep we evolving during this process. But right now, I think we in general. I mean, I will cover many many areas, but generally using like the computer or the oxygen part is not our biggest pain point at the moment. What is not your biggest pain point? Orchestration resource management is not the biggest pain point for right Understood. now. But it got it, got it, I see, and like here, when we talk about building on top of Kubernetes, like, you know, primarily on AWS and on AWS do end up using eks. Yeah, we are using cast. Got it, okay. And on G Google like GCP. Do you end up using a gke at all? Or like how is the training load running? Be our using gke. Understood. Okay. So In that case. Like Have you like when you first built out the platform on top of Kubernetes? Did you already use like terraform and stuff? Like Did you already build it in a multi-cloud manner or these gke scripts are more ad hoc Mmm. We're using tarot form but we don't use terraform for Google because that's only one cluster it hasn't been. When it has been built one, it was built it would kind of being a rush the manner and We rarely change it. So there is almost minimum operation overhead on that cluster. To certain degree. I think we got a vision, we're going to get rid of that. Workload. Actually, you want to move out of? Gke in general, but we just never got a time to do that and since it's running so we just keep it running. But so developer was used for your aka eks, but like for GK, is more ad-hoc, ad-hoc training script or something that you might have written, basically, and you want to move out of that. Yeah, we want to move out that, and we think the partially because we the model is train. You probably going to change the model architecture. Later. And we don't think we need tpu or it's not a good fit. And anyway, I see, okay. Literature. And besides like the use of tpu, do you see any other merit in using like GCP or like that? That was the core reason. So you're moving out. CPU is the only reason where you didn't have Google Cluster. There is no strong motivation to use. GCP anyway. Understood. But it, okay. And the platform that you mentioned that you have build on which runs on Eks, were you able to use the same components when you use the gke cluster? Or you're to like, redesign Some of the components are, was it like Fairly easy to do that. I think. There is not a lot of work to because communities abstract, the most of the part. The only thing you share is actually the The part that you you get the credential to talk to the Kubernetes API and you probably build some two, and a lot, maybe kind of lying our library, which retrieve the credential which you can switch from. Eks, cluster out, you can cluster. I think that's the only shared component that we have been building. So this makes our obstetrician system, call to the Kubernetes. Essentially has the same interface, but rather than that, I don't think we really share anything between gke. And yes, we we use Terraform to manage our eks cluster but we I think our Google classes just to have nothing to mandate because we never change it. Temperature. Yeah, got it. Yeah, so you mentioned right on Okay no. they're going good. So, just one, one thing that I wanted to ask is with respect for you, also brought the separate a little bit, right? Like our back roll-based Access control secret management. So on top of Kubernetes like Did you end up integrating with like something like a Hashicap World AWS parameters store for that? And also how do you manage the access control on different machines if that's a problem at all? So basically, like how what is your setup for secret management and our back on top of your ML workloads, basically. You mean you mean for the Coming for talking that there are many part of these things. So so in general speaking, the The two kind of one in the user to you there to service authentication. The second part is that part basically who can't talk to talk to this, who can't get the key, the secret, the secret kindly, you don't really You get that too part. Of course, you've got the public a CA which, which he was essentially started the crystal which we downloaded. So, so, we got the CA certificate, but there are other part, like, who, how you can get that key? The actual private key that mainly we are using essentially diem authentication. some part of our integrate using the company's integration but some are still in the In the Amazon. Wow, what was a key star or something like that? Let me just throw something here. That Yeah, that's how you get the key, all the token essentially, but then there are a lot of things service to service authentication certificate base. So so, part of it up, basically, the we have in-house mesh system with an image or TLS, so Mindy, the the basically, we're using these two authorization policy to configure, which service can talk to which service But it, okay. So you also use a mtls for for this thing here. It's a recently building out this integration with you. Yeah. Yeah. I mean, high level mtis for service It's great. Yeah. to service talk and I, I am authentication for user. For you. Talk to talk to service. Yeah. Make sense. Make sense. And given that you build out this platform, like, you know, you're decided to build out this platform completely from scratch. I guess, when you started, the other platforms were also not very popular like the, like, the now, well-done level of platforms. So, if you're a think about, where was the biggest complexity, Of building out such platform was it in the intra management layer, was it in the developer, interface layer was it in the Security and access layer. Is there like a particular part where you felt like that was the maximum pain point of building it from scratch? I think the Orchestration, part of the computer resource part is actually communities resolved, most of the problem. Your main thing in the unit to figure out first, you need oxidator. There are many of the many of the options available today. I think the part of the thing is, In the many of them probably have pretty rich function. I mean, we still run your airflow. That's you can kindly sync it as the first generation oxidation workflow, build before the Kubernetes state. But as you have enough, Python library to handle your most input output dates out for the date pipeline. It's working. It's okay. There are some new new actuation party with the per person more intended to do the To to which more companies native and also to the other part like try to be able to for example, like the, the flight and other things can start a cluster and the rest of the job. I think these things are good to have, but as I said, they are not the biggest problem. The reason is, we already have all the already abstract the most other function in the Python library, even before the Kubernetes stage so the value add-on from them and not very challenging. So, So from that perspective, we if you are similar situation that you already have this, all this kind of library, you just want to use a Python which is probably the prefer the language in the ML domain. You just want to use Python to build this kind of oxidation. So all them and in your community, with the language, it's pretty flexible, define a pipeline. I think you have pretty much fine. Okay. Yeah. Understood very helpful yet. I think I interrupted you, please go ahead. Yeah. No I think what I was trying to ask is that you mentioned that on gke, there were not a lot of share components so some of the things like using the model directory or Maybe other simple things like model deployment, etc. Did you like use any of the components from your ML of platform for this or You Like It implemented? Tomato solution for that. We don't need to implement app hot solution because we basically, you can think about our obstruction workflow is like we treat. We start this training job from the From the Amazon here, as we send the, the job to the gke cluster. And the point, then one left training job finish is just to save the model back to our S3, which we resolve some of the AWS S3, key problem. But then our model deployment system, It's still running on AWS, it's the same. So the only thing we do on the Google is just start a Kubernetes the job which you running on Tpu cluster and a training, the model and this sends the model back to S3. That's the only thing and the part of the work center the model back to S3. And other part, this is probably the I mean update all the model registry all the other information they are also this work I've done in the Oxford part not in the training job, part of trader. Part have the access to all the model Directory, API, internal API. So there this is not a problem. So we don't see a lot of needs to actually build something, share across different cluster. Understood. Very helpful to know. Chinmay. Any other questions that you had on your part? Otherwise, I'm also interested in getting some feedback from Lida.  no think that was more or less it. I think it's pretty good. I just wanted to understand so you mentioned that there is a Python library that developers use right? And and that's how things are abstracted for. The do you also provide some kind of a UI For let's a visualizing let let's say they have a heavy load which is like running on the test and form of a job. You also provide some kind of a UI for them to like visualize the logs etc. Or is it like that through AWS? The training log will write back to the, to the standard output, which will stream to the airflow job. So people can Actually, look at the log output from the airflow UI. For their job. Understood. So the UI that is exposed as part of the platform is mostly through airflows UA, right? Yeah, it's amazing through the airflow UI, but we do some work to streaming the, the part standard output to the airflow operator output. There are some work done there to merge the outcome, streams at the result, you will be able to see the from the airflow UI. You click it. Allow, you're not only see the operator lobby. See all the Hard part output log from the Kubernetes part. I'm not sure, I'm not sure and the same to understand. So when the developers are using this phyton library, let's say they have different kind of use cases is this library just for like the training part, like, you orchestrated jobs, etc or the deployment like the final deployment. Let's into a staging environment or uit environment. Is that also handle through the library, or the Devops team actually comes into play and helps them do that. It's all self-serving. So from the ML engineer perspective, the you can think about they have a Python library which they The trigger something called session, which is specify the resource requirement. And also the whatever, they also specify an entry point, which we essentially a current, which we call kernel, but it's nothing. But rather just a Python starting training script. And there are some environment variable like the ML flow integration will be set, but there's almost no operation team involved, it's all the imaginary. Basically you the right there are airflow job using these things to start and regarding the deployment. It's actually a separated system. We have all the every model has a unique ID. So During the training, they will specify, which was out of the model. There are other version information that will start any other places. But there is a separated place in the report they define which model they want to deploy, how many replicas, what's the results for the model serving? They want to. And once they added that south code, they commit, it goes through a cicd system, which essentially will run all the serving model servant deployment on a community. So it's also a self-serving system. Understood. And these are the when they specify this number of replicas, etc. This this API directly consumed by lecture production model, or this deployment is more of like a staging kind of a month. It's mostly directly go to the production. There is less things. Go to because there are many models in the experimental stage So there is another traffic control system because we draw a lot of experiment because whenever you change the model as a features you run an experiment. So The production traffic has always been splitated to different bucket, so you always redirect a smart. I mean the process is always You've got a new model then you you basically there's an experimental process. Workflow you go through, then you put some traffic compare. The two model performance in production and it's good you promote other one, show down it. The older one. This is for the model. If you change the model architecture, if you only change the, if that the model actually has never changed but just the wrong continuous training, which, you'll probably treat every day. If we run, can you training? We have no. For the good or bad, we have no canary or staging we will just directly deploy to the product. Understood understood. Well, that makes sense and let's say, the monitoring of all of these models that you need of. When you do a candidate deployment, what how the model is performing? Do you have you you still use the airflow UI for like visualizing let's say they want to know a few metrics regarding the model. How is that done by the Emily's? All the metrics are right to some. Time series database and mostly more online survey monitoring are done through the GRAFANA. Which essentially read the the Time series database. And permission make sense. For example, most of the questions I have Thank you so much Lida, I think, after you have like, given that you have already read the red thesis, I will show you a little bit of our product and get some feedback. But before that, did you have any questions or suggestions the time? Something that that like, you know, you you thought to ask us during the call about like why we are doing what we're doing. Yeah, it's a very interesting. You pick the multi-cloud path. I think that's a probably. I'm pretty sure you're probably heard this from some customer. I guess some specialty. Traditional customer, then tech large IT financial organization. Probably got this just a flying my like, background and No, regardless of Pinterest style core up. I this kind of company probably really don't have a strong desire to go to the multi-cloud even Marty region for the same cloud. The one of the biggest reason is this company a very, they'd have a ML heavy that's been there, a lot of data shifting moving around every day. And Ru training is a very date-intensive process. If you try to move in data across cloud or even a crowded region from the cloud, there is a big penalty in both the latency and also in the cost. Because for all the cloud, if you try to get the bytes data out, you need to pay actual price. This has turned out to be very significant. So I think that's the main reason in the past that we you might in my experience in these two companies. We We never really try to do that because when we look at the data, we need to shifting. When we look at the additional cost that we need to pay. It's always it's always a no Right. To go dramatica. Okay. Did this additional cost leader that you mentioned? Is this gesturing transfer? The data is it like a regulatory cost or like the storage housing cost? Like what? The data. Exactly is the cost? The data transfer and the storage. We're gonna. Yeah. And also this become most a big mall many model at daily training at daily. So you need to keep transfer all the data, keep the copy, around those places, it's very significant cost. So I think I think Lida you're right, that like, these are typically like a big, like traditional organization problems that we are trying to solve. We're still validating a few things, but I think the idea for them also is not necessarily to transfer data from one cloud to the other when they're training or optimize the compute availability, etc. I think it's the idea is that because they have multiple business units, which come through acquisitions and mergers and like a different leaders having different deals with the cloud providers etc. They tend to train their models on different clouds. Just like data is heading on one cloud model, getting trained on one, cloud and blah, and they are centrally optimize like, basically I see. one central layer so that they want to standardize how people are building and deploying models. So the platform teams job basically becomes that all the developers are able to. You see what I'm going with this that they have a consistent, we are building and deploying model so that knowledge is shared, I think that's the that's it. Mmm. Let's make sense. Yeah. Any other questions that you had on this friend, Lida I know it's just a curiosity about this, but your explanation actually address my question. Understood Okay, sounds good. So let me actually like, you know, spend a few minutes and by the way, like I know that we have like fully four minutes in the call remaining. Do you have like five seven minutes to go over? Just to give some high level thoughts. No problem. Okay. So I think I basically just wanted to focus a little bit on the on how they're trying to build out this intra management layer as well.  so the idea for us is that we have this one pane where, by the way, all everything and then Diabet form is on top of Kubernetes, okay? And the idea is that we will abstract away, not only Kubernetes but also any details related to any infrastructure, pretty much from the developer ideally that's where we want to get to and they ideally from the same script can be building. Basically like a person deploying a model on AWS will have the exact same interface as a person deploying a model on GCP or Azure, right? That's that's ideally where we want to get to, and for that, whatever Mm. integration that we will build out across cloud, we will build that out. So I'll give a couple of examples, but to begin with the way a company gets started is that if they have already a cluster running in, let's say AWS that's an eks cluster, they can just go ahead and connect that cluster when they do that, they can provide us the cloud provider. Okay. That okay, I'm running a an AWS cluster. Yeah or GCP Azure cluster, anything right? You can also like you know industry you can support a few things. Like Do you care about a particular type of machines, GPUs, some CPU intensive machines on that, cluster and blah, okay? And ideally in the same pain, you will be able to connect directly AWS GCP or azure. Okay, so now this becomes one one place where you manage all the clusters running in, whichever whichever cloud for you. now a few challenges that come is like, you know, the the secret layer, for example, how do you ensure that like you know your your secrets across different clouds are basically managed so like we are continuing to build out integration with more and more secret managers and parameters stores like Hashicop World. AWS Secret, etc. And we are build out our own thin layer of secret manager. So, what ends up happening is that as a developer, you only get to see like this one API key, okay? Now this, so this this fqn, okay, and this could be actually pointing to a secret that stored in AWA, secret store, or a hashcop world. It doesn't matter. You will end up using this. So you essentially, from your perspective at the developer, you get a unified way of handling your secrets running wherever on the clouds different clouds, actually. Okay. So like that's that's like one thing abstraction layer that we're built out. Some of the other things that we have done here. Is we have also divided a Kubernetes cluster into this abstraction called a workspace. Now a workspace is nothing but a layer on top of the Kubernetes. Namespaces. Okay. And we've tried to add a lot of other functionalities that is. For example we have added access control at the level of workspace so like you can give access to a particular developer or a particular team to a workspace. Okay. We have also added some other things like I'll just show this to you once again. I don't have permission to in this account. Yeah. So we have also added a few other things, right? Like basically like you can set up resource limits at the level of workspace that like if the dev workspace you you put less limits. If it's a staging workspace you can put more limits or certain types of teams and projects, etc. You can set that up. Similarly, you can set up like you know, different instance families that which machines become part of this workspace, the way you do the authentication on top of Workspace is by adding your AWS service accounts, this workspace. So now you know that this workspace can access this database this S3 bucket and blah blah. Right. And lastly we are also trying to build out cost which is not completely developed. But how much cost are you incurring at the level of every workspace so that you have this visibility. So those and from a developer standpoint, the only thing that they care about is essentially a workspace. Now, so long as they know that this is the workspace that I'm going to deploy things on. They don't care whether that workspace is part of a Kubernetes cluster running on AWS or GCP and anything like that. Basically, I'll just take a pause here to see if you have Questions or suggestions from an infrastructure standpoint about this setup. So in the system lyrics packed that's it's essentially deploy a dedicated to cluster for your workload. Instead of I mean do you expect it's always a dedicated cluster only for the two you know workload. There are some way customer care integrated with that existing cluster. Yeah yeah. So so right now I think our system is not very mature so we we actually ask people to have a dedicated cluster because I think some things around which version of istio, etc. We are we are a little bit more rigid but the plan is that people will be able to use the existing clusters as well. So for example here you see that some of the namespaces or workspaces are created directly from true foundry, some others are not which means that there is some sharing that is happening. So like you know this this already had some other name spaces that you're able to manage from here and then you ended up creating more more workspaces. But we are still refining our system Okay. to get there, where we can, we can share the existing cluster, but I'm curious. Like, Why do you ask, like, What do you think is what would matter or change? With this. I'm just, Curious to know because I I saw there. Many different level of the majority of the customer communities complexity, has been a, pretty much big headache for many organizations. So I think that's part of the reason. Some people would just use sagemaker or other things. If they are requirements for, the ML Right. is not very high. But the moment you go to. You have to provision your own computer resource, right? Then build things on it. Let me see you. You are so. Have some. I'm just your thoughts about the customer, actually. Ready to because once one side of the spectrum, that's a customer know, nothing. They probably just the one something available to use and they may favor, say, to make her all whatever things similar to that. Right. And on the other side of the spectrum, is the customer has a dedicated team. Maintaining, there are nice cluster you have very well viewed like all the security monitoring Accounting things on it and they the team probably don't want to other teams to start a new cluster in the, in their AWS account. Right. Right. So there are a lot of I can see in many cases that are some organization requirement simulator for the similar for the secret management that you just showed up. For one side of the company, they want to have the, you, you might create a new new secret style, Does some company, you want to unify the secret style. So, if we are products, I, I guess it's probably kind of challenging integration, but just want to know on the high level thought that. So what do you think the integration story about? It's probably channeling to cover of them, but To expect this will be. If you try to support something, you know, existing clusterial, in an existing secret style, this up, there will be some barrier for the adoption because you might have to spend a lot of time to other part. No, that makes sense. That makes sense. And I think that those are some of the barriers that we are currently facing. And that's why we right now say that okay, we will have our own separate cluster now that cluster could be managed by like you know, the company's Devops team. So like you know they can create the cluster in the same way that they typically create. But just keep things separate for their ML workloads or if they want. We can also manage that clusters like our platform itself can manage the cluster. So just like a data breaks would manage some of the question on your cloud accounts. Similarly, we can manage clusters on your cloud account. So, from an infra overhead, you don't get a lot. You essentially just grant us the access to be able to create clusters and manage clusters, and we take care of that like data breaks basically. I see. Any other things that we should be very often when we are going this loud lida. Like, I would love to understand because I think you have built out a lot of this distribut. Actually one other question is specifically that I wanted to ask for Quora, is how many people are there in the ML platform team today? Roughly. Less than 10. Less than 10 and roughly. How many ML developers are there in the company today? About three to four times the platform. People let me up. Three to four times the platform. So roughly like 40 developers or so. Yeah. Okay, understood. And do you think this is a fair ratio to understand like, for for mature platforms? Like Quora that the ratio of people who build the platform to people who consume the platform is, in the range of one is to four or so. No, I don't think that's normal ratio for people, I think. All right, very Special even quite different from the Pinterest. It has a way less platform engineer. I know have a lot of MRI engineer. So, what? Normally I I would think probably, it's really depending on how if you build everything in-house, just the utilizer Kubernetes. I think a normal ratio to me should be something like one, two, two, one plan for many year, 2 ML engineers have something like that. If you, if you using Say to make her off. That's probably a different story. Yeah. Oh interesting, so you're saying that if you're building everything in-house, it would be one is to two kind of a ratio. I mean, I mean, the almost not normal thing is a one to two, if you didn't say to make our properties reverse. It's you, you need a way less engineer. I'm just saying our current ratio. I, I feel, it's, it's a, it's probably not a normal norm. In most of the ML companies, Now, so you think, you think there should be more platform developers at QUORA, normally, or there should be less platform developers at quora. I think should be the mall if you decide to build the platform like what we do. If you adopt the Sagemaker, you can probably have this ratio. I'll have less engineer less platform it. Oh I see. Okay very very good to know. I see because Because like, if you think about it, this is practically one of the pitch that we are making to the end customer that you end up spending. Let's say a quarter of our 1/3 of your like, you know, the best engineers to build out the platform. And instead of that if you build on top of, let's say hours you, you can do it in lesser essentially. That's quite a different AI. I, I think, I think it's really busy. If you're running an Internet ads, business, your revenue and your user engagement is highly related to your ML model performance. You want to spend Right. huge amount of the resource because every one percent of the model performance, improve translate to Right. huge amount of the revenue income. I think that's true for Google Facebook Pinterest, Twitter, many of the company. So this company spend It's been a lot of results to make sure everything they do is correct, because the end result is critical. If for, for many of the companies, not in this category, I would say the result after ML will not directly, I mean it's such some kind of impact we are revenue, but it's probably indirectly to the top line of the business metric. it's What I've seen is you probably don't need to go to to that to what Internet ADS company scale and they are requirement. It will be pretty much different because this company will for example, they will push their model to be trained as fast as possible. Some company. I know they replace the model every five minutes because it won't be able to train the model with the latest data. So if you train a model, release a model in every five minutes, you need a very solid platform to do that. But for a lot of company, I know. Three months. Six months. They never reply the model. Just throw the model there. So you don't need that. Right, that's right. I have an engineer to do that kind of system. so, That makes sense. Yeah. Now that that completely makes sense. I think this is very, very helpful. Lida. Thank you so much for for sharing this perspective. Yeah. I think I think that's pretty much it from my side. Did you have any questions for me? Lida or us like January from here Chinmay? So how they all startup journey so far? What's your biggest? There are in this journey. Yeah. So we are like, you know, since the last time we spoke, the startup has been, we have focused on the enterprise segment and we are now working with two, very large enterprises to cobild the product together with them, we also acquired like five, six, four, five smaller customers to get more product feedback. And in the last three, four months we have we raised like, you know, small safe round from like industry leaders like Fortune 500 leaders. So like the onboarded CDO CEO CIOs of these Fortune 500 companies, like Transunion Bank Expedia and ETC Service now and now trying to acquire like a couple more. Large enterprise customers that may have a similar like cross Cloud use cases and all. So that's where we are. We are roughly 20 member team, out of Yeah. which 15 folks are full-time and five are part-time and interns etc. So, small beginnings. We are all having fun in the journey so far. It's good. Cool, good to hear that. Awesome, thank you so much Lida. I hope you have a good good evening and good rest of your good, rest of your night. And I'll connect with you again, as we move along in this journey, I'll keep you posted. By the way, are you to our Sure. newsletter? No, maybe you can. Can I add you there? Send me the information. Yeah, can I can. Yeah. Okay, Great, I'll add you to our newsletter. Okay. Okay, cool. Thank you. Bye-bye.