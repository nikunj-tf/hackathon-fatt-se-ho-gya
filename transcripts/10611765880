Before demo. Hey. Hi Amit. How are you? Good. Happy. Related devalue to all of you. Yeah, same to you guys. Yes. Where did you meet him? Was it in the Tekken thing? No, not tech. Trent. I think we ran into each other at one of these Scalia talks in SF. Okay, I see. Interesting. Very nice. How did you recognize him? He recognized me. I was wearing the Prodigal sleeves. Are you from Prodigal? I'm like yeah, our companies are talking. I'm from proof. Truefoundry. I'm like, oh yeah, it makes sense. Small world. Small world. Yeah. Are we waiting for anyone else to join in as well? I don't think so. This will just be the office today. Okay, got it. Perfect. Give me 1 minute. Also from our team, Chirague will be joining in. Can you quickly check if Chirag is joining? Yes. Just a quick recap of the last call and the conversation over the email. So last time I think we discussed some of the use cases at Prodigal and we were trying to understand the infrared as well. And I think Atul mentioned about some of the challenges. Amit, you also mentioned and we heard about some of the challenges from your side and also the state of the overall ML thing and some of the things that might be useful from you context to kind of take a look at. Just to summarize, based on the last time call, one of the things was basically maintenance of models and improvement over a period of time. Like once they're in production, that is something that could be interesting. How to potentially optimize for the model inference using one X right. On any other route could be interesting. Then a system that could actually have a cohesive environment to work on where you don't have to expose notebooks or for sharing things with the med, that could be useful. And then I think in Sage Maker the support on the NLP side was lower. So there I think you told that it might be something else. And then integration with things like label, studio, et cetera was things that were of interest at a high level. So last time we were not able to COVID detail of two. Truefoundry. So what we thought is we can do a deep dive into what we are building a meeting and based on that we can do a roughly live demo and from there love to understand based on your thoughts, like if there are things in that you feel could be useful and which we can potentially work with our pilot together and take it forward. Does that sound good? That sounds good. Cool. Anything else that you all want to COVID in this call? I think these should be it. Okay, cool. And just one quick check. We were trying to upgrade the production system a bit. So I just want to check like is that fine from our side and that's working or are we still okay, perfect. I'll then share my screen. I'll go ahead with just a high level overview once for 510 minutes and then I wish I can then walk through the India part and then Shirak can do the dive into the developer spot. And in between like these stoppers and keep on asking questions like Amit Akshay and Atul. Cool. I'll record this meeting. That's fine with you. Cool. So, diving into what we are building at Truefoundry, the goal is really to make it very seamless for companies to take their models from the development phase to testing and productionization. So testing means giving capabilities to the developers to actually expose models in production or in develop testing environments and be able to see as to how they will perform with respect to the models in production. And then if it was, then be able to seamlessly move them into production and do it in a very fast way without a lot of involvement with the DevOps team. And while doing that, like have things like monitoring that come b build. Now, not everything in the system is at this point built and there are things that are still in development. So we'll try to COVID that as to what is already built and what is not built. So yeah, if we look at two finally, our goal is to build a reliable ML platform so that you can deploy a model either from the simplest predict function to a complicated graph model as well. We want to make it very simple for people to learn so that even data scientists, even if they don't have knowledge of Kubernetes, it should not affect their work, so they should still be able to use the system. The whole system tries to kind of follow the best DevOps practices so that there is a single layer in terms of deployment of services and models and they don't follow two different parallel tracks that kind of increases the complexity. And finally, a monitoring part is set up by default with basic monitoring that is available out of the box. Now, in terms of the capabilities, we will kind of talk about three major parts today as well. First is how do you kind of provision the infrastructure in terms of how do you connect to your existing cluster, how do you manage the resources amongst the team and how do you kind of have a control over the amount of resources along with costs, et cetera, and even have control in times of datta and families or machines that you want to allocate to your teams? And how do you manage different environments in a very simple way. That part we'll try to COVID which is the super admin kind of view that allows you to have that control. Once you have kind of been able to connect your cluster and you have created workspaces that are located between your developer teams and you have integrated with your system the major part of the system which is the model deployment where we have tried to simplify and make it very easy for developers to deploy their models either as a batch or a real time along with functionalities for support on the Python way or the ML or the UI way. And we'll also kind of tell you about how we have added support for model servers and how you can use it to later on maybe optimize for it scalability in terms of CPU, GP or other machines and how the testing of models will come out that we'll also try to COVID then the monitoring part is something that is still being built. There is a basic monitoring in place but it currently works for structured data primarily so we'll touch a little bit upon it. But here we also have a few questions because a lot of the data that you mentioned last time was NLP unstructured kind of data. So I would love to hear as to how you are currently doing monitoring and what are the things that might be useful for you here. Any questions so far? So I have one unrelated question. So. How to find the security piece? Like if there is a PCI data, let's say we have a PCI data and we want to deploy our model and that kind of data will pass through license will pass to the model entrypoints or the server. Is the security. Being taken care of or is it in future plan? Got it. So do you want to take this? Yeah can be deployed on your cloud so everything all datta stays on your cloud itself. Okay. So some of the things we have both these mechanisms of deployment one, we kind of enable very similar deployment on your cloud like we just kind of give you a hand chart which you can easily run and install on your cloud. We also have a public cloud version but yeah, from a security perspective this is more better and then there are a few things we have done around security to enquire that the system overall is secure so data never leaves the cloud. Environment Authentication server sits in the two county cloud which authenticates the service and provides a license. That system can be put behind a VPN if you want. We have had like the security testing of our infrastructure layer as well from a third party provider and then all the storage policies like VB as well as block storage can be customized by the DevOps team according to their security requirements. So all of these things are there that we already take care of? Yeah. So this is positive. Okay, cool. So I'll not dive into individual features at the PPT level and maybe we'll rather do a live demo and we'll try to COVID things which are already there plus also things that will come pin as we talk along. So we should do you want to take it ahead in terms of the intro part. First. Share my screen. Is my screen visible? Can you see my screen? Yes. Okay, perfect. I will start from the infrastructure side like let's say you already have Kubernetes clusters and this can be across clouds like AWS, GCP or any of the clouds. So you can just integrate first all of the clusters here so you can see all the clusters in one place. That is as simple as you just go ahead enter the name of the cluster which cloud it is on the region and then you can also add your team members as admin member of viewer. So like developers if it is a production cluster, probably you want them as viewers. If it's a staging cluster you want them as members where they can go ahead and edit things. This is where you provide the interest of the cluster and basic monitoring like you can provide like Grafana located Prometheus, URLs if you're using something like Datadog and Uri, you can go ahead and use those things and then you can also decide which type of machines will run on this cluster. So maybe you whitelisting GP machines are more expensive a machine type database or GCP provides so you can whitelist which machines karen not this. And then you can just click on add cluster. The moment add cluster happens basically you can see the cluster entry here and then some of them will show as disconnected when you add the by default will show us disconnected. So the way you connect you existing cluster this is just adding it to the control plane. The way you connect is you basically install this on your cluster. We provide an agent handchart that you install in D cluster and the moment you install that cluster, install that handshart, it will start showing up as connected which means that our control plane can then start talking to your Google des cluster. And then once the cluster is added. Just before you go ahead I think last time it mentioned that they had been using Cloud Watch for the logging and of their services. So how will that work? Maybe it will be good to clarify if they want to continue using Cloud Watch. So I'll continue using Cloud Watch and the Cloud Watch console. All the logs still go to Cloud Watch. So you can continue using that part as you're using it that will remain unaffected. Basically if you want to switch to Loki or something then we can also help switch to Loki. I mean Cloud Watch is kind of very expensive but it depends on what you want to use. So one problem we have been facing recently is like trying to get metrics, like operational metrics from the containers. So is that something like miski where people can help or something like Miskie kind of comes under our Investopad or the people who are setting up the clusters. So is that something very people can help like getting metrics around GPU usage and CPU usage, all those things. Yes, those come by default actually. I mean I was going to show that. I'll show you a quick sneak peek of that. But that comes by default. I think 1 second. Let me just. Quickly show you that actually comes automatically. It doesn't need any extra things. I will go ahead in the demo. I will show you 1 second. I think there's something wrong with the metrics thing. Okay. So this is the metrics. This will come by default with every single service that you deploy. Yeah, that would be cool for GPU clusters. Does it also show GPUs? GPU we haven't integrated yet, but we do plan to integrate GPUs also here. So that will also come. Currently we show like CPU memory discount network. I think GPUs is also will integrate here. Is there also equivalent log? I think just monitor built on top of logs on this. So they're pin form of logs also. Yes, you'll see the logs also right here. Okay, so logs will also show up. So basically I showed you the clusters part. Once you add the clusters automatically, all the namespaces inside the Kubernetes cluster will start showing up here and then you can filter for the cluster that you already have. And this is kind of we call it workspace because you kind of build a little bit on the Kubernetes namespaces. So every namespace status showing up here. And then what you can do is you can allocate you can divide the workspaces, you can share the workspace with the different team members like admin editor and viewer. You can also put a size to the workspace. Like let's say you are giving a workspace to our data science team and you don't want them to like by mistake. Also it might happen that you run something and you enter a lot of cost in one night, right? So you can limit like four CPU. And eight CP ran. So there is guarantee that nobody, even if they make mistakes, they cannot exceed a certain cost. So this kind of helps a lot different workspaces among different teams. You can also mark workspaces development, staging and production. So you can also do that and then you can also further whitelist like okay, this workspace that you're giving to let's say software engineering team, they don't have access to CPU so you can further whitelist like okay, this type of machine is accessible to this team and this type of machine is not accessible to the Steam. Maybe if you karen having a workspace for training you can whitelist GPU there. But you don't need to add GPU to a workspace where you karen probably doing inference, something like that. So that allows that fine grained access control so no developers can make a mistake and screw up. The cost and service account is basically you are adding access to different systems. For example, let's say the Data Science team is accessing two s three buckets where the data is placed. So you can just add service account that gives them access to the two s three buckets, and then they cannot touch the rest of the infrastructure. So the rest of the infrastructure is secure while you're giving autonomy to the developers to move fast and execute themselves. Also, so this is kind of on. You are creating workspaces and then you're adding clusters, and then you're seeing all the workspace and managing the workspace among different teams. So this is basically on the side of the infrastructure and then the developer view status from here where basically as a developer, you can go ahead and deploy services, jobs or models. And this is the part where I think Sharag will take over. So Chiag will discover this part from here on. Any questions on the cluster and workspaces part? Where do we consider the scaling aspects. On this cluster? We usually like bootstrap it with cluster auto scaler, so it will automatically scale up and scale down the cluster. We actually are using pin. AWS, I don't know. You're using a plus, right? Native plus. We're using this carpenter thing which has very cool cluster auto scaling already built in. It's actually open source, so it works out very nicely. It's not a scaling part. Any other questions so far? Okay, sharak, do you want to take over from here? Yeah, sure, just give me a second. I'll share my screen. Okay, hopefully screen is visible. Can you see my screen? Yes. Okay, so primarily I focused around some of your use cases that were asked in the previous call. So I guess one very familiar use case for you guys is deploying any NLP model. And so I prepared two services which, let's say if you have an existing model and your developers already understand how to write a Fast API web app, then they can build a service Fast API web app and deploy it. And that will sort of generate a deployment on our platform with the service and with the entrypointtype video to use and share with other people. I'll go into the exact code of how we do this. But the idea is basically I've taken a summarization model, CNN based summarization model for this purpose of this demo, and once deployed with a Fast API wrapper, basically it gives you entrypointtype video which can be used for conference link. Let me paste some example over here. I might have something. Okay, just let me see. Okay, yeah, I wayve some example over here, I can take this and I can paste this over here. This might take a little bit of time. So there's like the summarized text and with the inference time along with this, basically you get everything, like all the metrics for this service, CPU, memory, disk usage, all the request stats, like how many requests, what is the success rate response for all of this just comes with automatically. You also get your service live logs as well as we allow you to maintain logs for quite some time. And basically you can redeploy this however many times you like as well as you can roll back using this redeploy function. Okay, so now I'll go into the code of what this looks like in code. So here I have the normal model from Hugging Face which is this bart large and I will just return a predict function summarize that data scientists can very well write on their own. And then there is Summarize batch also same variant just taking multiple text. And then there is a Fast API if the developer already understands how to write the Fast API. This is just creating an end pin infer which is using the same function summarized from before and just returning it in a Pythonic model which gets converted to JSON as we saw in the Uri, what extra code they have to write for deployment comes in at in deploy PY where basically you import our SDK and you define a service, you just basically give it a name. This Python version is optional we captured from your environment. But in case you want to provide it explicitly you can you can give good command to run. In this case, since it's a Fast API app, I'm running it via Gunicon or Yubicon and you can customize CPU resources. Again, there are defaults for this but in case you want to explicitly provide them how much CPU or how much memory you want and what kind of machine you want that also you can provide a conference link with this service defined. You can just call Deploy and you give it a Workspace. So like a visheig explained before, workspace is a collection of resources where your pods will be deployed to. So with this service definition you deploy a service and this will behind the scenes generate a docker file, build the container and roll it out as pods on the Kubernetes cluster. There are many more options we have in the documentation. This is just a minimal example over here. And there's also an option in case you don't want to deploy via Python. You can do the same thing via a YAML file which sort of looks very similar to the Python code that you have. It might look a bit taller than the Python version just because of the formatting. So with this, basically your existing code base or web service can be deployed as a service on the platform and then it can be scaled up and scaled down accordingly. Another service that I wayve prepared in this example is something which we want. To. Where do we add the login pieces? Is that something that is model specific? What kind of loggers are coming out of the box? So in this example, particular example, you can configure your own loggers in app fi like import logging and you can configure a standard out handler if you do that and you log there using that standard out handler, you'll be able to see all logs over here. So here you can see already unicorn and hugging face are outputting something your app. You can also use the same functionality for your app. Another very similar example is I've taken the same model and I've run it through onix pipeline and that just results in another service but I won't go into the lot of code for it. Basically I've used hugging says optimum to convert that Bart model into an onix model. But what we want to do further down the line with this is you can do this process yourself or we plan to build out an automatic pipeline on the platform itself that you can point your existing Bart model and that will try the optimum pipeline and give you our service directly. Right now we don't have that built out but basically it's the same concept where I'm downloading the model and I'm running it through hugging face optimum and that is just converting it to an onix model. Practically it looks and works the same, it's just using the onix and time in the background and again the deploy file also will look very very similar to that. Okay, so info so basically I'm loading it using the optimum on the front time like sequences, sequence, wrapper and because this is a converted model okay, before I go forward any questions, this is a services part of it anuragin please feel free to stop me and ask me to point out if I forwarded something. So maybe just this part. Now it would be good if you can quickly just socase the UI for this. So while this is a simpler example in the UI. Some of the other options available. Okay. If you use Python and switch. So is the optimization coming only from the. Optimum package or like there are some. Other optimization also right now it is only coming from the optimum package but what we have planned down the line is to take in the machine preference that you've given us. From that we can infer let's say what is the instruction set supported on that hardware? Does it support FP 16 intake codes, things like that? And what can your model be converted to? Let's say, can it be reasonably converted to Onix? And with those optimization and Pruning, can it be made faster? So all of that is on our plan to build to try out these various combinations and generate a report for you. That way you can make the most optimal decision which is the best format and which is the best machine to deploy to. Okay. Further few other things is you can connect give your environment variables and secrets. We also have a secret manager on the platform if you deploy Python and if you wish to let us switch to YAML down the line. You can grab the YAML spec from our UI, no need to write it by hand. Other things you can also completely deploy from the UI itself. You don't even, let's say, need to sit down and open the terminal. You can basically push your code to get a repository and you can basically connect via git over here. It will automatically detect if you have a docker file or not. And you can basically override parts of it and as well as, let's say, consider other things like ports and environment variables and which machine type, CPU, memory whatnot, and submit it. So this can be handy in case you feel like I want to quickly increase the memory and roll out immediately, and then later down the line I'll commit it. So that can be done very quickly using our UI. You also get a rough cost estimate based on the CPU and memory requirements. You enter over here. Further down the line there'll be a GPU instance also which you can take, and then your ports will be scheduled for GPU. Instances, not Spot instances. This again, you can jump in over here, but I guess we can configure infrastructure provider and possibly we can integrate with Spot, Dot, IO, something like that. That way we can use whatever nodes are coming from there. You can actually do Spot instance if you want to work that route. Okay, all right. Okay, I'll go to jobs for now. Just one thing here it will be good to COVID Like last time, Mohan mentioned that models and their Depends, sometimes they have to test it against six to seven machines using a Sage maker notebook to see, and then the numbers are published and based on that they decide how to deploy. Is there something like that that we can enable wherein you can test across machine types maybe and even do load testing or something? Right? So my next demo is exactly around that. Again, this is something we plan to automate for you down the line. But for now I've created a sort of a semi automatic kind of a demo where let's say the executive is that I have a machine learning model now. I want to try it out on different different machines and figure out what is the min max and average response times for different parameters and different inputs I throw to the model. So for that, basically what happened is I used Jobs on our platform where I'm deploying the model and I'm running it against a fixed data set and I'm scheduling those jobs on different types of machines. And what I'm doing is I'm collecting all of that data from all of these jobs and putting it into a database. In this case, I'm using the Experimenting metadata store on our platform itself. But you can feel free to connect to any database pin dump your data there. So these are the temporary jobs that have launched on different types of machines. So in the end, the last three letters are named with the machine name like C six I is the Compute Optimized Intel machine, m six is the memory optimized one t three is the burstable Compute machine. And basically I've deployed my same model on different machines as a job. And what these jobs are doing is they are dumping the data in my experimenting framework over here. So basically, let me just clear this compare, okay, so basically I've started each of those jobs is starting a run and dumping some information like okay, how many CPUs are there, how much memory, what is the memory limit configured? What is the model we are testing? In this case, I'm using the same model and then it is running some benchmark and logging some numbers. So for example, since I'm doing summarization, what is the main max and average time for trying to generate a 200 length summary with four beams and similarly other configurations. And once I have this data especially for our platform in expertise and I can sort of do a comparison plots over here that I see, okay, for this model, basically I'm getting better on M six A, better inference on M six A and I can compare for multiple settings that I've done. So, what does the code look like for this? So, I have this benchmarking folder where I have a run pin and again I'm loading the same model over here, tokenizer and model. Then I have the same summarize function that I would use later down the line I use in the web service. I am reading some benchmarking data. I'm creating a run to you can connect to any database. Earlier I'm connecting to two foundries experiment tracking platform over here. I'm logging some metadata over here. What is the CPU, what is the memory? Then I'm running some benchmarks with different settings. What is the max length I want to generate, what are the number of beams I want to use? And I'm measuring those times and I'm logging min max and average for these and that's about it. So that's like the code of my job. To deploy this, again, we use a very similar concept we use in service, but this time we create an instance of job instead of a service. So we have a job. So because we have the ability to deploy via Python, you can do all these sorts of complex orchestrations where you can basically I'm deploying six jobs at once by using two for loops over here. If I had to do it, let's say manually, one by one, this would be very cumbersome because we have the full power of Python. You can do all sorts of loops and conditions in deploying these things. So I'm creating those jobs, I'm basically giving them some extra metadata info and I'm scheduling them on different different instance types. I guess I mistakenly removed things from you machine. Type. Yes. Sorry. So basically I can give okay, what type of machine I want to deploy this job on. So I'm using these three machines that I mentioned earlier and yeah, so when I run this Python file, I'll have six jobs deployed for me in the platform. And again, all of these have the same metrics and logs available to you right now. I guess I haven't done it in a very long time now. So these are empty, but you can retrieve them on demand. And basically this will log data over here. Any questions before I say anything more? This one, we have not built the automation around it. Depending on the use case, we can actually build it like this use case hasn't really come so far. So this is more like small hikey approach of doing it via the platform. The same also be automated later on. Yeah, the project which I spoke about earlier, by taking the model file and trying it out on different hardwares with different optimizations, this will just become a part of that where you get a report of all different combinations that we can possibly try. And again, the same thing, just that Jobs don't generate any input and Jobs only consume resources when they are running. So we are incurring no cost. When the job is sort of in suspended state rest, everything will look very similar to how service looks like. And this feature, again, job can also be used to, let's say, do batch inferences. You have the model and you have the predict function. You can connect to any database to pull in data predict and push to another database. Or you can connect your S Three buckets in a very similar way. Yeah, so that's a very brief on Jobs, I'll go to the third part, which is a beta feature, and we have just started work on it. Okay, let me switch to the dev environment for this one. Okay, so basically in services and Jobs, at least when you have generated a model, you have to still do the Lego for writing the web app or the extra code to call the model and generate predictions. We have sort of come up with a model deployment feature where once you have logged a model, let's say on S Three on our experiment tracking dashboard, or from anywhere else, you can take that model and directly deploy it as a workable service. And once that is done, you can directly start using it for protections. You don't have to sit down and understand how to write fast API code or how to configure a web server and things like that directly. We do all of that work for you and you can start using it. So I have an example over here where I'm training Iris classifier on skelon. We do have support for skelon like Dbmxgboos Pyta TensorFlow. At this moment we are planning to add hugging face also. But for now I have an Iris classifier over here and this is deployed as a service. So this model directly comes from a model registry that you already have on the platform where this is a model and I have a version three of this model over here. This was logged using our experiment hacking framework. And this basically internally has a model pickle file. That's about it. To convert this into service, what I do is I come over here, I do a new model deployment, I name my service. Over here I can put a Uri, this Uri I get from this experimenting dashboard. This is the FQN over here. I take this, I put it over here. Let's say this is a version three. I can configure CPU memory and storage accordingly and I can also configure what instance I want. And then I can do submit with this will automatically take all the all the files from your model, figure out the environment using requirements etc. And convert it into a fully working service. So if I go over here, this URL will fix. But you get a Swagger UI because behind the scenes it is using ML server which generates a Swagger UI for you. And actually I already have it open over here. Okay. And then I can use V two models infer endpoint. So this does take a little bit of understanding on what kind of payload to send to this service. But we are also working on how to make this very intuitive so that you can just use a Python library and just pass in data or any data frame and you don't have to worry about, okay, what is the exact format I need to send the datta in. And so once you send this, basically you have your predictions and datta. So in this case I send two examples. I got back to classes one and zero and again, this is a fully functioning service, just like in the service tab. The difference is just that all that middle work of writing the Python code to generate a web service is just taken care of by us. You get the exact same features of metrics and logs over here. Since this is a new feature, something is wrong with metrics right now. But yeah. Any questions before I go ahead? Hey Shiranki, for one use case, let's say I have a model on a three and I also have a license wrapper. So a lot of times like we don't want to just get license and we want to serve out of service. Is there an offer where we can post, possess some kind of a output and return the result? Got it. Okay. That is also something we are planning to build. So that is something we call Transformer. Technically our back end supports it, we just haven't added the support to our UI and client SDK, Python SDK yet. But with that basically you can write a preprocess function and a PostProcess function and you can choose if those functions need to run in a separate POC or you can go the whole Python way where everything is in one class and we just don't infer anything about your framework. We just treat it as any normal Python function. And in order your preprocess is called, then your model is called and your post process is called. So we plan to build exactly that with this feature. Right now we have just built a middle label of directly model to a prediction service. Yeah. So I like to draw some kind of analogous with the ML Pro Python thing. Right? So we do have something over here, something like say I have this and basically we have a concept called Transformer Pin which will do all the pre processing and post processing for you. So very similar to this, you'll be able to write preprocess and post process and then we'll take that and deploy it alongside your model. And we can configure the same container with multiple models and multiple containers with different models in pipeline. So that is a valid request and to be honest, we haven't planned that far yet. But yes, multi model serving is also something we have come across before. Will definitely note it down. And so in some cases we have realized that multi model serving does not work well until you do a lot of optimizations by hand. So which is why we have defered that decision. But that is something that we can definitely think of. Yeah, so I was just trying to explore and sense what all features we have. It is not like immediate need for. Action also, but is definitely possible because you can see we have like this V two model and you can give a different model name over here. So you can definitely take, let's say multiple of your model Uri and pack them into one model service and then you can use them accordingly. Yeah, so we do have like this is a very new beta feature. So we do have docs on it explaining examples for cycle and sense of one Keras. This week we are working on bringing models from hugging face report directly to our service. So yeah, exciting word. Yeah. One thing like last time, also one question came about like public and private end points in terms of the security authorization for the service. Like if you can speak about that. Right, so, yeah, so far in this week, we have these three things in this week on our roadmap that for every service that you have, you'll be able to generate API tokens so that you can do access control on who can call this API, who cannot call this API. As well as if you deploy it on your pro, you can also configure private VPC URL so that you don't have to go the public internet. Second thing is we'll also allow triggering a job with different inputs. So imagine you wrote a job for batch inference and you're dropping a file into your new file every day and you want to retrieve the job with that new file. So we'll allow doing that and as I spoke, we are bringing Hugging Face more as a pipeline for this direct service conversion. These are the three main features we are working as well as we also have a few more features planned for it like onyx models and Triton server in this same just to be clear. You will support pipelines. Pipeline is the kind of wrapper over. The is that right? So initially Hugging Face has pipelines like text classification and token classification things like that. Initially we'll support directly those but we'll also see if we can directly go from a Python model without these pipelines to service and that probably will be achieved via the same approach you Karen also talking about that is a custom Python file which can do pre process load, pre process predict and post process. As per use case different things could be right? Yes, makes sense. Basically we also have plans for automatic benchmarking onyx Models Titan servers. These are like little bit far out and might take till let's say end of year but these are definitely on our roadmaps. Awesome. Two more things based on last time if you might answer this, that will be great. One question came was are we more cost effective than Sage Maker? Could you give some context there in terms of maybe for batch real time or sync. Compared to Sage Maker. Sage Maker charges like flat 2020 5% markup on top of AWS if you do like if you're just to use the machines or covenant directly like 2020 5% markup is there and on top of that usually face makers enquire to use spot instances and all if you use for instances and auto scaling in Kubernetes like further 2020 5% savings you can have people that have used like a platform versus salesmen you have seen around a 40% reduction. 40% to 50% reduction in cost. And one more question was like something that we don't have right away but I think for data set annotation you're connecting with Level Studio and how we can enable that connection where we shake. Or like right now we haven't completely planned out data set as an entity on our platform that you can make data sets and manage them but there's one way at least you can integrate with Label Studio is we have a monitoring solution for at least built out for structured data sets for NLP we honestly would like to work with design partners where we can understand what kind of metrics do people track for NLP models because monitoring is never one size fits all. So we'll definitely like to work on that in the future. But at least in a monitoring solution, what you can do is while you're running inferences you can log data using our SDK, basically raw data, which means features, as well as any custom metrics you want to calculate as well as any additional metadata you would like to do. In this case, I haven't logged anything, but any JSON serializable metadata is fine and over time you will collect data points in the monitoring DB. Now the monitoring DB itself has APIs to stream data out of it and commit actuals into the database again. So you can imagine, you can connect label studio with the data source of our monitoring solution and all of these data points will then start surfacing on labelstudio and then once you start annotating you can do a callback to our table to commit the actual true labels. That way you slowly accumulate data and then from here on you can again query this data resource and use it for your next training iteration. So that is at least one way we can think of it in case we think of integrating data set as a core and datta platform. Again, that might change a few things here in there. By the way. Good to know that you said let me see in the conference link script or the conference link standpoint we can add basically add lot of metrics and we can see those metrics in real time, is that right? Yes. And basically this monitoring solution allows you to then do histogram comparison, trend analysis, things like that. These are for features but you can also do it for custom metrics that you have logged and we also like report drifts over here and then you can also set alerts on these drifts and conditions that okay, which metric went about this and that then send me an email alert or any other alert. So multiting is still work in progress for us, which is why we only have focused on structured data set in the beginning. But yeah, definitely would like to expand the capabilities to NLP and vision models if we can. Yeah, this is cool. Any questions in the meanwhile? Like love to answer and I might have to drop off because I have another call and for that but I'll let you all continue and Richard Shen and answer the questions and we can discuss the next steps in terms of what you think in terms of a possible next step forward. No specific questions for me. I also need to drop off and kind of have you jump on another call. Okay, what's the overall demo of the LPUs gets cleared by the way? Yes, I think from the last meeting today we got a lot of detail on how this platform is and what kind of things might be useful for us. So thank you for setting this up. I think making it energy specific was helpful for us to kind of understand it. The two capability that you have currently. Okay. What would you recommend as the next steps in terms of potentially figuring out a way where you could try out the platform for a use case if it adds value to whatever you're doing currently or even if not right away from a longer term perspective of whether you want to invest internally to build something or we can potentially help in increasing the capabilities that you might be looking for. I think that definitely something that we'll look at the team and get back with you on some potential next steps. I think both are valid directions to consider. So I will definitely circle back on this. Okay. Does this from the demo like anything specifically stand out? Like, okay, this is something that sounds very interesting or as a whole the platform seems interesting or you know, generally most of the things we are already able to COVID now through whatever we are using initial comments and then we can connect back once you have an internal discussion. Yeah, I will. Let akshat and I'll answer this. So I firstly like the monitoring part. We have been struggling a bit with the monitoring setup, so I like the monitoring part. I also like the job part. So those two things like a strike a bit and. We'Re that could be okay. Anything from Yorkshire? I think the wrapper on Kubernetes and having the kind of Uri way and just zooming out. It feels that this might help us speed up some of our deployments, which is definitely a plus. So yeah, I think considering those definitely positive directions there. Okay, got it. Cool. What we'll do is we'll send out the recording of this and then maybe you can review it internally and look forward to potential next steps. Like we'd love to kind of hopefully engage in terms of use case that we can add value on first, improve the platform and we can hopefully build something that is also custom for you if need be, depending on the requirements because we also love to take this forward. So is there some kind of like trial, like trial and kind of figured or something like that? Yeah, we can try and set up something like a demo account wherein we can give you access to you and some of your team members and we can allocate like a workspace with certain limits there and you can actually play around and try to deploy models. Like, that should be possible, right? Yes, that will be possible because. We can try to set that up for the team members if you let us know for whom all you want to set that up until. At high level, this is roughly the source of comment we can try and set it up and send to you. And all the features will probably not be available because some of the things have not been pushed out to be a public version. But you will be able to at least play around with a decent number of pictures using the documentation. Okay, cool. Sounds good. Okay, awesome. Thank you so much for the time, team. I really appreciate you taking out the time for this. Thanks, everyone. Bye.