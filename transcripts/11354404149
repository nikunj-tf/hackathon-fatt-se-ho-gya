How's it going? Hey. Hi, Ansul. How are you? I'm good, doing well. Nice to meet you. On Rag. Did I pronounce your name right? You pronounced it right. Did I pronounce your name right? Ansul? Yes, exactly. Exactly as it spelled. Awesome. Ansel, where are you based? Are you in SF area? I am. I'm in the Mid Peninsula. I'm currently in India, by the way, so it's pretty late for me, but. Sorry to keep you up. Delay. Are you normally based there or are you traveling? Yeah, generally right now I'm based in India. I used to be in the US and Singapore before this, but a lot of our tech team is currently in India, so I'm currently here. One of my other co founders, Nikkunji, is in the SF area. Cool, very nice. Well, thanks for taking the time to chat. Thanks a lot. Like, my pleasure. Really grateful for you to kind of spend this time. I'll give a little bit of context and so I would love to hear from you as well what I wanted to do on the call. So I can tell you a little bit about my and my co founders backgrounds and what we are building at two foundry at a high level. I would love to dive into a little bit of how Proof uses ML, what is the current infrastructure, what are some of the challenges you have seen along the journey and if there are any pain points or challenges that you're seeing right now that you are looking to solve for. And then if time permits, can dive a little bit more into what we are building. Otherwise I can set up a follow up call to show you a high level and a detailed demo of the platform as well as to what we are building and hopefully see if there is a use case where I can work with you and take your help in the journey. Sure, yeah, I'd love to definitely find out. Okay, cool. So my background, I did my bachelor's in electrical engineering from IIT Corrupt in India and then post that I worked with a hedge fund called World Quant, which is a part of a Linear Partners based out of US. I was working as a portfolio manager and a quant there. So initially I was building trading strategies using a lot of data sources, and then later on I was a portfolio manager managing around 600 million assets for them over global markets primarily. We used to trade equities at the same time, I used to angel invest into startups like starting in 2017 and have invested in more than 22 startups, a lot of them being Indian, South Station and US startups primarily, and always wanted to build something of my own. And my friends and my current co founders Abhishek and Nikkunj, their background, they were my bachelor's at Undergrad and then Avishek, went to Facebook as a software engineer, was a senior staff software engineer when he left, led the Videos team, the in priority design team and the Cashing team there. So he brings a lot of experience on distributed systems. And Nikonj, who is my other co founder, he brings experience on the ML side. He was the lead ML engineer at Facebook and prior to that led the ML for a form called Reflection where he built the ML system grounds up and prior to that did his MLS from UC Berkeley. So all three of us, both of these were on the same boat where we wanted to build something together. We all left our jobs in 2020, moved to India, built a startup in the talent space, sold it to Infoge, which is one of the bigger tech players, and then later on started Truefoundry over the last one and a half, a little more than a year. The goal of building True Foundry is to kind of bring what we saw at Facebook in terms of the ML platform there to every company around the globe. So Facebook has an internal platform and on top of that there's an ML platform that allows ML developers to quickly kind of test out and deploy models to different environments and do that with the best engineering practices, including monitoring, auto, scaling the right authentication and the security systems and at low cost. So our goal was to kind of build something like that and provide it to every single customer company where they also have the flexibility to kind of build on top of it. But at the same time, the developers are able to increase the speed at which they kind of test out and deploy model. That's like a very short background. Love to hear if there are questions and then I'd love to hear a little bit about you and Prove. Yeah, no, definitely interesting. I mean, I think the problem you described I think is very prevalent just to kind of think through the target customer of True Boundary, or maybe I should say the target user. Are they primarily the ML engineers that you're targeting who are focusing on building models? Is that kind of your target customer, if you will, or how would you describe your target customer? Yeah, I can talk a little bit. So the target user who will use the platform, there are different users. So one is the primary user who would be the ML developers who are building the models. And then they would want to ideally test out and deploy these models either in a depth test environment or in a production environment and so on. Or while training out the models, they would want to kind of do virtual training, right? Like over machines or over shared classes, et cetera. That is the primary user. The secondary user is the intra engineer or the DevOps team or the platform teams in companies who would want to have a full view of what is going on in the resources, in the infrastructure that they have allocated. Which part of the infrastructure is allocated to which ML team? How many models or services are being deployed there, ensuring that there is the right reliability and security system. So that is the secondary user. And then obviously it's the ML engineering lead or the manager who wants to have an ROI faster for the NML models. That is like the ultimate decision because that's how I think of the user profile. Got it. Yeah. Basically it's really making it overall more efficient. Yes. Essentially what you're targeting. Right. That makes a lot of sense. Okay, so this is very concentric, right? Because everything is really about infrastructure. Deploying the models in the cloud and being able to iterate that on pretty quickly is kind of the goal here. Right, okay. Yeah, that makes a lot of sense. Just very briefly, definitely we have struggled with this and I think every group struggles with it to some extent. We have built quite a bit of stuff, kind of we used to do a lot more custom stuff, if you will. Like, we set up an ML Flow server and all sorts of stuff on tracking and things like that. We've recently tried to move towards leveraging Sage Maker a bit more to kind of take a little bit of that maintenance work off our hands. Because I think the main thing really is that a lot of the stuff that our team is building right now is, I would say more kind of early development and RMD. So we haven't put like a model in production in the cloud. And to be honest, it's not clear that we would necessarily do that because a lot of the stuff that we're building right now is actually targeted for OnDevice ML. Okay. And so that introduces a different at least from the way I see it, it introduces kind of a different dimension. Right. A lot of the things that we're testing and training is not meant to be directly deployed to the cloud and like serving customers through APIs and whatnot it's really about. Okay, we need a test framework that is essentially kind of replicating, if you will, what we are going to be doing on device and optimizing that. And some of it's, like it's not all going to be the traditional, let's say, cloud native software, right? It's like, oh, we're going to go into the world of C or whatever because it's going to be optimized. But right now we leverage a lot of Sage Maker everything's containers to kind of run in Sage Maker pipelines to kind of run a lot of this stuff. But I definitely think that the general problem is always there on how we become more efficient. I don't know if you guys ever thought about kind of an on device kind of angle to this. Yeah. So on device is an interesting thing. We have recently been talking to companies that actually deploy models on device. And the problems are very different from the normal deployment problems. Right now, we are not building or focusing on that as well. But I'd love to hear, given you kind of care about that when you think of like an ML deployment on a cloud native infrastructure versus on a device. I'm sure there are a few challenges that come in respect to size because ultimately every device will come with a limited hardware. But I'd love to hear at a high level, what are some of the other challenges that you end up seeing that will be great to kind of hear. Yeah, and I'll be honest, we are working through those right now. Right. Like, as I said, it's still very much in an R and D phase. And I think at a high level, it really is what you described. Right. Basically what we are when you talk about doing the, let's say the research and the development aspect of the model is one problem, but then it almost is like basically with that, you're trying to optimize for speed of improvements, iteration, and I think that fits more like what you're describing with what you found you optimized for. Right. It's like, oh, I want to be able to train, I want to be able to really quickly deploy model and try it out and things like that. And so we definitely have been viewing that as kind of one kind of angle of looking at it. And then at some point it becomes an iterative process of, okay, now we know algorithmically what we want to do. Then we have to take a step back and also say, okay, now if you were to try to put some device, it's like, is it even feasible? What do you do? Is it like, oh, do I use TensorFlow on the device, TensorFlow lite or whatever, right. Is that the right solution? Or is it still too big? Because then you have to bring in the TensorFlow dependencies and it's like, okay, is that the right way to do it? There are different ways to get around that. And I'll be honest, we don't have a perfect solution for any of that right now. But it is kind of an iterative process where it's like, we think about the on device part, but at the same time, you won't really know what the bottleneck is until you've kind of figured out what model or algorithms you want to run. And then you kind of have to go back and forth. And we haven't figured out for sure the right kind of optimal way of doing that. Right. Because I would imagine it's going to be related to something like you tried to find a way to essentially build the on device thing as much as you can in the cloud to be able to iterate. But at some point you're still going to have to get closer and closer and closer to like, let's say SDK, right? And so where did you get that? Do you end up using any sort of distillation techniques or for reducing the size of the model? Like, let's say you train the model and it's up to 500 MB size, and for device you need maybe a 50 MB. So do you use something at currently or you try to train models right. While at the point of training to be of an optimal size? And I'm curious to know before this also like this kind of models, what are these use cases when you have to deploy a device so that I have a sense of what the actual size of the models, et cetera, could be that you are building, actually. Right. I think the general problem statement is really how do I say the model sizes vary, to be clear, because I think it's obviously use case dependent. The way that we think about our problems is like if we're trying to, as you probably know, prove is trying to do a lot of identity authentication work, right. And a lot of it is trying to use signals like, let's say from the device to help figure out how much risk on this device there is, things related to whether or not it's still this person using the device or some behavioral biometrics related stuff. And so, depending on the specific, let's say, factors that we're looking at, the size of the model will be very different. Some of them are very small, which case it's not really a problem. Some of them are not even a model. Right. We're like logic. That's great. On the other end of the spectrum, you have stuff along the lines of we're trying to use a gate as a way to determine whether or not it's a person or not. Right. That is a much larger model. But the key thing is that a lot of it is privacy driven. Right. We don't really want a lot of this data to leave the device. And so that's why the goal is to try to do a lot of it kind of on the device. And in terms of model size, I think we are currently kind of at a point where it's like we want to build the models that we think we need to build in order to meet, let's say, a certain performance capability. Right? Yeah. So we recognize that that may lead to bigger models, but as we said, it's iterative it's like, okay, let's try to establish upper bound on how well you can do and then start iteratively going down and making the trade offs likely. We could use techniques that directly whatever quantities, the weights, still, whatever, all sorts of kind of techniques for compressing the model. That would obviously be one approach that we would take. Right. Because it's kind of not out of the box, but kind of out of the box. Use standard techniques, but also just redesigning the model to make it smaller than what we learn. So I don't think from our perspective, very practically all of that is on the table. Right. Yeah. But I think maybe what you're getting at is if there were the more tools we have that could easily try a lot of that stuff for it, the better. Right. It's like, here's a model, you can now go and compress it, which there are capabilities out there. That's kind of the thing that you would probably try first because of course okay. Right. Fair enough. I get a good sense. A few things I wanted to ask also on the infrastructure side, I was seeing you all end up using AWS, and you mentioned also that you use Sage Maker. When you're using Sage Maker, you'll not be using a case, but do you have Kubernetes in house in the company that you use like an EKS or something? Yes. Okay. So you mostly deploy all your software on EKS, I'm guessing. That's correct. Okay. Is something that is owned by the ML team. So I'm guessing the DevOps team is not getting involved and it's completely managed by the ML team. Right. So for all this R and D effort, the ML team owns it. So that's why we kind of chose to go something simpler like Sage Maker, just to be as simple as possible. Right. But that's actually part of the reason we did it, because we didn't have a lot of dedicated ops support. Like we could do it, but it wasn't kind of the best part of the team's time. And then this Sage Maker, which component of Sage Maker does the team end up using the most? So, like, Sage Maker has this hosted notebook, sage Maker notebooks. And then you have a way to kind of obviously deploy in different environments, do multi model loading and testing and so on. Which component does the team maximum use? And that is one question I had. Right, yeah, no, we definitely use the notebooks. Right. That gives the easy kind of prototyping interface. Right. We used to actually have our own Jupiter hub that we maintain and that was just a lot of work. So we definitely use that. We also use Sage maker pipelines. As we start building more of the as we start kind of solidifying some of the code, then we basically package it up and use pipelines to run and we run experiments with that regression. We use it as well. I think that is definitely one of the key things we use. Just allow us to kind of simplify the running of a lot of these things. Okay, got it. When you are using and building models, you mentioned earlier that you use the ML flow as well. Was it the experiment tracking piece of ML flow you used, or was it more the ML flow server for quick hosting and everything? Yeah, we've been trying to move away from it because it was the experiment tracking piece. Okay. We found that pretty effective. It worked pretty well. Again, it was something that we hosted ourselves and so again, it was more overhead. And what was the reason for moving or wanting to move away from it? Because from what I understand, Sage Maker also does not have a very good experiment tracking interface as such. Is it like the need is not there or you are trying out some other tool in that regard? Well, I'll be honest, I think we're still trying to figure out what is the best way to do experiment tracking. ML Flow definitely worked well to a large extent. We do definitely like to use it for a very long time. I think a lot of it was kind of the maintenance aspect and I think generally we were trying to say the more of a kind of a managed service we would move to, it would help free our time. So that's why we tried Sage Maker. We haven't yet fully figured out the best way to fully replace I see. ML Flow. Right. Because Sage Maker has some capabilities for that. It's a little bit clunky, right? Yeah. And so we're thinking about building other potentially just setting up our own databases to kind of track it and potentially optimize it for our usage. But I agree the decision to not use a malflo came with trade offs. Got it. I have a few more things to ask and I would love to take some time in this call to actually give you at least a high level picture of what you are building so that you have more context rather than just asking questions. So just one last question is how do you take care of some of the other good practices from an SRE perspective which would be important for you as well, like things like the CI CD or authentication or say secrets management ensuring that none of the secrets are written in the code? Like do these things matter at this point or is it a little too early in the journey? They do like the CI stuff for sure. We run everything through GitLab and so actually we post up Sage Maker with GitLab, we do CI and we basically just use from a regression testing perspective. For example, we basically have the CI pipelines kick off the Sage Maker pipelines to do regression. And so that's kind of how we've managed to address a lot of that stuff. Got it. Cool. One thing anchor I would like to kind of mention is for us we are still very early in the journey and really I'll be brutally honest is we are working with a few companies and our silly to work with a few other companies who could actually be early customers or design partners as we are building out this platform and releasing it for mainstream usage. And during that time we are also happy to build out custom features for people we are working with. So I'd like to take this opportunity just here, maybe a little bit about what the platform is currently doing in terms of easing out the testing process for developers. I'll just try to walk through it, but obviously I would love to set up a follow up call to do a more detailed live demo and also probably explore with you if there's any possibility of any problems or pinpoints that you are facing which we can potentially address to our platform, if that works well for you. Sure, yeah, absolutely. If you wouldn't mind, I did glance at your website and it definitely seems quite nice. Thank you so much. The website is a little outdated, I think we never revamped it well. But yeah, I think I'll give you a picture through. Cool. Are you able to see my screen? Yes, I can see it. Okay. The core thing that we are focusing on from a perspective of truefoundry is the deployment layer. And when I say deployment, it does not have to necessarily mean a production. Production, deployment, it would be simple. My connotation of deployment is anything that you have to run on a virtual machine, right? So it could be a simple training that you are running and you want to test out different models and then track their metrics and compare what is doing well. Or it could be actually a deployment on a test server so that you can expose it to someone who can test it out and say it's working fine before you finally do the deployment at the corresponding scale that you want. So it could be anything in terms of the deployment. So we have built out a deployment platform as a service over Kubernetes. We use Kubernetes as the base layer, but obviously we can obviously work without it as well if there's a need. But so far we have seen that option to be maximum, so we try to use and build it over Kubernetes and so on. The aim is to speed up developer workflows while providing them flexibility to build on top of it as well. So I'll just show you what that means and then enable doing that with full security and control for the intra team. So we believe that the intra team in itself is a very important layer. Like while most companies fill shortages of intra teams, like, they are still an important layer. And at least if you are able to enhance the visibility, you can actually make them much more effective in the overall journey. The way the platform works is it enables you to deploy very quickly, like single click deployment, either through the UI or through CLI or through the notebook, enabling it to be faster. We make it simple to learn for the ML developers, basically in terms of exposing the Python libraries that they easily understand and ensure that they don't have to depend on DevOps or intra people to kind of deploy models. So we kind of try to enable full independence for them automatically. The Platform ensures that the SRE best practices are followed and inculcated into the team's behavior. So things like auto scaling, security management, authentication, secrets management and so on automatically come with the Platform, like the CI CD and all of those stuff. And finally we try to ensure that the Platform runs on top of the same infrastructure you are using for software, so that you don't have to run parallel to two set of infrastructure for your company. So for example, in the old case, if you're using AWS and EKS for your software, we would ideally want that your ML also runs on EKS without bothering the infra team and your developers are labeled to access and use it. So that's the thought process behind building it. So if you look at the Platform, the Platform Idly consists of the following parts and I'll walk you through a short demo and you will see as to how that comes into play. By the way, I'm showing you the deployment but we also along with deployment, as you do the deployment, we have a layer of tracking out your offline experiments as well. So basically what ML Flow does, we actually use ML Flow behind the hood and we kind of augmented further as well to improve the tracking of metrics, the logging of metrics including system monitoring and so on. So you can potentially then use that part of the platform and I can dive into that a little bit more in the follow up. But primarily the workflow is as a developer. The first part is you create a secure development environment for your ML team. So this part is either managed by a DevOps or the head of ML or the engineer, like the ML lead engineer and so on. What they do is they connect to your own cloud, we make the connection easy and then you are able to provision and allocate resources to your ML team. And once you do that, basically the system ensures that in no way those resources are exceeded anytime. The system is controlling for the costs and the right access controls are there. Here you also integrate with your existing systems like GitHub or GitHub in case of yourself, whatever docker registries you are using and so on. Once you have done that, then the interface is open for the developers to keep on trying out. You can start trying out simple functions to complex models real time as well as batch models. You can use the best rate practices by default like the CI, CD authentication, auto scaling. We also use and support model server. So if you want to optimize your model in terms of size and an OnX server feeds better, you can actually do that. More customization. Here is coming, but basically at a high level the model server part is supported as well and as you deploy we make it easy to do it via the UI as well as from the CLI or the Jupyter notebook as well for developers. And once you deploy, we kind of automatically show you things like your system monitoring us to CPU memory usage and so on. And if you are using structured data, we also automatically showcase you the model performance and data drip so that you can easily debug your system. Can I ask you a question? Hello, just a quick question. I'm just thinking through like this model deployment part, right? How does your system generally manage the, let's say reproducibility? Right? Let's say I have some ML developer, he's got some model he's figured out on the notebook and he's like okay, I want to really try this thing out. How do you track that model to kind of ensure reproduce it? Yeah, so basically in the model like that two things like suppose you have an option to register the model with us as well and you can actually define your schema. So we provide the model registry component so that way every model is version like v one, V two and so on. And if you're directly deploying event from notebooks without the registry, you can actually track your versions in the deployed tab like I'll show you that and you can actually roll back to any older deployment. So when you are deploying, basically the system creates the docker image automatically and then it will deploy so that way again, it preserves the reproducibility aspect of the platform. Cool. Does that answer your question? It does, yeah. It sounds like it was the registry kind of the probably the recommended way to do it. Yes, I believe the registry is the recommended way. Yes, absolutely. Yeah, that makes sense. But by the way, you don't have to necessarily use our registry if you are already using a docker registry like AWS docker registry or any EMR registry or anything, you can actually load models directly from there and deploy and the system will still enable versioning. So it will basically tie back and ensure that that is version and you can easily roll back to an older version as well. There. Cool. Okay, do a quick touch, a couple of minutes and I do need to run at once. You said do you want to set up a follow up call? I'd be totally open to that. Yeah, would it be great? It would be good if we can do a follow up call. Yeah, I'm totally open to that. Okay, cool. What day or time will work best for you? Take a look what times work for you? Do you want me to email with you? Yeah, we can definitely coordinate our email. Yeah, we can do that. Not a problem. I wanted to quickly show you a high level glimpse of the platform. Sure, okay, I'll quickly do that. So. I'll not take more than two minutes. So this is the part of the platform until where you kind of integrate with your docker registries, GitLab, secret store and so on, and you actually integrate new clusters. Suppose you have an ETS, so you automatically kind of create that cluster. It could be in any cloud. Like in case of your AWS, you have the control, you can configure your monitoring as well. And in case of ML, you sometimes want to configure what kind of machines you want to provision or blacklist. You can also control that. So once you do that, your cluster gets connected and your DevOps infra team or the ML leads can actually create workspaces which are smaller groups for your developer teams to work on. Each workspace basically comes with resource limits and proper access control. Again like admin editor viewer. So with the resource limits you can actually configure how much GPU, GB, Rap, etc. You are allocating so no one can exceed that resource limits. So this is the first part of the infrastructure. Once that is done, the developers are ready to actually start deploying models and they can go to this deployment tab and you can deploy like preprocessing functions, services, jobs, like training jobs, inference jobs, as well as models directly. The new deployment is as simple as just you click the service or whatever, you select the workspace and then you can deploy from a source code or even from a docker image if you have a docker image. If you are deploying from GitHub, you can just do that here. GitHub is connected, but you can easily connect GitLab as well. Similarly, if you are deploying using a Python build package, just give this one. You don't have to do much, we automatically generate the docker image. And if you want to configure this, you can do that. But otherwise you just click the submit button and literally the model will deploy at your end and you will see the endpoints, et cetera. So you can deploy to a test environment if you want developers to quickly test out and you will have access to metrics logs, et cetera as well, so as to quickly debug things. You also asked about the versioning. So I'm not showing the model registry here in this recorded demo, but basically there's also a model registry which I can showcase in a more detailed demo. But here the versioning happens automatically and you can redeploy any old version here without having to go back. All the spec files and everything else you see are there. Cool. So similarly you can deploy jobs, deploy models and then there's a monitoring which I'll skip for now in the interest of time, but I can showcase it to you later as we talk further in the next call. Okay, yeah, you mentioned that the website is a little outdated. Do you have an updated slide deck or anything that I can look at offline as well. Yes, I can send you a brief slide deck and also include this video in it as well. Hopefully that will be helpful. Okay, cool. Yeah, no, that would be helpful. Just give me some. Also offline set up a follow up call in respective to kind of go over a more detailed demo and yeah, sounds good. No, definitely appreciate your time. Sorry we had to run, but look forward to talking more later. Thank you. Thanks you.