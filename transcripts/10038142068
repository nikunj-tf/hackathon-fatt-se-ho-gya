Hi, honeycomb, can you hear me okay? Yeah, I can hear you, yes. Okay, great. Is there a lot of background noise that you're having trouble following what I'm saying or you're fine. Okay, so the reason is I'm actually attending a conference. I don't know. Are you based in the Bay Area? Yeah, I am. Okay, so there's like a Ray Summit conference that's happening in San Francisco. Great. So so, yeah, try to find the quietest corner. Quietest corner, I see. Okay. Nice. By any scale, right? I think they are actually hosting this one. Okay. Any scale? Yeah. Do you guys use any scale? No, we don't, but I'm aware of them. Got it. I see. Where are you based? In the Bay Area. So we are based in Menlo Park. Okay, I see. Got it. And by the way, I know that. Cannot hear you. I think you got cut off in between I can hear you now. Yeah, I can hear you now. I was saying that I know that Abishek had reached out to you, but he is currently based in India. So the time zone difference and all, I decided to take the call on his behalf. I'm also one of the founders at Truefoundry. Nice to meet you. I'll give you a very brief background about can you hear me? Your voice is getting cut off in between I don't know. Yeah, I think something about my airports now. It should not get cut off. I completely disconnected it now, so it should be fine. Okay. Yeah. So I was saying that I'll give you a very brief background about myself and I think the purpose of the call is kind of clear from the LinkedIn message as well. But yeah, I'll set some context there. First of all, I am batchmates with Abhishek. We did our undergrad together at Adikarakur and both of us also worked together at Facebook as well. While Abhishek was leading the videos.org, I was leading one of the conversational AI efforts, building portal product. Prior to Facebook, I personally have led the machine learning team at a startup called Reflection where we built out a lot of recommended systems for the ecommerce industry. And between Facebook and Building Proof Foundry, abhishek, I myself and our third co founder on Rug, we built another startup in the HR tech space called Entire and that got acquired by Info H, which is one of the largest HR tech players in India. Right. And for this call, the primary focus for us was to figure out what kind of machine learning problems that you all are trying to solve. At one concern, what are some of the most interesting challenges that are pressing for you? And also get some feedback on what we are building and see if near term, medium term, long term, there are avenues where we can work together. That's the agent, what we call as well, would love to learn a little bit about you. Shabbat. And the first few minutes I'll probably spend some time understanding your work and the team and everything and then we can get into what we are building at True Foundry. Is that okay? Sounds good. So about myself. So I did my undergrad in IIT Heatherbad and then came here from a graduate school at Stanford. Then after during the graduate schooling, I started a company around machine learning operations, focusing more on machine learning implementation. During the phase of fundraising and everything, we got acquired by this company where I'm currently working at called One Concern. I'm basically leading the data science team over here. We are a team of 30 data scientists building the models for natural catastrophe modeling utilizing AI and machine learning based models as well as physics based models to understand the impact of climate change impact of the natural catastrophes. From flood, seismic and wind, hurricanes and typhoons in Japan and then understand on how we can provide that information and data to various customers ranging from insurance reinsurance or emergency managers, governments as well, so that they can take advantage of that data, which we can provide them, and then take the right decision based on that. So we are basically a data provider. Our core focus is mainly around the data science because we sell the data which we generate from our models at scale. So the scale aspect is important for us. Very interesting. So the business model here would be that these customers that you're talking about will actually pay you for the data that you are providing them and some subscription of the data, essentially. That's right, yeah. I see. And the mode of delivery of the data science output actually is actually like you would get that in a flat data file. I'm probably simplifying it and directly the output of the model is shipped to the customers. That's how it's working. Seems like. Yeah, that's one way of doing it. We also have our own GUI as well, which allows them to understand and interact with it. That's more for private sector so that they can visualize and play around with the data. That's one aspect. But at the same time we also provision using snowflake as well. That's one way of delivering for us. I see. Okay, understood. So as I understand that in that case, most of the use cases of machine learning is more like batch processing and then analytics on top of it as opposed to any kind of real time serving. Is that a reasonable it's a combination of both. There are a few pipelines which are real time. We have two different products, Ready and Domino. And Domino is more offline predictions right now, but eventually going towards online predictions, whereas Ready product is usually online predictions. What's the second so you mentioned Domino and what was the second one that you mentioned? Ready product. Ready? Yeah. Reay. And what kind of online predictions does Ready make? That's more on real time flood predictions. Forecasting of flood predictions or forecasting of impact from seismic aspects. Oh, wow. And who would consume that data in real time? Emergency managers, governments. I see. Interesting. Wow. So we forecast a flood three days in advance or two days in advance so that they know which regions to evacuate or which regions to focus their efforts more on. Wow. Has there been use cases where you all have forecasted flood or an earthquake and some people have responded to it? This is out of curiosity. Not for true foundry. Yeah. So we have deployed it to seven cities today. So it's being used in this flood season. So we are just looking forward to quite interestingly, even though we don't want it, so that we can respond to those. Always the thing about companies that work with disaster management, right. It's like you want to have customers, but you don't want them to have problems. But then what? They don't have problems. It's such a hard thing to sell. I get it. Okay. Makes sense. So I was just basically saying, our problems basically range from starting from monitoring of data sets to now going towards building a platform around data science platform is what we call. But internally, basically, we want to make sure that we can enable the data scientists to be quick to be able to deploy and reduce the time for handoffs between the data scientists and engineers. Of course, there are many tools out there for this purpose. We are trying to figure out what will be the best solution for our business problem. I see. So here when you say that, you mentioned two main problems, right. One is monitoring of the data sets and the second is kind of building a self serve data science platform to increase the velocity of your data scientists, right? Yeah. In terms of the increasing the velocity of a data scientist, could you zoom in a little bit more? Like what kind of use cases are you thinking of that data scientists can quickly do and what is it that they're doing currently that you think can be improved? Sure. That is to do with we have Argo pipelines. Think of it like kubeflow. We are building our own model registry and data registry which can be used and having a more of a data centric approach where our pipelines are usually the same. The code inference code doesn't really change. The configuration of the model can change. So updating of the model configuration in the registry should be sufficient for us to update the model pipelines. I see. Okay. I'm curious on what does Truefoundry refocus on because the ML stack and the workflow cycle is pretty large. Sure. Usually companies start with one focus area. So I'm curious on what's the focus area that you all have? For sure. So the focus the focus for us is what we call it a post model pipeline. So essentially, once a data scientist has built out a model, how do they deploy that model to production, which could happen through batch inferencing or real time inferencing while still following some of the best engineering practices like logging, monitoring, alerting. These things which are very frequently an afterthought or like companies tend to build them after the effect of deploying the model, these things just become default for any single application that you are deploying. And everything that you do is basically like version. So that if you wanted to roll back to a previous version or so, it's very easy. And everything that you do is reproducible that. If any model made a prediction at any point in time, you can basically trace it back to which version of the model, what was the data set that was used, what was the configuration of the model. Basically you can trace it back down to the data level. That's the part that we are focusing on and I'll actually dive into that in the details of it as well as we move forward in the call. Shabbat, I'm curious, given the space, there has been companies like Algorithmia which has been acquired by data robot, then there has been Sage Maker. Sure. I'm wondering on how do you differentiate in this set of ecosystem where there are many players in the space even any scale has deployment solution as well, right? For sure. Yeah. So I think the answer depends on which alternate tool that we are talking about, because a lot of tools specialize in their own way. Algorithm, I think their main thing was building like a more of a model marketplace. So I think very different problem that they were trying to solve. Sage Maker is clearly in a similar space that we are in, but our focus is on developer experience. I'll give you a concrete example. If you look up the documentation of how do you deploy a custom model using Sage Maker and you give that documentation to a data scientist. I don't know. Do you guys use Sage Maker internally? No, we don't. We use our own tool. Okay, got it. I see. So like you give a Sage Maker documentation to a data scientist and ask them that, okay, follow this documentation, deploy a model and see how long it takes. Our bet is that it's going to take them a week, a minimum of a week to just understand how to do that and be able to deploy and repeatability. It will still end up taking them four to five days for any new model that they're doing right. And if the data scientist does not come from an engineering background at all, it will probably take them a month to do the first thing that they're doing right. Because it just involves so much understanding of docker containers like infrastructure DevOps, et cetera. There's a lot of assumptions that are baked into doing that. And we focus on user experience, a lot. So meeting the data scientists in their own tool chain. So if they wanted to work on a Jupiter notebook and only in Python, how can we talk to them in their own language? That's like our main focus. You also mentioned about any scale, and literally I was having the conversation at the conference right now. The focus for any scale is literally like any scale, right? Like when you are building models that you want to deploy to hundreds of machines in parallel, your major focus is how can you get to that? Distributed training and deployment, essentially. And that to something that Sage Maker cannot do. For example, for you, Sage Maker has like one model per container, but any scale is like our value prop is like we can do five models per container. So they're really optimizing at that level. Right. But of course, if you are solving that sophisticated a problem, you also assume that your user knows all the basic things until a certain point. Right. So you understand the segment of the use cases that they are trying to solve is going to be like a really niche segment where people are dealing with extremely massive data sets, extremely distributed and high performance systems. Whereas for us, the segment that we're dealing with is, yes, you are building a model. Yes, you're dealing with a scale, but it's not like you're dealing with such a high performance system that you really need to understand the nitty gritty of exactly what's happening in Kubernetes and exactly you want to optimize every data loading and offloading operation, essentially. So, like, simplicity of use case is where our focus on with a reasonable scale, as we call it. Like, we don't say that we operate at Facebook scale. That's kind of how we differentiate ourselves. Okay, so the number of requests, which I assume would be medium scale requests rather than large scale requests, and of course number of requests are feasible. Large is a big term, subjective term. Right. But the point I'm trying to make is so long as you don't care about so long as you can imagine that your system can be horizontally scaled. Like you wrap things up in a docker container and you can imagine that you can scale it on a Kubernetes cluster. That is something that we dearly support, that's at the core of what we do within a docker container. Optimizing reading data from s three and how do you batch it most efficiently. Those kind of things we don't do, we let the user do it. So if that's the use case, then we are not a good fit. Okay, got it. I would say the way how we think about it right now is of course horizontal scaling. At some point, that's how many of the team get started. Just assume it's a black box, scale it horizontally as much as possible, throw money at it. But at some point, we are facing a difficulty of the compute becoming very expensive for us. And we are looking at HPCS and we are starting to see how we can do vertical scaling as well. We are in space of that one, but that's one of the goal. But of course still want to have a platform for the data science to be able to deploy, but at any scale similarly. Right? Yeah, makes sense. Understood. Zooming back to our conversation that we're talking about. So, like you mentioned that you're building your own model registry and data registry. And the goal is that somebody is able to deploy a model directly from the model registry. That's kind of what you alluded to. If something config changed and you got a new version of the model, somebody can directly deploy that one. So have you explored some of the other existing tools in the market? I'm curious, how did you decide to build that in house as opposed to, let's say, using something like an ML. Flow or we have not made a decision yet on what to use. We are still in the development phase and we are working on developing it. Yeah, we may end up using ML Flow or any existing set of tools which are already present or build a very similar simple custom solution as well. We have to decide that. I see. Understood. And in terms of the current deployment process, shabbas, could you help me understand how does it pick up one model that you recently saw getting deployed and stuff? Right. Like what process for a data scientist, what are the main concerns that you are trying to solve as an engineering leader, as a data science leader within the company? Is it the data science productivity that matters most to you? Right now? It's two things. It's productivity and repeatability of the pipelines. Okay, I see. So we want to make sure that we can provide updated data to our customers as well. So there's a business use case associated with it. We need to be able to accept new requests coming in from the customer. But that's again, also going back to the velocity as well. The higher the velocity, we can deliver quicker. So it's an implied output business value which we are able to get out of it. I see. And in that goal right now, according to you, what is the biggest blocker when you think of you mentioned about deployment, but I want to zoom in a little bit. What about deployment is so hard currently? That kind of it's becoming like you feel like it's a productivity sync almost. The models are basically shared with the engineering team. They repurpose the code sometimes or update the pipelines to actually have that. It may be hard coded by the data by the engineers in the pipeline. We want to remove all of that hard coding and then have a configuration driven pipelines. I see. Got it. Okay. Makes sense. So that makes things a little bit more repeatable. I see. This is an interesting one that you mentioned. So the team is structured as like, you have a team of data scientists who are building the model, and then you have another team of engineers who are focusing more on the deployment and maintenance of it. Yeah, for our needs. That's the best way of actually doing it because the expertise which we have in data science is very niche into construction engineering or hazard modeling or they who have their own niche areas of expertise. They're not necessarily engineers. So they don't have yeah, it's a huge burden for them to actually learn new set of skills and be bad at it. Instead, it's better to just couple them with good engineers. I see. Okay. Got it. And what would you rather have? Would you rather have that the data scientist is kind of becoming a little bit more independent? That's like a philosophy of some of the larger companies. Right. Like, for example, when I was at Facebook, we used to have that philosophy. Are you familiar with FB Learner? Yeah, I'm familiar with Michelangelo. Yeah. Oh, nice. Got it. Yeah. Actually, I have not talked about a little bit about your startup as well, which I was curious about. But anyways, so our philosophy used to be that empower people with the right set of tools, make it easy for them to do the right thing, make it hard for them to do the wrong thing, but give them more independence, right? Yeah. Is that the kind of mental model that you have in terms of where you want to take the company? Yeah, that's where I want to take the team. I see. Okay, understood. Nice. Got it. And lately, have you been evaluating some tools that can potentially help you in that aspect? What are some top of mind tools for you? We are in early phase of that evaluation, so I'm letting the team actually come up with some of their opinions and then I will provide them the inputs as needed. I'm not really pushing down some tools which will not be accepted by the team. Makes sense. Got it. I see. Got it. From our message. Actually, before I get there, a little bit about your startup. If you can share, like, you are building an experiment management platform. So is it like when you say experiment, is it similar to the A B testing experiments? Or is it more like the weights and biases or ML flow type of experiment management? Yeah. So we are basically focusing before even ML flow existed or reads and bags existed, that's when we were focusing. That's basically in early 2017, late 2016. We are focusing on developing experimentation tracking because at that point, experimentation tracking was one of the big issues. There was another issue which was being focused on as well. Diego from Algorithmia. The CEO at that point. Yes, they were focusing on marketplace, but they eventually found that Marketplace is not giving them good revenue. So they also went towards Enterprise Mloc's offering. Right. So Marketplace was not the main focus for them as well as their end. So we were focusing basically on experimentation, versioning and deployments at scale is what we were focusing on. I'm curious on how you have found the market responding to your product as well. Honestly, as a founder, when you go and say, hey, I'll change the production servers, be using my software, rather than using something you're already depending on, there's usually a pushback, and I'm sure I need to be able to convince the team, and team usually won't be convinced as well. And I'll be giving a pushback as well. Right, yeah. So I'm wondering on how are you finding the market given that challenge? You want an honest answer? Yeah, please. So let me be prank. We also get a lot of pushback from the industry when we talk about the production use cases and the major pushback is around. Generally, no matter how small, how large the team, people have built something internally. Right. Something, they're like, okay, I'm able to deploy my model using EC Two or I have these five engineers who work with me something, and a lot of people understand that this may not be the most efficient approach and this could be improved but then there is always that friction to take down what's working and get a new thing replace it with a new thing. Number one. And number two, I think there is also an emotional attachment element that we did not anticipate so well, which is people who have built out these systems are exactly the people who will take the decision of upgrading or changing something and they are emotionally attached. I built this thing and it works for me. I don't think external things are better. Right. That's a pushback that we are generally getting. Honestly, as founders, we are still trying to navigate what's our best route to solve for this problem, because at some point, we have to figure out a solution to this. Otherwise, usually, honestly, I found that Silicon Valley is not the right customer base. Yeah. You want to go to places like retail in Minnesota, some random place where they don't really have good software engineers, where they basically want to replace software engineers with tools. Right, I see. Is that an approach that you took at that moment as well? Yeah, we were trying to do that as well when we were trying to do it. We basically can start getting revenue more from these regions. First I see where they value Silicon Valley talent, and then at some specific stage when your product is so good to be that it cannot be denied by Silicon Valley engineers, that's when you can come back again. I see. That's a good point. I'll actually keep that in mind. One thing that so far has worked for us a little bit is prototype. So I have actually had a conversation with Chip You, and I don't know if you know her. Yeah, I know Chip. Yeah. So she's working on Claypot, right. Real time inference as well. Similar. I think there may be some competition between both of you, but real time inference is what she's focusing on as well. For sure. Yeah. So you're saying something. I'm sorry, I didn't mean to cut you off. No, so I was just saying that one thing that people have responded well to us is kind of the speed of prototyping and that a much lower bar than getting to your production. Right? So what our existing design partners are using us for is once they start a new ML project, they want validation of the project in some way. Like they want feedback from, let's say, the product manager. They want feedback from some end user. They want to maybe even deploy quickly as an endpoint and expose it to the end user, like a small percentage of the traffic and see if the model is working well or not. Right. And at that point, because you don't know how well it will perform or what kind of feedback are you going to get, you don't want to spend a lot of engineering resources around it. Right. So what companies today do is they end up spending a lot of time on prioritization where they're like engineering resources are limited. I only want to be able to deploy like five models and they spend like hours and hours just prioritizing the place where people are like we are adding a value is they just quickly deploy. It takes like 15 minutes to deploy, right? So they're like be it an app, be it be it an API, whatever you want to do, or be it some batch inference, right? It takes 15 minutes to deploy. You quickly test it out. If it works really well, then we step back, you go put your own engineering resources and then deploy that. If it does not work, you have saved a lot of planning and engineering time that you could have taken to experiment with that model. Makes sense. That's one place where our thing has worked. I was actually discussing similar thing, but I think for us as well, we have something called like Solutions Team where we explore like a POC project. And for them, agility is important. For such kind of teams, a quicker way of actually approaching it is easier. Right? And then the good news with stuff like this is that when you are doing this kind of thing, the whole point is that you want to get feedback from people and be able to visualize is it working or not, right? And then the platform that we are building actually comes with all these dashboards that you care about. So I'll give you a couple of examples. Like when a data scientist is building a model. The sole thing that they care about is model accuracy, right? Like how is the model performing? That's the only metric that they're optimizing for. But when you actually try to take this to production, suddenly like latency and all these things start to come into picture, right. That you can only figure out when you are actually taking into production. So we are like you get that latency dashboard as soon as you deploy thing to production. Then is something broken in your data pipeline, right? Can you improve something with your data pipeline that is something that people are interested in doing? Now your monitoring dashboards, like data monitoring dashboards are up and running as soon as your model is deployed. So if you wanted to compare it against your current champion model, right, you wanted to just check this is a model that is in production. Can you not do that with datadog or elk stack? You can do that with datadog. But datadog generally, if you think about it, it's designed for your software monitoring, right? So like the drift dashboards are actually very minimal. We're talking about latency here, right? Are we talking about latency? You can do so latency is like well known in data dog is captured in this one. Or would that be yeah, data drift. Is also captured in our dashboards, basically. So data drift and also model performance. So if you have one Champion model that is deployed in production, and you have a Challenger model that you wanted to try it with, then as soon as you route your production logs to go to this new model, which boost deployment is very easy, you quickly get the performance of this model that you can compare with your existing Champion model. And then you already have data driven decisions that okay, yeah, this seems like it's working well. Let's spend more effort on this thing basically. So that decision making becomes faster and that's what our current teams are, essentially. We tried to go through that route as well. We called it like optimizedly for models. Okay, I see. Yeah. So that people can try to relate, at least investors, we can relate with it doing it. You guys actually ended up building an internal tool for this guy. We actually had a Pivot and that's why we ended up going towards model deployment. Because I have a focus on computer vision modeling during Stanford. So we started with that focus as selling algorithms as a service that wasn't really picking up well. So we decided, okay, let's go through tooling in itself. And then we went through Tooling. Tooling had a more traction than algorithms as a service reasons. But then at that point, there were not many tooling companies. Now of course there are many. Wow. It seems like we have repeated our journey all throughout. I can probably even share my screen and you will relate with what I'm saying. So when we. Fundraised last October or so. We did not raise it on what we are building right now. We literally raised it without writing a single line of code. With an idea in mind, we went to Sequoia and we're like, what we are building is marketplace of machine learning models. They were convinced of that for some reason. They were convinced. For some reason we were convinced based on the initial user research that we did. And we thought that it can be a pain point. Honestly, I still think that there is some value in that segment. But what happened is when we started talking to further users, they were like, yeah, model is not my biggest pain point. When we actually were like, let's work together, let's build out this platform, they're like, model is not my biggest pain point. Model we are able to make. Can you help me deploy that? Can you help me get value out of it? Basically that's my biggest pain point. We actually pivoted. When we were pivoting, we actually were also building experiment tracking and we actually have a nice experiment tracking solution built out. Like I will show you that we have a pretty good, like much better than ML flow, maybe 30% worse than weights and biases right now. Okay? So actually I would say like right now our experiment tracking has reached a point where it's pretty much the third best solution in the market. Right now. We have that and then we realize that there is very little core differentiation in experiment tracking anymore. Like essentially what you will end up doing is this feature war. Feature war? Feature war. And that's not the market that we wanted to get into. So that's where we landed into this entire deployment and monitoring thing that I just described to you. It can still evolve. Who knows? And as I mentioned that we did get some friction from adoption. But at least for now, we are getting prototyping types validations from the market. That's where we are. I seem to have repeated your journey just a few years later. Basically a few of my friends I don't know if you know Radio tool. Yeah, we have deep integration with Radio. Okay, so Abu Bakr is one of the founders. He's also from Stanford. When he got started with Radio, they started with more on they started with focus on model deployments. No, actually something around synthetic data generation. That's how they started. I have found most of the companies focus more on deployment at the end of it because that's basically where everyone says the problem is. I can share more pointers on what I have gotten a pain from. Would absolutely love to learn about this. Yeah, I'm very curious to chat and. Learn about your I'm also writing a book along with my ex co founder and another friend of mine from Amazon. So if you want to be joining the effort or give some case studies from your experiences as well, that would be great. Absolutely. I would love to contribute and work together on this one. If you want me to through freedom or something, I can do that. If you want me to contribute certain case studies, I can do that other thing. I think that's all something, but yeah, I think you have a lot of experience having done it in Facebook and many other companies and I'm sure since 2021 you have gotten your pain on this journey as well. I would love to do that. Yeah, absolutely. You're right. Deployment is where a lot of companies tend to kind of focus towards. But if you look at the market shabbat, I don't know what has been your reading, but if you look at the tooling space, you have leader in experiment tracking, like weights and bias is a clear leader. Right. When you think about the model monitoring, Arise AI has emerged or is emerging. Maybe it's not Fiddler or arise. I think it's Arise that people know of more at this point, but yeah, it's still emerging. I think that space is still new, but at least you know that Arise, Fiddler and maybe true era as like a runner up, right? These people are catching up quite fast. Right. If you think about feature engineering, Tecton is like a clear, probably the only mention worthy player in the feature engineering space, right. Model building KDA. Most people just end up using open source libraries. But if you think about the distributed deployments, you have grid AI run AI, right? People know of them very well. There's also Domino Data Lab at some point, but I'm not hearing about that anymore, even though they have huge investments from Sequoia as well. Yeah. Three companies like Domino Data Lab, Data Robot and Data IQ are like this end to end platform. But I think all three of them target a different segment than all these startup segment that I'm talking about. Companies target, I think the Midwest segment that you were talking about. He goes sell to a company in Minnesota who does not trust engineers, who wants UI to build machine learning models and don't want to write code basically. So these are like almost no code solutions in some way. That's a route that they have taken, whereas all these companies are low code solutions where they're like making engineers more productive. That's the segment that they are taking. But if you notice actually in deployment, there is no clear leader currently you have Seldom, which is like a series, a funded company, 15 million maybe funded very few number of customers right now. The market penetration is very low. You have Bento ML, which is an open source library, but does not have a lot of coverage still they have almost no enterprise solution right now. And then you had Cortex which was trying to do something, but then it got acquired by Databricks very quickly. Pretty much that's it. Besides that, you have the cloud providers which are there for everything. So Cortex was one and which one the other one which we thought was good ones. Bento, ML and seldom. These are the two other. I think the challenge in deployment is everyone has their own specific way of deployment. You can either deploy it as a pipeline, you can either deploy it as a Restful API. Sage Maker started with Restful API as the main focus and then EB test on Restful APIs algorithm did the same more as Lambda as a service better UI interface. Yeah. I think when I was trying to do this for a company in gaming industry, I started as a Restful API. Then the scale was so high for images and videos, it didn't really work out. We had to move away into using serverless based approaches. I see. Given that variation in deployments, even if you come to me honestly, the pipelines are so complex. It's not emotional attachment to the pipeline. The problem is more about the foot of even moving from one pipeline to the other is so significant that it takes one year that I cannot justify the cost to any of makes sense in the company. Okay. Because the platform honestly, like the platform as a startup, it's not proven that it's so robust that it can be relied on for the production scale we will have in the product. Right. So it's a difficult justification process, not just to me, but even to my manager, for example, on why we can rely completely on a new solution which can be helpful here. Right, but then the other thing is the effort which is required to actually go onto it. For example, for us, we initially started with Postgres, then moving on to Snowflake. We performed multiple Pocs, understanding what's a significant advantage out of it. Then eventually we are now moving into using Snowflake. But that entire process of getting everyone on board and then moving on to using Snowflake after Series B companies, it becomes a very long process. You may have to convince everyone and the solutions, the architect and everyone within the company. Right. Then there should also be a product stakeholder who has an opinion on is this really helping in the product roadmap as well? Like for example, Snowflake, one of the important inputs which we had was Snowflake has something called Snowflake Marketplace for data delivery. So we can see that as one of the delivery mechanism through the marketplace which will help us get easy data delivery mechanisms as well. So all this decision went in to actually select what tool we want to use. Now that's a robust. Of course, it's an IPO company, so we can completely rely on that company. Right. If you use similar structure of judgment on startup, it goes about in the first few set of discussions itself. Right, completely makes sense. I'm totally with you that it's really hard to justify moving your existing infrastructure to a startup altogether. I think it's very hard to do. That unless I think the place where I found is, like, you have to basically figure out the perpendicular value proposition here. Like, for example, Monte Carlo data. I don't know if you know about them. Yes, I do. It's a data monitoring solution. Great Expectations is, of course, similar. The QA team is not used to doing data QA. Traditionally, the QA team is only focused on functional QA testing, not data QA testing. That's where I found a huge market opportunity as well, where they are actually focusing more on perpendicular value proposition for the pipelines. Right. They're not really going into the pipeline development. I will not let you touch my pipelines, but I can monitor my data with minimal breakage to my pipelines. Makes sense. Yeah, it's basically like, what minimum thing? Can you touch my existing pipeline but still give me a new value? Makes sense. Yeah. So I completely understand, like, anything that is adjacent to your core work people are more open to accepting new solutions. But, like, core work, people are like, no, I want to keep this close to my chest. Yeah. And it's also because huge amount of work has already gone in. Right. Like, for example, for a company in the past, in 2016, 2017, moving from VMs to using Kubernetes was a huge. But now everyone's using Kubernetes. Once new startup starts using Kubernetes, from the onset right now, from using Kubernetes to going completely serverless, forget Kubernetes. That's again a challenge for many companies, which takes time for actually transition their entire product. That makes sense. One question. Are you guys also using Kubernetes? Yeah. Got it. Okay. Your machine learning models are also using Kubernetes? Yeah. Argo pipelines are built. It's a Kubernetes native solution. Got it. Okay. I see. Is Argo CD. Basically, the UI that Argo comes with. Argo. Is that the only UI that you use on top of Kubernetes? Or you have also integrated with some other tools on that? Integrate with datadog we integrate with, yeah, that's basically the major tool. We also integrate with great expectations. I see. Understood. And one other thing is what is it? Do you guys use apps like Radio and all for kind of, like, demoing your model to someone? No, we don't. Those are very nascent model development, in my opinion. It doesn't really work for our use case. We still need to be doing it at scale for our analysis. So we end up actually doing for example, ours is a geospatial analysis company, so we basically on a city wide scale and then show it on a notebook and share the results as graphs and stuff. We don't really have testing the features by playing around. It's not really that helpful for us. So that's why we looked into Streamlit. We looked into radio. We have used Streamlit a bit because it has still shown us some value to quickly deploy something and show it gradio given it's within the notebooks, it's not been that helpful. I see. Okay. I seem it is shown directly to the customer. We cannot show notebooks to the customer. Pardon me? Seeing it is directly shown to the customer or just getting the feedback directly. Right. For quick feedback. Of course that's not good for us to show it to enterprise customers, but just to get a quick feedback. But we cannot show gradio which is within a notebook to customer. I see. So like you mentioned, the notebook may like the embedded graphs and all is sometimes helpful. One of the issue with the issue that I've seen in that approach and tell me if you all have a face that issue as well is that it's not relatable to the true business user. Like somebody who does not understand code. Like if you throw a notebook at them, they will have a hard time understanding what's happening and that's number one. And they cannot interact with the graphs because the only way you interact with those graphs is by writing more code. Right? Yeah. So streamlit app kind of I know solved for that problem. But how about the other segment? Yeah, it depends on the stakeholders you're communicating it to. Ours is a data solution at the end. So when you are selling it to insurance or reinsurance they are all quant focused. So it may end up actually working for us. But when we are focusing on private sector then streamlit is an easier way of communicating rather than the graphs. Got it. I see. Understood. Makes sense. I know. We are almost nearing our end of time. Shabaz, there are two things that I would like to do and by the way, it's fine if for your use cases like what you're building does not work out, it's totally okay. But what I would like to do is kind of show you the platform and get some feedback on where we are headed. Especially given that you're built in the space and everything. The feedback itself would be helpful. And if you see some value, it would be great to figure out a way to work together. Otherwise it's totally all right. Second is whatever way I can help. In the book that you is this book that you're talking about more of targeted towards technical audience or more towards generic audience? Technical audience. It's more around data centric machine learning operations. Got it. Okay, take it. So on that one, if you let me know if there is any way I can help, I would love to do that. So should we maybe set up like one follow up call where we can go through the platform and get some feedback? Yeah, we can do that. Do you see a slot on maybe Thursday afternoon? Thursday? Yeah, we can. 430 would work. Does that work for you? Sorry, 04:00. 04:00 works for me. Yeah. All right. I will send out an invite to you in that case. Okay, sounds good. Doing a startup is always hard, but especially having seen in the developer operations or ML Ops is also difficult. So all the best of that. Thank you so much. Thank you so much. I need all the good luck, good wishes right now. We'll get in touch. Okay, sounds good.