I'm doing fine. Nice. Where are you based these days? So I'm working from home. In India. Where is your home? In Punjab. Nice. Good. So you are in that team is remote. Yeah, like offices are open so a few folks they do join the office. We have office in Hyderabad so some folks do join there but it's not mandatory as of now. Okay, nice. That's good. Hello, how are you doing? I'm good, how about you? Doing well, thanks a lot. Good. I thought she was saying something. Yes. I want to start off with a quick formal introduction but if it's not necessary we can also get started with the agent at hand. Okay, sure. So we have been doing some great work in the space of MLPs and similarly Nicole is a vice president of innovation team. So all the AI work that happens is in our innovation team. So we also internally have been using a few open source tools as well. And I think overall the idea is for us to identify if we can be a good fit for each other in terms of identifying common goals and widgets. So with this I will hand it over to you. Thanks a lot for doing the introductions. So riddle just a little bit about myself before we get started. So I personally come from a machine learning background. I used to be at Facebook where I was driving one of their conversational AI efforts. Have you used the product call portal? The portal is the one no I have not. I think I guess have you seen Alexa or Google homes? Yeah, yeah. So Facebook launched a similar device with a video calling feature enabled like virtual assistant device with video calls essentially. So I worked on that, basically the conversation AI part there. Prior to Facebook I was at a startup called Reflection where used to be built out a lot of recommended systems, personalization algorithms for the ecommerce industry. So there I got a chance to scale the system to about 600 million users because I was like one of the early engineers, second engineer on the ML team there and also build out a horizontal ML platform for the company. And my education background is it corrector undergrad and UC Berkeley masters. But basically what we are building is two foundry. I think some of the inspiration comes from my time and Reflection and some from the time at Facebook. So at Reflection when we built out this horizontal AI platform for the company, we noticed a huge productivity gains for all the developers that people who were taking a certain amount of time to deploy models and maintain models. We saw roughly 40% production in that time and we're super proud of what we have built out until I joined Facebook and I realized what are the other things that are possible as part of NML platform? I felt like at that point at Reflection we used to think that what we have built out is already great. We actually don't need, not a whole lot of things can be done on top of it, but I realize that is possible and that's part of the mission of what we are trying to do at Giri Truefoundry as well. That basically bring out Facebook level systems to all the companies in the world. And obviously we'll need to adapt the systems to the requirements of mid market. It's not like all the problems that Facebook faces, all of us outside would face as well. Right? So we need to adapt there, but that's what we're trying to solve. And we'll tell you a little bit more about what we're building, but would love to spend today's call in doing a couple of things like understanding your current tech stack a little bit and also what are the problems that you all are facing in this entire ML domain. Deployments, monitoring, stuff like that. So that's what I have to focus on. And then generally what we like to do is in our next call we can then give you all a demo of the platform and that can be personalized to the problems that we are learning in this call. So that it's a little bit more relevant essentially. So, first of all, thank you for such a nice introduction. I'm like glad to see like minded people who have worked in the domain of ML. So great to meet you. First of all, a little bit about myself, I'll just do a very short introduction right now at Mafia, the VP of Innovation. Basically we look after all the AI implementations, basically productizing AI for our customers. We are very big in public sector in the sense that a predominant customer base is state agencies in the United States, right? So as soon as you hear state agencies a lot of things rings in your ear like regulations, data privacy, model hosting at scale, data resiliency, model resiliency, having wide replication of your services so that at no point it goes down and then they're very struck about uptime budgets and so on, right? So that is a very small brief into some of the problems that we face. But apart from that, I mean, I have personally worked with fortune 500 companies. I work with Nike, direct energy, general motors, home depot, basically. I used to work to help set up their data sciences team and basically build the models to start off with some of their business use cases. So that is a little bit of background about myself. Now here at Mathew, basically we are building a couple of products, right? I think what you would have talked to us about is predominantly one of our AI products for clinical trials. It's basically trying to help pharma companies better predict the lifecycle of the clinical trials, when are they going to complete, how are they going to compete, and so on. So this is basically machine learning models, your xg boosts your random forests and so on. So over there model lifecycle is a little simpler but again, we are looking for optimizations there. Our other product is intelligent Document processing which is basically where we hit the public agencies in states. It is basically a complete end to end document lifecycle management as well as information extraction through AI models in it. And we basically build custom models for their specific document types and we host those models so that they can extract information from the documents, right? We are talking about documents and the volumes of 1 million to 10 million on average. And then if you're talking per year and then for large agencies it obviously goes up multiple times. So that is what we deal with. And basically what we provide is not trained the model for them, we provide them a product in which they perform very minimal things required to provide the data, to provide the annotations required and then the system is configured to train a model using the data. Right? So basically what we're talking about here is a lot of ensemble models packaged together as containers, as deployables, which can with the data input can be trained on cloud and then deploy themselves to be available as a service which they can use to infer using. Right? Again, I think you touched upon it. Horizontal auto scaling is one of the things that we continuously use in all our workloads. So apart from that, we need the productization of horizontal scaling so that a customer can control it and not just rely on a configuration that a DevOps engineer or someone has already set in the product because that would dictate their costs. What configuration do they use? Do they want to change it or not? Or is the throughput that they're getting good enough about? So like cost is a big factor, obviously and then because cost is a big factor and we are talking about probably eight working hours a day in which people will use this because this is public agency you're talking about. We want to scale down to zero all the way for all the models that we deploy when they are not in use or they don't hit traffic. And if there is a new request that comes in for that model which has been scaled down, it should be up again in a couple of milliseconds. Not to give a lot of latency, but that container should be up again in minimal amount of time. So we're talking about all the way down scaling it and then bringing the models up. So this is all to optimize costs because we are a product company, we play on subscriptions and we want to minimize our cost as much as possible and basically being able to track the cost, optimize the cost at an infra level is very, very important to us. And then one important bit about us is basically our USP is that if you look at other document AI processing companies out there, whoever else has a product, they provide a generic model in general, right? We allow the agencies to train a custom model using their own data set. And basically they can train ten models, 100 models, 1000 models or 10,000 models. So this entire system should be able to handle the HPA and the scaling of the scaling down to zero of entire hundred or thousand of models concurrently. Basically, if we keep 10,000 models running all the time, we are getting a huge bill, but they might not be used all the time. So that is one bit which is important to us and then something that we are asked very, very frequently, disaster recovery and resiliency. So we build in resiliency, but we are still looking for things that we can bring in for recovery. Basically, the questions that we are asked are what is your RTO? What is your RTO? How can you guarantee us in case of an entire region going down, you will be able to still provide the services in a different region with a minimal downtime and budgeting of that downtime, obviously. So I spoke a lot, but this is basically us as a company, what we do. And from ML practice perspective, what are some of the challenges that we are facing from I would say ML Ops perspective, but also as a perspective of how do we productize ML Ops into a product in a way that it becomes the most beneficial thing for a customer? ML Ops at the infrastructure level is good because we can configure things and then we can work without configuration in bound parameters. But when we productize it, that's when things become interesting because then we would have to control a lot of parameters for a customer to be able to minimize the costs. Got it. Okay, there's a very helpful background. Thank you so much for sharing this. I have a few questions for you. So one point where you mentioned that you want to be able to productize horizontal scaling so that someone else can control it, right? Yeah. What do you have in mind and what do you currently use it for? This? So, right now we use a configuration driven auto scaling, just like I said, that is generally used. But where we want to get to is that there might be agencies who want a throughput of ten documents per second. There might be agencies who wants a maximum throughput of, let's say, one document per second. And that choice will drive the cost enforcing that HPA from a product perspective, like they have a UI in which they can configure, okay, this is the throughput I need. I have a season coming up, back to school or something, and then I have to process a lot of applications or something. And they have to enforce that particular HPA for that time period. After that they can reduce it down. Your response times might increase a little bit, your throughput might decrease a little bit, but they would have absolute control of the cost and ensuring that the system will respond when they expect the most to. So that is what I mean. HP has a product type feature that they can control using product configurations itself. Got it. Okay, understood. The second question is that in terms of cloud, currently, like, are you all using multi cloud? Are you using a specific cloud? What's the same multi cloud? A lot of our customers. So, again, public agencies come up with a challenge that they always have their own preferred cloud. So all our solutions, machine learning models, and everything that we do in terms of machine learning is completely cloud agnostic. We basically work on developing the models to be in a replicable fashion and bind them as containers so that they can run in any infrastructure. Does your solution typically get deployed on the customer's cloud? It can be either. They can be customers cloud. But in a lot of times, when the customer doesn't want to take the ownership of the product, they ask us to host it on our own cloud, our own accounts, and then expose a secure endpoint, which they consume from it. I see. Got it. Okay, understood. This is cross cloud. All container based horizontal scaling is a big feature. Do you all end up using Kubernetes? Yes. Okay. Is the entire infrastructure based on Kubernetes? It is. I mean, for very specific custom modeling, which is going to be completely one time, we deploy it on Kubernetes. But the development can happen on a number of different things. Collabs and our own Jupiter notebooks on AI servers and so on. The deployments are exclusively on Kubernetes. Is what we have as a devop strategy. Understood. And do you also end up using some kind of pipeline management tag orchestrated like Arco CD or Airflow and stuff like that? We don't use airflow for CI CD. I mean, for CI CD, we have our own CI CD tool that we use. It's basically an open source tool that we have configured for ourselves. I have to check with my DevOps team on that. Understood. The other question that I wanted to understand was how is the team structured? Is there like a data science and a back end engineering team and a DevOps team, or is there like an ML engineering team and DevOps team? What's the structure of the team who deploys models? What's the process of that? I guess from a point of view. We have ML engineering team, which is basically entirely my team, the innovation team, which develops the models, which develops all those binaries and containers that we host the package into. So our outputs are basically containers. And then we hand it off to the DevOps team, who basically works with the infrastructure part of it, with the CI CD part. Of it. I see. Okay. So there's basically two teams. One that builds and containerized the models, and the second is who would deploy those containers? Deployment in the containers. It precisely. Got it, okay. And in terms of things like the offline training jobs and retraining jobs, do you have any kind of any models that require like very frequent retraining? And if yes, what kind of tools do you end up using for that? So, I mean, our document processing product that I talked about, IDP, that is completely human in the loop online training mechanism. Whenever a user wants to add more samples or give feedback and upgrade the models, they can do it directly through the product. And what we basically trigger is a Kubernetes job, which takes the data and then obtains the model using the previous rates. Got it. Okay. Completely online. Okay. How does the resource allocation work? Does the DevOps team, for example, when a machine learning engineer needs Kubernetes machine to launch a job right, or even experiment with it, not during the production time, but even experimentation time, how are you allocated machines? Is there like a DevOps team that you have to create a ticket for or what's the current process? So if you're talking about where does our team work when we are experimenting or we are training the model for the first time, we're just playing around with the data. Let's say we have our own AI service, which all our engineers have access to, and they train their models on those AI servers, and they are basically our own machines hosted in our offices, which we have provide access to. Once. We are ready with containers and so on, then obviously it's all handed off to the DevOps team. Got it, okay. And is there a concept of, like, a lot of permission control on these like internal permission control on these machines and models and work that you do? Everything is open to everyone. Currently, for the product, there is a bunch of permissions authentic. Then there are field level permissions as well. So basically, product uses completely binded by a lot of permissions for our AI service. I mean, all developers have access to it. So it's basically they can concurrently use their big enough service to run concurrent jobs and so on, so multiple people can use it at the same time. I see. Got it. Okay, understood. And then in terms of environment, do you have like a dev staging fraud set up? And do you have any complete test suite that runs each stage to stage? How does that work out? So we do follow the DevOps practice of having four environments, sometimes five dev testing, performance, UAT or RC, basically release candidate and the final production for our products. Right. So any product that we build which has AI models in it, goes through all these things. The testing is involved manually in the testing phase and then the performance testing is there where we have scripts which put a lot of load on the system to check what is the throughput and so on. And then there are test cases for each module which basically talks about the code coverage that we have. Got it. Okay, cool. And then for monitoring of your machine learning models and even exposing monitoring to your customers who are kind of training their own models by providing like these sample data sets, do you use any tools, external tools? Have you built out something internally? So for internal monitoring, because we have to keep a check of the system load, at least our responsibility to the customer, we use Grafana that basically catches all the logs of Kubernetes Pods and then gives us a very nice view of memory utilization, CPU utilization and so on. For exposing metrics to the customer, I mean, we have not done that yet a lot. Basically what we give them is an admin dashboard which talks about the usage, not the infra level metrics of how does the system help look like. So if they request, we can give them Grafana. But to their specific use cases, we only expose the information that they need access to, which is basically how much have they used the AI services, like things like number of API calls, how much they have burned versus how much they had quota for and so on. I see. So in this case we describe the system monitoring, the service monitoring level. Is there anything around ML model monitoring, data monitoring that you're using currently? I mean, it's all custom built. We are not using any open source tool for it. And it's very minimal in nature as well. Like for example, for different model versions that we create, if it is like a simple model like Random Forest and Xg Boosts, we basically capture the important metrics across each experimental and each model training. That's what we capture and document for the product part of it. It's all stored in the product database itself. So we have not exposed it in our product because it's down in our roadmap. But those metrics we can expose, but there is no tooling around it that we have used to do it in an automated way. It's all basically done and stored in databases so that we can retrieve it at a later point. Understood. And I'm assuming same thing for experiment tracking, like the offline experiment tracking as well. So you kind of end up doing logs, but you're not using any tools like ML Flow or something. Right now we use ML Flow for our machine learning workloads. Okay, I see. Cool. So this is very helpful. Now, in terms of the problems in the beginning, you described some of the problems around data resiliency, reproducibility of the pipeline, data privacy, security and all. And also scale is what you call out as a thing as well. But I'm assuming, given that you already have customers in the public sector, you have kind of solved for some of these problems. Otherwise they will likely not work. Right? We have solved for them in a manner in which we control what the system configurations look like. Where I want this to go is us able to productize this in a way that once we do a one click deployment of the product, after that point we don't have to manage the info at all, right? So that if they, let's say, did a subscription for 1 million documents and later on they have a load of 100 million documents, they can just go into the product, change those configurations. That completely trickles down at the info level, upscale the infra, and after that gives the customer the environment that they need. Right. So one time configurations on Kubernetes for horizontal part scaling and all of that, we can do. But how do we productize it in a way that basically a couple of things happened? Number one, it completely scales down to zero when no traffic is there, so that there is no cost at all. If the traffic starts coming and the model is scaled down, then a new replica or a new pod is created in less than a second so that latency is not there. This is basically one of the objectives that we are trying to cover. This is one. And the second is this should be model agnostic because we have a lot of different frameworks that we work upon. The document AI models that I talked about, it's basically an ensemble of three neural network models which are deep enough. I mean, they are each 800 MB neural networks. And then there's an ensemble of it that goes from one model to the other. So, I mean, they are completely custom workloads, right? If you were just talking about, let's say, deploying one model in its dedicated runtime, we can do that easily. But this is three models which can be on the same runtime or different runtimes. But they need to be packaged together to run to give out an incident. So the custom runtime thing actually is a problem that we are trying to solve as well. That basically takes us away from a lot of good open source things out there which can actually scale down to zero if required. So that is one very specific problem that we are trying to solve as well. I see. Okay. Got it. I guess for these models, do you also end up, first of all, for scaling down to zero is the thing that you're trying to achieve. You're not there yet. You're saying we can do it for a dedicated runtime. If we know that our model is going to use only one runtime, it's XGB, we can scale it down to zero. But if it uses multiple run times, then the open source products out there don't support that yet. Like scale down to zero on customer. And times, especially when we use GPUs, is the problem because GPU provisioning is a pain. You have to either keep one GPU engaged all the time, you cannot scale down the GPU to zero. And then when you requested the cloud provider immediately provisions of the GPU and it starts. The cloud providers don't provide any SLA for GPU provisioning on demand. Got it? Understood. So here one question that actually did not understand this part. Maybe I'm just not familiar with this part. That what happens when you have multiple run times. What is the biggest challenge that comes when it comes to scaling down to zero? And where do open source tools here break? So I'll give you an instance. One of the tools, TensorFlow serving or some derivative of it, we were exploring it sometime back. They basically have runtimes defined in their offerings that if you use these runtimes, you just need to load your model object into this runtime and then it will start serving. So they're basically talking about if you have a TensorFlow runtime, load your TensorFlow object into this runtime and it will start serving, which is fine. But then if you have two models which both belong to different runtimes, let's say one is onyx runtime and the second one is TensorFlow runtime, then you cannot use that particular tool set unless you make your own custom edits to the entire code base and try to figure it out. Right. So that's where some of the limitations we faced, that our model. One inference requires two model. Let's say both of them require a different runtime. Then it's problematic and basically we are moving towards being model agnostic. The framework should not worry about the runtime that you have. It can worry about runtimes, but it should get it from the configuration and it should be able to then scale down or up. Okay, got it. I will have to discuss with my team a little bit about this because I'm personally not very familiar with this part. So glad to know that you called out this problem. Cool. Awesome. So, Riddle, I think this has been a very insightful conversation. What I'm going to do as the next step is set up a call with you and we can also do it later this week if you all are available. Or sometime early next week. Whatever works best. Where we show you a demo of the platform and we'll try to cover a couple of points that you have called out here in terms of the infrastructure scaling down to zero and also the entire runtime thing. What can be done now before we get there? Just a couple of things from our side. We are a relatively early stage startup. Currently it's been over a year that we're around and the way we are working with our current customers is in a design partner mode. Irsh might have mentioned this to you as well. So what design partner mode means is that we have a basic set of capability on the platform already built out. And by the way, our tech stack is very similar. We're also completely on kubernetes. We are also cloud agnostic. We can work with multiple different Kubernetes clusters on different cloud all in one view, et cetera. So from that perspective we have a lot of alignment there. Now the way we work is that we pick up one or two problems that are like the most pressing. So for example, scaling down to zero is the most important thing. Then we basically work together with you guys to make sure that we are able to take your models and kind of scale them down to zero and bring them back up quickly. So how can we set the entire thing up right? Like that's just an example. It could be any other thing as well. So from that perspective, whatever features are missing on the platform, we basically work together. Together means a weekly meeting or something where we understand the requirements. What are you guys thinking, what are the timelines and stuff like that. And then we go build out that feature that can be used by you and your team, essentially. So that's the mode of engagement that we are currently working with. If you have any questions, I'm happy to answer. Yeah, it sounds really good. I am looking forward to looking at the product demo platforms demo that you mentioned. I discussed a lot of the problems that I'm facing. I would have a look at the platform that is built out so far. We'll find out synergies and after that point I think if you wish to go ahead, we would have to go into some form of NDA to ensure just because there would be a lot of discussions with respect to things that can be potentially IP. So we would have to follow that path. But I am first of all waiting to look at the demo of your platform. I'm really interested to see what you have done. I'm always intrigued with ML products, so looking forward to seeing it. Sounds good. Cool. So when should we do this? Should we do this? Let's actually take a look. By the way, does something similar times work for you all better? Do you guys prefer in the morning times? What's your preference? A little earlier in the evening would have been better. I can also do by the way, in the afternoon or something like I'm generally up until late in the night my time. So if you guys want one or 02:00 P.m., that's fine. I don't want to keep you up. No, I'm generally up by the way. So my schedule is that literally I have meetings until two or 03:00 p.m. IST very frequently. So I can easily squeeze in that time. No, I think we'll do it at a time. I think this time works for you as well. Probably. This time? Sometime later this week or next week? I'll have a talk to you about that and we can set up a time to go through the demo. Got it. Okay. Will you then help coordinate with Riddle and help us set up a time then? Yeah, sure. Okay. Sounds good. Awesome, then thank you so much, Adele. Thanks a lot, Ash, for setting this up. I really enjoyed the conversation. Likewise. Thank you so much, Nicole. Bye. Thank you. Bye.