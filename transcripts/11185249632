Hey. Hi, Stevetha. How are you? Hi. I can't see you. Are you able to see me? Not yet. Yeah. I have Vishak with me. He's more of a hands on data scientist kind of guy who's the other AutoML platforms hands on practitioner perspective also. Awesome. That's great. Thanks a lot Sivasa, for kindly also including a wish taken this call. What I wanted to do on this call was primarily Sivasa like I'll give maybe a two minutes please without diving into much details. And what I wanted to understand is you have been in this space for quite some time, like earlier at Musicma and now at Maverick. I'd love to hear a little bit about the ML infrared Maverick, the kind of use cases and what are the challenges you encountered and where have you like and even a vision from you like handson. Where have you faced problems, what is it you have seen in the systems? And then briefly towards that I can give a thought overview further of truefoundry, but then I would love to do a follow up call to do a more detailed overview of proof and also hear your feedback on what we are building. Is that something that will work? Sure, I think it should work. Okay, awesome. Where are you all based, by the way? We are based out of Bangalore. Okay. I'm also in Bangalore right now. So just a very quick background. Like two foundry right now is like a 15 member team started by me, Vijay and Nikkun. We were all batchmets from it. Krakpur we have been building for the last one year and the goal has been to build a platform for enabling companies to test out and deploy models very seamlessly. So around that we have been trying to talk to companies to understand more about the different workflows, the kind of problems, what are the experiences they have had with other tools and so on. Background for me has been primarily I was working with a hedge fund trying to build trading strategies using a lot of data. And then Avisha can Nikun were at Facebook. So they actually work a lot on infrastructure, large scale infrastructure as well as on machine learning infrastructure. So Facebook has its own internal ML platform which they use for building and then productionizing models. So we are kind of trying to see if we can bring that similar thing that we had at Facebook, other companies around the globe and help make developer developers more efficient. So that was the thesis around which we were trying to build. And we'd love to dive into more details, but before that, lock me a little bit more maybe if you can share about what kind of data science, which part of the data science work you look at at Maverick and then we check also you and generally about the data science team. Shinmay is here in the call. He has joined us on the product side. So he oversees the development of the products. He's also a graduate from It, corrector. Briefly, I joined Maverick about three years ago and as you said previously, I was working with musicma, math, Co. But the one thing that is different here I'll come to what we were doing there. In the other places it was more about custom build models using Python. There was no platform as such. So it's literally every time you take on a new project, you do a lot of things again from scratch. How you clean data, how you do like univariate bivariate everything right up to model based. So there's a lot of work that keeps getting repeated here what we have done is we have a partnership with an AutoML product called Data IQ. So that is what we primarily use. We found it to be very useful in terms of one thing is getting people up to speed in terms of being able to build models. And then there are other roles in projects like people like business analysts, data analysts who look at data and try to get insights out of it, right? So there's some amount of exploratory data analysis, some basic visualization capability that brings in that if I were to be honest, that adds a lot of value to businesses rather than the actual model, right? So a good EDA is sometimes as good or better than a model in a real business kind of situation. So that's what we primarily use. We work only with banks and that's been my background. Also, generally I've worked mostly with banks across consulting and analytics. So the kind of problems that we deal with are obviously around banking, so they could be around risk like predicting default for loans or one of the things that we've done internally and client got very interested in was how we were predicting home prices. In the US. We got lot of publicly available data and we shake worked a lot and he kind of turned it into fully automated things. In the sense every three months the data gets updated, it gets like on a website, it gets downloaded, the models are rerun, the prices are calculated and everything is available then and there's a dashboard at the end which gets kind of repressed. Similarly, other areas we work on are around customer problems like cross sell upsell, marketing analytics, looking at the whole marketing funnel, say for a wealth management firm and how many sort of leads turn into qualified leads turn into actual customers. So can you kind of predict what the time frame will be? So broadly, I think very, very focused on banking and we cut across customer marketing, all the areas in banking we kind of work on anything. Yeah, so all the use cases that she was mentioned, so I work primarily on Python and R. So as you see, we are working on AutoML platform called Data IQ. So it has a lot of wrappers around Python and R. So you don't even need to go out. And you can write all those scripts there. You can use the custom built, all the UI features of Data IQ. And similarly, as he told, that EndToEnd deployment part. So it can also take care of that. You just add a trigger using the Pui tool. It will add a trigger that every three months, everything will get refreshed. The data sets will get rebuilt, the model will get retrained and you can log the models and you can do everything. Basically. One more thing I want to add. This is kind of very new. I was talking to an Israeli company a few days back. They have built something very similar, but on the data engineering side, it's called Big Bi. And their claim is that they can help you build pipelines like five, six times faster with disparate kind of data sources. And it's again, very visually driven. So drag drop. And she showed me a demo sir, not worked on the actual tool, but you can pretty much create an SQL with the joints and everything just on the UI, get the output, do some visual analysis on it. It looks good. So it's interesting that people are looking at that side. Also make the workflow more streamlined. Question. Yeah, I'm sure. So, you mentioned a bunch of these different models. Just trying to understand, do you sell these models directly to your clients or do you run it yourself? Are these batch models or live inferencing models? So when we do it for the customer, it's completely on their environment, how they want to do it. Usually with banks, what happens is that team is usually very separate. You build something, you give it to them and then how they deploy it is kind of a black box for us. But with Data IQ there is some capability to deploy. We have exploited to some extent something on that. What they have done is that basically they have created a separate basically. They. Have divided into nodes. Like there is a design node where you can do all the exploratory work. There is an automation node where you can do batch processing and there is an API node. If you want real time predictions, then there is an API node for that and they have separated out the logic of it. Interesting. And just following up on that question. So when you are building these models. That'S fine, we can just wait. Hello? And now you lost the last 10 seconds. Okay, I was asking like when you are actually building the model? When you are actually building, not after the model is built in the build out phase, do you actually use by chance any remote machines to actually train these models or is it mostly on your local systems or is it like Data IQ? Does it provide some sort of a post? Data IQ is remote. Yeah, it's fundamentally a VM that is somewhere in our data center and it's. A browser based tool, so it's completely remote. Okay, and is it hosted on your own infrastructure? Yes, right now we have hosted we have a version on our own infrastructure. We have a cloud version also that can be set up on wherever AWS you are and all that. Okay. And you use the notebooks of Data Two. Like Data Two has its own notebook. Like when you write Python and all. You can do two things. They have something called as visual recipes, where you need not actually have a lot of programming knowledge and you can kind of say that this is the model I want to run and it has its own thing. But if you want, you can write code separately. There's something called as code recipe. You can write it in R or Python, whatever you want, and then you can import it into data. So both models work with data. There's one more very good thing which I liked about that is that the exploration part, they have sandboxed it separately. Okay. So there are five data scientists in the team suppose. So everyone can go in something called a lab and build their own notebooks and everything is tracked. Even if you don't set up a GitHub account or something like that, it is all tracked like which people did what and what were the changes. And then once you get a final model, you can just with a click of the button, you can deploy it. So basically there is a sandbox to perform the experiments. Some of the things were at least not that developed in data. Things like NLP or even when we were doing forecasting this house price forecasting their models weren't good enough. What we did was we just moved to R, built all of it in R and added it as a code recipe to the overall kind of workflow. Okay, interesting. So there are certain models you use out of the box and then there are certain that you will automatically build yourself as well. One question that I'm still not clear on is these models you are deploying, like ultimately you're building models for your clients. How do they access these models? Because as you said, for banks they will always want on their infrastructure. How does the retraining happen on your because the data will be mostly residing with the banks. I just want to understand when you are deploying on your own data IQ. What does it mean when I'm saying our own data? I'm almost never using client data, publicly available data, scraping websites, doing all of that and creating pipelines for all of that. We have a fully functioning database with all the data that we need to build models. I see. And what is finally sent to the client? Is it like the model file or something? What do you. Varies by client sometimes because they have their own version of an AutoML. Also that they use something like H Two, for instance. That's what one of our customers uses. So again, you build it and you either do, I think you have all the options available, you do a pickle file, you do API or whatever. And it depends on what they want eventually because everything happens on their environment. So that's how it usually works. I see. And in your case, once you've built out, you deploy it. You said that the business metrics are also generated. So it helps. So finally, basically, you basically see that, okay, the model is good at giving good action and then after that, you kind of convert it into a pickle file and ship it to the client. Like at a stage where you are confident. Is that how it works? And something like testing of these models or maybe monitoring while you before you retrain, like figuring out how do you retrain? How is that taken care of? So as I mentioned, there are separate nodes for that. So automation for batch and API for that for real time scoring. So all of the data that we score that will be kept as logs. And there are some, like, there are servers, which what they do is that they get the data drift, they get the model degradation, they will plot it for you and all those things will happen. And we can set up like scenarios. Also, like if the model accuracy goes below some specified threshold, then you retrain it, you run all the recipes again and all that and it will happen automatically. Got it. Okay. One more question is how do you kind of handle preprocessing functions and postprocessing functions? Like if the model needs some additional function that it is relying on. Is. There a generally it's a single code that you are deploying? It didn't get your question. Are you talking about while scoring, while. Building the model, the feature creation, et. Cetera, in case some feature is not direct and you need some preprocessing for that? Yeah, those recipes are present. Like you can do all sorts of transformations. Okay. And then how about pipelines? Do you actually end up building pipelines like data itself supports or do you. Even it does, it does support also data engineers on my team who build a pipeline. So they start building the model with a dump of data. But eventually the idea always is to have like a fully functioning data product EndToEnd, starting with a data pipeline on one side and like dashboards on the other end. And these data pipelines are also executed. On data IQ, is that there are some limitations I think we found on data IQ. But you can build them outside or you can build them within the IQ itself. They have a plugin for basically everything. Basically if you want a MySQL database, you want to run some recipes and then you want to store in SDFs or somewhere with a click of a button. If your hadoop environment is set up, you can just point to it and then it will do all the things. It's sort of a wrapper around all those things and it will just orchestrate it for you. I'm just trying to understand the capabilities of Data IQ. Let's say for a production model, is it like full fledged? It can handle high production throughput or what? Like full fledged. Okay. And it also supports GPO inferences, check prices. Yeah, plugins are there. But we haven't explored that much of the deep learning capabilities of data IQ. Okay. It's 100 million dollar revenue company. They have global customers, huge customers. They are like the second largest. Interesting. One other question was I wanted to understand little bit more about the infrastructure at Maverick. What is the general like what do you use for deployments generally internally, like for general code? Do you end up using Kubernetes et cetera by chance or is it mostly like one of the cloud platforms or is it multi cloud? So there are many teams working on that, right? We have Azure teams in which they use all the data warehousing capabilities of Azure and automatic capabilities of Azure. We have teams who connect snowflake to tableau if they want warehousing. Then we have teams who use data I do and so on. It depends on what the client is asking. Okay. And how big is generally the ML team, like the data science team overall at Admin right now, overall engineering, apart from that, I'm guessing there'll be more people on the engineering. Yeah. So biggest chunk is data validation. That will be so as data will be about 350 people. Okay. I include ML with some data analysts and tables and all that. That's what kind of comes under me. That is about 40, 45 people. Data validation will be I think around 150 people. Rest would mostly be ETL, data engineering, that kind of stuff. Got it seems like from a perspective of data, a lot of the things at least what you are doing is able to fairly take care of. And also I wanted to understand is there still anything where you face the bottleneck or a challenge? So they mentioned it quite clearly on the website itself. Like if you want to do heavy computations, then use your own database. Okay. What it will do is it will just point to that database. So if you want to run that pipeline, then in database computation and then the pipeline will go on. So tell that. So if there are heavy computations, do it on your own machine or your own database. But as you said, it's already deployed on your internal platform and your resources, even after that, do they require to do it on something else? What can happen that in one Linux server there is the installed okay. I have given it like 120 HB or something like that. Then in another server there is a MySQL database. In another server there is a PostgreSQL so data you can just point to all those servers and make a connection. And if I tell that run this computation on this MySQL, then it will run that. If I tell that run this computation on that so it orchestrates all that basically in a graphical interface. Has this caused any challenges to you or what kind of bottlenecks are you guys because of this? If you push the computation to Data IQ, then it's like for big data sets it's a bit of a trouble. This means that pushing it on the server, on the data it is running yes. Then it will depend on the machine itself. Right. Okay in that case what you have to do is you have to spin up another instance and then run it on that what exactly do you do? So you have many servers running spark clusters, you have many servers in the company which are running heroku clusters and then you push the computation and you just tell Data to point to that. Place. And is this process cumbersome? Data connections are very easy okay you just provide the server name, you just provide the credentials and then it will point to that place. Okay and you mentioned about a lot of AutoML capability so most of the use cases, are they solved by AutoML models that you're currently working on or what part of custom development. Especially see, the big value that a good data scientist brings in according to me is in feature engineering. Yeah, you solve that. The models you can use the standard models, you can build them and then you can deploy it. And then, as we mentioned, there are some specific use cases around. Deep learning, NLP forecasting. All of these are little, they're not that strong. They keep coming up with, like, upgrades where these things have gotten better. But for some of these things, we just prefer to build models outside and then make it like a cold recipe. So AutoML is native to Data IQ, but if you want to use deep learning capabilities and you want to set up Nvidia clusters and so on for that they have plugins. So for now deep learning is not natively supported, it's supported. Okay, do you see also developing a lot of deep learning models in the future or is it not on the. Pipeline equipment, not in banking so much? I mean there are very few use cases where you will use deep learning. You might doing like text or images or something of that sort, then you will end up using deep learning. But most of the standard business problems, the data is pretty structured and you don't really need deep learning. Okay, understood in terms of model explainability and inference understanding is that of concern. To you because. Of those supported natively by data. Okay things like subpopulation analysis, model feature importance, then all sorts of things. Shaft, lime, et cetera. Everything is natively present there. Okay. Actually, on that model, interpretability, they are focused a lot. Okay. Understood. Also, from your side, from Munichigma days and later on, at least, how easy was it for, say, data scientists or ML engineers to kind of test out models and even deploy them? Did you see any challenge in those organizations as well? Music Modesting would have been again, music operates in a similar mode of maverick. Right. We work mostly on the customer environment. We use whatever the customer wants to do. But Musicma had taken us, again, a slightly different approach to sort of solving this whole workflow problem. Right. And I think it's revolved quite a bit. Now, they have something that they call, as you see, I think you'll find it on the website as well. Okay. There the idea is what they've done is they've taken that entire problem solving lifecycle and split it into, say, data wrangling, data cleaning, forecasting, classification, whatever. And all of these are like readily available components. So when you want to build something, you combine these components and build your own custom monolithic code to solve that particular problem. So it has evolved quite a bit. Honestly, when I was there, the whole deployment thing was not such a big deal in ML. I'm talking about 20, 16, 17. Right. The whole idea of ML started around 2017, and it's evolved quite a lot. So there the focus yes. For validation, they had, again, their own product that they could use, and it would generate sort of some basic documentation as well. And then you could kind of add on to that. Right. So the testing part was okay, but deployment was not such a big deal in those days. Understood. And in maths, I think they've tried to build something very similar to an AutoML. They call it codex. I don't think it's that evolved on the deployment side yet. These companies are good at doing data science, so they've built that part really well, but not so much on the deployment side. Okay, got it. So I think that's pretty helpful in terms of the context. Maybe we can take three. You had something yeah. Just one more question you mentioned, since most of your models are being deployed at the customer end, do you often face up, like, challenges in time in terms of building something custom, let's say supporting something custom for each new customers, like some sort of plugins or like feature request or something like that? In terms of model capability, when it's deployed, or is it like yeah, force deployment, do you need to sometimes like, container? We usually don't do support, and as a company, we usually don't do support or any kind of projects we do. So that, again, is generally handled by the clients themselves. Okay. So client would get the ML model. And then after that and everything okay. See, validation is a big deal in banking. It's usually like a separate team and separate exercise in itself because how banking differs from other industries is excessive regulation, right? So the explainability is important then the model, everything about the model has to be documented not just from a statistical standpoint, but also in terms of the code and everything has to be explained. All those things are very important. So all of this is done and then it kind of gets handed over to someone else who usually end up supporting it. Someone else is on your client side, is it? Yes. Usually they keep this thing in house. And then testing and et cetera. They also do it on their own or you generate standard reports and share with them. So testing for a certain period we do it, but not on an ongoing basis. Okay, let me give a high level overview of your thoughts and then we can maybe do a more detailed follow up call as well. So I think in this space, if you look at it like there are two sets of companies. One company that actually is more on the AutoML side, which kind of help you take data, and they have model templates and a lot of things, and they're able to build and generate models for you and they manage the entire workflow post that, which I think data Iqh two and all in that genre, we probably do not fall into that space. We do not do anything around the model building. Our platform is built more as a support to the ML team to do things after they are building the model. So suppose you have built out the model in that if you need to do trained model deployments, then the platform supports that. So it will connect to your own cloud, whatever you are using. And there you get like instance on the cloud and you are able to run the training and it automatically takes care of managing the resources and oppose that. If you have built out the model and you actually wanted to deploy, which could be deployment in a test environment or a production environment, the platform makes it easy for the data scientists to do it with like a few lines of code or from the UI. So you will not have to worry about anything and you don't have to also worry about any language. You can use any language, any framework basically for writing the model. And then after that you pretty much wrap the model around, wrap the code using one or two lines and at the end you do something like a 2000 and deploy and then it will deploy to a corresponding space that is allocated to a data science team. And as you deploy you can add another line of code for basic monitoring that you want to do around the model in terms of data drift, in terms of the model performance and so on. So we are more trying to make that deployment process seamless, which involves like deployment on any virtual machine. So it could be a training deployment as well as real time batch, etc. And then in that we are trying to build a bit more deeper. So, for example, if you have a model in production and now you have a Challenger model and you want to kind of roll that out, then it allows you to do something like, okay, shift 1% of the traffic and then slowly shift to the traffic so that if you see the model, can you. Do that in parallel? Multiple challenges. And yes, you can do multiple challenges in parallel as well. You can set the limit of the traffic that is going to it and then you can say the condition on the basis of which it will sold out to the entire production. So it's more a slightly different pathway. I wanted to hear your thoughts on this kind of like what have you seen around this kind of companies and what do you see? If there is any value, where could we be a more better fit in terms of companies which could find value from us? I can tell you of one company that actually uses something like this, which is Netflix, I'm sure you would probably know. But they try to predict what score you will give something. You watch and they come up with that prediction. And they have five models running in parallel doing it. One is a champion and four challengers. And as soon as a certain threshold is crossed in terms of the challengers outperforming the champion, the model automatically shifts to production. This has actually been happening. So I think anything in that industry or even I think e commerce is another place where something like this will definitely it solves a lot of problems in terms of you don't really need to worry in terms of what happens if there's a better model you can just shift. Why I'm skeptical about industries like banking or insurance is that here the cost of a bad prediction is much higher. I'm just saying that in banking the cost of a bad prediction or a wrong prediction is very higher. Amazon gives me a bad recommendation, which it does a lot of times, right? It's okay, I'm not going to lose anything. Amazon is not going to lose anything. I think that entire space will see a lot of value with something like that. Be it retail, ecommerce, be it OTT companies in terms of because that's what they thrive on and they want you to spend more amount of time watching their platform or on their app or whatever. Right? So you need the best models there to work and to try and do a lot of A B testing all the time or try to manually figure out which model is working better. Something like this can be a tremendous value. That's what I think from what you have told me. So there's a lot of industries where it can be really useful. One thing to clarify the AB testing or whatever you call it, that is a small feature of the platform. But the more important part is actually being able to write any code and be able to deploy. Like basically write any model and be able to deploy. I wanted to know if that is something like I don't know if you have previously worked in any or on the Ms side as well and if you face any challenges there and eventually what's up for you? Like building a deployment platform that enables developers to be fast. Is that something that people generally need or generally you feel that the trend is more where people are trying to adopt more of AutoML solutions. AutoML is one part right? You come in after AutoML part is actually more important. The deployment part is definitely very important. The only thing is I'm not aware of the capabilities of these cloud providers in this space because they are throwing away this entire thing, their AutoML and all of them say that they have ML Ops, they can help you deploy faster and all that. We have not used any of this. I think that is one thing. You should possibly explore our research because we have not done it from our side because that's your competition. Things like model monitoring, after deployment, logging and these are like quite important things which we haven't found or we haven't. There in the AutoML platform. They're also kind of building and evolving but from an economic standpoint, I think the AutoML guys, the cloud guys they kind of give out some of these. Since you take AutoML, I'll give you this. So that part how good they are. At the end of the day, just because something is free doesn't mean it works, right? Azure throws power bi for free but people still want to use Tableau. They want to pay more and use Tableau. It's not an area that we are that aware of. But I think the cloud players are like your competition there. That's what I would think that is true. I think all of them have some platforms vortex and Zero. I think this is very helpful. One thing I would love to do, if it is fine, what's up for you and Aishek is for a platform like it allows you quick deployment of the model. I think a Vishek has been building models. I would love to see if it's possible for you to a Vishak test out the platform where you can take any of your models or even like basic models and try and see if you actually are able to use the platform to deploy and if you actually like the process. The aim is just to get feedback maybe because you have used data IQ and other things. I would love to get some hands on feedback, we can provide you access that should not be a problem. But even if you are able to kind of do it for a few hours, would really help test the platform. And we have loved to hear some feedback there. I think from a UCS perspective, like what I understand, at least right now, from a company's perspective, it's well sorted. So I don't see like it being used in the company right now. But at least at a personal level, I wanted to see if there is a possibility of trying to test it out once a week. But what is the time frame? Are you looking at any time frame? I will be flexible because this is more than half what we'll do is we'll share a login demo to you and a walkthrough of how you can use the platform and any time. Like if you can make some time over the next three weeks or so, then we can do a oneonone call maybe with Chin web and we can actually guide you on exactly how to do it. And then if you can test it offline with one of your models yeah. I think that can be done. Okay, that will be great. So if you can send a visit, email as well sure. With the details of what we have done, access to the platform and will be great to kind of help you. Definitely. Thank you so much. This is very helpful. Thanks a lot. Great talking to you guys. Always good to kind of know what is new and what is upcoming. So detail, more details. I think because of the time period, I was not able to share, but I'll share a Ppt as well, in which we'll be able to see more of what we are and happy to do. Any follow up. Bye.