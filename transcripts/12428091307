Just a second. Can you hear me well? Yeah, I can hear you. Okay, so nice to meet you. I think Nikon is traveling at this moment so he had to avoid the call. But it's fine, I can discuss with you. So thank you very much for taking out the time you for doing this and I just said some context and introduce myself. So do you know anything about what proof only as a company is building? No. Understood. Okay then I'll also brief you about that. So this is Jimmy, I work in this hello. I lost you in between. Sorry. Okay. Yeah. So. Hi. This is Jimmy. I work in the founder's office here at 200. I have been here for about six months. I work mostly in product and customer development and sales. But yeah, before this I worked at McKenzie as a management consultant. Graduated from It, did machine learning there. Mostly interested in NLP. So yeah, that's a bit about me. Would love to know more about you, sir. Yeah, I have a total of 15 years of experience with last year, eight to nine years into data science and then I completed my master's from I did leave and currently I am working as a principal data scientist with automation anyway put up in India. Understood. So do you guys have a central team of data science or is it. Like no, it's not a central team, it's mostly like associated with the business units. Okay. Business use cases that have been identified. So there are two main themes. Understood. They come under one sort of a substructure, but each of them as of now has a separate engineering team associated with them. It's mostly business driven rather than data science center. Understood. And what are these functions if you're fine to share? Like are these use cases or product specific? When I said business use cases, they are product lines, product features, the central product being Ram automation anywhere procession product is which tries to automate document processing and various other stuff. Process discovery and the document processing, it's all about automating the manual tasks that users might be doing of gathering data from multiple files and coming up with another file and emailing somebody or something like that. So it's what related to that. Understood. Understood. And what's the second product besides discovery? Yeah, the process discoveries based on the events that we gather, click points or whatever user is doing, trying to identify the exact process and then coming up with a port automatically without having to have a user manually fire it out. Understood. So automation of like that process no, that makes a lot of sense. Thanks for sharing that. So I'll just give you some context. True foundries trying to make like an ML of platform which is cloud Agnostic. We're based on top of Nets and our core competency is model deployment and everything after that. So you can deploy models on the platform and that's like across cloud. So we provide like a unified framework to do that across clouds and then also to productionize these models and have things like scaling, EB testing, et cetera. And then when the model is actually in production to also monitor it using data drift tracking. Concept drift? Yeah, concept drift. Or you're setting up alarms on Slas like latency, et cetera. So everything after that point is something that we work in. This is like a very high level introduction of what we are trying to do. Do you have any questions about that? Yeah. How different is it? Understood. Some of the things that ML Slow does not actively take care of and which is really needed in enterprise, et cetera, are things like role based access control, secret management, all of your security layer API authentication and also things like scaling of these provisioning resources for that managing Kubernetes for the scaling or parallelization of training with GPUs, et cetera. These are some of the core competencies that we have over something like open source ML flows. What companies actually end up doing is to build a lot of these layers on top of the open source software such as ML flow or you can even have metaslow, et cetera. But we out of the box support all of that. So it's more about the abstractions and the variety of use cases that we solve for. Also we are less opinionated in terms of use cases. So in the sense that if there is anything that is to be built on top of our platform, a platform is designed to enable the dev teams to actually build on top of it rather than being opinion and a lot of it. That's nice. I understand the ML flow when you said GPU side of the things, but vertex does allow us to the GCV part of it allow us to take and the business use case, to be honest. In our case, for model training purposes, we are sticking to one platform only for the deployment purposes we are required to go to. And even in deployment per se, we have containers, the images that are built of the train. Understood? Understood. I would love to understand the setup better. So you mentioned are you guys like Gcp heavy, natively, like the internal training? I would say everything is on Gcp. There as of now. With respect to training. Yes. Okay. To be honest. Yes. And what kind of access is given to data centers or ML researchers? Do you guys spin up VMs on top of it or you schedule jobs for model training? How is the interaction of ML researcher or developer with the cloud? Okay, so you know AI projects, right? I mean to say when it comes to Jcp, we have projects that we set up so production projects, the developers don't get any access to it, people have access to that. And we have another project for development activities. So in that project we generally get the master data, whatever the user non pi data to be specific. So we look to train our models on those non pi data. In another project of gcp. So effectively files don't move around, it's only authorized files and the production environment. Generally every developer has access to. Whatever. The data that has been NONPI data that has been pulled into this development environment. And he can spin up VM as per his requirement. Generally how we go is we have an image built out of the training project that we want. The standard bitter bucket is our code repository. So from there we pull the code, build an image, push it onto the Gcp with whatever changes that we want and adopt trainings are triggered and then for continuous learning that setup is like triggered, that is continuous training. We haven't as of now though it is ready for continuous training, scheduled training. We haven't felt that as of now the drift is as heavy as currently. We are working in an image. Image domain really doesn't have that kind of a drift. Where my earlier organization which used to do people churn protection for telecommunication operators, the feature used to vary over time. Even if the feature sets for same, the drift used to be very heavy. That is not the case with the images that we are currently working in. Understood. So there is no Cron job kind of a thing that is set up right now. Not in this because the thrift is very less. So you mentioned you guys use bit bucket and then you push code to bit bucket and like a CICD pipeline pick it up and build it. You also mentioned about VM. So what's the distinction for what? Currently we are going on adding on the features with respect to automating the automation part of it. Okay, so we understand to automate something we require require additional classes to be added to the objects that we have that we are already detecting in an image when a new application comes into the picture. Things like that. The only thing is we are trying to ensure that a user who purchases our product, when he realizes that certain controls are not being identified, he himself has to annotate it and then additional data into this one. So once we have that additional data in the pipeline so as of now we are triggering it. But certainly it's nothing to do with Cron jobs or any of those setup. We are relying currently on Vertex AI. That's why I related also most of our data sets that we prepare and the way we store it, the new data that comes in, we take it into that data sets. Then based on if it is a TensorFlow related model or Pythons related model, we do the pre processing and converted into appropriate file structure. And then you can move ahead with the training side of things with respect to if there are any annotations that have to be reviewed, that the user himself would have done it somewhere in the production. So we rely on the data set feature of the Vertex to see if we review once in a while if the new annotations have been done properly or not. And the main important for us with respect to annotations when we felt that we require additional training data, the Vertex itself lets us design a labeling task and assign it, outsource it for it to be so we are not really worried about that. So that is a very important feature with respect to for us that we can outsource the labeling task without having to really worry about something. So that's the ML ops part of it. The vertex has all that. But currently as our training is not multi, the use case from Truefoundry would have been helpful for us if, let's say the applications really vary from production to productions. And if you are not doing the non pi part of it, I mean converting the data into non personally identifiable data set if you are not doing it and training has to happen in the clients deployment space itself. If we had forced us to do that then suddenly a truefoundry because whatever the client's ecosystem that's where if we were required to train probably truefoundry solution would have been much more useful for us than the way we are currently moving the data out of the environment and bring it to where we have our centralized training structure. So that's probably the fit as such for us. We have understood. So you mentioned the client data is flowing out of there. So also how does it work? They provide that data, you pull it to your gcp bucket basically. It's not like they're providing the data. Okay. Slightly different. Our recorders record the data. So when the client knows, purchases our product, he knows that whatever the recorder that runs on their laptop. So process discovery, what is it? It runs on a particular laptop and that particular laptop, whatever the user is trying to do manually, it captures those events, the images of those events and automatically stores it into the cloud that's been deployed. They know the laptop data, whatever the individual user is doing, the nonpa, it is converted to nonpa and uploaded into the cloud. Okay, this conversion is also automated? Yes, it's a model and model specific to that. Okay, makes sense. Yeah. Okay. And the software is shipped to the local laptop. Is it like dockerizing? The local laptops have a specific set of software that are installed in individual laptops? Yes. Okay, but not the ML part of it. Understood. And just trying to understand. So do you guys also provide like a self hosted version of the platform or in which the users are not comfortable with sharing the data across? That's not the mode of operation right now. Sorry. Users also yeah. So do you have clients who are not comfortable with sharing data back to automation anywhere and want to sell posted version maybe on their cloud or something? Yeah, when clients say that, we really don't, I mean to say take the data out of the system. So he has a local deployment over there. So in those use cases, we tell them whatever the model that has been trained upon, that's what it will be supported as if they are docker images that we send out. So we tell them about deployment, only the inference part of it and no new applications will be supported over there. Okay, so is it like you have. Learning never happens on the client. Okay. So basically in this type of client, what you'll do is you'll send them like auto pretrained models that you have with your own data set, proprietary data set, and they can just use that, but there's no contextualization or retraining. Yes. See what are the applications that you run on Windows machine? Mostly the windows. If I take Windows as a use case, there will be Excel sheets, most of the robotics, all we are talking about is from the context of robotic processor automation, excel Sheet, all the Microsoft applications and standards app and various other applications around that. Okay. That ensures that we cover 90 to 98 more than 90% of the use cases that clients require. But do you like taking data directly from these Excel or Microsoft or the world or how do you fetch your data? Is this like a screen recording? For what? For training purposes? Yeah, for whatever data you're trying to use. So are you trying to say, sorry, I didn't understand. Are you trying to say that for Excel Autodesk and all your procedure, this. Is a robotic process automation? In robotic process automation, all a user would be doing is opening up multiple applications and trying to come up with another file out of those data that is present in multiple files. And he would be lucky to push it out. So that would be his process. Or else HR might be there who has to onboard multiple new employees. He will open up their individual information that is presented in Excel Sheet and try to upload it into their SAP or whatever, the business application. And they might be required to verify the certifications that have been put into a specific set of folders. So compare the marks that has been present. So we try to automate those kind of manual tasks with those. So that is a process. So verification of the file that has been uploaded against a mark sheet, the information that has been provided, and then the mark sheet. So that is a standard process that has to be automated. That is the process discovered through itself. So for these kind of processes, you would already have a free trade model. Right. Okay. And in the scenario that client shares their data back with you. So your application, which is deployed on their system, that way we'll send it back to your cloud. Is it for influence if they have taken cloud first ever? We are trying to push clients to cloud first because they don't require to keep on updating their software. So if they agree to cloud first, our cloud first, then automatically whatever is happening, we try to do the process discovery out of it and we deploy the bots back into their local machines. If they say okay, now we want it in our local deployments, the A 360 is provided in their local environments, but the training as such is not newly required. The way we are, the way the bots we are trying to use and the data that we require we are able to generate it the way we want it rather than rely on client data specifically as these applications are publicly available as well for us. They are not a very specific client specific application that has been that only the client has access to it. Those kind of applications are very few, requires automation. Okay, so you're using the majority 95% of your case. You can like ship a generic model to the client and it will work. Right. So there is no retraining part that is happening from the client, from the date of clients place. We don't do any continuous training or any of this. As I said, in a given application, in an event, it's rarely, it varies, it's not that frequent. Yeah, the way I was talking, in telecommunication or any other banking domain use cases where the data is mostly about a user doing various activities, these are mostly standardized. But if there is a use case that comes up, even we are in the journey, there might emerge tomorrow. That's a different use case. Yeah, I'm talking about till late. Understood. So the training then that is happening and the research on the front of ML resources and data centers, that is mostly on the generated data. And then you just retrain and ship to the cloud or release a new version of the software. Yes. Okay, understood. We continue to update our software, whatever the new features. If he is interested to purchase, the client will purchase and he will move. And he will not. Understand? So for developers like do they do local training on the laptops or in Gcp, do they use VMs? Do they use VMs on it? Yes, I was referring to yeah. In Vertex A, we can submit a job training job. We build an image and we give a reference to the point to that image and the command, Python command to initiate that training. It completes that. We don't explicitly create a VM, but the Vertex training job, model training job takes care of creating the VM and also killing that VM once the training is complete. Understood. The model folder has the model which we then converted into PB and then store it in the model register of the vertex here. Okay, so if the data center wants to explore the data, right, or do some visualizations, make reports, do they then spin up? What kind of reports you're talking about? With respect to basic data, understanding data wrangling which people do before the training of Ivorasm. Right? That you try to play around with a few architectures or in vertex. We can have that. Gcp has the bigquery, big table. The basic processing is sufficient over that, once the basic summary statistics that we get out of it, they are sufficient. And for images, you know, we cannot visualization the vertex. Visualization provides the required speed for object detection or classification. It provides the required statistics automatically. Understood. And once you push this model to the registry, from then on testing of it or pushing it to a higher environment, promotion of the model or actually deploying these parts are handled by the developers themselves or not by the developers themselves. There is a separate team which makes a complete build of the product. Everything goes to the product, not just developer pushing it based on this. So that is not this one yet. So we hand it out to the QA and the QA ensures that they gave the sign off for it to be part of the separate register handled by separate dedicated infrastructure team for that. Okay, so the QA team will test it and then the infrastructure team will deploy it, right? Yeah. Okay, and then monitoring dashboard. Do you like also monitor these models and all? How does that work? Let's say probably system metrics or maybe drift. Generally we don't get direct access to the production environment, not developers are not provided. So if there are only when there is insufficient, I mean, not happy with the object detections that happen, we get the required data, nonpa data on that. We look to generate additional required data and see what exactly is the use case. Okay. But as they are standard applications that we are trying to detect an object out of, so if we know the application name, we know we are happy enough to generate the data and play around with it. Okay, makes sense. Thank you. We really don't require the data from the production itself. We look to generate that data. Understood. And you mentioned on deployment, you are on multiple cloud, right? Why is that? So, like mostly it is a single deployment only once somebody wants it on premises, that's where it becomes slightly challenging for us. It is still an image. They provide the required, I don't know the on premises deployment part of it, but I have seen all the use cases till now. It's on the cloud, gcp cloud itself. Okay, but you do see like on prem kind of thing, but in these on premise, are you aware, are you aware, is any model training or anything shipped to them or anything like that. Like this. On prem use cases, training, we don't ship to on premises, they don't provide us the GPUs or any. Luckily for us, it's an image, image specific to so it doesn't take change with geography, it doesn't change with time or anything. There is no time based drift or it doesn't change with geography. So we are lucky enough, else we would have requirement for those things. Understood. These on prem kind of thing. So the data scientists still have the same flows. Like you push the model to your registry. Now packaging it, making an image, that all, who handles them? That for the on prem part, if you're aware. On prem part, the infrastructure team takes care of it. So once QA gives the sign off, so we check in the code to register this one. So there is a standard part of it. So they have given us the instructions where it has to be kept in other stuff. Understood. In terms of ML Ops, do you think there are any problems that the team is currently facing or trying to solve for or you guys will handle by vertex? Vertex AI we are relying on Vertex. It is solving most of our problems, though there are lectures around it with respect to features, the way we visualize the data and other stuff. Okay, so visualization when I said the Glitches, the alternative that we had evaluated was Label Studio for object detection because we were evaluating the data through data set feature that is made available. So every few, five or ten images that we validate, we see that it hangs. So Label Studio provides the same feature where we can scan through the unauthorized data and determine the quality of it without having to really worry about it. Understood. Actually, what exactly are you solving with respect to this data other than Vertex? Have you tried vertex. I personally haven't, but someone from the team must have. We are targeting this is a hypothesis that we are trying to test out, I think shared with you. We're trying to go the multi cloud route. So we are seeing like some companies adopting the multi cloud part, especially in enterprise software, because typically they do see clients where the clients don't want to share the data or something. I think yours is like a unique case because of the generated data. But a lot of these times we try to find places where people even have to ship some ML Ops components to their clients. And in these cases, typically the hyperscaler own ML platforms, they don't work because the client could be on any okay. So the training happens on the client environment is what you're saying, right? I mean, yeah, sometimes the client themselves want maybe just simple retraining after a certain set of data flows, some simple things like that or tracking the drift or some stuff like that. Making the data flow. I used to work with the previously a company called Flightex where they used to deploy models in telecommunication operators and this one so the clients over there? Yeah the data features itself vary from geography to geography. Forget about time. So that would be more through fundraising solution would be more relevant to them. I can understand. So you are trying to ensure that a few of the ML Ops frameworks are part of the deployment at the client side. That's what you're saying, right? Yeah that's like a niche that we are starting to explore. Obviously like the product does much more than that. We are. The solutions that truefoundry provides. So something like monitoring as you mentioned right? So one of the problems that we solve for is like we try to empower the developers themselves so that they don't have a blind side when it comes to their own deployed models. So something like even in Prod, if you want to see the logs or simple metrics or some stuff like that, if you want to set up your own monitoring dashboard or monitor like a few features which does not directly expose the data to you. That's also something that we try to provide or things like direct promotion from one environment to the other. It could involve like a few additional steps from the QA team et cetera. But actually keeping like a DevOps, making them self sufficient for a lot of these things is something that we try to do. Okay, yeah that makes a lot of sense. So you mentioned Flight text right? Would it be possible to make any refer, anyone or someone that we can talk to who works on DevOps or ML Ops there? Let me check and get back to Nico tomorrow I'll drop a message to Nico, the contact of it. Okay, understood. And is there anyone from your ML Ops team maybe who has worked more on the on prem side? Just trying to understand. I really don't have any contact with respect to that because the photos I really don't think so it is deployed on any of the on prem premises though. There is Automation 360 entire product but if anybody has purchased specifically that for the on premises, I really doubt it. Is there anyone from this infrastructure team that you mentioned? Right. Maybe they would know better. I don't have much of a contact with infrastructure team anybody? Okay, that would be difficult, but yeah. Understood, that makes sense. And if you would want to try out the platform we would love to give you a demo or something because we weren't able to cover that in the call today. I think that might be useful. Do let us know if you're coordinate with you on that. Let me check with one of my colleagues and see if this use case for MLS really interest us. Is there anything in future in our pipeline? If we are interested we both will get on the call and try to understand it. Matt is my director. It's good to know what exactly are being developed. So if we require any sufficient, we know whom to contact if we come across any such requirements in future. You mentioned Matt, he's your director, right? He's the director. He's my director. Yeah. Okay, sure. That makes a lot of sense. So I'll also give you like a brief. I'll send you some documents on the product itself that should answer some of the questions. And we would love to connect with you on mine. Yeah, but as I told you currently the solution that you'd understood business use case the way we have it. But still, there's no issues I'll anyhow informat about this and if required certainly. Thanks a lot. That will really help us. Okay then. Thank you. Have a nice day. Thank you. Yeah, bye.