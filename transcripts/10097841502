Hey. Hi, Chen. How are you? I'm good, how are you doing? Yeah, I'm good as well. Thank you so much for taking out the time to speak. Yeah, no worries. May I record this? Yeah, please. No. Cool. Sergeant basically telling that we had reached out through our LinkedIn and the goal was like, we were talking to companies in the machine learning domain who have been using machine learning, try and see what the use cases are and what kind of challenges they have been facing. So with regards to that, I wanted to know the stack for simpler and what are the kind of situations, use cases that you have internally. And what we can do in this call is I can give a quick introduction for myself as well. I would love to hear a little bit about you as well. So I'm one of the co founders at Truefoundry. We are building in the machine learning domain with the aim of making it easy for companies to deploy ML models. Right now, it take a note of time to set up the entire deployment in Pro. We want to make it as seamless as within a few hours, your entire deployment happens whether you use it for your testing environment or for your prod environment. All of that is completely supported. So getting started is simple, you can scale it and so on. So that's the goal. Like, how do we make machine learning productionization simple, but around that we want to understand more challenges and therefore talking to a number of companies also where we find fit, like we try to work with them. So I thought it'll be good to kind of connect and know from you. Yeah, sure. Background wise, like, graduated from IIT Karakpur 2013 batch, spent six years with World Quant, first in India and then US Singapore for three years. Then built my first startup in the talent space, sold to Inferage. And then this is the second startup we have been building over the last one year. That's very cool. Yeah. So I've been in the industry since 2018, so about four years now. So I graduated from Nsit in Nsit, Delhi last year, but during my tenure there, I was actively involved in research at IBM and did a couple of internships at some other companies like Geo and Oil in the field of machine learning itself. After that, wanted to personally, I'm also quite interested in startups and aim to start up myself someday. But I joined a startup called Skate AI. It's one of the first voice icon. Sorry. Skate. AI vernacular. Right. Yeah, it was vernacular now when I joined, it was vernacular. A few months into it, they changed their name to Skit. So exciting company they were working on, Voice AI. So I worked there, solved a number of machine learning problems, and after that I wanted to get back to NLP. So I joined simpler, more mature organization. And so I wanted to work on this side of building products which are scalable. In case of vernacular, the case was that mostly the solutions were very client specific. So though they were handling a lot of calls, but the microservices were just for one single client, they were just replicating the same microservice and deploying it for same client. Because of certain conditions here, the case is not the same. So I would say in simpler right now we got started with our machine learning journey about a year ago. Like a year ago we started shipping our first ML models. Right now the ML team is small, like it consists of seven people. But we are trying to solve a range of problems. And right now I think in the last year we shipped the number of services that we were doing this stuff. Like, so it's an internet platform. The platform is basically like social media, but for organization. So somewhat we are going in the direction of where Slack is going. So it's product for a customer, but all these product level changes are shipped across all customers. There's nothing happening specifically, no customer specific customizations. One quick question, what does exactly Simpler do? Like I did not have a very good context of what exactly do you do around that? Internet communication. Sure. So Simpler is an employee experience company. So the first product they started, it was the Intranet. So on the Intranet it's like a destination where company can put organization level announcement, people can talk about their interest, form groups. Basically most of the stuff you can do on Facebook I would say, but within an organization, that's the thing. So we are working on problems, the ML team is working on problems like sentiment analysis to analyze the overall sentiment of the employees in the organization, then trying to find out the topics that they are talking about and trying to understand what the people in an organization want with the help of machine learning. So it came first as a platform to just facilitate these communications. Now we are in the space of also finding out insights and delivering those insights to the leadership of a company so that they can make decisions based upon them. So here like one quick question. So suppose there's a company, what all channels do you plug into? Like do you plug into Slack link? Yeah, we plug into slack. So confluence all Jira based, all Atlation products. We are also connected with Google Drive. If someone's using Microsoft teams, we are also connected with that. So the platform gives right now the internet platform gives people the accessibility across all these different sources via the platform. So if you're searching, for example, holidays in India, so you can search it on the platform and if there are any relevant docs on Confluence or Google Drive or any specific conversations that happened on Slack for example, then you can all find it via the platform. You don't need to look for all these documents across different applications that get Gmail. The reason I'm smiling is my co founders were at Facebook and they used to use something internal at Facebook I forgot where. Anything they can search intra and it used to return search. I don't know what it's called. I'll probably find out and let you know. Over here, we work from range of problems which are specific to the employee experience industry, like trying to predict attrition rates to very conventional problems that you face on any platform, like search. So I think the biggest problem when it comes to scaling ML models at simpler is that all of these products are very the platform. Many customers use this platform, right? So the search and all the other models need to be fine tuned on the data from on their personal data. Right? So the problem that we are facing is that, for example, one of our clients is Zoom and one of our clients is Nutanix. So the kind of conversations that are happening over the Internet on Zoom and Nutanix would not be the same. So we can't use the data from both of the organization to train one sentiment analysis model, or we can't fine tune the search results based after combining the data. So we need to have individual models per organization. And we have over 400 customers at present. So maintaining deploying these models, having an infrastructure to train, deploy and monitor these models is presently a challenge that we are continuously working on. So over the last year, most of our products were sort of like trained on data combined from all organizations. But we are seeing that the quality of the results that we are getting is not what we can get if we fine tune it for a specific organization. So we're now moving in that direction. So I think when it comes to ML deployment, this is one of the biggest challenges we face that how do we deploy these models and then how do we monitor these models along with that? That's interesting because that's exactly the problem statement that we are trying to solve. So I want to understand a little bit more achienza. So you mentioned seven members in the team, right? So the kind of problems you mentioned, so where is this data? Like, does this all flow to your system? Is this purely like a SaaS product? So, all these data sets, we are using Snowflake. So we have a data engineering team that have set up the Snowflake infrastructure. So actually the platform is based upon Salesforce. So Salesforce is the so we get all the data on Salesforce and then the data is aggregated onto Snowflake for our experimentation and building model stuff. Okay, I'm just trying to take notes. Okay, so once we have all the data on Snowflake, there is this seven member team. Are all of these data scientists? Yeah, the seven member team is completely. So when it comes to the AI team, we have seven data scientists and then we have a different data engineering team. And right now we have an AI software engineering team which works upon the deployment part of the stuff. So once the models have been developed, then though we are the ones who are sort of designing the infrastructure. As in like what, for example, what AWS service would be the best to use to deploy that model. But we are not the ones who are doing it because these are separate team which does the deployment for us and also the integration of these models onto the main platform. But as we ship more models, we feel that there's a need for us to get an ML engineer to do the ML ops work, which we don't have right now. Here like one quick question. So when you say one is deployment, this deployment will happen on your cloud. We'll go into what that stack is, something on AWS and then integrating onto the main platform. So when you say that, what does that mean? Like, what does the software engineer here do exactly? For example, I've built a sentiment analysis API. So they would be the ones who would be deploying and creating the CI CD pipelines, et cetera. After that, they would also be the ones who will be consuming the API, integrating it with if we have to integrate it with some back end service, then that's their job, to add this component into the whole pipeline. Okay, so some back end service that is consuming this and fetching result to the understood. Now in your phase of building models and you do majorly data science work, or you also do a little bit of engineering work, because I saw your title was an ML engineer, right? Yeah. So earlier on I used to do everything from building the API to writing services to deploying them, monitoring them. Right now it's more towards just the experimentation part and building the API. That's where my role ends. And besides that, helping out the engineering teams to select the best architecture building, help them in designing the data pipelines, et cetera, but not getting I'm not doing the hands on stuff. Okay, so model building, when model building is happening, there will be need for training on data. Like, do you end up using a lot of GPU based models like GPUs for training or is it yeah, we. Use GPUs for training the models. And you run your training on where do you run your training? Generally we use AWS instances. EC two instances. Okay. AWS instances. And do you use a hosted notebook or something? We started out with hosted notebooks, but right now we have more defined pipelines, I would say. So it's not as good as directly connected to snowflakes. All the training is happening on its own. We are in this phase where we are using dumps like we're dumping something on S three and then we have scripts that we just have to run those scripts and it would pick the data from an S three bucket and train the model and dump it back to an S three bucket. So we use BBC for versioning our models and data files. Basically. Okay, so there is this and once the training has been done, do you use anything for tracking all of this experiment right now? No. So that's where we are working on right now. We're currently in the face of exploring multiple frameworks like ML Flow or Neptune AI. So we are trying to find out what works best for us. So that's one of the targets that we have in this quarter to finalize one of these ML ops frameworks. But the difficulty that we are facing is that someone needs to set. So there are different components, right? There is the monitoring, there is the model experimentation tracking bit, then there is the performance monitoring bit, then there's the deployment and scaling, then there's versioning. So all of these have different set of best tools? I would say so connecting them also requires a lot of effort. Making this whole ML model infrastructure ML deployment cycle requires a lot of work. So if Truefoundry is working on something that takes in this whole cycle and it would be really awesome. Got it. Understood. So experimentation, suppose whatever right now, wherever do you do you find out, okay, this model is good and now this model is to be deployed. Do you end up creating an endpoint and you give it to someone or do you kind of. Right now we create an endpoint and we share it with the AI software engineering team. Okay, and do you use any models. Server or something like I did not get your point. Are you using any model server currently for running your models like say TF Serve or Triton or any other thing? No, we were exploring triton but we haven't adopted it at the scale. At scale right now most of our models are consumed by APIs. So these APIs are deployed on Eks, so Kubernetes and we have Auto scaling enabled but we are right now not really exploiting benefits of GPU based infrastructure. Yeah, that's true, a lot of okay, fair enough. Understood. Okay, so now the endpoints are hosted on Eks, so you are using Kubernetes. But is your stack generally like AWS still? Yeah, we use AWS for everything. AWS, okay. Do you have a person who has expertise on Kubernetes in house or is. It more like yeah, we do have but that person is not part of the ML team in general engineering lead who has expertise. Understood. Perfect. Now I understand monitoring. Right now you are not doing anything automated kind of a thing. If I understand it's more ad hoc, is that fair? It's more of ad hoc only. We are doing offline evaluations on batches, but we don't have alert mechanisms? I would say so that's something that we are trying to roll out from the coming feature, like the models that we would be shipping this quarter. But most of our evaluations on model quality are happening offline that. We get some data out of the system, we evaluate it and get the metrics and see if things are happening. Well, that's one of the problems. You see we have a small team and we don't plan to expand the size of the team significantly over the coming years. So we would hire a few people, but not make it a 30 40 people team. So we are looking for ways to make most of these things automated. Understood. And I'm guessing ML engineers hiring would not be easy, right? It's not necessarily the predominant skill in the market. Right. So where is the biggest challenge? Like out of say getting an experimentation framework where you can track whatever you are training or say, deployment, where you can easily deploy a model and that can auto scale downscale and can serve the right optimization for your machine utilization and all and monitoring which is the highest priority. Do you have a sense of that? Yeah, we do have a sense of that. Mostly it is the combination of scalability and monitoring, I would say. I think deployment right now is not a very big problem that we face. As I mentioned earlier, we have over 400 clients and we're trying to release fine tuned models. So we're building a general model, but then we are fine tuning it on customer specific data. So problem relies on how do we first of all have 400 models, 400 rate files, then how do we monitor the quality of these models? For some clients something might work, but for others the same model might not fit very well on their data. So how do we keep track of it? An alerts mechanism to tell us where is there a need to work upon? Since we have released a lot of products, we are now starting with the process of reiterating and improving the quality of results of our services. This is where we are seeing this problem. How do we create an automatic feedback loop? Automatic feedback loop, okay, fair enough. Understood. This is very helpful achienda. So this helps me understand. I'll give you a quick context of what we are building. So for True Boundary, our goal was really to kind of help organizations that are in similar stages. You, wherein they have a few data. Scientists have a few models deployed, and they want to make it more scalable, which is you ideally want that instead of five ML engineers in the team pretty much like the data. Science team and with basic engineering skills, is able to manage this entire pipeline and they are able to go through that pipeline of CI CD, able to continuously retrain sorry, get feedback on the models, retrain it. And the retraining group is completed and models are updated automatically. This is exactly the problem. So the way we are building our platform agenda is actually enabling you to actually deploy your models in whatever way, like as an Eml way or a Python way, which you can do from your Cli, from your notebooks or from your UI. It kind of automatically creates an endpoint. It will deploy it on your Kubernetes Cluster. So our platform connects to your Kubernetes Cluster or you can use our cluster, whatever it's built on top of AWS, but it supports other clouds as well. And then as it connects, like any model that you deploy, it will come in with things like auto scaling. You can also kind of stop a service it automatically does and gives you some cost insights as well. And then you can deploy different kinds of models. For example real time models or batch based models and so on. And as you deploy a model you get system monitoring and ML monitoring by default. So system monitoring is what you are tracking in terms of usage of CPUs, memory, resources and so on. And then ML monitoring is some basic monitoring around okay, what is the drift in the data and features, which features are important and so on, how many features are deviating, what is the performance degradation and so on. And based on that you receive some alerts and then you can set up some customized alerts which will then flow to your email and slack. And using that you can again run the retraining of the model as well. So that's pretty much how we have built the platform. And we are building the platform first of all. Any questions here? So are you using Dags, some tools like Airflow or something in the back end or do you have compatibility with these tools? Yes. So Airflow and Dags like the pipeline system, that is something that is part of the roadmap. So it's not yet shipped into the product. But it's like basically we did first simple real time, then batch jobs and now we are adding pipelines to it. So that's how it is. So mostly behind out we'll be using one of this like either a Prefect or an Airflow or something and it will integrate and work on top of your own tax as well. So what is your system? Do you use a lot of pipelines? We have started experimenting with Airflow. We're not sure if we are going forward with it. One of our senior data engineers, they have worked with Airflow a lot and they have really pushed us for using Airflow in the future as well, for creating these pipelines. Because one thing that we so we work a lot with text data mostly we are mostly doing NLP. So there are a lot of standard transformations that are happening on the data, text cleaning and if we are text cleaning, stuff like that. So those are standard blocks that are used in every service. Instead of adding this block in every service or calling this API in every service, we were thinking that we host a different service for this and then things flow. We are also seeing something we want a lot of parallelization as well. So for example, if we have a single data point, we might want to know the sentiment, we might want to know the emotion, we might want to know something else on that single data point. So if we are creating right now we have what I would say linear pipeline because it does one operation at a time. But we are looking to do something like parallelization in order to reduce the latency of our overall system so that all these three operations or same data point is passed on to those three services. Parallel they run and then we just aggregate the results and put it back into snowflake. So we are working on it. Every time we start something like this, the tooling which we want to use, that comes in and we have to think a lot about it in terms of we have some architects so those guys always want us to if we are adopting a tool, they want us to at least use it for five years. So our evaluation for every tool is that whether it would still be relevant in the next five years or not for the sort of problems that we are going to approach in the future. So that makes the adoption process a bit slow, I would say. I see. Is the engineering primarily everyone? Yeah, primarily. So most of the Back End team is based out of back end is based out of India. But we have offices in San Francisco, Redwood City and in London. So I think Redwood City is mostly sales and marketing. I think a lot of people from the Front end team work in UK, which I have not been here. I still learning about the company. It's been like three months. Perfect. Basically I'll tell you. Like the way we are approaching the problem. No, I think it's very much aligned to the kind of solution you are looking for a product it might be useful to kind of see. And the good thing is Kamlok bought early stage fair. So we are working with a handful of companies at this stage. So we are also kind of making changes to the product in line with. Based upon the demands from your that. Helps us make the product better. But at the same time whatever value you need, you are able to get it. And we kind of do both like post on your cloud as well as host on our cloud. So that way everything is secure and it's not like a black box. So even if we kind of at any point you decide to let us go, you still have access to everything. So all your things on Kubernetes and everything remain. It's just that post that you have to continue to use it yourself. So that's the only thing. I would love to kind of actually walk you through the product once, showcase that to you and actually see if you feel from a perspective of utility there could be utility and if there's a way to work together. You are based in Bangalore, is it? I'm based in Guru. Given that you are already evaluating a few platforms, have you already evaluated any tools? What is the way in which you are thinking? So one thing you mentioned obviously five year duration, but anything else that's most important here? So most of our evaluations have been like for single tools. So we evaluated ML flow and we were doing an evaluation. We are in process of doing some evaluation for Neptune AI. So factors that we generally keep in mind when doing these evaluations for example, for example, when it comes to model management tools, then we are looking for how easy is it to do versioning and how easy is it to do experiment tracking and resource monitoring. Then we also look into the UI. So if the UI is very user friendly, then that's obviously a good thing. Besides that one thing that we also follow trends a lot. So we want to ensure that the product has a continuous support. So most of the tools I would say honestly, we were evaluating our open source tools that are community supported because we were planning to set those up on our own servers. So the support of the community and how actively versions are being released, that is one of the factors that we look into because we don't want something that doesn't get updated often and problems are not addressed. Yeah, we were also thinking of trying out Sage Maker. We haven't tried it so far because we felt that it doesn't have a lot of flexibility. So we are a bit hesitant, but there's some push because we have access to it, we pay for it, so why not use it? I can definitely initiate a conversation amongst the other members of my team that if we want to evaluate, we were thinking because the hiring for an ML engineer is becoming a bit painful. So we were thinking if there is a platform which can eliminate all of these challenges, if not most, then we can at least get started and we can have one ML engineer that can just maintain that platform instead of things. Actually around your point. The way we do it is we actually give if you need like cubectl access, we give. That way your developers can build on top of it and it's actually very seamless to use the way we are building it. And a lot of features, basically forward features, but user flows are being built. Keeping in mind the developers because actually, I don't know if I told about the team. But the team is if you look at it there are people Facebook, Amazon, Gojek, et cetera. So they have really seen very good engineering cultures. So Yellow, Nikonjan, Avishek, my co founders, they were at Facebook to offer internal platform type billander. A lot of learnings from there. Badal was at Amazon, one of the founding members at Sage Makers. A lot of learnings from there. He feels that Sage Maker is a city product. So we bought work is going on, how do we make something really nice and convenient and then there are other people from Mad Street then go check postman. So the engineering team is really good and we are building with the similar principles in mind. How do you make it scalable? How do you make something that actually supports the best in class infra like all the best practices like using Kubernetes, using Terraform for infrastructure underneath on experiment side we actually improved a lot on ML Flow, what ML Flow was doing because that was not really our core product. Our core product was deployment with monitoring but workflow complete connect like basic experiment but that is much better than ML Flow and all. So Eggbar would love to kind of seek your possibility as in there. Put a platform if you want or if you have close friends who will be willing to also come one or two people in the call and just see happy that also no problem there. But happy to work with you, to just kind of even evaluate like even if you want to try it for a beta period. Overall experience you can think of as like an extended team up under Lou engineering team. So all the support and all can come in. But yeah, basically forget about all of this right now. The main thing is if you would be willing to kind of see the platform and we can take it from there. Given that need that you have is. At a personal level, I'm very interested in checking out the platform but I don't want to give you unrealistic promises. So what I can do is I can initiate this conversation with my team. So we have a small team, I can initiate this conversation and see if they are looking to try something out and if we want to explore in this direction, if I get a go from that side then we can have a demo and possibly then I also invite other people from my team for that demo. Right. Thing is fine. That is not the most important thing I think irrespective of them. I would love to kind of show you the platform and learn from your use case. If you are interested use to say it, if not you use it for your personal use case. At least give us feedback, how many feedback and in that meanwhile utility then it's fine. Like you yourself feel good, you will maybe yourself recommend. It to the team to try it out or whatever. So that is also fine. Is this something we can do? So if it comes on to personal level, I think I'm down for going through a demo. Actually. Platform update gather. So next Thursday or Friday freedom. Thursday, I think 8 minutes next Friday I think would be better if Friday we can have something around the same time. As of now, generally my afternoons are all the product managers in my team are based out of us. The problem is that most of them call them set up in the evening and morning we have a stand up. So the afternoon period is I'll send. Out something for Friday next week ticket which is nine similarish time. We'll take like 45 minutes or so for the demo and answering your questions and taking feedback and anything. So I will also have like badal or avishek from my team. Joining basically is the CTO Bhadal is the founding engineer. So they'll be able to give much better overview of the platform. So we'll look forward to that. But this is super useful achievement. Like very helpful to get on the conveyor. Thank you for letting I think I made you guys wait for the login of feeling guilty myself. I'm just pushing this so I just decided just get on a call. I think you guys are doing a good job for trying quite hard to get feedback and love that spirit. Yeah. Thank you so much. I look forward to showing you and getting your feedback. Thank you. Thank you so much. Bye bye.