Hi, Vivik. How are you? So slightly super tired because we were running this initiative for one full day, and then I had to write the newsletter doing that. Hey, Michael. How are you? The other rooms are all taken today. It's very busy, to be honest. I have to tell you that this dark room setting is actually amazing. It's like the face. It actually looks really nice on a video. Maybe I'll take more meetings here. It seems like a very professional setting. Professional? Not in the sense of the work professional, but, like, somebody who's recording some things or go broadcast and stuff like that. Right. Are you guys thinking of making one? I guess, like when you move to your own office? I don't know. I think we operate out of WeWork for now, and we are happy with we work. It's easy. Convenience. Maybe you can convince them to get a dark room. This room looks like AI will take over the world. From this room, the monument will go. That's the control room. Yeah. Nice. How are you guys doing? How's everything doing? Very well. Like, I'm still spending some time here in India with the team. That's nice. And today I have a family function that I'm attending, so good too. That's good. Yeah. How have you been, Michael? Very good. I would say just a little bit busier than normal. I bet. January is always like that, and it's starting to calm down. I would say after this week, we had a few publication deadlines. I have one left for Monday, and then it calms down a little bit after that. I see. Okay. You said you have a deadline. I have one on Monday. Okay. And then another one, like the abstract has already been accepted. The conference is in May, so that there's time for that one. Got it. Okay. Just one quick second, Michael. Just give me 1 second. Okay. So I think the goal for, like, you never signed up on the platform so far, right? Like, you don't have an account currently. I thought I did, and I remember you had to resend me an email. Yes. After that, did you try signing out again? Let me see. Let me just go through my email and login. I would have the username and password too. Can you drop me a link to where I would log in? Oh, yeah, for sure. Okay. Do you want me to do that app? Truefoundry.com, right? Yeah, truefoundry.com. Yeah. Okay. 1 second. Login. Yes. You're logged in? Yes. All right. Do you want to share your screen? Maybe? What we can do is we'll probably share a collab notebook kind of a thing with you. Okay. Then what we can aim to do today is that you are able to run this notebook and kind of get to a couple of services deployed by yourself. And then, of course, later you can feel free to modify the notebook, do whatever things that you want. At least you'll have that much amount going for you. And then later we can work on adding the GPU part there. Does that sound okay? Fantastic. So you see this, right? Yes. It seems like I send you a linked to a collab notebook in here. Yeah. On the chat? Yeah. Okay. How would I get to the notebook? No, on the chat. I meant are you on the chat of this call? I'm sorry. Okay. Vivek, do you mind calling she and checking if he's able to join the call once very quickly? Let me do that. Let me check while Michael and I. Can continue getting started with this thing. Okay, so you said in the chat there is something where is the chat? In the chat of this call. There we go. Okay. So now you see this, right? Yes. So we can actually start running on the cells earlier. And obviously, as they always keep asking me questions, if there's some part of the code that is not clear, I can always clarify. Okay, so I'm going to talk through it. Right? Like installing pip and installing. So these are the python packages associated with true foundries. Right. So what's the difference between ML foundry and service? Foundry? Great question. So ML Foundry is the library that you use to log different artifacts. Almost think of it as an experiment management type of thing. So what you would typically use call it preproduction. Right. Or metric logging framework, where Service Foundry is more where you deploy things. So it helps you deploy your services, deploy your jobs, whatever you intend there. Eventually all of these things will be clubbed into a meta library. We'll call it true foundry. But like, we want to keep this modular. And that's why, like, we are building it out as separate components. Yes. So it might take a couple of minutes because they also have dependencies that it will install. Meanwhile, if we can open the next cell, we can actually at least start talking through it while this is getting installed. Okay. So here essentially what's happening is that we are authenticating this notebook with your account. Because at some point, we have to basically link this notebook to your particular account where the services will get deployed. So that's practically what we are doing. The hook there is an API key, essentially, that you have unique to your account. Essentially, I think it's done right. If you click, there is an error. That's fine, right? I think it should work. I think it's still saying that. Yeah, I think it should work if you move forward. So it's all finished. Okay, so we'll run this. Yeah. And now if you go to that link, basically. Create a new API key. Sure. Should I copy this or it doesn't matter. I can generate a new one later. You should copy this, and you will need to place this API key in the notebook. Basically, if you go back to the notebook. Okay. Just copying it for later. I meant permanently. Should I persist it? No, permanently, I don't think you need to copy it because you will always have access to the account. So that's fine. Okay. Now, once you've entered the API key, it's basically authenticated. Your notebook is now authenticated, basically. Right. And one of the things that you notice here that we have done is the way this is happening is you never actually need to save your API key in the notebook itself. That's one thing. So that way if you check in this notebook on GitHub or something, you're not checking in your secret. And the second you're also updating your environment variable. So that way, moving forward, you don't have to keep doing this. So those are a couple of things that are happening. Do you plan to have like, SSO integration sometime in the future? Yeah, we already built out with Google and we are already connecting now. Next, we are going to connect with Octa. Okay, very good. Now, this workspace, by the way, let me just explain this very briefly. I think probably you would not need to understand any more than the fact that there is a workspace in which things get deployed. But really the idea here is that think of a workspace as a small segment in your cluster, actually, that you have access to. Okay, so like a logical partition of the cluster, right? Like a slice, maybe. Yeah. Basically imagine that you get a slice of cluster for yourself, and everything that you do will get deployed here. Got you. Okay. And also some more magic that happens here. For example, your intro or your platform team can allocate certain permissions at the level of this workspace. So you need access to this s three bucket. They can give these permissions at the level of workspace. So that like over and over again, you don't need to go ask them for different permission, access and stuff like that. I see. Okay. So it's very important for things like when you configure, like your dev staging prod environments, you may have different workspaces for dev and it will only have access to talk to dev databases, whereas staging would have access to talk to staging databases. Some of those things get covered with this. I see. Okay. So we're creating a directory for Job, right? Yeah. Okay. Yeah, go ahead. Sorry, go ahead. I was just talking through it. I think it's best if you talk through it, then I talk through it because that way you will also remember it better that way. So I think it's better and I can just pitch in whenever you need help. Sure. So we're creating a file that just it looks like it's got some dummy computation to just go through a loop and wait 1 second for each loop. Run that pie. We're creating it in this directory. That's right. Yeah. So now basically the goal here is Michael, that you may be implementing a job by the way, before we move forward. Like meet Tree, she is on our back end team and free meet Michael. Nice to meet you. Yeah. We have been talking to Michael for maybe three or four calls so far and Michael is willing to michael basically found the entire concept of this fractional GPUs that we are building interesting for his use case and he wanted to try that out. So the goal for today is just to onboard Michael on the platform itself, like have him try out deployment of a couple of jobs and services so that he is at least familiarizing himself with the platform. And then later we'll add the complexity of discrete GPUs and the fractional GPUs as well. So that's overall the goal here. Right, okay. So Michael was just trying to clarify this one thing to you. That what you see here as a dummy competition, right? This is basically you can imagine this as it could be any arbitrary job that you have written essentially, right. It could be a model training code, it could be inference code, any of that. So that's practically what did you want to record us? I just wanted to check with everyone if we can record this call for two reasons. One, we can share back with Michael and second, it can also be shared internally. If there is any wants to go through. Sure, that's fine with me. Yeah, I think it will be helpful both ways. Good idea. So, yeah, Michael, basically it could be any arbitrary job. And you see that the cell with the run dot Python actually has no truefoundry specific code. It's like arbitrary python code. All you're importing is like a time library which is Python specific. Right, right. Now, once you have written this function, how do you deploy this function? Or how do you run this function on a remote cluster is what the next cell is telling you. Okay, so it looks like we're creating a job object and we pass into it. So the name I guess can be arbitrary. Right? Like whatever label we want to give it. So there's a build object and to that we're passing Python build. So I assume like there's other kinds of builds. Yeah. So I think the idea here is that this is practically almost like a boilerplate code that ideally you would just copy paste from the documentation and change a couple of parameters that you need to specific to your job, essentially. So passing in the Python version, run that PY. Will we be able to pass in like an environment. When you say environment condominium? Like a condo file or something? To say what dependencies? Yes. Okay, correct. We actually support passing like a requirements TXT or whatever that you want here. This is a simpler example, I think. In the subsequent examples of the notebook itself, you will see that we are passing some more things. Okay, cool. And then a deployment is so I guess job deploy returns. A deployment workspace equals workspace. I believe we define workspace above. Okay, so this will deploy the job to the workspace. Correct. Outdated version of service foundry. Should we upgrade Service Foundry to the latest version of something or is there a specific version that we should be running? I think the latest version should work. So I think you can just remove the version number from the pre installed and I think the latest version should work. Okay. And by the way, I think if we do this, we might need to restart the kernel after we do this. Let's see, it's likely we will need to restart the kernel. Can try running that now. Job deployment once again. Okay, 1 second. I still have the API key. I think you'll need to. I think it. Will probably this should fail, right? Because I'm making it. No, I think it's MP. So that argument tells us that if it's there, then just ignore the command. Okay. Why is this returning an error? Let's see. Okay, so I guess we have to research the kernel, right? I think so. First of all, before we start the colonel, can you go and check up on the log if the service found, it actually got updated? Like if you go up, scroll up all the way up where you installed things. Looks like it right downloading service. Truefoundry. Already satisfied? Yeah. So I think what ends up happening sometimes is. It installed that it installed the new version. Oh, no, it installed the same version. Okay, perfect. No, I think the previous one was 0.60 point something. Oh. 1 second. 1 second. Found existing installation of service truefoundry. Successfully uninstalled. Successfully install. I think it's just safer if you do uninstall Service Foundry and then pip install service and then just restart the kernel, basically. Okay. Yeah. The latest portion is 0.6.6. I think it is. Cool. I'm not used to code lab. What do you typically end up using, Michael? I use like a regular Jupiter lab or Jupiter notebook. Okay, understood. All right, I think we got the telephone uninstalled. Now let's go ahead and reinstall the previous month and restart it. And by the way, just notice that we uninstalled 0.6.6 it did install the right version. No, I don't think that is the right API key. I think you pasted service boundary in there. Oh, you're right. Okay. So if you read them, then just. Copy. That should be the right one. Okay. Now let's see if does not exist. 1 second. Let me just so we're going here. Go to work spaces. So I'll copy this name, right? You have to copy the sqn. So I think it comes from the Kebab window. Extreme right. No, in both spaces only. Just go to the extreme right. I think you can copy from there. Let me maximize I couldn't see it before. Okay. Under deployed up once again. So if you go to the UI, I think you should be able to see the build from there as well. You can see the locks here, but if you follow the link that you see in the locks, you can also see it from the UI as well. So you can see that it's currently building the image on the actual status. You can even see the if you go to the blue loading this thing, I think you should be able to. See the blue loading. Where is it? This thing? Yeah. What do these mean? I think I understand what clone means. What this promote means. Promote is basically the idea is you have different workspaces that you're using in different environments. Let's say staging goes to production. Got you. We sort of provide between your production and staging so you can make decisions on what to promote and whatnot to promote or what are the changes that you're pushing production. So it's more like experience that we're providing in terms of promoting applications to the production stage. Got you. Yeah. So I think while that is happening, maybe we can go back to the notebook because I think we can see the actual deployed so that we can actually move ahead. Sure. So is this blocking or is this no, it's blocking. Okay. So I can actually do anything. You should be able to run more, right? Yeah. So if I press stop, the deployment will still happen? It just stops the logs. I mean, if it looks like it finished anyway, but that's important. Okay. So it looks like we're creating a second direction. Once we saw a very basic job. We are just giving an example of an actual ML model that we're deploying as a job. And this is also a very simplistic model with IRS data set right. That we're also using the ML Foundry Library as well to do certain logging as well. So this is the ML flow equivalent of what we have built out. Nikunj. Truefoundry.com. Okay, so what does it offer over ML? I'm going to run this. What does it offer over ML flow? Is it like mostly the integration or. There no actually there's a lot of other functionalities that we have built out. So for example, number one is the entire authentication layer that we have built out on this. So like role based access control and stuff. The second thing is ML flow is actually very good for logging scalar metrics and plotting scalar metrics. But anytime you want to do any non just 1 second. Sure. Give me 1 second. So anytime you want to log anything nonscaler, you would notice that the ML flow actually fails. And that's the part that we have. Just give me 1 second. Okay. I think my sort that out. So I'll run this in the meanwhile. So you can trigger the job from that UI itself. Where would I trigger it from? Run job. Actually, I just triggered one for you. So I had an admin access to the sports page. So here you can see one complete, but if you want, you can trigger it again to see what it looks like. So like if I hit Run job, right? Yeah. In this case there are no parameters. But the idea is that in case you have parameters you want to pass, we can edit it there as well. Sorry, Michael, about the interruption. No, it's not a problem. The idea was they were talking about the ML flow, like what we offer. So ML flow, if you notice that you can log like scalar metrics and plot those. But anytime you want to log anything that's non scalar, like confusion matrix or any plots, basically things, you cannot make it interactive with them. We solve for some of those problems. And also you will notice when you go in the dashboard that we build out a lot of additional data logging functionality and all that MSP does not offer. I see. I missed this part. We are looking at the Irish train. So the previous one went fine. Let me check. Actually, I think Michael just opened one of these services that is there, but the one that we deployed the first time. That is the counter job. Yeah, the counter job. That went fine. Right? You can see the log. Cool. So that was like the first thing that we deployed. And now we are just looking at the iris thing. Yeah. Okay. I think it triggered the Irish deployment. I believe so. I think I saw it running. Yeah, I think I saw it there as well. Iris? No, this is an older one. I think the one at the top is the one that I just deployed. If you go back okay, that's fine. It looks like it's building the image. Yeah. So by the way, Michael, there is one thing that I want to add here just to give you a perspective that for example, you see how you can launch a job and then forget about it and then come back to it later. Right. Basically, it will just build and run it on a separate remote cluster and stuff like that. So stuff like this is also very helpful for your hyperparameter tuning and stuff. Like you can practically spin up like 16 jobs and launch in parallel. And they will just go figure out run in parallel, come back and give you an aggregated metric in one dashboard itself. So that way you can make things much faster, at least from a horizontal scaling perspective. But yeah, while this is happening, maybe we can go back to the notebook and move forward. Just so we understand. Okay. So I can stop this. Correct. And the deployment still happened? Yes. Okay. All right, let's see. Okay, so this was the question I had before, right? I mean, not just the Python environment, but it looks like there's also environment variables you can pass in. Right. So this is another layer that we have built out ourselves to manage secrets much better. And it actually integrates with your existing secret management layers as well. Like, if you are using whatever AWS parameter store or hashicop vault or something inside, we can work with those things with this layer. Got you. Okay, so service. No, I'm sorry. Go ahead. No, I was just saying any secret that is stored with two truefoundry, you can basically use any of the application jobs without having to refer them directly in the notebook or on your repository, wherever. Right. It's just the indirection there. Got you. Where is this hanging? Oh, this didn't actually interrupt Tom. I hope I didn't run it twice. Okay, now let's try. This is multiply normal uniform distribution. Okay. I think the point here being that these are like arbitrary Python functions. And what we're trying to showcase here is that how do you deploy these functions as endpoints, like API endpoint. Right. So you see that the idea here is that you can be writing any of these Python functions. All you do is you register these three functions with an object that we provide, and then you just deploy that object and you will see all the three functions are now available as an. API endpoint that somebody can that's very cool. Yeah. I can't tell you how many times I've had to do this. I have complete seriousness. Sorry, pardon me. I lost you. No, I'm completely serious. How many times I've had to deploy a small Python function as an API endpoint. Yeah. And I think when I think about it, just being able to quickly do it from like a jupyter notebook without any infrastructure management or something, I find that very useful for myself when I'm using the plot. Yeah, exactly. I remember at my old company, I had toy with building something like this, but just for how many times he would have to do it. Yeah, for sure. Okay, so I can same thing here, right? This is the deployment. I can press stop. We don't have to wait beyond a certain point, right? No, I mean, these are the logs I think there's an option to disable. Okay, well, we've not done that. So you see that the functions are visible. That's the one that's currently getting deployed. Once it is deployed, you should see a link that's getting generated where you can actually go and play around with the APIs, essentially with the function that we just discussed. So after this is deployed, we can play with the function from the dashboard or from probably from the Python SDK as well, right? You can do both. You can do both. Yeah. Oh, cool. Okay. A standard rest API you can even share with other people in the team, and they can also invoke these functions after that. That's the point here. Wow, that's cool. Okay, so this is instance request. So I think the point have you written Fast API code before, Michael? Not me, but we do use it pretty extensively here. Sure. The point of this function is that you don't need to do all of this that you're writing here generally with the platform. Truefoundry, but I think the reason we provide this is sometimes to specify that you can write code the way you want, like your standard Fast API code, and we will be able to deploy that as this as well. So I think that's one of the points that we're trying to make here. If you're deploying something simple, instead of you writing the entire Fast API boilerplate code, you just use our function deployment and that would work fine. In this case, we're just giving a more detailed example that you write your entire Fast API code and we will deploy that. That's what it says. Okay, so how is the infrastructure managed in the back end? Is that all handle that when we in the workspace. Say you want this function to execute on the GPU, right? Yes. Okay. Once you run this code in the next two cells, I think we'll show that. By the way, this is an example where you're passing like your custom requirements or TXT, as you can see. Right. So when you are actually doing the service deployment code, you notice that there is a requirements path parameter that you're passing, and this is where you are passing the requirements TXT file. Does that make sense? Where are you passing the requirements in here, right? Yes. You created a requirements TXT here. Right, I see. Okay. And then you're sending requirements. Yeah. Is there support for the new one, pip file or. The Pip file? Yeah. So are you talking about the poetry or the Pip file? Pip file. Pardon me? I know, requirements. I'm sorry, what was that? So you can either possibly requirements TXT path or you can just pause the list of libraries as well. So that option is there as well. Okay. It's a different key, but that is also possible if you just have three for libraries, you can just pause it as a business as well. Okay, yeah, no, I was asking about the new I don't know, relatively fast. I believe the new standard is tip file lock. The lock file. Yeah. I think that's a poetry lock. Yeah, we support that. Okay, cool. Is there as well. But we don't support Pip file yet. Okay. One thing that I want to show you michael, can you see my screen? I was looking at let me bring you up 1 second. If we just quickly look at my screen, I think in the context of what you're doing, right? Like you're basically deploying a service where it's a Fast API service that you're deploying right now. Right. And you notice that you passed some requirements or TXT, which looks like your code basically, like, in the notebook, looks very similar to what I'm showing you here. And there is another argument called resources that you can pass, where right now I'm just passing, like, a memory limited memory request. And this is where you'll be able to pass in more details around the GPU as well that I want to use, like, whatever, 0.2 GPU, for example. So the thing that you're asking about that how do you pass the GPUs? This is kind of how you can support, pass whatever resources that you want to consume. Very cool. Does that make sense? We'll see some of that as well as we move forward. But yeah, let's go ahead. Okay. So this is creating a survey. Is. Our service employed so we can go look at it? Hey, like she did we actually keyboard interrupts sooner than it was. Okay. Because I saw that there's an error that came this time. Can you go back to the notebook once? Michael I only see the keyboard interrupt there. Okay. So you said there's a way to turn this off, right? Yeah, we will turn this off. At least, I guess I was going to say, like, provide a link to the dashboard, but make it fully asynchronous. Yeah, I think that's more intuitive behavior to me. At least that makes sense. Yeah, there's actually a lot for that. It's not in this notebook. Okay. But I think there might be a problem here because if you look at the code right, there's a model version FQM that you're passing, which is basically a reference to a model that you should have in before. But I'm not sure if you're passing a system that belongs to you. Oh, I see. Okay. The way to generate that would be. To run the previous job that we deployed the job that we deployed before for training. And from there, you can get a model. Okay. So this one right? This was a training. Before we go there, I think let's actually look at the function service, the API endpoint before we do that. Michael okay. If you go back to deployments yeah. And you see that the second line, right? Like, you see that there's an endpoint link, fifth column. Okay. Yeah. If you open that endpoint link, it basically brings up the three function that we had implemented. Right. The normal, uniform and multiply functions, basically. Have you seen this interface before? This is a fast API interface, right? Yeah. The Swagger fast API interface. Correct. Yeah. So these three functions are already deployed. Maybe if you want, you can just quickly try to play around with the multiply function. Okay. So I would make an API call. Open up the multiply one. The multiply I got. Okay. I think you can go to the green multiply thing because that's where you can actually try it out. Right. On the top right there's. Try it out. Try it out. Right? Yeah. Okay. Basically. Right? So this should give me 132, right? Yeah. I want to see this right? Generating a select what will it look like? Zero. And. See how this looks. Yeah, there it is. Very cool. Yeah. So that way if you have implemented any preprocessing post processing function, you can always deploy it quickly as an endpoint and then just go play around with it. Share this link with whoever you want in the back end team or something. So people can just play around and integrate with your models or APIs or whatever are there. What is the max payload size? So this one, we have deployed it as a rest API endpoint. But if you wanted, you could also do gRPC based deployments and that will allow you to whatever pass larger objects like video files, et cetera. That's also just like passing like a single flag and it will just get to gRPC types. Okay. Like the idea was to copy the model. So if you remember when we trained the model, we had logged the model as well. So we just need to pick up that API endpoint. So if you go to experiments tab. On the left okay. I don't think we run the job. Yext, we did not run the job. Okay. We didn't run the job. Okay. So model this is an upcoming feature that allows you to go from model this one up here. But if you can go back to Jobs tab on the UI right, you can run it from there. Which one? This one? I think you can run that. This one. The one we did. Okay, so I want to run the job. Okay. This part of the logs here. Okay, let's go to the logs. Logs, right. So basically what's happening is, Michael, that we were trying to run our training job but I think we never ran the training job. So the moment we run the training job, we will see that it's generating a model and logging a model. Essentially right now that model logging. Imagine this is essentially an ML Flow model registry, right? So we have our own version of model registry. We just get the FQN, the fully qualified name of the model from there and that's the one that we will use in our deployment, essentially. And the other thing that I want to call out is you don't even have to use ML Flow or whatever true Foundry for your model registry. You could literally be logging a model on an S three bucket and you can just path of the S three bucket and that will work too. Got you. Essentially any URL that this code has access to. I think that's the main point here. Actually. I think you can close this logs. I think it's not streaming real time. If you just close this and just refresh this page once. Okay. You should be able to see the logs. Okay. Yeah. Complete. Yeah. To the registry as well in the logs. But you can also go from the experiment structure tab that you see on the left. Yeah, you have the experiment here. If you go to the model tab, you can see the model, but you also have all these metrics. Got you. So these are the metrics models, Irish classifier. And the way to identify the model is the Fqune again. So we had to provide this Fqune and the service that we just deployed so we can do it again. And you can even do it from the UI as well, because we just have to change one parameter in this case. I didn't catch that. So you said we have the FQN? Yeah. Can I get the FQN here? And let's go back to the deployment tab. Okay. Go to the Edit button on the extreme right for IRA service. Okay. So here you can click on Edit and you can scroll down so there's a UI version of whatever the deployment so that you see they can come down, keep scrolling down until we reach the enrollment variables. So here you can just replace the model version FQN with the one that you just copied in enrollment variables. Got you. Okay. And now we can just submit, I think, all the other parameters you can keep access. So, by the way, Michael, one thing I want to call out here is how you see there's, like, a clean transition between a Python code that you're writing with the UI, because I think we had triggered the job. From the Python code from the colab. And now all these things got popular in the UI and everything was basically just changing, like one parameter. Right. If you make any mistakes, if you want to parameterize certain things, it's actually fairly easy to do that. Yeah. Adam services live right now. If you go to the UI, you should be able to see the UI for the IR service as well. If I go to where. The endpoint that you see. On the right side. This one's. Okay. If I tried that out and I sent I don't remember what the IRS looks like. I think it's fine. Anything should be yeah. Some probabilities. Yeah. Okay. Yeah. Garbage. That makes sense. Okay. So I think we deployed this as a standard, like, fast API code now, and you notice that you did a lot of work to be able to deploy this. The next bit is what you see here is a model bit if you scroll up a little bit. So starting right here, right? Yeah. So, again, if you can change the model version FQN with the new thing that you copied. Okay. And the idea here is that, realistically, you don't need to write any fast API code if you just want to deploy a model inference endpoint, because all we really need is the model FQN itself, and we'll be able to deploy it from there. Basically got you. So you give it the this is the FQ and specify the resources. CPU limit, extra field not permitted. I think this notebook is a little bit old, so we might have upgraded some of the APIs. Do you remember this? Can you change the key model URL to Model source? I think maybe they'll fix it. Model of source. Okay. I think it might have been a recent change. Also the model version. Is that the one that we just deployed? Yeah, this is the one. Yeah, we switched it. How do you bring up the documentation? I know. Jupiter is like shift tab. How do you do it in Google collab? I'm not sure about collab, actually. I'm also forgetting the shortcut here. Yeah. But yeah, that would be important here in cases like these, actually to debug debug issues like these. Yeah. Value is not a valid DICT model source. Oh, so like we have changed the parameter, like everything to be a DICT here. Yeah, I think the structure had changed. I'm just taking a look at the documentation. One. Yeah, just 1 second. I think maybe we'll just copy paste a demo sample code to you, Michael, and you can use it. Sure. You can also go to the two of our docs, by the way. Okay. While you check this out. I think this might be more important. Yeah. The sun always comes up every time I use Google collab. How do you do the documentation control Shift? Is this for the docs? Pardon me? Yeah, to just link me to Docs Truefoundry.com, right? Yeah. It should. Pasted the structure in the chat as well. But you can find it in the documentation as well. Model Source and then it looks like. It gets you probably find a better example down there. Truefoundry. Yext Model Registry model Service FQM. Okay. Interesting. Why did you do that? I think we're making it more generic. Remember how I told you that you can do it from whatever s, three ML flow, et cetera, et cetera. So we are saying that you just need to use ours. You can use pretty much like a lot of other registries with us. I see other classes you have is hugging fees as well. So we have a class of that as well. You might have to do an import. As well for two Foundry model Registry. Right. I guess just copy paste the import code from their documentation. Do not remember the full path. Would it be in Service Foundry or would it be in I think if. You just go back to the documentation, we should have it there. Oh, yeah. From service foundry. Okay. That makes sense, right? Yeah. I think from spelling. I misspelled it. I'm just going to put it here. Let's try it here. Double promo. I think the format of this is also a little bit different now if you go back to the documentation I see. I think there's a spelling mistake. I think it's resources. I think there's a typo. R-E-S-O-U-R-S-E-S. Yeah. Okay. That's not here, is it? It's here as well. I think this is more elaborate, but we only have a couple of I. Don'T know where the spreading mistake is coming from, if that's what you're asking. Okay, let's see. It looks like it's running. Okay, it's good because we have a few minutes when our battery is also about to die. Yeah, I think we should be good. I think we have covered pretty much the entire thing here that we were able to deploy like a job, a service, a few functions and a model directly using the FQN. And we were also able to use the logging library as well a little bit. You can explore that more in depth if you want. I think if you go back to the documentation, I'll just help orient you in the documentation itself a little bit. So just look at the left. I think you will notice that the way the documentation is organized is you can deploy like a service, a job and also the model, like the beta feature of the model server, call it. Right. And if you wanted to understand some theory around what's happening, we have a lot of these concepts that are described moving forward. We also have if you scroll down, the parts that we have not seen today are like the monitoring components. I think we can pick that up later. And also the deploying on your own cloud and stuff like that, which is not relevant to this conversation for now at least. Right. One of the other things that you should notice, tree, where is the documentation for the tracking, the logging experimenting? ML foundry part? It's actually down below there. I'll tell you what to search for. Can we scroll down and see it. In the index and search for ML? Fountain. Here? I think if you go to the search, I think we have it's inside. The deployed job tab. You can see a tab called Job. In the documentation or in on the left side. On the left side you should be able to see deploy job. Like I say. I got you. Okay. Yeah. So this is where we have a lot of things like how do you log a model, how do you log a data set? How do you create runs, parameters, artifacts, et cetera? You can log all of those things. Got you. Very cool. Okay, cool. So I think as a follow up, Michael, from here, as a next step, I think it would be really nice if you can play around with the notebook that you have. Maybe edit it to your own use cases like the end of the model that you are working on. Try and build that out. Maybe deploy this even a smaller version of it. I know that you needed GPU. Absolutely. But at least familiarize yourself. And we also have models that we support, so you can try and deploy some of that if you want. And if there are any issues, just ping us on WhatsApp or something. Or we can create a slack channel if you prefer, and just ping us so that at least get to that point where you are comfortable, like running your own services on the platform. And the moment we get there, let's also try out running these on GPUs as well. Okay, fantastic. Does that sound like a good Yext step? Yeah, I'm excited to start playing with this. Nice. Awesome. Great. Vivek. You had a point. Yes. So, Michael, you were mentioning someone else will also be part of the experimentation process. So is it going to be someone else as well, apart from you? Right now? Probably just me, but we're bringing in more people as we speak. Eventually, I'd like to have some of the engineers try it. Okay, do let us know if we have to create more ideas or user profiles. We'll be happy to do that. And any timeline that you're looking at, GPU is one thing that we are working from our end, but something from your side. So probably like to play around. I was going to say a couple of weeks to a month or so. How does that sound? Got it. You mean, like, try out the entire GPU between you and us? Like between a couple of weeks or so? Yeah, to play around with this. I have a few tasks that I think I was going to try running on this. Like an NLP task. Okay, sure. If we can squeeze that in, whatever, the next week or so, take two weeks at max, I think that would be nice. I think if we push it to one month, I think it will we. Don'T need a month for that. For playing around with this? No, not a month. I won't start until next week, unfortunately. I still have the publication deadlines, but I will play around with this. After the deadline, basically. Yeah, sounds good. Okay, sounds good. Awesome. Thanks, Michael. Let's get going. And good luck for your deadline. I know this is always, like, the most exciting time of submitting a paper, so have fun. Yeah, the most stressful as well, but no, I'm excited. I really enjoy this. I can tell there's a very coherent vision behind the product, and it's much better than Sage Maker. Thank you. Very good to hear this, Michael. Okay, thank you so much, guys. Have a good one. Bye bye. Bye bye, guys. Thank you. All right. Thank you for joining in the last minute. I was just wondering, like, I feel like a B Hamara UI is pretty good as well. So for people who understand terms.