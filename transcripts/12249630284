Can you bring me the link? What's the name of the dock? After we take the notes of this. Thing, I just put it on. WhatsApp? Right now? You'll put your five minutes back. Okay. I found it actually. Multicloud report. Hello. Hello, how are you? Hi Muthu. I'm good, how are you? I'm too. Amazing. Awesome. Great to connect after a long time. Now. By the way, I have to thank you that your tips about Telegram got me my visa. Thank you. Yes. You said you are going to Singapore or something, right? Yes, I went to Singapore. Basically what ended up happening is in Singapore appointment. But then people told me I talked to my lord and they said you should not risk it because you don't have a strong enough case to apply in Singapore because you're anyways going to India after that. So if they notice that way and you say that you're going to India, they will not appreciate it basically. So you're just risking things. So then I cancelled that appointment and then we went through Telegram groups and one good day they opened 100,000 slots in 2023. So that's the one that we picked up. Basically. I followed the same thing clearly. I got what the date, what I wanted and was there in India for a book of months. I was in India for a couple of months. I see a lot of things to catch up after three years. Are you back in the bay now? Yeah. Awesome. Also Muthu, I don't think I have spoken to you since you decided to invest Nikunj Truefoundry.com as well. So really welcome on board and. It'S great. I think we connected first maybe almost a year ago now and joining us. That's interesting actually I do invest in some stance through investment funds. This is definitely an interesting piece if you ask me like what you are working on maybe like close to my heart. So from that perspective, like I thought okay, it's good to keep a follow up and it's also learning for me in that way. Glad and mother, you get our monthly update emails, right? Yeah. Okay, great. Awesome. All right. And by the have you met Jerma before? No, I know only you and I think I know one more guy, forgot his name. Akshay maybe? Akshay is it actually like he first meeting when we were having discussion with implementing in Balbix at the time we discuss with. Understood. Okay. Anrag is one of the co founders. We have third co founder of Bishik and Chinmay is member of founder's office. Chinmay, if you want to briefly introduce yourself, either you're on mute or something, we are not able to hear you. All right, no problem. Anyway, so basically like Chinmay used to be a data scientist at Apple and then he worked as a consultant with McKenzie and then he joined us in the founder's office role. Right now currently overseeing a lot of new initiatives at Truefoundry Customer Success as well. So he's helping on that front. So the idea was that we are recently kicking off a new initiative where we are trying to learn about a specific hypothesis. And the hypothesis being that does our platform serve the needs of companies that are trying to deploy across multiple clouds? Deploy machine learning models across multiple clouds? Right. Either across multiple clouds or hybrid cloud. And like, one of the other hybrid cloud, basically. So on Prem and the cloud. And we had a few questions to you regarding that we wanted to understand from your experience in this domain. And it's fine if you don't have that specific and experience in deploying across managing platforms. We still have a few more things that we wanted to learn. So the idea was basically just take this call and learn a few things about this call. I think I have a hard stop at 08:00 P.m. PST, but I think our call is blocked until 815. So Chinmay might continue for the last 15 minutes, but let's get started anyways. Sure. So, first of all, maybe one of the things that could be helpful is you're just walking us through that currently, either at Balbix or, like, one of your past companies. What was the overall cloud setup like? Were you all using one specific cloud? Multiple clouds? What has been the situation so far? One cloud. It's mostly AWS. That's the only one using it, though we do use multiple clouds for other purposes, not from the perspective. Got it. Understood. One thing here, has any discussion ever come up about using multiple clouds or there were no discussions. What was the overall decision making process like, if anything? No, I think maybe, like, if I consider our current capital bulb, I don't see a need for it because we collect the data into our site and then we do it. So we may need data from different clouds of our customers. That's different. But otherwise we being a SaaS product, web based. There is no need for multicloud, at least at this moment. So maybe in the future, if we get a large customer and they come and say that okay, you need to deploy your competency in my cloud, and that cloud intra is different, cloud intra, then we may consider. But at least I don't see that happening anytime soon. But in general, I think there's no need at this point. But if it has to happen, it has to happen that way. Got it. Understood. Actually, like, you bring up such an important point on that because I would love to understand that. At what point do you think so I guess maybe help me understand what is the current ICP ideal customer profile for Balbix? What's the segment of companies that Balbix targets currently? Okay. The thing is, like us, we are into cybersecurity quantification domain. Any companies? Probably. I know the sales team has a number of devices that they expect in a company so that they can on board our product, which means maybe, let's put it a number, like 15,000 devices, maybe combination of servers, laptops and any kind of hardware, software kind of things. So from those perspectives, they do have a kind of a minimum requirement on how many of these devices in a company so that they can substitute our product and that will help them because our product is mainly for CSOs. So the CSO varying, especially if it's a caesar's presentation to the executive. That's one major part of the play, what we have. So thus, from that perspective, once we invest money in us and of course, like that, they need some kind of non human manageable requirement, which means that they have so many devices and kind of things and they have to track all those devices on their vulnerable test. And if there's an impact, they are able to quantify them in that dollar terms and present it to the board. So from those perspectives like that's when they will need it. So any company so we we are varied in our customer landscape, different domains. And these are not like typically Fortune 500 companies or so. No, not understood. Because it turns out that Motor is like exact use case that you mentioned. In fact truefoundry. Because we deal with ML platform, we actually deal with machine learning data immediately, even for the first customer. Like for example, even if Baltics were to use Truefoundry, most likely Balbix would have us deployed on their cloud. Because you would not want to send your data to us, right? Yeah, that's one part. The other part is from Baldwin's perspective I know what happens is being a suburb of dominant there are a lot of security audits happens here. Most perspectives they are pretty much worry on involving third party and taking a risk kind of that. So that's one of the main reasons like they put forward to me saying that okay, at this moment we are not ready because we have limited resources with all those things and we have things to handle the customers and won't be able to take those additional audits and compliances. No, that makes sense. I was not suggesting in terms of usage or something I was just saying that for such that even a smaller customer when they adopt Truefoundry, they want it to be deployed on their cloud so our business by nature is multi cloud nikunj Truefoundry.com already at this stage is serving across three clouds. That's what I was suggesting yeah, it's the same thing. So your world and my world is seeing the only thing is like at this moment at least we have not come across a customer who says that it's not all customers from our site they ask us to deploy all components in their club. They don't do that. So it's like whatever you provide them and then they use it. But the only thing is we go with a large customers and they are still very like what we are and then some of them do ask like can you come and deploy in our environments? And then until now we are lucky at least I would say we are in the same cloud with our customers and we are fine deploying everything but anytime they come up with a requirement saying okay, we need to deploy in a different cloud that's when we'll have some kind of transit. Understood. So basically so far you have not actually deployed on any customers cloud even on AWS. Some companies have asked you have not done like everything is safe model there is no on prem models per se. We have on prem model but I can't discuss further on the customer side. Okay understood ICS. Okay understood. Got it. Now I will switch gears a little bit and I will just try to understand what are some of the updated deployment side of things that you are experiencing today because obviously multicloud is one aspect and once you get to multi cloud, how you manage each individual cloud is another aspect. So maybe we can reuse some of this time to discuss a little bit of that, if that's okay with you. Sure. In terms of training and by the way, first of all actually I know that last one year you were focusing a lot on the data platform side of things and there were some operationalization of machine learning that you all were thinking of doing. Actually if you can just help us update a little bit on that front. Did the team actually prioritize the ML platform side of things and actually build out some things there or most of the efforts were on the data side. From your perspective, like coming back to the training process, we train our models in GPU node and then we the artifacts of the model. What the train models look, we store in our history packets. So we want to have a kind of we have multiregions AWS kind of thing so we want to replicate and we leverage the existing model of existing AWS history replications functionality earlier. Like there is to be kind of a manual approach where we use runner script, couple the files industry and further run scripts to do all these things. But we started leveraging the Airbus internal list, three applications to copy once we train the model and then of course it will be copied to our digital kind of that it's a breaking for us during deployment time. Of course it will pick the latest model, we version the models industry so it can have a manual approach but it works for us. So we use SDBC for our version and of course the code remains in Gate lab and then that is version with what's on deployment kind of things. Model architects are versioning in the SD packets so whenever we want to deploy it takes the latest based on our configurations and then moves forward. So until now it works for us. So other than that, from ML perspective he asked me like we introduced Airflow for our jobs. So Airflow was one major thing that because we were running crown jobs and as you may know that it runs fine but only thing is like you can't have a proper orchestration or kind of a dark functionality or anything like that but that was amazingly super if you ask me. Like with the kind of functionality is what it provides the way we can split our jobs into DAX and ensure that we can execute our jobs in a much more precise manner as was like whether there's an issue or failures, the way it's restart the task and we integrate their Kubernetes. So we were basically a kubernetes shop in AWS, and Airflow very well supports us kubernetes based execution parts for workers. From those perspectives, we integrate that way, and then we were able to successfully migrate our few jobs, and we continue continuously to do that. And we have more, much more jobs across the organization that we need to migrate. But the intrawise it's all set up and then we are able to track it. It's really going good perspectives. That is one major thing from a ML perspective. Bonus on the infrastructure, what we needed for our jobs to take care of the database we had some work that we introduced in graph databases and other things. Oh nice. Which graph are you using? Neophorger or Tiger graph of New forgery. Okay. That'S another part. But for me the next priorities would be like introducing Jupiter hub wearing like user by need Jupiter notebooks. But for me my thought process is always like how can I have a Jupiter Hub? Like the node group will come up in the back end if it can bring up a GPU node or something like that on demand basis kind of things. That was my thought process. Today we have people use notebooks individually though we have a common location varying like this for all the information kind of things though it is not lost, of course like it's being in a node and then it's not. In version or anything like that so it works but I would say it is not a proper management managed kind of service going to Japan will provide us access to user defined access and all those things perspective but still I'm not fully convinced with respect to how bringing up. So today if I want to train my model, I have a note that runs always good for us. Sometimes we don't use it, but we use it mostly, but we don't use it still I pay for it GP runs I want on demand GP nodes and also if possible I can have notebook based access in the front end and then the process will happen in the GPU, the back end of things. So that is my next priority that's work in progress and also parallel. We are looking into ML flow kind of over here. I was initially thinking about the Sage maker which was me most of the things what I needed. But here from a cost perspective we are a little bit concerned about rated with cost going things. So mostly even if I can save a dollar probably we look for handling open source systems. From that perspective we want to have our own things. Even for Airflow, there is support from AWS but we say that we will go with our US, we'll deploy our Airflow on our home and then you don't manage it. So that's what we are looking into. It understood. And you mentioned like you're trying to move to these hosted notebooks like currently how do the developers do it? Like do you spin up a VM or like where do they run these notebooks? Because I don't think the data might be like loadable in the local devices, right? Yes. Great thing is like we have our GPU node that is running always. Mostly the notebooks are used for training purpose running like the data pre and features, all those things happen over there. So that has been up in the GP node and then access from a local host and being upon. So that's what we do today. But yeah, that way the data is shared among users. It's not lost in a local perspective while still not fully convinced. It should be in a node. Rather it should be if it can, or something like that, and then bring up a note of such a magnitude and then process it. And then once your passing is completed, you just take it out rather than keep it running. And right now, do they submit jobs as scripts currently? Is it like done through notebooks? Like the development part? Come again? Sorry. Yeah. Do the developers submit jobs scripts on this node or do they have access through notebook or something? Like both, they do both actually. So for certain purposes they open up a notebook from the node and then accidentally locally and do it. And sometimes they directly go there and then try to run some kind of scripts in the node itself. So both they do. Just trying to understand like usually we've seen like with notebooks there is sometimes a problem of debugging etc. So just trying to understand like at what phase do people prefer using notebooks and when do they move to the scripts? Is this like a generalized thing that people do or how does development usually work? They start with notebooks and then go. Okay, so there are different use cases over here if you ask me. From micro, people deal with microservices, people deal with exploratory jobs and people also deal with model training and anything to do with model kind of things. So for a micro services perspective, they don't need a no doc in our case, like they handle it in a different way and do it from an exploratory phase perspective. We look at different different use cases. For some use case we need Spark for distributed processing that we use kind of like EKS or something like that and we use that. For some cases we use notebooks. I think earlier we were using a lot of these exported jobs using notebooks, but later we decided when I introduced the database packaging for snowflake from then on, what's like we reduce the usage of notebooks. Thing is all the data is stored in a structure format in the snowflake so it's that they can query the data anytime and it's easy for them to work with that. So they don't need to write a job as such because our data is in our data lake. The data lake data is in history bucket variants in the JSON format. Every time you have to run through it, you have to load the data, all the data that is going in several terabytes and all those things rather than that. Introducing data pipeline for our snowflake and teams are able to query the data faster and only the specific data what they need without having a concern of all the back end infrastructure this is dynamic enemy like whenever I need it, I just want to go and complete it out. So that has almost killed our Explorer data analysis using notebook that part is gone almost. So as of today, the notebooks are mostly on the training purpose. Of course, during the training purpose, even though the data is coming from, they will query the data from the snowflake so they need to have some kind of a preprocessing and all those things, labeling and etcetera. So they will do all those things using notebooks. So notebooks are still heavily used on the training side of things and yeah, that's mostly where we use notebooks when. They'Re using it for training. Is it like for the initial model? Because I believe that you don't still have Githubs for the notebooks, right? Is it just for the scratch testing of their code and then they move to Vs code or some other ID? Other jobs are continually submitted, the training continually done through these notebooks. Okay, for training purpose, let me put it this way we do use GitHub for storing the code that will be the notebook code that still resides in the node, I would say okay, but the implementation of this functionalities from our models into our code is in GitHub that's different. The notebook, whatever the output of the notebook exploration or exhibition still reserves in the node itself. Understood, that's what mostly we use for I think there's nothing at least I would say that we would like to have in the maritime notebook using a kind of version controlling things which we don't have today. But at this moment we are not concerned much because the notes are always pretty stable and the teams are able to cross utilize things because it's in a common node rather than like that and basically pre prep data. That's what we do, that's what we use, notebooks. Understood. One quick question. Are these notebooks currently running over a Kubernetes cluster or some. Yes, we are. Everything is understood. Okay, I see. Actually, one other thing, by the way, most of the reason we are asking a lot of questions on this is there are two things that you mentioned, which is, as I mentioned that this call, initially we thought that we'll talk a lot about multi cloud. But given that we're not talking much about multicloud right now, there are two things that you mentioned which caught our attention. One is this Jupiter notebook thing. We are actually actively collecting requirements from people who care about hosted notebooks. Okay. So that's why we have a lot of follow up questions on this. The second thing that you also mentioned is that you all adopted Airflow recently. And we are also exploring a Dag orchestrator for our system. So we would also love to understand if you ended up exploring a lot of other alternatives, were you already familiar with Airflow, etc, etc. So that call, I don't think I want to go deep into that discussion right away. I think given that we're talking about notebooks, we can actually finish maybe that discussion. But I would love to set up a 30 minutes call between you, me and Abhishek. Abhishek is our CTO and the third cofounder that we have anyways, not met. So it would be nice for you to meet him as well, where we can do a deeper dive into your choice of Airflow and like, you know, like basically that infrastructure choice overall. Sure. One other question that I wanted to ask on in the context of notebook itself, muthu was you mentioned Sage maker is cost prohibitive. You are currently using Snowflake, it seems like, for a data warehouse. Right. And you are never considered data bricks for your delta lake, for your data warehouse. Basically we are actually so there are different use case for which we use that. So we have a data engineer. Are you using them? We are introducing that I would say things like it's not data picks up that data picks. It's not just the spark. The open source version of the delta lake is what we're using. Open source version of delta lake. Okay, understood. So you're not getting like managed data brick solution? No, we don't. Because here, like I said, cost is a big consideration. The only thing that I have paid for is Snowflake. Other than that, nothing. So that's what it is. But Snowflake was a big decision because I had prior experience working with Snowflake. So I was able to convince, I would say with my CEO and other CTOs. Understood. And it's been a year, it's been fine. I could demonstrate at least from a cost, because there's a lot of concerns from the cost perspective when using. So using my experience and I told them, okay, this is the cost and if you agree for it and it should work and they agreed for it, it worked on me from the perspective. Luckily, that's why I was explaining to Jin by saying that, okay, earlier the team used to spend days writing notebooks to load data from history package of data leak and then query the data and then do a lot of exploration. Anytime you have a new customer comes in, they ask a lot of information. So it's all in our data and we have to provide this information in a meaningful format every time. You need to go and load this data in a single format rather than you put in a database, okay? And probably you use a data lake and the presto database or something like that, presto to go to the data. Or as simple as it may be, like you have it in three bucket, the data and you can use a kind of like Athena if you're not wrong and use it. But Snowflake had a lot more functionalities and based on my experience, but you can use it for a lot of team members. Like this thing is like when I introduced it, like, a lot of the current developers, they used to come up and tell me, like, hey, I used to take two days when someone asks this question to provide an information because I have to do all these things today. He gives me ten minutes. He writes the query, gets the data and presents that done. Apart from that, it simplified the training model also training process earlier. Like first they have load terabytes of data and then it sends some format in the data, like and they need to convert into their own format whatever they need and filter out and all those things here, nothing. You write your inquiry, what you need and whatever conditions you want to add it over there some part of the preprocessing or most part of the preprocessing in the query you've done. And you get the data set output into CSV format, which you can easily load into your partnership framework to whatever you want. So that part so that's why he was mainly concentrating on the data pipeline last year. Thing is like ensuring the data is there simply in the overall development process and the training model process with respect to data. So once that is done, so remembering everything is like you are the model management and all these things pending. So that's what we are looking into at this moment. So that definitely Snowflake was really helpful, but I think otherwise the other approach was Delta Lake. So for me, I still debate with team members why Delta Lake and why can we continue? When I look at comparison between snowflake and Delta Lake today when we were starting developing for different purposes, I did a comparison to show them. Okay, hey, this is the X amount I paid for Snowflake. And you see the ICD idle because I introduced this. I introduced the pipeline and then the pipeline is working fine. No DevOps, no comment. DevOps is not looking into it. I am not looking into it. I did the whole pipeline. I'm not looking into it. It's all working fine. Now you introduce Delta lake, opens a Delta lake. Now you need to manage you have make sense. That is completely true. When you look at the overall cost, I would say either the Delta Lake, even though it's open source, either similar cost. It's a little bit slightly more than that, maybe, but I would say it's similar cost, I would say. But it's not just the cost for me, it is more on the. Sorry to interrupt. Actually I do have to drop off for the next call. But just one more thing is Chinmay, maybe you can continue and understand from Muthu about the notebook thing today. And Muthu, if you can follow up with Chinmay towards the end of the call and set up like a 30 to 45 minutes session with us together with Abhishek. We would love to discuss a little bit about the Airflow thing with you, if that's okay. Awesome, thank you. I'll drop off. Okay, bye bye. So I just also wanted to understand currently if these models are trained, you're not using a model registry or something. Right now we are just using S three packet as a registry, if you ask me, and manually versioning it kind of thing. Okay, do the developers then you need to store some kind of metadata, et cetera. Do they write it in a file along with the model and the storage or how does it work? Yeah, so when we store the model artifacts, after the training process complete, it includes a metadata section with respect to all this information. All these things are captured for that specific model training process. Consider that you have a unique ID for every training. Every time you do a training and then the output of it is stored in a bucket with a specific unique ID. Then within that you have all this information with respect to metadata, model output, data, all those things are okay. And just to understand, trying to understand the workflow of the developers, let's say data from Snowflake and then they did their model training. The final model is then logged as an artist, right? If they had to conduct tests on this, like, is there any way that you facilitate that? And is it through Jupiter's notebooks you have separated tools for that staging test and something like that? Okay, so we have staging an airmen to test, if you ask me. So once the training is done and then the developers look for the parameters with. Those different score and other parameters. Whatever it is, they are convinced that, okay, this is the right model to use. So the way we approach what we take is we have an active model in the shadow model approach production, though it will be tested for a small set of data in staging, but we even deploy in production. We don't care because we have an active model approach and shadow model approach. Excellent. It's kind of a switch flip. Okay. Yeah. Whenever I decide, okay, I keep monitoring the inferences from both from the active model as well as the shadow model. And then if I know that the new model, what I introduced as a shadow is performing better, I'll do a flip and then the shadow becomes active. Understood. That's how we manage. Just to understand the developers do they containerize the application, then give it or they just save the model with an ID and then the DevOps team picks it up from there for the deployment. Okay, so the Train model has a unique ID, okay? Once that is done in the GitHub as part of our application, the developer goes and updates the configuration file saying that, okay, hey, this is the shadow model ID that we want to use it. And when you deploy the application, it's all dockerized for us, it's all parts and docker is container. So when you deploy, of course it takes that it knows that, okay, which artifact with that ID is the shadow? Which one is active? And then it just processes it based on that. Understood. That makes sense. And do the developers set up some kind of a monitoring, let's say, if they want to monitor certain metrics? Is that done by the developers or by the DevOps even? What kind of tools do you use for that? DevOps doesn't do much. It's mostly application metrics. What you are asking for infrared by the application metrics with respect to whatever is happening internally, the developers take care of it, the integration. We use a Prometheus graphana, open source again to publish our metrics and tracker metrics. DevOps has created this, the intro for Prometheus and Grafana and all those things for developers, of course, like they add the metrics library and then publish the metrics and we have the URLs to publish our metrics. That's how it works. So the system matrix, something like whether to scale the model or not, some things like that, what's the latency, et cetera. This is managed by the DevOps team and the performance of the model would be managed by the developer. Is it, if I understand correctly. Okay. So the way we asked me, it's kind of like the auto scaling perspectives of our models or components, I would say, rather than models auto scaling today it's simply based on CPU and memory perspectives. We maintain flow traffic and the traffic varies. And based on that but we don't have any parameters based on which we do an auto scaling at this moment, but we do have auto scaling. But the only thing is like we do based on CPN database settings rather than anything else. Ideally I would like to see that as the number of record number of the amount of traffic that's coming in, the rate of incoming traffic varies, then probably like I would like to do not skidding, but I have not seen anything assets at this moment. I'm just trying to understand in terms of the cost perspective, when developers are also using a lot of notebooks, how do you keep a track or limit resources for the developers? Like do the DevOps team allocate certain resources to the developers have to request a certain memory or storage for their notebooks, et cetera? How do you do access control on these notebooks currently? Or like resource control itself. First is like from an access control perspective, there's no access control today except for how do you access the underlying node as the book. Okay, so from that perspective, we are trying to implement a Jupiter Hub. If I use a Jupiter Hub or anything like such a tool, but I can have an active user management of my notebooks rather than just depending on my node based access. Okay, so that is one thing. We are looking into it from those tools perspective. As of today, being a Jupiter notebook, it's open source. There's no control on developers to how many notebook they open or anything like that. But the underlying node, if you look at it, because they are not using the local host or local laptop to run it, the node specific restrictions are there. So I will only give a node or two in which they can of course bring up a notebook, whatever they want, but it's always limited by the resources of the node. Say that like four team members, all of a sudden they say, okay, all of us are going to open a notebook and then I need everyone needs a huge memory and CPU does not work. So unless until there's a requirement just like they need it, we don't bring it up. So that's controlled by the DevOps team. Okay, but how are you allocating the resource, let's say a lot of requests come in for splitting up these notebooks. Is there like an SOP or people like talk among themselves what kind of resources are free on the node and then use it? How does it work? We have a dedicated ESG for this, what we call groups dedicated for our teams. So for different, different teams we have AWS, auto scaling groups, we have dedicated so whenever they want to work on their team has their own ESDs and their own set of nodes they will get it to work on. So it's not like there's no really a contention between developers when DevOps has to come and mediate or anything like that. So it is just about like when someone wants to use their node, the moment right at that moment saying that okay, how many resources do we need, how many nodes will you need? Then based on that we created ASG and then provide to them and then within that, whenever they need it, they bring it up and then they use it kind of that otherwise and it doesn't impact other teams from that way. Okay, you mentioned that the node itself is auto scaling. So can they request any amount of resources or is there like a form or any interaction that happens with the DevOps? Yeah, there is a formal meeting kind of thing where we discuss the cost because it has to go through approval process from a cost perspective. Of course, when we want to create an AI, it goes through a Jira process and the Jira process has an approval requirement because the cost comes into picture and it has to be applied to the corresponding executive leadership. And they understood the cost and they agree upon it. Very like they put all this parameter saying that, okay, I need at least up to two nodes, five nodes, things like that, for this purpose list. And this is the functionality. And that gets into the implementation, let's. Say Jupiter, and it's using like a certain amount of resource which is blocked, but that's not actually being used for training or something actively at that point. Is there any way in which you are trying to optimize this when the resources are not actually being used and do you actually monitor one part of. It we are lacking is when they actually use it. I am not at all worried about like they are under utilizing it. When they actually use it, they are always over utilizing it. That's not my worry always. But a lot of times they don't use it. That's the problem. Thing is that you take a note that keep on running and I don't bring it up to zero and that we pay for it when they are not using it. When they start using it, of course I don't see much concern. They ask for tenGB of memory and they're only using threeGB of memory and at least we are not tracking at that level at this point. Okay, so you mentioned this class node itself is auto scaling, right? So you would pay for minimum resource that you request, is it right or when the node is not actually in use for the training, is that correct? Yeah, the node will scale down basically number of nodes will scale down to the minimum requirement. Yeah, but it's not scaled down to zero. So you still enter the cost. Yeah, we don't yeah, we can't. Do you have like a centralized monitoring system? Do you use the native AWS solution for like monitoring these costs and like flagging it to the leadership? If something is like it's all methods we ask me like governance of course we use metrics and then the building from AWS with individual service level, service level, cost kind of thing that we use and that creates differently. Apart from that, whenever there's a new requirement comes in, we know that what are the kind of nodes someone is asking for and how many nodes? And based on that, we project a cost and we show them. Okay, you're going to see a jump in the cost in your next bill as per this requirement that is put forward to the related team. That is a projection, but actual tracking is mostly using AWS. The cost, I forgot the cost analytics or something like that for that. So then we can see that like what's the cost? Understood. And just one more question. From the multi cloud perspective, I assume you use multiple regions because of data housing norms or something like that. So all of those compliance issues got resolved with AWS regions or have you faced instances where there was a need to move to a different route? I assume you haven't faced that rate, right? No. So different cloud and different regions are various, has no relation at all if you ask me. So different regions are basically like because our customers distribute across the regions. That's one of the reasons why, that's the only reason if you ask me why we have in different regions at this moment. For performance optimization or cost reduction by being like across clouds. Have you ever considered something like that? Cross load is not from a cost perspective, we never constrict that cost. When I look at cost, of course I need machines and I may have machines I never cross looked up figure okay, this machine is cheaper in this cloud or something like that, we never did that. But maintaining multicloud is also a management overhead. So if you need to consider cost, I need to consider everything. So now I need to have resources, either they know both or I need to have degree resources for each one of them, which is very costly if you ask me. And other than that, of course from a management perspective, debugging perspective, everything is like when you go with a multicloud unless until you are sure you need it. I don't think so. The cost is like that. We can directly do an apple table compulsory or anything like that. It's going to be like we need to look at multiple levels of cost additions in that perspective. Yeah, that makes sense. Thanks for the insights, I think this was really helpful and we are running out of time. So I just want to understand, as Nicole mentioned, if there is any slot for like 30 minutes when you can discuss with Abhishek, we would love to learn from experience on airfare as well. Okay, so when you guys want to meet. It'S fine by us anytime. Since you're in IST early morning IST, which should be like evening for you or your morning, which will be like evening for us. Is this yes, sure. This time is fine. You can take a call. You can let me know what am PST or something. What time are you thinking about? I will just quickly check on the. Calendar sometime Thursday morning. Thursday is tomorrow, right? Oh, you are Thursday. Okay. My Thursday morning is Thursday night for you. Okay. Yeah, tomorrow, what time? Thursday morning. What time? Okay, Thursday night. Up to what time can you and I can look at based on that? Oh, yeah, you can let me know. So I think 10:00 a.m till 10:00 a.m is fine. 10:00 a.m. Or my 10:00 A.m.. Your 10:00 a.m.? Yeah, my day is like 1130 for you. Yeah, that should be fine. Okay. 10:00 a.m. Right. Tomorrow. Yeah. 10:00 A.m for me. My time. For me, it's not tomorrow. Day after tomorrow, thursday, 10:00 A.m., my time. Or you put nine you will put 09:00 A.m., which is fine for me. Okay. Makes it easy. Cool, I'll go and do that. Thanks a lot for that. Thank you very much. Thank you. Take care. Bye. Thank you.