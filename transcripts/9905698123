Hello. Hey. How do you pronounce the name? How about yourself? Nikhil. Nikunj, okay, nice to meet you. How are you doing? I'm okay, I'm just pulling the different resources you share with me. Yeah. Fun stuff. I briefly went through through you LinkedIn, you worked at Meta for a while and then you sold your previous company and now you're starting a new one. Yes. That's ambitious, I admire that. Thank you, I appreciate that. The most fun part of this entire startup journey is working with my undergrad batch sales. Turns out my co founders, IBM, Ragging, Abhishek, we went to the same school, we're in the same hostel and everything. So we shane spent a lot of time together. That's awesome. Where are you based? I'm based in Toronto. Calendar. Okay, understood. They're almost days about to end for you. Yeah, absolutely. Awesome. Were you able to discuss, I think we discussed on the email on the LinkedIn that you'll probably talk to some of the machine learning engineers in a company and see if they are interested in the call business, other folks. Yeah. So in order to projectpro that time, I did not further that I was considering it, but if you don't mind, let's just have that conversation and I can direct you to certain people to reach out on LinkedIn. You can let them know that I've referred you to them, but I will let you do the outreach if you don't mind. I see. Okay, understood. No problem. Cool. So I think I've already seen the background and everything. Just a brief a little bit about myself in terms of the work. I have been personally working in machine learning for the last few years. At. Meta I was doing more conversational AI stuff so working with a lot of natural language datta. And prior to that at Reflection, I led the machine learning team and built out a lot of recommended systems for the ecommerce industry. Sure, just give me 1 second, okay? Yeah, of course. Actually just give me 1 second. Hello? Hi, Shelley. Hop on a different call. Can I call you in 30 minutes? Okay. Is we are having soumen fundraising conversations. We're just talking about that. Important ones. Yes, makes sense. Cool. So basically I was just saying that I built out a bunch of recommended systems, personalization algorithms and scaled the Reflection machine learning systems to about 600 million monthly active users. So yes, I work both on the modeling side and the engineering side of machine learning. And now what we're building at True Foundry is very similar to the work that we did at Reflection. We built out a horizontal AI platform for the company and now we're trying to build it out for other companies in the world with some inspiration from our time at Facebook as well. Makes all sense. That's where I am. I would love to learn a little bit about you. So from your linguist, seems like you are product manager, a bunch of Datta science projects. So we'd love to hear about you current work as well. Yeah, that makes sense. And I try to specific addresses a piece that might be of interest to you. So as a product manager on the Data Science team, my goal is to bring data Science projects to the customer. Right. And one of the challenges we're facing is that our data pipeline, whether it's the data lake, the data quality check, or the MLR platform that we use, is not very mature. So the team, the Data Science team end up spending a lot of time doing manual model decision and promotion on whether we promote recent models as well as bring a R and D project to deployment. Right. So my team specific at this time is working on assessing which MLS platform that we want to use in the future. And I do have a few slide decks that I can show you. For instance, when you're asking the problem description right, this is what we're currently looking at. We have a lot of issues when it comes to dependencies, right. Like, a lot of data scientists run stuff on the locals, but then once you want to kind of like they don't have a similar same derivative notebooks that they work on. So there's a lot of differences in terms of like the code base. And right now we are at the phase where we kind of deciding whether we go with Sage Maker or Kubernetes, not cuber flow. My bad. Okay, I see. These are the problems that you're facing and right now you're in the middle of deciding between Sage Maker and Kubeflow. Understood? Okay. Yeah, that's right. Got it. And what are some of your decision making criteria here? The one that I'll describe here are the main ones. I think cost is definitely one set up time, how managed it is versus how flexible it is. That's always like kind of preference base. Let me see if I can find other things. Slides. Yeah, I think that's the main one. The other piece which does not play in your favor is that generally speaking, teams are looking for a platform where they can find a lot of features. Right. So that in the future, if they want version control, if they want, I don't know, like monitoring system, they just want more and more features. Right. We may not use it right away, but just because of its maturity and existence, we prefer to go with something that we believe is going to be more sustainable. Gong term. Right, I see. So this is an example, like currently comparing the two and trying to see the difference between the two. Got it. Okay. And in terms of this decision making, for example, how far are you guys from actually housing a platform and starting to use it? And how large is the team who will be kind of like building on top of it internally. Right. So the POC itself and that assessment has been running for about two months and were scheduled CTO complete within a month. So the POC and the assessment itself was three months. Once it is deployed, it would be supporting three data science team, which would be around four, five, six, let's say seven to eight data sciencemaengineers combined. I sep. Okay. And then just want to call out. One thing is I'm a product manager, but I also have from an organization structure perspective, I have an engineering manager who will end up making the decision. I'm involved in the criteria that we kind of want. Right. What are the benefits that we want to bring? But he's the decision when it comes to costs and feasibility and kind of implementation complexity. Got it. I see. One other question is, whenever you onboard a platform like Sage Maker or Q Floor or any of this internally, you need some work. Like somebody in the team has to build on top of that. You can't, like, generally directly expose like Sage Maker and all to a data scientist because it becomes too configured frequently. So is there like a team who is handling that as well? So right now we have one data scientist that has a POC project, and then we hand it off to the two ML engineers who are doing that assessment. Got it, okay. So basically the same team who will be kind of evaluating these platform will ben the ones using the platform? Essentially, that's what it sounds like. Yes. To a certain extent, the ML engineers are the one evaluating the data scientists are providing their feedback and opinion. But ultimately, the implementation would be owned by ML engineers and the use of it would be by data scientists and engineers. Got it, okay. And you mentioned seven to eight data scientists and melee combined. Out of that, how many are me and how many are data scientists? Three ML engineers, five data scientists. Got it. Okay. I see. From your standpoint, like, as a PM, what is it that you're really looking for? There are a lot of engineering considerations. I'd love to hear your thoughts. Absolutely. Ultimately for me is to accelerate deployment from POCs to being able to demonstrate that to the customer. Right. And looking nat you platform, it's pretty fast where you can bring that to the UI. That's essentially my intention. Right. It's acceleration of experimentation to be able to demonstrate it to the user. I see. Okay. So do you frequently end up doing this where you build out new models that you have to demo to the user and once you see some positive feedback, that's when you end up doing with the full integration and engineering and deployment around it or you don't have CTO demo too much to external world. What's you typical use case like? Yeah, let me think about this. It depends on the project. So that's why I'm thinking out loud because there are projects that are directly to the end user, and there's project that is more internal. Right. Like a data drop. And then the user is the UI team or the application layer team will take it and they will use it for their purposes. Ideas in IO will be we demonstrate that directly to the customer because then we remove that layer and that risk of, like, having the UI application team pick it up as well and then deploying it and then end up being as useful to the customer. I see. Okay. Got it. Okay. So I really would like to get feedback from the customer, but right now you're not able to do that because there's like a UI layer in the middle, which ends up taking a lot of time and effort. Assume yes, that's correct. The second piece then. That being said is that we have customers who are more data oriented. So as long as you can give them a flat file, they're okay with using that and providing input and feedback. So the UI component is not necessary. It's necessary in certain situations only. Okay, I see. Understood. So even for the exposing the data to them, what tools do you use? Because even that needs to be hosted somewhere that you would expose that they can play around with. Right there. What do you currently end up using? We are just on AWS. So S three and then exposing s three buckets. Okay. Got it. And currently, in terms of the models, do you already have a bunch of machine learning models in production? Yes. Like, what kind of models do you typically end up dealing with? It's very simple regression models, I would say. Those are the key ones. It's basically given a list of features, what is your self estimate? Like, how many this product has been sold per week or per day? That's the main one we have. We also have certain kind of given this set of input features, what is the keyword, the volume and so on. So those are the two main use cases. I can come up with more, but that's on top of my head. Got it. Okay. Do you envision that with the platform that you guys are onboarding? Would you rather want to serve more machine learning use cases or kind of like the major purpose just to optimize the existing use cases? We want CTO serve new use cases. We are challenged where we have a lot of R and D projects that are just sitting there because we're not like the ML Ops team or the ML engineers is over utilized or under resourced. Right. So as you can see, we mentioned five data scientists and three ML engineers. You can appreciate that this is creating deployment and introductions, frictions that make sense. Yeah, I understand what you mean. Okay. And are you guys completely on AWS of the cloud? Yes. Okay. Got it. No on premise, nothing on prem. Okay, got it. Are you integrated with any existing monitoring systems at all? Like, I don't know, like data dog nearer, like something about AWS. You know, we use Datta dog. We use MF flow. I believe that's the term. Yes, MF flow. We were looking to Great Expectation as a test unit case. I don't know, that's less of monitoring, but eventually, like, unit test, data dog. That's for the data engineering piece. What else? Yeah, that's pretty much it. Got it. Okay. When you say that you're looking to create unit test, do you mean like, unit test for machine learning models? It means yes. Given this pipeline, the output, is it within a reasonable rate or like it's like there's a certain percentage that is outside the France. Right? Because, like, usually the unit test is a yes or no kind of a problem statement. Right. Whereas Great Expectation is able to work with not ambiguity, but percentage or kind of right. Distribution. Yeah, sorry. Yes, we're actively using it, actually. Oh, nice. Okay. Got it. You're going to say no, just asking that. You're basically saying that the team internally ends up using that heavily. Great Expectations. We want to use it more. We have started using it, but we want to introduce more of kind of testing around it. I see. Okay, understood. Okay, that's good to know. And then the other question is, in terms of the libraries frameworks for machine learning, do you know what kind of libraries the team ends up using more frequently? I mean, Pandas data framework, usually one we use let me check the documentation. Different people have different use cases, but if you're specific, talking ML heavy, like when it comes to neural network. Yeah, it's definitely TensorFlow. Chris is TensorFlow. I was just going to try. Is there anything else? Yeah, otherwise it's the usual ones, pandas and Sci-fi and so on. I see. You got it. Okay. And do you know if the other, like, the non machine learning part of your software, is that getting deployed using Kubernetes or how is it typically deployed? Do you know? Right now we use a program called Cadre I don't believe is the equivalent of Kubernetes. This is where I'm a bit out of reach or out of depth. There. IBM familiar with Ketchup. But Keto is not not equivalent with Kubernetes when it comes to container and stuff like that. That's why you're asking. Yeah, sure. It is more like they allow you to manage your pipelines a little bit better. Yeah. They have a concept of this entire projects and everything where you can set up, like, your training and like yeah. So it is more for your pipeline management. So think of it as a layer on top of airflow for data scientists. Right. I don't believe we actually use Docker or Kubernetes, but then that's a good question. I'm trying to find out what alternative we use then given that we don't use those. Yeah, sure. Sorry, I'm trying to get to my GitHub repo, trying to figure out yeah, technically sufficient offense to end stuff. No problem. Cool. Google. So this is already helpful. The other thing is, you mentioned that you saw the demo. You liked a few things about the demo itself. What caught your attention? What did you think? That we might be able to add some value on top of the platform that you're already evaluating? That's where the challenges? I think for an organization, a simple organization, it's really about reputation and the risk of having to change a different platform in the future. Right. The one thing I really like, based on the example I've seen, is the speed and the velocity. You can do this, right, because you have the basic building blocks, CTO, able to quickly kind of deploy something in a container and then being able to monitor it. That being said, I'm wondering whether the use case is more for data scientists to kind of experiment and then kind of push the POC a bit, an extra layer, or whether ML engineers will also want to use a platform like yours. That's why I'm wondering. But the one thing that definitely spoke out was the simplicity and the ease of use and getting into it and being able to deploy something. Got it. Okay. And by the way, that demo exactly demonstrates basically showcases the ease of use part of the platform. So you got the message from that, right. And you're right. It would appear that it's more for demo use cases as opposed to the more heavy production use cases, basically. So I'll actually explain you. The way we think about our platform is that people who are starting to optimize, companies who are starting to optimize their machine learning development workflows. So imagine that companies who are at a sweet spot of about, let's say one ML engineer, full time, focused on just managing the info, building out the tools. Essentially, that's kind of where our sweet spot is. And at that scaler of the company, not everything in the machine learning operations platform is generally needed as features for the company because they are actually getting started with something. They're solving more basic problems at that stage. Generally. Yes. The way our platform works is that we expose the basic features that companies might need in the beginning and as their needs evolve, the platform is there to skip. And I'll explain why it goes from a data scientist prototyping to a complete production maintenance is because we are completely built on top of Kubernetes, just like so, except we have exposed a lot of simplistic APIs for the data scientist, but at the same time we expose the full power of Kubernetes to our users. Essentially, technically, if true found it did not exist at all for whatever reason, and somebody was using proof on the platform, they can actually continue to use it because we shane a layer on top of Kubernetes and everything. Kubernetes is exposed to the user, if that makes sense. So we actually don't take away any flexibility from the user. They can actually do every detailed thing that they want, but in most cases, people don't need all that. So like, the abstraction layers on top of it helps gain the speed and usually what would end up happening, by the way, I'll tell you this from my personal experience, is take a note of companies that we speak with, try to build out layers on top of this, these platforms. Nobody builds it from scratch. So everybody will be building on top of Sage Maker, on top of Kubeflow and stuff like that. But even building out this layer and getting to a maturity, that okay, people are frequently using this platform. Coming up with the right set of abstractions typically ends up taking about twelve months when about three to four engineers are working on the problem. And after that, like, there's about a 30% ratio of maintenance. That is like for Qovery, three data science is working. There's like a one full time person who's just maintaining that platform. Essentially. That's kind of how the platform scales up from our conversations. And our goal is to reduce that number. Basically, that your time to production of the platform itself. You don't need to take like twelve months to do that. You can actually get ready in a week or two, basically. Right. That's one of the value. And in terms of the maintainability, you don't need like one is to three ratio. You can probably maintain like one is to ten ratio. Right. You will always need soumen internal, we don't believe that you take a platform and that's it solves all your problems, but can you have more people work on the product facing features as opposed to the platform facing things? That's what we are trying to solve for. That makes a lot of sense. I think then the follow up on that is which is harder to establish, but like the trust and the reputation around your platform, right? Because what you're saying makes a lot of sense to me. And there is a need, there is a need within my organization to kind of have that, so that we're not spending that much time just QC and then starting to use it. And our team is also spending a lot of time on Toyota work. Right. You said a ratio of one to ten. We have a ratio of one to three as well, at least on the deploy models. Right. So it's the same kind of things that we're trying to reduce toy work on. That's a huge value add for a business like ours, or any kind of cost optimization or efficiency. Right. So I think then the question is, if you present your platform to an engineer, how much they would trust it to be able to meet a need. Long time, long term, right? Sure. So there are two main things that I want to call. I would not be like at the end of the day, decisions like these are internal, like how much people are willing to trust. There are a few things that I want to call out in this context. Like number one, that our backgrounds, we have actually built similar platforms. We have used platforms at Facebook and we have like 15 member engineering team whose fulltime job is this? It's unlikely that somebody internal, like three to four member engineering team will be able to move faster than, let's say 15 member engineering team who come from similar, like ML operating accounts. Right. So basically in stuff like this, you kind of bet on the team that you're working with as opposed to the current state you're looking for, how can I get to the future as fast as possible? That's number one. Number two, as an insurance to a new platform, we understand, we appreciate this concern from any company who's trying to build out an internal MLS platform. And as an insurance for that, as I mentioned that everything Kubernetes is exposed. Like tomorrow is Qovery. Literally. No CTO. True foundry. Switching from True Foundry to a Kubeflow is like probably one day of effort. Right. Because everything is Kubernetes and all the things we do go infrastructure as a code, right. So nothing is like platform specific. Right. That's number two. And number three, the trust that we have received from the community, like for example, founder at Kaggle, the person who created Facebook's internal MLS platform is one of our advisers being sequoiacap. So the trust that we have built out from the community and the support system that we have generally will give confidence to people that, okay, these guys are backed by good enough people that they will have that support system that you need as an early Shane startup to continue to grow. Right. And at that point, internal leaders have to make that decision that do I want to take this? You are taking a chance. Any time you work with a startup, I will call out that you are taking a chance. But do I take that little bit of a chance and have a shot at building something ten X better and faster? Or do I want to take the safe route? Like the old saying goes, right? Nobody got fired to take an IBM, right? But nobody got rewarded ten x to take an IBM. Right? So that's a bet that somebody internal, internal, like there has to be internal. Champion would take that bet and yes, you get the ten X reward for the little bit of the chance that you take. Yes, that makes a lot of sense. Yeah. That's amazing. We use doctor instead of Kubernetes? I don't believe our team uses Cuban at just yet. So that's something I can investigate. But either way, kubernetes is like the way to go anyway when it comes to deployment in the future. Right, cool. And maybe one question for me then is if I bring your platform to the team, your system is currently marked as early access or better, is there certain concerns or known issues with it that we should be mindful of? Yeah. So the thing is that we have not exposed the platform to folks because our goal is to only work with six companies altogether for the next one year and our team will entirely work with these six companies. Almost think of this as an extended platform team to these six companies. So the thing is that all the features that we're talking about only get built for the requirements of these six companies and the platform basically we will not make it generally available until then, basically. Yeah, that makes a lot of sense. The Gmail thing about the beta where we don't want people to sign up on the platform, start using it, it's more like you have to work with us closely to deploy things, which makes. A lot of sense. You are still in the discovery phase as well as kind of. The idea here. And obviously our pitch to these six companies is that you essentially get. ML. Engineers who come from Facebook or Amazon working for you for your use cases without you needing to exclusively pay for their salary, essentially. And you have a head start for the platform that has been built out like their full time job is doing this thing, essentially. So that's realistically our pitch to the companies and then we figure out the rest. Yeah, makes a lot of sense. Cool, well, I look forward to kind of monitoring your progress and seeing how it goes. Anything that I can help you with? Yes. So I want to actually understand in terms of your internal teams, what do you think we should do as a next step? What's your recommendation? And I have to jump in two minutes, but basically in terms of for your discovery, kind of talking with ML engineers or just getting to understand the problem a bit more, is that what you're looking for? Yeah, understanding a little bit more in technical detail. What kind of things are you looking for, what matters to you? And we also want to evaluate maybe you care about features that is three months down the line for us. Then we're like, okay, go ahead with the platforms that you want. Or maybe you're looking for things that we already have ready and we can ship it in like get started in a week. So then a good progress. Right, so that's what I want to figure out by talking to people. So ideally, if you believe personally, by the way, this could be a meaningful collaboration, then it would be great if you can loop in a couple of folks and then we do a call, join call together. That makes sense. Cool. All right, I pitch it to the engineer manager, and we can follow up on that then. Got it. So let me follow up on LinkedIn about next steps, then you tal internally. I'm happy to let me reach out. I think it will ben. Yeah, I can do that directly, internally. Okay, got it. So you reach out and then let me know about the next sep. Is. That yup, that's right. Okay. Got it. Awesome. Thank you so much, then. Yeah, thank you. Take care.