Hi, how are you doing? All good? I'm good, how are you? Yeah, I'm good here. Thanks a lot for taking out the call. Taking out time for the call and also scheduling, that's really helpful. Lucky will arrive to be joining him. Yeah I'm just texting him, just give me a few minutes. So occupied he might join in between but we can continue our discussion. He can. Cool, sounds good. So actually just kind of to give you a quick introduction, I am anrag I'm, one of the co founders at IIFL Truefoundry, graduated from IAT Karakura in 2013, spent seven years working with a hedge fund called World Fund. And then during that time, apart from managing like 600 million in assets for World Fund, I was also a member of the CEO office looking after various strategic initiatives. Worked in the US and Singapore for a part of like three years out of those six and a half years and then later on started investing into startups and then would say wanted to build something and built a first startup in the talent space called Entire which we sold to InfoAge around September 2021. And then we kind of started building True Foundry to make it easier for data teams to kind of get access to a better platform for testing out and deploying models in a scalable way to production with the right security, with the right authentication and with monitoring in building. So apart from me, like Nikkunjana Vishek and my co founders, they come from Facebook background, same like educational background is same as me. They saw how internal platforms are Facebook have been built and we wanted to kind of make it that easy for other companies to also get access to the speed at which ML can be done. Apart from the three of us we have like a 15 member team. Vivek is a part of the founders office and he has been helping us in terms of getting the initial customer conversations and so on. Right now we are working with 350 billion dollars plus companies plus have two other pilots in the B to B, SaaS, Unicorn Space et cetera, covering mostly health care enterprises, Enterprise, SAS as well as we are talking to a few companies on the financial side as well. So I think Arihan had actually seen the platform and he actually had reached out in terms of understanding what are the kind of things we can support. So wanted to use this call lecture to get an understanding of the data science initiatives within IIFL. What are the kind of things your team takes care of? What are the challenges that you all are facing? And based on that, we can kind of discuss further. And before moving ahead, one thing I wanted to mention is we are still very early stage in the journey. So the goal has been more kind of understanding the problem statements from the companies than trying to see if there's any way we can help. And in that. Would love for you to be as candid as Frank and not think of us like a vendor but more like somewhere where we are kind of discussing more insights about the overall Data science. Got it. If I talk about our Data Science team so right now we have some four thoughts. Models. What models again? Four thought models. Okay. So basically for underwriting we are using them but right now we haven't deployed them so they are not running in a production environment. We do compute this course in a patch compute manner. Right. Ideally speaking we want to get live with those models and the deployment part is something that we are looking at. We have initiated some of those activities where we are working on model deployment part. Once we touch that we will be definitely touching upon the modern monitoring aspect. Right now we are looking out around these opportunities where we can deploy it first and then start the monitoring activities along with that. Got it. And if you can give me a sense of like what is the size of the Data Science team currently at IIFL? As in the team bandwidth you're talking about? Yeah, like how many members are there and how is it distributed between the people who are working on building out these models versus people who are, say, on the engineering or the infrastructure side of management of this model versus people who are actually doing say or who might be involved in say the production deployment. We have a couple of folks, a to ten members are there but if I talk about the specific segregation that you are asking for right off, that has not been built in. But yeah, in near future we will be segregating the Data Science team into Data Science and ML of the part. Okay. Got it. So currently so you lead the overall Data Science kind of efforts. Lecture I want to understand what parts you lead and where does the Arians come into the picture. I would love to understand that also a little bit. Yeah. So Aryan is the director of the Data science team. Apart from that he is also a couple of things creator of the writing and poor team and I have to talk about my experience. So I am working as a Data Science manager and I have a past experience with this money. So in this case I was working with Data Science team and specifically I was working up on the MLPs part. So currently this model that you are building, most of them are in batch mode. So how are they running? Like are people running this on local notebooks or do you have like a hosted Jupiter notebook kind of an environment internally? Lecture yeah. So we are using Azure right now for that batch computer process thinking to move towards AWS also. But that is the ongoing process right now. Okay. And why are you thinking of moving to AWS? Yeah, so basically our data warehouse activity is something which is more inclined towards redshift. I wanted to cater the single cloud platform and use that because it is quite easy to use build data engineering pipelines and use modeling in the same environment. Right. Okay. Given you are on a zero and you're looking at AWS, I am guessing, IIFL in general is multi cloud from an infra perspective, is that correct? No, right now we have every solution in house only and we are using our own server, but we are initiating that process. I was meaning like, you are on multi cloud, as in you are not specific to kind of say, SEO or there are some companies which will use everything SEO and will not touch AWS and GCP. So your case I'm guessing is a mix of SEO and AWS right away, right now? Or is it more like a plan will come into the picture or does the software team already use like a multi cloud kind of a thing? So we are moving towards AWS from SEO. Okay. We are moving in general from Edo. Okay, got it. Do you kind of use by chance like say in Se or AKS or Kubernetes in any way to kind of host your general services, et cetera? Or how is it currently done? Like for spinning up of machines and other things like to use Kubernetes behind the hood or how is it yeah, it is there. The team has explored that opportunity also, but it is not currently large. Okay. The exposure part is still working. Understood. I love to kind of get a quick overview of the overall workflow. Like at what point, where is the data, for example, residing from data to modeling, how are people doing it, how much time does it take, how is the validation of those models happening currently? And then when the batch compute is happening, where are the results being finally stored and how is it being fetched right now? I would love to understand that a little bit. And also, while building of models, like, are people using anything to compare across different training runs and so on? So, yeah, if I talk about that, it's just like we go with an extensive exploration data analysis around the data and then we try to build some of the models. They can be posting and bagging models. And once the data is there, once the model is fitted, we have the saved models. And using that saved models, we try to call the scorecard model and try to predict it. Right now, if we are dumping some of the scores in our old database so that the other teams can also utilize them. Apart from that, if I talk about. Other things like that, this is the. Basic flow that we are following right now. Okay, sorry, go ahead. No, that's all I was asking this dumping of results to like an old database. And then how often do you run this scorecard models? Is it like a daily run or is it like a weekly run? So that depends actually. Like, if I want to start upon a new initiative, the new initiative can be pre approved program where we want to go for customers. We have lacks of customers. That depends. That depends. If I talk about underwriting, we are using some models, but I'm not sure, like at what time we are using them. But they can be like weekly models. Okay, but nothing is real time right now, right. Or close to real time, like customer. And you have to underwrite it. So it happens online. Right now that is not happening. Yeah, right now that is not there. We want to develop that. Okay. And as you are using SEO, how did you find the overall system in SEO in terms of, say, doing this research part? And then if you had to continue to use it for deployment, were you facing any challenges? Lecture there? Yeah, so I started an initiative where I wanted to make a container using Tokle shipping that container into Kubernetes. Right. So with that approach, I was inclined and I was moving in that direction. But there are some restrictions in terms of IIFL platform that we have, it comes with a lot of restrictions which the credentials they have shared with us. So was not able to explore it. But we have reached the point where we were able to containerize model or application that you're trying to. I see. And when you would want to host these models, you would want to host them as API endpoints right. And do you have like a Fast API? API? Do you have any preference there or do you want to use any sort of model servers? Lecture so I would want to go. With Fast APA only because it is as incremental for my purpose. So I think if I want to move ahead, we'll be in front of you. Okay. And one thing around the model part, which I miss, from what I understand, most of these models would be okay to kind of run on CPU. I think nothing is so complicated that you will need GPUs at this point. From what I understand right now, we. Are not using deep learning. Yeah, you're not using deep learning. Okay, so that's fine. And this Fast API endpoint, what is the expected traffic that you would expect to see once these models are deployed? Like how many requests, say per second, per minute, per day that you would want to that you would generally expect this model to start seeing once they are deployed? And any ideas around that? So, yeah, that will depend the funnel very exactly. We want to deploy this model, right. So that will depend upon the nature of the model that we are trying to build. Either we want to call that model after calling the civic score or not or before that. So that will vary a lot. But if I talk about an estimation, it can be ten to 15 APIs hit per minute. Ten to 15 /minute okay. That's fairly okay. So it's not like very in the range of like hundreds of thousands or so. So I think latency and all hopefully should not be a problem there. Okay. One more thing is the data scientists in the team who are building this model, do they use any sort of like git repositories, like GitHub or GitLab anything for maintaining their food or do they kind of do it currently on the Jupiter notebook as your notebooks, et cetera. So there is something else which is being provided by Microsoft. Is it not bit bucket? No, it's not bit bucket. It's something else. Yeah, I think it's not bit bucket. It's something else that. Something is being used for like as a code repository and for pulling. Okay, I think. Probably understood. Got it. If I understand correctly, the challenge right now is how do you kind of provide like a simple platform? Any of these models that you are testing, which are like batch compute based models, right now, people are able to take it and quickly deploy it to some sort of like a staging or depth stage. And from that if needed, promote it to a production stage wherein the product team can then pick up and use the API endpoints at respective points of the product. Is that like a fair understanding? Right, okay. And do you have a sense of how will you kind of consider building it, say either in house versus say, buying some sort of platform? Like, is there a bias towards anything? What are some of the core necessities that if you have to buy a platform, what is it? That lecture you will be looking forward to from an internal perspective? So, yeah, first I select a deployment part should be taken care of. Apart from that, a dashboard should be there regarding performances or regarding the failures in API calls that we are getting every second or every minute. So that kind of dashboard we want. Also there should be some kind of intelligence around the retraining model of the model and how model is performing. So I would want to consider some of the metrics around population stability index, AUC k or characteristic stability index. So I wanted to know whether my features are varying a lot. So that can also be catering will be like wholesome product, which gives me a lot of flexibility to look into a specific models and how it is performing and what are the key factors that are affecting its performance. Got it. And I'm guessing like IIFL because of being a financial firm, like you would want the system to be hosted in your cloud and data and all privacy and all is there. Right that is generally I'm guessing the cool. I think this is very much aligned luxury actually. I'll take 510 minutes to give you an overview of how we have built two foundries and what are the core components and I'll show you at a very high level like simple recorded demo which is like three minutes. But I would like to schedule another call where I can show you a more detailed demo which is more hands on in terms of from a code repository like from a Jupiter notebook, how people can easily you had a question? Sorry, I just wanted to ask one more follow up question on the same lecture. Is there any timeline that you're looking at from a deployment point of view or from the solutioning point of view. That'S currently like we are not looking at a timeline, we are exploring lot of possible way to work in that direction but yeah, this is how it is right now. Understood. So I was telling we can show like a more detailed demo, actually lecture in terms of keycase, but a notebook say you can do like training comparison and from there you can deploy as an endpoint if you want. To promote the production environment. How you can do that and then how you can trigger retraining well apart so that you have a visibility into that later on. If you want to expand it towards tracking matrices and tracking monitoring those you can do. But the core part of the platform is still enabling like very seamless deployment and every deployment is actually dockerized containerized. So automatically even if you provide like Adobe you are using on SEO like Get repository or you can provide a linkage to that and then automatically it will gather the requirements. The external to containerized script will deploy it. To a corresponding environment on top of a Kubernetes cluster on whatever. Like if you use AWS AWS SEO as your however you want and it will allow that management and visibility to the managers IQ and even Ariana and all so that they can see all the models that are being deployed at once. This detailed demo we can showcase. I needed a little bit of help from you to understand key. We are again like early stage so while we have kind of deployed in a few good organizations, what we try and do is we like to work with folks closely in terms of understanding, say a few more needs from your side and we can even co build for you. So for example, if there is some part of the platform that is missing that you need, you can let us know and we can build it out for you as you kind of use the system. So you can think of this as more like not necessarily all the features, everything is necessarily present but if something is missing you can just let us know and we can actually build it because most of these things will have to build. We just look for companies quickly, so you can think of it in that way. So while you are looking at the platform and the demo that we do later on, think of it from that perspective. If you let us know these are C mode requirements, we can get it done as well. Sure, I think that makes sense. Once we discuss upon the demo part, our team can also contribute and we can definitely touch upon the best practices around this building an entire platform which can cater more data center solutions, right? So yeah, I think we can look at that way. Just let me know when we can have a discussion about this demo. Let me show you a brief luxury platform. You have a sense and a mind map and then we can discuss the demo. So basically, I'm not diving into very specific parts, we can cover that later on. I'm just showing you very quick overview. So we are like a machine learning deployment platform built over Kubernetes, supporting all three clouds. So whatever you want to use, you can use it. And the goal is really to make it very easy for developers to kind of deploy things from their own independence while providing full security and control for the instructions. So when you look at the platform, you will see that, okay, we have enabled very quick deployments for you, wherein with a code or even with a docker image or anything, you can easily deploy it to a corresponding environment of your choice. We make it very easy to learn by providing Python libraries or Python SDKs, which are generally understood by data scientists, ML engineers, everyone. You can also use the ML configurations and all that completely fine. So it's very flexible in that way. The best practices are followed from get go, so anything around user authentication, secrets management, the platform automatically tries and insured. So even if say, a developer does not know about it, the platform will have those prompts to ensure that the best practices from a security standpoint are taken care of. And this becomes very important from financial services perspective, healthcare perspective, that people are really border about key, which APIs are exposed to whom, whether it's protected by an authentication and so on. So those functionalities are there. And finally it sits on top of whatever your actual team's workflow. So whatever your team might be using at IP, if your use currency AWS use currency without creating a separate pipeline. So when you look at the product, there are three major parts. The first part is more connecting to your infrastructure so that it enables just give me 1 second. So that it enables you to connect to your cloud and it allows you to allocate resources to your ML team so that they can use that and easily deploy, et cetera. And there are guardrails in the platform to prevent mistakes from happening as well. So this is the part that also ensures that everything is installed in your cloud and no data flows out of the system. So that way everything is secured. Your intra team or the It team that whoever is involved is happy and they don't have to worry about things. And at the same time for managers there is a complete view of everything that is going on in terms of all the models that are being deployed, what cost et cetera is being incurred and so on. The second part is the part that is this is more of a setup. Once that setup is done, the second part leads to the deployment wherein you can deploy like a simple function to complex model. Sometimes like when you are deploying models you might also need preprocessing functions. So you can also host and deploy that. You can deploy batch inferences as well and schedule them. Like right now suppose you have a batch inference, you want to run it every Monday or you want to run it every day at a certain time. All of those cron scheduling can be done. And if you want to host like an API endpoint you can do that as well. Auto scaling, CI, CD authentication, all of those are automatically taken care of. And later on when you want to say worry about performance optimization and all you can do that. And all of this like Real Time for example is hosted as a fast API. You can also host as some model server but that is more complicated. Models when you build that is not needed as of now for you. And as you deploy like you get access to your basic feature drift and your basic performance monitoring so that if you see something is going wrong you can actually go back and retrain the model. So there is a full visibility on that as well. This part of the platform is still work in progress there, but there's a lot of refinement that needs to happen to make the pipeline fully complete. But yeah, that is roughly at a high level. I'll show you like a quick walk through as well. Any questions? Such as so far? Yeah, no questions. Okay, so this is one thing. What we try and do is we try to ensure whatever you are using, you continue to use and nothing you have to change. So for example, docker registry which use you can use that if you are using your cloud accounts. You can use that if you are using any of the depositories, whether it's an SEO, bit of anything like you can use that and integrate. This is where you connect to a cloud. So this is an example of connection to an AWS cloud. You can also connect like Azure et cetera. Once you connect to a cluster. Like this allows you to set up the system. You can have user access control and if you want to configure your monitoring URLs which use you can do that. You can also restrict the type of machine. So you might restrict, okay, we don't want to give access to GPU machines. So all of that you can restrict. And once you set up this cluster, basically it's installed on your cloud, your cluster is onboarded and within cluster you can have multiple workspaces which can be like a dev workspace, production workspace, it can be for different teams. Here you also see the source limits. So every workspace comes with some resource limits that you want to allocate so that the data scientists or people in the team do not exceed that resource limit that way. Cost is always controlled here in the actual new version you also see this, how much percentage has been used. There is also cost tab that shows the actual cost that is being incurred. At the compute level you get full access to basically okay, which boxes are there in that, what cost is there, what resources are going on? This setup is done. Then the deployment part comes wherein people can deploy like a service or a model, et cetera. So the deployment is also very simple. You just click like whatever you want to deploy and you click as a developer, whatever workspace you have access to. Here you can deploy from a source code, like if you were using Azure, you can do that. Or if you have a docker image, you can directly upload your docker image. You can also configure, you can enter your docker file as well. And all you need to do is you kind of enter your path to build context. If you have requirements or text, you can give that or we automatically pick that from the corresponding repository. And if you have other packages you wanted to install, you can do that. You can expose ports, like for security, if you wanted your ports to be exposed to only certain people, you can give like a user protected code as well. Here authentication, here you can manage your secret variables and then you just click Submit. And once you click Submit, like your service or your model is deployed as an endpoint basically, and the end point, you can actually go and access it as well. These are the endpoints. So if you click on it, you will be able to access the fast API swagger documentation as well. You can also deploy like a web application through grade IO so you can access that. You also get access to this metric. What is the memory usage, CPU usage, all of this is in one place. You get access to logs at one place so that if anything is going wrong, you can track that. You can go to different versions if you have deployed and even revert back to an older version very quickly by keeping on redeploy and things like that. And then from here, like once this is then similarly you can deploy like a job, which is like a batch job, and you can schedule it and you can deploy like a model which is posted on any other model registry as well. And then at the same time you get access to basic monitoring dashboards. Like what is the performance of the model, what are the metrics that you should track? Like if you want to log your own metrics, you can do that as well. You can see the recall f one score, whatever. If you are using something yourself, you can do that. You can also track the predictions and actuals by logging them and comparing them here. And if there is any drift between the two versions of the data, you will be able to see that drift here and you can push that drift again to an email slack, whatever you are using. And you can go even to the level of raw data to kind of see what is going wrong. So here you see the data distributions and further, if you want to drive and go back to the raw data, you can also do that. So this part is also enabled that if anything is going wrong, you can kick off that retraining loop for your model and so on. So roughly, like very high level. Just wanted to show you. So you have a mind map overall lecture. Yeah, so although I was able to follow it completely, but I have a quick question from my side. The cloud will be ours only, right? Yes, it will be your cloud. It will be deployed on your infrastructure, right? And you'll be connecting to our cloud and giving us the platform that you have just shared with us. Your intra team or whoever, you or anyone who has access to say, your cloud. Just to give an example of AWS, in AWS you'll be using EKS, right? So you'll just give us an EKS Credential or an EKS cluster or you can also do it remote where we guide you and we'll provide you a helm chart and with that you can just install it on your cloud. Or if you give us access for one, we can install it for you and we can ensure that everything is set up so both these things will work. So after giving or sharing credentials with you, I can access your platform and do a bunch of stuff which you. Have just it will be installed on your cloud. Everything will be on your cloud. Like once you have given access, once you have installed, then after that you can take the access back and we don't have access to it, it will be completely yours. And then within that you will have your hosted UI that I sold. I stored only the UI method, but you can also use it from say, your CLI, or your Jupiter notebook or YAML, anything that you want. It's a platform that you share, right? Yeah. Exactly. And then later on, if you want to further like if there are upgrades, we easily push upgrades as well. The only information that flows to us is okay, in your team, these ten people are using it is the only information that goes apart from that. No client data, nothing, no model data, anything close to us. Even the UI can sit on your club. Yeah. So I have other questions. If something goes hey, while the system has like so like, how much technical help we can expect from if something goes up at midnight, those kind of services we can think of that way also. Yeah, completely. Like, generally we are working with three enterprises, Lecture, and all of them like anything goes, like our team is available, we create like a slack channel or anything that is convenient to you and all the times our team will be available. Also, there is like SLS that we try and maintain depending on whether it's like S zero severity, s one severity, or S two severity. So zero is like production related severity. So if anything goes wrong, we'll try and ensure that that is resolved within 1 hour or two. So that way we are fully there and we kind of provide the entire technical support that nothing goes wrong at your end and your team is kind of working like anything around it all goes wrong. You can just rely on us. Like you don't have to worry about it. Got it. And whatever data that we might be pulling or pushing our platform, also consuming that for your internal analysis, it will be all yours. We don't take any client data. It will not slowly to us. Lecture will not have access because it's installed on your cloud, so all the data will flow through your cloud. So we don't even have access to it. The only data that comes to us is how many users are using it. So, for example, Laksha is using Adhesive, so we'll only get okay, there are ten users signed in and they are logged into the platform. That is the only access. So that is more for billing and all purposes. Any cloud platform that will, when deploy on infrastructure will need that. Like, no data apart from that close to our system. Yeah. Cool. I think I have inputs from my side so that I can connect with the audience on this. And definitely is it possible for you to share this demo with me so that I can discuss with the audience also? I can share Lecture, but this demo was not very detailed. I wanted to actually give a detailed demo. So if we can connect maybe once either later this week or early next week, we can do a detailed demo and you will actually see all these things in much more detail. But it will be very easy for Arian to come up with some inputs from his side so that he can discuss more about the platform. So that's why I'm asking. Sure. This request request demo with us. I will share like, a short version of the demo that I saw. I'll also share like, a small presentation. And could we kind of think of maybe Monday or Tuesday next week or maybe even Friday this week for the next step in terms of a detailed demo? Yeah. So for that, I try to connect with Arian because his availability has to be there. Right. So that will depend upon his availability and calendar booking. So I'm not sure about that, but I can really push to have this discussion early next week. Okay, so let's try for that. We will follow up on the email and let's try to get that done. We have a lecture, your number, I believe, right. Like, let's take this head. I would really love to kind of help you and in that we'll also kind of help get help as early kind of company. So we really look forward to working. Thanks a lot. Great talking to you. Have a good one. Bye.