I spoke to Sudhart on phone. No, this is my uncle. And all that. Not yet. Not. There was no parasol because he wrote we are talking. So I just got confused if there is any. So we had Convulsion and Siddharth that he will introduce us to two teams. One is AI team, which is Myank and there is a second team, which is Empora, which he has not done at the follow up and doing that. Have we sent him the questions for. Multicloud questions you mean, right? No, because he said yes. Some of these questions. I think that makes sense here. Otherwise, it is much better to send the hypothesis at least with questions. Especially if you have confirmed your company to definitely multi cloud notes in this seat. By the way, I hope you see it becomes much easier. Makes sense. So I'll add into this only I'll not directly create here. I'll continue creating in my sheet. But generally. The point that you are writing, I'm writing same. Any additional point that is there, which I write on notion you drop that email. Actually, my case he's on another call. He is joined soon. Okay. He messaged. What I've done is I've created a WhatsApp from the Joe Amara outlival accountant. I've created a separate WhatsApp in the name of even someone sees you. It's like mentally Abhishek is not there on the call. After every ten message, there is at least one response that I get. Okay, that's good. Yeah. I mean, some of these techniques combined you will start getting response in us. Like sometimes WhatsApp does not work, text will work better. Text may problem. That does not work really well. WhatsApp works especially it may not be okay for someone who is a US citizen, but it works for Indian introductions. Generally trigon Western ebay. I'll set the context up because I'll just tell them that Abhishe may not be joining, but I'm in touch with him. I work very closely. I'll set basic context agenda. And then you can start as I called someone from high radius and after a long email from Abishik's phone and he asked me how you I asked him how he said not good. Tell you a live magazine and not good. Is that good? How are you? He said not good. Is it not the right time to talk? No, not the right time to talk. So I don't know what might have happened. Hello? Hey, can you guys hear me? Yes, we are able to hear you. How are you? I am good. How are you guys? Good. Are you able to hear us clearly? Yes, I can hear you. I'm on my phone, so I will be not turning on my camera. That is fine. It's a good time to talk, ma'am. Yeah, no, this is a good time. I'm just like jumping from one call to other. Let's just quickly get in. Let's just do a quick round of introduction and then we can dive in. Sure, I can go first. Basically I'm unrag one of the co founders at Truth Truefoundry along with Abhishek and Nikkunj just to quickly give background like we are all from 2013, Batchman from IIT, correct, you are in the same. Also I'm from Electrical, just like you. After that I went to work with WorldPoint which is an asset management firm. I'm sure a lot of your friends from it, they leave it also. Do you know anyone from your back who is in World Point? I remember a couple of folks went to World Court. Basically spent like six years at Worldcoin. Initially was on the research side, used to build trading strategies and then moved to US and then Singapore three years spend my time doing portfolio management so got a chance to manage 600 million assets for them. I was a member of the CEO of System of, looked after various strategic initiatives as well and during that time I used to also invest in startups myself as a very small angel, like five to ten K check sizes, just because enjoyed it and it was good interacting with founders and then finally wanted to build something. Then left my job in 2020. And Abhishek and Nkunj were on the same board, so they were with Facebook, so they like abishek was on the software engineering side, he led the infra team there and the videos team in the later part of his career at Facebook. And Nikkun was on the machine learning side. So he was the lead ML engineer in one of the AI teams there and prior to that he worked with a company called Reflection Building Recommendation Systems. So that's how we all came together in 2020. We built a startup in the talent space, sold it to Info because it became very hard to scale from an optical perspective. Adoption was very low and we were trying to kind of build standardized systems for companies to hire with interviews conducted automatically on our platform with some interviewers as well. So we had bootstrapped that we found it operationally hard there to scale and it's like we were building models like machine learning models for matching candidates to jobs and next job prediction and things like that. So build out of the models was fine, but then when we had to kind of integrate it with our website and web app in a reliable manner so that things don't go down and everything is well tested, one model is able to replace the other with some basic A B testing and things like that. We faced a lot of challenges like the system took us more than one and a half months to build and after that we understanding of cloud systems, open source and all of those things together. So we were discussing about Facebook internally, how the system is, and they told about the ability platform of Ishik and Nikmanjar mentioned, which kind of abstracts out everything from a data scientist perspective and from a researcher's perspective and exposes the infra in a way that they can easily test center models and the entire system is really awesome. So that's where we kind of thought we will start building. And if we can build something that kind of brings an ability to like platform to other companies around the globe and do it in a way that it's more simpler and is also kind of multicloud by nature, it could be a valuer and that's how we started. Truefoundry. It's been a one and a half years working with a few companies right now and doing a few pilots. And because we built the entire layer for machine learning deployment in a multicloud agnostic manner, we wanted to also talk to companies that are multicloud by design, want to understand what are the challenges they face, and also understand demo pipeline. So that was the motivation behind the call and then like trying to understand the problems if there is something that kind of aligns maybe even good to kind of take your help later on to see if there is a potential to work. So that is the big background and Rebecca is a part of the founders. Obviously works closely with me and nice to see you. Thanks for sharing. That's an amazing story. And also this is a big problem you spot on in general in industry people do like even in my team we do a lot of AI development but the main frustrator is a separate team and they are basically working day and night to make sure that what we build to get deployed. And sometimes these deployments are on the phone and they're on the cloud. So I just want to make sure that we are not missing the phone piece which is actually from our experience now on top of it, we also face another challenge which is being in the healthcare space, being the regulatory body. There is a lot that needs to be done from an infrastructure viewpoint to make sure that when we deploy something we call it change control in a way that when you are changing something, either you're retraining the model, you are reconfiguring the parameters on an active system, one that is not allowed to be done in that way. If it is a product which is life critical but in case there are some ways we can do it then there has to be a lot of processes on top of it to make sure that the right people are signing off on it before the change go live. We do enough testing and we call it verification and validation to even like ensure that the changes that we are making are not degrading the models, let us say. So there are a lot of those concerns. Now those concerns are mostly performance oriented but they have implications in infrastructure as well. As far as infrastructure is concerned I would say that yes, there's no solution out there. I know there are solutions like Amazon, their own solution. And I think we are using ML more and more lately and they have their own solutions as well. People do go with one or other. And I have seen fair amount of success with Azure ML lately, with several team members using it. Not for live production environment yet, but mostly for research and development to make sure that internally we can deploy and test things out. But then, yeah, to go live. We do like hands on building of solutions all the time for anything that is of critical nature. Like any product that Striker makes. Even before Striker, I would say when we were part of Gauss Surgical, we had this problem. I would say that we were mostly concerned with deploying on the phone at that time. And for deployment of AI models is not easy. It's like the wild, wild west at this point where you have to do things every time differently. If you're on Android, then you have to do like TensorFlow Lite and then there are all these things that can't go there and then all those on top of it. Very heavy on C to make it all work. So, yeah, when you do an iOS, then there's different set of challenges. They don't talk to each other quite well. So you can't use the same pipeline to deploy both on iOS and Android and then you have to worry on top of it. Like are the models similar in both platforms? Things like those. So I would say that the problem is there. I would like to know what you guys have come up with. In general, the problem identification is like amazing. Perfect. We'll go into that. I would love to hear a little bit how is the structure at Stryker? You are more a part of the research organ ML based on what I read on LinkedIn. So how is the overall ML or structured? And is it central to say different units or is it like different units? Stryker is quite big. We have businesses that sell a lot of medical products. So if you think about endoscopic cameras, communication equipment for operating room, lights, beds, then we have surgical tablets that allow nurses to improve efficiency in operating room, where we sold our blood loss monitoring technology to Stryker a couple of years back, which is now part of the portfolio. They have sucks on the system, they have take a note of things. And now over time, the company has realized that the next value add is to start adding automation and also some intelligence not just to the device, but also to imaging where we have cameras and systems already in place, where we actually sell those endoscopic equipment or cameras for operating room. So we formed a central AI hub within Stryker last year. There were a lot of teams across the company who are doing or starting to do AI development and we brought all of them together under one umbrella. And so that is where we currently sit. There are couple of I would say pillars to that. One is AI research, which I lead. The other one is AI systems which is basically what you guys are talking about. And deployment and monitoring and post deployment monitoring and also like versioning and things like those. So that is AI systems groups responsibility. Also AI systems looks into some deployment hardware because many of these models require edge compute. Particularly when we are talking about hospitals, they don't want their data out, things like those. Then there is an app team which is like AI application team. Now what we do is basically it's end to end. So the app is basically taking all these models and then bringing up a UI on top of it user flow and like making sure that the product and marketing team understand what we are building and then we build it as a product effectively. So that's why the app team is just not the models. At the end of the day it's the total application lifecycle. And then there is a bit of analytics as well, which is like business analytics, understanding how products are doing in the field and things like those. So this is all like sits under the AI. We call it striker. AI is a central entity within Stryker where we currently are working with different business units. So to say to help them further their product development. Either new product development or improvements to existing products. This is generally a lot like what I was reading is mostly a lot of the devices are being sold by Stryker, right? So where does the mobile development come into the play? I was guessing that a lot of this will be deployed in devices and I understand some of them might need edge. But where does mobile deployment? We are kind of selling iPads to operating rooms as well for deployment target in itself. So what happens is a single UI interface for managing the whole UI. So they can use the iPad camera to scan sponges to imagine also look into kind of a dashboard for every process that they need to track. So it's like an entity in the product itself. So that's why and then since many times we do not rely on the WiFi in the operating room or like in the hospital, 5G is not yet completely available. So you want to do like edge compute in these phones. So that's the reason why I mentioned that. That's one of the other aspect that we do worry about. Understood. Awesome, got it. So I would love to know more about the multi cloud and the research side. But before that, let me give you some context to what we are kind of building and it will obviously evolve as we work with more companies. But just at a high level. Primarily the goal is that anytime someone is building a model, starting from the process of building of a model, wherein you have to use any sort of virtual cloud, like say for example you are running a model and training it on GPUs or training it over virtual machines, you would need to spawn a cluster and run your training. And then you'd want those training runs to return and then kind of return all the logs. You will want to compare those locks and see which of these are doing well and be able to select one of those and say that okay, this is one good one and I want to move it to say a test environment. So we start from that perspective and then we continue till the end where you kind of are able to deploy to production, including some basic monitoring. So imagine like as a researcher, you kind of spin up different training instances. The system takes care of kind of optimizing over the kind of cloud costs or over the virtual machine costs. And then once you kind of finish the run, the system will automatically shut it down, all the logs will be returned on the system. You can select, you can launch it if you like it, you kind of can deploy it to a test environment or you can create a basic demo out of it in an easier way, like a basic front end demo where like the inputs and all are there and someone can go and play with it. Now after that, like depending on the companies, some companies allow say the data scientists to themselves depend. Some companies that is taken up by the ML engineering team. So then once it is deployed to a test environment, from there the ML engineer can pick it up and they can kind of run like an A B test on it and they can move it to the prod environment by testing with the challenger model that is in Prod. And the movement of the prod happens automatically through a promotion flow. And once you have moved to Prod, like the entire auto scaling and everything is taken care of so that the system is cost optimal and it does not consume additional resources. And all this while you are able to kind of get monitoring for your service that what is running, what is not running and so on. And you also get basic drift metrics. If something goes wrong in prod and your model is drifting or your data is drifting, you get that basic drift and you are able to send the signal back to the corresponding system. Hey, you need to re trigger training. So that is the kind of pipeline that roughly it is and then from an intra perspective it supports like multi cloud. Suppose a company has Gcp, AWS, SEO, all of them and they would want to kind of optimize on cloud costs for their deployment. So it is able to automatically efficiently select which instance site it should use for best output as well as low cost. So like different instance types in say GPUs if you're using it might have different costs for different clouds and there like it is able to optimize on that. So that's basically the goal. Some parts that have not been built yet is the cost optimization across cloud is not built but the infrastructure supporting that is built and the AB testing automation retraining is not built but the rest of the system is already built and also live with a few places. That's basically the pleasure. Thanks for sharing indefinitely. I have seen or at least heard similar pictures from your other companies out in the Bay Area as well as in the US in general. One thing is I think there's a lot of scope here for several players to be in the field. One thing that I would say is specializing for one vertical may make more sense in general. I think the pitch that you gave is something that I think Azure ML also provides to a certain extent. So you ideally do not want to because the big player will always be like big player, right? Like big companies will go big players no matter what. Because one of the other reasons why at least I'll just share the decision making process to a little bit is that long term viability or long term stability of some choices is a very big factor when we make decisions like these to go with. So since for a general pitch, that's where I guess on a very specialized pitch on something that is very specific to a certain industry, it would make a ton more sense to focus on. Now, I'll tell you why I mentioned it. The process that you mentioned is not something we follow, okay? And that's fine because healthcare is a very complicated industry. The processes are way more complicated. It does not just go models don't go from research to development to testing to production from a click away. It has to go to a lot of testing and other things. I would not recommend you guys building for healthcare either because healthcare is in itself a lot in there to consume and it would take like unless you guys are really motivated and have some domain experts who can help you out on how to build processes around, that would be a killer product, by the way. I can put you in the five years or three years down the line, several healthcare companies would jump on the AI ship and would realize that. But having said so, I think specialization is important in general to go with. I think the problem is there is just that what you just told me looks very similar to what I have heard from several folks and that's okay till the point you're willing to work with companies on a one on one basis where your proposal fits and then you keep refining and growing. Basically that is the idea. What I told you is more like an overall vision and the way we are also building is very kind of API driven. So for example, you don't have to necessarily follow this process. So that's why we generally kind of try and understand exactly the organization's workflow. And based on that, if we are able to identify I just told you so that you have an overall perspective but the main core area is the deployment optimization. So multi cloud deployment optimization which generally like the cloud providers are not good at, that is where the focus is. So multi environment, multi cloud deployment environment which is more systems oriented, that is where the major focus is. But we kind of try and understand in this early stages like the use cases and based on that we are able to focus that okay, this is the part that best fits and this is how we'll be the best in that corresponding part. So just wanted to kind of share this at a high level so that. You have some definitely yeah. And do you guys have an online demo or something like to go with like how do you guys interface with new people? So generally what we try and do, we try and do customize demos ma'am. So just say the two things I was telling you. Right. I would love to know the workflow once and then also around the multi cloud thing that I wanted to know and post that I would love to set up another call where I can show you a more demo that kind of probably you will relate more with. So that is how we generally do. I can send you a short demo video if you want. But generally like demos are better than life. So that's where like the reason I ask is because the two things we have our ML Ops team or AI Systems team effectively figuring out a couple of these things for the whole team or for the whole and they are also searching for solutions and I know they are right now heavily inclined towards Azure ML and also associated processes. I don't think at an organization they will striker is like Microsoft, effectively like Azure. That's how it is. In general, we do not it's also about stability. It's not like well, you want to build a medical product and you want to see it through for next 20 years. You don't want like cyber security issues. So you build it once at one place and speech table around it. So one thing you guys can do is show this to the ML Ops team who would be the right audience to see if they can see how it is relating. The other one is in general the need I can tell you is like a bit more complicated than you mentioned and it's for good reasons. So we heavily focus on data and data iterations in general. So we have processes around, like, within my team to try out different data. And data burgeoning is one of the key pain points because if you try on one data and then you keep adding new data, as you have hospital partners provide you more data sets or you are collecting data on your own, you have like a whole stream of data and maybe the data is coming live from customers. That also happens for products that are released. And one of the things that we have been like unable to figure out or I would say we know a couple of solutions out there that works okay is to how do you like tie the data version to the model that you are training. And there is a good reason why we want to do it is because at the end of the day, any model that we deploy, it has to go through FD, and FD wants to know what data it is straight on from both the distribution viewpoint and also from what sites were part of it. Are there agreement with them, data privacy and things like that. So there is one key open ended question that the team is still figuring out. So that's where a lot of processes comes in in terms of training and deployment. We have been a bit old school at this point on there, which is like we train on either on Prem or on cloud, but R and my team would keep training, improving it rating with data. The model is not the ending game, right? Like model is just like one aspect of the system. We work with the application team to build that model together with all the extra logic. We even have team members who would put logic around Model because the model is not necessarily right all the time. So we have developers who would take the model, put it into a testing environment within AI research still. So it's very custom because you have to then do it on the device or on your computer or on your on Prem computer, develop a software system and then develop an API for the app team. And then they take that whole algorithm. We call it just not the model because it's just a model plus all the extra logic that goes in preprocessing, post processing, anything. And then you take it to the app team. Now, app team can deploy it on the cloud, can deploy it on phone based on the product, but we do not generally deploy it in all right, because for any product only one thing makes sense. So for a product it could be Nvidia Jackson where we deploy or something like that, for another it could be mobile and for others so then we actually also adopt our delivery. Is it a C plus package or is it for cloud? It could be a Python package like a docker image. No, we also adapt accordingly. So I would say right now it's so hands on with everything and every other product that there is no process in the development side. There is a process, but there's no consistency among one product to others. You can't generalize it at this point we go through and then once the app team actually incorporated, we go through like a million iterations of testing before we go to production. So by that we build internal apps, we let them test, then we have a formal testing that happens through a protocol that we call the vNP protocol. So it has to be done with the protocol needs to be routed by that. It means that once we have to send down the protocol before we go and do the testing and a separate team does the testing, it's all done for objectivity and then we are like a lot of processes happen and then the deployment actually happened. I would say that it's a bit more a lot involved there and I would love somebody to actually address all the steps there, but it's something that's like too complicated. I want to understand it. At what point does the research team, like when you all are training models? First question is do you end up using say GPUs et cetera in your training or do you end up using distributed training or is it primarily mostly a lot of CPU machines, et cetera? No, we have on Prem as well. So we try to do on Prem for major development. But then when we have to run a lot of experiments together then you send it to Azure ML or like AWS cloud, but mostly Azure ML these days. Like before acquisition we were like on AWS but now we are removing our AWS dependencies to remain since Striker is all Microsoft. But then yeah, between these two. But then I would say that it is very developer dependent and very much like sporadic. Like we have performance leap to gain. So then somebody would throw an experiment run on the cloud and then we'll get back over the weekend or after a few days to see where we are. It's not like very much like every model has to go through these stages. Not like that. I would love to bring the team to that level, but our velocity is a lot from work. But then the number of products we send out is like five or three per year and a lot has to go through before we can send something out to the customer. Yeah, understood. Quick question in terms of just understanding the flow a little bit more. So after this training is done, do the researchers themselves kind of host like. The API Endpoint, which is yes, we do not call ourselves a research team in that way. We are research and development team. So the API endpoint is something. That it's not the making of the API endpoint, but I would call it like the interface with the application need to be built. So if you think about the image that is passed by the application, any qualification of that image? Is the dimension correct or not? Is there anything wrong in the image? We need to check for that? We can't throw it over the wall to somebody else. Any logic that goes on, like either image or if you're doing NLP related things, the app team responsibility is just to make sure that data from the user comes to us. Okay, I see. That's why I say that the model is encapsulated in the whole app, our own algorithm code base and exposed as an API. Now, we do not necessarily say that, well, are we doing rest? API is on the cloud. Not that we can offload it to a cloud team, but we can do mostly we have to define like, okay, what are the input expectations to write unit test for that? What are the outputs? We have to write unit test for that. So we do that, all that. It's just not the models. Okay, understood. And currently for all of this, all the trainings and all the models, you are registering it in Azure ML Model Registry only? Yes, right now? Yes. Azure ML model registry. And we are also using ML Flow experiments there. We were using waste and bias. But then again, we are moving to ML Flow has more native support with Azure ML. Yeah. Then one question. Mayan is around the multi cloud, like I was seeing in the JDS that generally Striker is multi cloud. But you are saying your part is only you are using Sqml. But you also said that the deployment team then can go and deploy on multiple environments. So there like, where does the multi cloud is there a theme towards moving to multicloud? Is there a reason for it? Or is it more like what is. Not the right person to answer that question? There is a separate team who can answer that multi cloud aspect of it. I'm pretty sure there are some discussion, some strategy there, but I'm not privy to those and neither am I equipped to answer cloud related questions. Okay, but from your research, primarily, you all don't have a reason for your research to be multi cloud as of now, right? Unless the cost becomes a bigger concern from one provider to other, which generally is not the case. Usually most cloud providers are very comparable in their rates. Then we don't have to worry about it. The other reason is that we have on Prem, like for the exact cost reason we have on Prem servers. And this is not just like it managing it, we actually manage it ourselves. So we have our own server room, we manage all the GPU servers. We buy them and we throw them away. But we do it because we want all developers to have the latest and the greatest GPUs. We can't rely on that key to actually provide those. Okay, who manages the intra for the research? Like okay, quit training is the ultimate like provisioning of whether you will do it on Prem versus your ML. Like is it managed through some orchestration layer like a Kubernetes or something? No. Yeah, so the team is like 15 to 20 people. We are kind of doing it very hands on is like finally going to on cloud so the only thing we do is we do dockerize everything so you can go from on prem to ONCLOUD pretty easily and then I definitely want you to connect with like in case I'm more curious to the ML ops. ML Ops in the AI system is helping us also put up these standardized authorization and processes in place as well so that you can move from this to that without much but usually there's nobody. Every developer has his own choices to make whether he wants to do on Prem because the previously stays and he is just like being we want to just see code in front of them and make sure that everything is correct. Whereas when you go on cloud and execute on fast iterations. Is there no restriction, say multiple resources start using the on Prem and that limit is exhausted. Then of course there is. We keep buying new service. Okay, keep buying new service. It is cheaper than going on cloud actually I can tell you because I've been budgeting that for last one to two years. Buying a server is $50,000. High end, four GPU, 30, 90 sort of a server or even like 6000 as well as around $23,000 and you can easily recoup that cost in like three to six months of life. Generally the service lasts for a couple of years at least. If we are not even taking the headache of fixing any problem like fans or anything we just toss it away or give it back to the company we bought it from. So we kind of use it and then give it back to them. They assemble systems for us like this assemble system for many companies here and they can take the old system back when you are done with using it. Understood. So currently what is one problem that you would ideally want? This is one thing that if someone works with me and makes a really good system for me it would be interesting. Like I want to understand that what would your ideal kind of system look like? Or it could be a system that does this workflow. I want to understand that data. That's one. The other one is just that keeping track. One of the things that we are still missing is when somebody does experiments on device on prem and then take it to on cloud. We haven't done many, there is no continuity yet. Then you have a different set of monitoring than on Prem. And so that continuity. That's why I've been asking the ML Ops team to dockerize things and provide processes to the team members. But I would say that there should be a continuity. Like no matter whether you're doing on Prem versus on cloud, I would see that multi cloud or on Prem is kind of like just an extension and then continuity around tracking. And we are not yet in the deployment because the deployment is very complicated for us. So it's mostly like Rnd and making developments to the model till the point we are all very satisfied about the model's performance and then go live. Got it. I think get the overall perspective. Okay, where are you? I am in India, but Nikonjo is one of my co founders. He's in the Bay Area. And Vishik is also in India. Most of the team is in India. Which city? Bangladesh. Okay. That's what I was asking. Okay, cool. Thanks. I need to jump, but nice connecting with you. Let me know if it makes sense for you to connect with the ML Ops team. They're still figuring out a lot of things setting up for the whole Arg. They are the right people to talk to, I would say in terms of your detail explanation, I can set it up if you're up for it. I already told you the data. If possible, one is set up another call with you just to kind of show you once the system and get your feedback. And then obviously it will be great to connect to the MLS team and understand their challenges. You already spoke about some of them, but also trying to understand their priorities and block to see if there are any potentials there as well. Okay, yeah, I can set that up. And like the follow up call, I'm kind of busy in the next couple of weeks. We are trying to wrap up our planning for the year. We could do the week after, which is not this week, not next week, but the week after that. Two weeks is a good time. Sounds good. Then I can connect there. Okay, cool. I'll do that. Okay, thanks. Take care. Thank you. Thanks a lot, man. Great talking to you. Thank you so much. Bye bye. Thank you. Like most of the health care companies from our two, three costs that we have seen production mainly data, but they are more focused around iterating. It fast. And that could also be experimentation. It could also be in house production curriculum and then spend test results. Those kind of a setup. So I feel that. And second is, again, health care. The point that he was mentioning, more focused, the regular pitch that we have versus the need of health care. Level. I could have just talked about research, but only but it's okay. It's not a big deal. Mainly he will help with connect to the. Account on WhatsApp I'll create a WhatsApp group include you. Great talking man as this will connect two weeks later in order with you but at the same time do let me know who is the person. He'Ll do with that. Connection? It's hard to figure out but they are looking to solve they are looking. To solve. The way we have been. Doing it for Merck and for Synopsis I think one healthcare company point that he mentioned was very exciting for healthcare other vertical he mentioned two things. One is health care vertical is not a great vertical to start off with because the needs are very unique in itself. But it is possible it might work out well for developers. It's hard to say but again, the. Way he is saying decision making of money the way he casually said we don't even care about repairing it. Does not matter. I think that is what storming to. Deep pocket decision making power and deep. Pocket he also has research side me he has the decision making power. That is definitely true. He is the budgeting people know budgeting. Because that company is 100 to 50 million so they have independence. Let's see how it goes. He has not responded to my message. Not sure whether. He will not be able to tell much on multi cloud that I think the Ops team will be able to tell so that we should write on multi cloud he is not the right person to talk about it. He just was able to tell locally SEO MLS from his perspective, it's not multicloud anymore, right? And the only reason it was multi cloud for some time was because it is hybrid key on prem versus that is there. He has not responded john for the time I'll put in my notes I will written it down after you have finished your part I'll do. That. Take care bye.