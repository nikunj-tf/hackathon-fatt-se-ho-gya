Hello? Hey. Hi, Bobby. How are you? Can you give me okay, yeah, I can just hang on, let me turn on the line. Yeah. Okay, sorry. No problem. Bobby actually my co founder. I wish I had reached out to you, but he's not able to because he's not feeling very well, so I'm taking the call on his behalf. We are patchmates from undergrad and like our third co founder and Ruggage also, all three of us went to It for our undergrad. Great school. Yeah. Where are you based, Bobby? Yes, I'm based in New York City area. I was in New York City, but just moved out a little bit. Just a little bit north. Yeah, I see. Okay, so it's like 03:00 P.m now. Yeah. And you are in India right now? I'm in India. I'm generally based out of San Francisco. I'm traveling to India for a team of site, basically. Okay, I got it. What's your role then? If you can tell me. Like a quick background about myself. I come from a machine learning background, basically. After that I moved to the US for my masters. I went to UC Berkeley here, and since then I'm working in machine learning. Like, I worked at a startup called Reflection where we built out a lot of recommended systems. And then I worked at Facebook where I led one of their conversation AI teams. And after that I did one startup in machine learning as well in the HR tech space that got acquired. And this is like my second startup with the same set of co founders. And at co founder, I'm the CEO of the company. Okay, got it. Okay. What is the company that you sold, you said? Yes, the last order that we sold to Infoyage. Okay, what did that do? Do you know about Northwest.com? Yes, infoyage is the parent company of Nockley.com. Yeah. So we were building an HR tech space. They were also building a similar product, but they did not have a lot of traction back then and we did not want to continue building in super operational space. Like you wanted to build more technical platform, basically. Got it. By any chance do you work closely with Abhi Abe? Yes, he reports to me. Okay, nice. How do you know him? Mutual connection. He's a smart guy, I like him a lot. He's brain behind lots of this stuff. You've got nice. Yeah. Awesome. So Bobby, basically like today's purpose, the reason I reached out to you was primarily in the context of Truefoundry. So the idea is that we are building this machine learning platform similar to what we saw at Facebook, where we improved developer productivity by simply building out tools that are used across different machine learning models for deployment, for monitoring, for tracking, for versioning, all those things that you end up repeating right across different models. So we're building out a similar platform and now working with a few large startups and enterprises where we are helping them solve their problems. Right. And in this journey, the idea was that I wanted to learn more from people who are solving similar problems. Just figuring out are people facing this problem, is this a legit problem or not? How many people care about it? That's what I'm trying to figure out right now. Yeah, so there's a way of a little bit of my background as well, right. I run all of engineering classically in lots of ways as well, product design as well, for not to meet. So I joined them from DND, where I was chief finance officer. DND was, I don't know if you remember, roughly four years ago it was taken private because they stopped going 150 year old company. So plan was to turn this data company into analysis company because you can charge more to analytics and then take it public. So I got a deal I couldn't refuse and challenge and we did this and two years later we took it public. So I catch my chips and join them. Let me write so clearly there it was all about DevOps because if you think about it and then my previous job was about that as well. So let's talk about this job. So DNB, obviously data company, they put large amounts of data coming in, they want to provide insights from it, they have customers who want to get insight from this data after combining with their own data. Right? So what we build was analytics sandbox, if you will, where old data was already curated. We already had template models for like a better word, already built. And then all they had to do was upload their data with certain information in a particular format, if you will, and then model would run and custom model would be built and it will tell you how much uplift it will provide. And then if they agree it gets deployed, then we should charge $500 per API call for scoring. Big part of all that, certainly data science was there. Building the model for risk was primary space, but sales and marketing, those are two primary spaces, right. Government was also there, but actually government was pretty big. So the entire Tart money that was given out, trillion dollars that came out of that sandbox, my team did all the answers for White House. I was on call because a US government can't figure out how many companies there are, the real or not, because you are not allowed to require to necessarily register, especially with the startup companies, right. So they could explain for example, how much impact certain things would have or COVID was having at an early stage, at least in Rustwell as an example. Right. So the point to be made here is it was all about big chunk of that offering with MLS, right? So we actually charging minimum $200,000 a year and then depending on how can you use it, how many times you run the models, how much data you take. We charge for the data also and then of course, once you deploy the model, we provided the service and we charge you for scoring $500 per scoring. Right. So they're paying a lot around that, like companies like American Express, other companies american Express, you can guess, is about risk, right? Somebody wants to charge $5,000 on a corporate card and they want to risk quoted before the charge is approved. Before that, before the charge is approved they would take the information on that business entity or the information on what is being charged, what location, blah, blah, blah. And so they build their own models combining with data we had on various entities and then deploy it in our environment. They'll make a distributed call and they would just score it and send it back to them. And then that would in real time, well near real time reject or approve the car. Right? That's an obvious use case. I mean, if you could do that, you can make money, right? So there are companies that are coming in I recall two years ago that came out of Georgia Tech Atlanta that was providing a sandbox for risk trading and whatever else basically for those kinds of things in the cloud, right. You just upload your data, buy the data that is housed in the sandbox and you pay for the data to various providers and now you can within matter of minutes start running by building models. Right, right. Makes sense. There'S. That just like a long introduction. Last one I'll stop it. Something similar I did more constantly, which was in house because financial companies are lot more regulated and so they are not as open to using cloud where all the things are solicitly moving in their direction. Right? So we had 120 node cloud database for same thing, like 10,000 fees would come in on a nightly basis, all the data would be, you know, ETL curated, validated, combined together in a meaningful fashion and distributed them through distribution service, but also kept right there to be able to build models. To be able to build models. Because more recently the whole strategy was to grow wealth, which is still growing at the rate of 20% a year, even though the biggest wealth provider using machine learning so it would take 360 view of the customer, what you bought and sold, research on products, it could combine it and have various recommendations, actionable recommendations for customers. Right. So again, the biggest piece of that was really analogous. So the thing is that for ML Ops, you have a lot of data you need to be able to basically build models on it, have different versions of it, be able to activate old version and explain what was or compare version two with version one and explain what has changed from two to one. Right. Every six months you're required to explain why you think your model is still working and then generate alerts when it stops working, the genie and things like that and generate alerts. Okay, it was working. Then user behavior change, data change, now it's not working. Right. So generate an alert and then to say okay, now you have to retain the model. That makes sense. Yeah, for sure. What's your angle? Because now if I turn that around you do have a lot of competition in ML Ops, right. So you have to find at least something that differentiate yourself from somebody else. Otherwise it's a doggie dog race. Right? That is very true. What do you end up doing at Netomi for machine learning? Bobby Kennedy are you using a bunch of machine learning? We are a company before anything else, so we use a lot of deep learning. So we have a large number of models to do wide variety of things. Right. So from understanding what you just said for help, for customer help. Customer care, right. To basically have a conversation right out of the box using AI, just one thing to understand what somebody said is another to actually respond to it using it, which is much higher bar. So that to things like anticipate the problem person might have do the recommendations, things of that sort for upsell and cross selling all those classic customer care problem which you know, you can take it in the direction of productive care because now even IoT and things like that, if somebody bought something you got even. So you could take the events that are coming in and combine it with the data and practically make a guess out this customer may call in because their field just probably stopped working. Right. So you end up using AI for a lot of this customer experience, other things. Where are you using a lot of this AI? On the cloud itself. We use some services from Amazon sorry, google, like Google Translate and things like that. Got it. I see. And do you end up using like a Sage Maker or something for machine learning or like everything is on? Yeah, you have to realize that I use Sage Maker at Morgan because they are more process centric. Right. So they need to have different versions of the model. You need people to explain what changed from version A to version B and there is a separate group that actually reviews your model and approves it for deployment or not. Right. So it needs to be reviewed from the point of view that let's say is bias against women as an example or things of that sort. It doesn't increase company form by risk, things of that sort. Right. So it goes through all that. So we have to produce certain metrics before they would approve the model to be deployed. So you keep it, you use Stage Maker, you have different versions. Not only you have to snapshot the model that it's not good enough. But you have to snapshot the data conceptually, right? Right. So now you can see MLS becomes really complex and horrendous when you have to do that kind of stuff. Right, Netomi, yeah, it's just a startup. Right. I mean, you build a model, you run a model from a bucket, you make a copy of the data and you're not going to pay for it and anything big startup like yours. Right. So it's less about that. Right. And although we do deep learning and things of that sort, data definitely is not always as large as it was modern sunny right. Terabyte type of data. Right. So do you all end up using Kubernetes in that case? Yeah, we do use Kubernetes. We use ECS, certainly that I see. Even the machine learning models are deployed on Kubernetes. Yeah. So machine learning models are deployed a combination and they deployed on ECS. We do use GPD three for some of those. For some of them, Kubernetes just get in the picture. But I think it was more easier than GPD three combination than Kubernetes. But yeah, we are trying to use more and more Kubernetes. I see. Another problem is Kubernetes, to be honest, as compared to ECS, right. You need a residual staff of two, three people to manage a large info of Kubernetes. Right. So just because you have a few containers, it doesn't mean you can get away with quarter head count. But three head counts is a lot in a startup. Right. So that's other part, so you have to have critical size even to go for Kubernetes. And certainly migration from EC to ECS was fairly straightforward. So big chunk of our stuff is still in ECS. I see. Okay, understood. Yeah. A big chunk is using ECS, including machine learning models. And we do some do some catching in elasticsearch and things of that sort too. So other big part of machine learning is which nobody solves is real time scoring. Right. The building model is easy part. Right. So I can build a model easily. Okay. But I built it now. What that's? The time services I need to score in a performance way. And quite often the model you build, maybe that was a common problem at DMV. So my data is large. I built a model using Spark, but I'm not going to say deploy it on Spark for scoring. I mean, how do I do scoring on Spark? It's not that easy. Database always trying to solve that because now Database has almost like their own operating system that runs on the Spark. It allows shared sessions and things of that sort. So multiple things could be doing multiple things on this park. But until that came along, there was no way to really, in a regional fashion, put a service in a Spark that does real time scoring. So what would you do? You end up taking that model and take a position that help. But you know, scoring is much faster than building a model. So I'm going to take that model, but actually I'm going to deploy it in standard Linux hardware. I should say Linux hardware. And only challenge is really quickly accessing the slice of the data that I need. So I would use Elastic search for it, cache it in Elastic Search for long term caching, right? So that was classic approach we took. Right. So do the modeling on the spot deployed on the standard hardware and do the Caching and elastic search. Our luck here. If you have a better solution, there isn't for large data, then you have to do on the fly fast aggregation and then bring that into model. Right? So there are some challenges that one faces with large data, extremely large data. Of course those challenges are not as severe and we bypass them obviously in a rudimentary way because nobody is beating on our neck to say you have to be able to go back by a version or two. Understood. So Bobby, if you have to think about your current like in the startup Anderson, there are not a lot of governance challenges. What are some challenges with respect to your operationalization of your machine learning today, if any? Yeah, one of them is obviously the real time is coding almost fast. It is still more time consuming than let's say running a simple database query, picking a record and giving a response if you will. Right? And that's a challenge because if you think about it, for me I'm doing a voice, right? So I need help with luggage. Okay. So it goes through AI, whole thing goes through collection services. We turn into messages. So we are using Kafka Kafka to scale it. But at the end of the day then eventually the other science will say, okay, I got the message, this is for me, I need to score it and then it basically scores it. So I've solved some of this problem by making everything as synchronous so that my user will not hung up. Well, all things are going through this and AI going to score it and eventually send a response, right. User can say something else in the meantime or whatever else. But still you have a challenge of scoring it within 1 second or so realistically and sending a response. If it's a phone call, it's going to time out. If you didn't do that, most of the voices stuff times out after two or 3 seconds and so you face those challenges. So you have to really still scale it. The only way to scale it is really cache this data, right? And so you're not going to cache that is not durable. So you have to end up caching in redis elasticsearch. So that at runtime the service that's going to do the scoring can quickly get to the data, do the scoring and move on. Got it? So what's the server layer that you end up using for these models? Is it fast? API. Yeah, we use fast API. We use that. There are a couple of other things that are being used, but depending on models but that's been used. Do you know if the team has tried using any model servers like TF Server or Todd Serve? No, it's all in house, it's all open source. These are also open source frameworks that I'm referring to. We build a little extra stuff on top of those, right? We use all of those. Right. But it's not not in that form. Right. You have to build more of assets and frameworks to be able to use all those. Retrieve the data quickly from a search and figure out which slice of data you need those kind of things and feed into them and go from there. Got it. I see. Okay. The case that any of these can be used out of the box entirely. So you either buy something from MLS product, I mean, HTO has got something and a bunch of other ones have other mechanisms for deploying it, or you built some glue to combine a bunch of things. Do you use h two internally? No. Okay, understood. I see. Payments. So HT was, to be honest, used a little bit at Morgan, but it was never deployed. It was just more like prototyping. They tried hard and really printed. So we build our own thing on top of Spark and hadoop infrastructure cloud era in this case. And again competition of elastic search for Caching the data. I forgot. How have you structured your machine learning team, Bobby? Is it separately data science team or separate engineering team or how is that structured overall? It's a small company, you really do that. But there is that structure, right? I mean standard structured, you've got DevOps DevOps does standard DevOps like network to setting up containers, Kubernetes, ECS and those kind of things. And then the data science team does I would say ML Ops to an extent with the help of DevOps. And then there are data science vertical teams that build models for various things. Like we are in voice of customers. There are teams that's the voice of customer. There are teams that build models for certain aspects we have around the studio, if you will. That one could be described as special purpose data robot the robot robot, if you will, that offers not only view into the data by doing initialing supervisors and unsupervised learning initially to figure out what should be done with the data. But then once it's deployed actually, there's an optimizer in the studio that suggests, based on again, AI models, where AI should be retrained so user can review it and accept it or continue with the refinement of the setup. I see. Okay, understood. The thing is that lots of these tools like if you look at S two O data robot and stuff. There are general purpose, general purpose tools that do general analogs they really have no regard to the domain and things like that. It's the same issue at data bricks when I build a ML sandbox with databricks database has no idea about machine learning. So we have to think what are the subject spaces? But the whole end to end flow would work from the whole user experience point of view. Somebody who wants to build a risk model as an example, right? So the user experience was as follows, right? And you can't really build that custom user. My experience in an auditor robot so basically there was a portal somebody could say oh, I want to build my own model. Let's say it will ask you do you want to build this model, get sales and marketing model, whatever else is this model? We are five custom models, we support custom types of models. So you select one or within that what aspect of risk are you looking for? Right? So you select one. Okay, fine. So to be able to build this custom model, I need this sort of data from you give me the s three bucket where you have digital setting, right? So it sucks that in combines it with the data they have purchased the DNB data and business entities. I mean, American Express, if they want to build this, it would churn and say okay, come back, come back after 6 hours or tomorrow, when it's run, you can look at it in the meantime, it's independent. So they are worth flow aspects, right? Independent state, it's running, they come back next day, it's run, it gives you then all data science metrics preceding the call doesn't make sense if no business user would understand precinct recall, right? Exactly. You give the upgrade this new model provides what's a traditional model that DNB has purely based on this on data set as compared to traditional model that DNB has it provides additional 30% uplift, right? And they can give you it'll tell you for what type of businesses, large and small, where it works, where it did not work, right? So that you could just use it for subset of customers or all of them you need to it now you're happy, you're happy, right? So you would say okay, deploy it. So it would say okay, come back after 3 hours, it will be deployed when it's deployed automatically, more or less automatically, it's just a bad job that does it would take the data cache it in elasticsearch, it would do appropriate aggregations where it needs to do because there were well defined mechanisms for doing that. So that because you can't do 100 million records aggregates on the fly. So if your model requires that, well, then you have to do that beforehand. So it would do that automatically, right? And then it would put it in the cache and say okay, here's the end point makes sense, right? So to be able to do that you have to think about your business problem domains, various constructs you have and things like that, right? There is much to be made there because then you are counting on that's one angle so you're counting on business expertise, right? So I don't know if they ever build it. I left by the time I had this thought that there were three parties that had expertise in various areas. So the question became why should we build the models? Or a customer may not have live stream first of all now value to American Express is that they don't have to have in house this expensive livestream that's dealing with spark this and that they can upload their data within a matter of hours on their way, right? So they just could do it themselves, they don't even need it more or less, right? For deployment now they need ID but now they don't because it gets deployed on our side and it uses the security mechanisms that they want single sign on sample to whatever else they want to use it and they trust DMV so they don't have to deploy it. So all they have to do is have their service point to wait for scoring at real time when somebody applies for that, right? So then question really became that. So we got three things DMV builds the model that can do the scoring on DMV data a customer can build a model combining their own data and then it can be deployed in DMV site and we can charge on scoring and from building the model, right? And then the third part is, why should we do all this? And think about it? When building these templated models, the third parties may have insurance, the insurance companies, maybe they want to build their own model, like have Apple has Apple Store. Apple App store. Essentially, the third parties can build their own models finding created bases, using DMV data and deploying or maybe some of their own data and then basically hosting their models. And they can sell it to we can sell it to resell them or they can sell and we charge a small cut. So there was a plan for that last one ever happen? Understood. So Bobby, from your perspective, are you thinking of potentially working with any tools in this domain to kind of optimize for your dev workflows and stuff like that, like delivery in general? I mean to be honest that has not been a priority. We can talk about it certainly there would be no appetite to take too much money, right? That's the point, right? Because that's really not a priority. Nobody's complaining that I don't have versioning as an example, right? It's not a smart company that does become more and more important as we engage with large companies, enterprise size companies, which is where we are heading. So probably a year from now that next year, it might become important. At the moment, priority isn't that priority is to build a model, thinking about new ways of doing things right. So that's where most of the energy has been going. I see. Actually on this one, I think there is a little bit more to it than model versioning and tracking and the more bookkeeping stuff. What I would love to do Bobby, is kind of show you a little bit of what we have built out. Would love to at the very minimum get your feedback and sure I would love to probably have your team try it out, but at the very minimum we'd love to get your feedback here. Yeah, and I'm happy to talk and certainly we can compare and try it out depending on the cost. But certainly it wouldn't make sense to try it out at all if the cost was not reasonable. Trying to help me? I'm curious Bobby, you mentioned about the cost. So for a startup of like Netomi size, right, for a problem like operationalizing machine learning, like in an year, what's a reasonable cost? What would be exorbitant? What would be reasonable? What would be too cheap? It depends on use. Don't forget most of our team is in India. All the sellers have gone up. It's really still not as expensive. Right, so most of the energy isn't really spent in ML Ops entirely, right? Because you already have DevOps that manages in physical container. And I would have to see that if you solve the things end to end because at the end of the problem is nobody provides entire MLS solution. If I define MLS as basically you would let me house the data, point to a particular slice of the data, build a model and then deploy the model in a container. But also you would also do the aggregations and cache it automatically in elasticsearch and figure out what needs to be cashed. Nobody provides it and then you would assure it to me to convince that you do all of them even. Right, so then what you end up doing is that you buy a product and then you spend shitload of energy putting extra good on it for what you might as well do a little more where you have full control. It works just your way for your specific frameworks that you build on top of it. That is a response. That is a fetchist response. One would respond to it question. One would respond to in terms of cost, you were asking how much cost maybe you made. There are two people, two people in India were doing it. That's the cost because open source stuff becomes really good. You have made some simplifying assumptions because you're not building product, you're building product. But I am not right. I make simplifying assumptions based on my service framework, based on what particular technique I'm going to use Elastic Search for Caching as an example. How I'm. Going to cache it and think like that, I can make those assumptions. And then once you do that, there is not as much incremental work. Once you do it, same thing on the product. Then what you end up doing is you end up dealing with limiting well, I should say, let's say idiosyncrasies of the product. Right. So adoption of the product itself requires a team. Right. That makes sense. Yeah, it makes sense. At DNB, it made sense, actually. I was paying $5 million a year to data bricks for building that genetics center. That makes total sense. There's no way we could have done it ourselves at that level. Right. Because once you combine the data level, security aspects and things like that, it being client facing system, not that easy. Got it. Okay. So as I understand the cost for probably two people in India, no more than $100,000 for you. So that should be the cost. Yeah, that's the point. That's the reality. Right. So I'm not saying that company, but probably you are not looking at right market. If you are looking at a smaller size company, they're just not going to have appetite to pay that much. If you're in a position sell it for $20,000 a year, probably you can. Right. But then you have to price it that way and then have a model that works in that direction for low end use. The real money is elsewhere. Real money is really, I'll tell you, is there at the peak of a database. I'll tell you, I was paying database $6 million a year by the time I left. And American Express itself alone was paying us roughly $3 million a year to use this inbox. Wow. It's crazy. But it was because it provided end to end stuff, because we had our own data, so they would pay for the data. So first video of it was not only they were paying $3 million for this sandbox, but it was like the best way of promoting your data. They would need the data they see, then they list, oh, I want to build a risk model. Okay, so you need these data sets from DND. Do you already have access to them? No. Okay. There is your credit card if you need it. Right. More money. More money. It would allow you to build the model. It would deploy it, and it'll allow you to score it'll, set up the security aspects of it, like which slice of data, who has access to, and things of that sort. So there were those aspects. So if you had not acquired DMV data, it's not going to give you access to that data where it was sitting there. Right. So it would apply the filtering when model is built based on what you paid for, things like that. Right. So that does take quite a lot of energy to build. Right, that makes sense. Yeah. No, I think I completely appreciate your suggestion on the market itself, bobby, I would love to take some time, like next week to show you a demo of the platform. Sure, yeah. So if you ever do that, if you could set up in the morning, I'll invite a couple of people from my data science team too. Yeah, that would be lovely. So what kind of timing would work out for you given that you are on the East Coast? Actually, to be honest, it doesn't have to be morning because one of my data scientists is in US hours. So let's see next week. This time is generally good. Like Thursday is good. I have to see if it works for my. Let me follow up with you over email. Yeah, we can do it by email. Tuesday maybe too short. Thursday probably would work. Okay, understood. Cool. Let me do that and I will have someone from the team follow up with you and set up a demo call. And it would be great to invite a couple of data scientists as well. Sure, yeah. Happy to spend some time. But I think probably the goal should be if you want a feedback, I'm happy to give you feedback and that's why I accepted meeting chances of it. Unless you're telling me that you sell your solution for ten, $20,000 a year probably is not going to happen. Right. It's just not a priority. Integrating any new technology like that and building process around it all the way down to zero to whatever else takes so much energy. It's a distraction when you are overloaded and you're trying to really build the functionality and get the deals that make sense, I'll be honest. As opposed to no, I really appreciate you being honest, Bobby. I will be open to the fact that it may not work out and at the very minimum, I'll get some good feedback from you. If it works out, I would love. To work with you and your team. Sure, yeah. We can chat, but I would put a problem of it being working. No more than 25%. Okay, awesome. Thank you so much, Bobby. Really appreciate it. Bye.