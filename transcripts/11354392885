I'm good, catching up again after a few weeks. Yeah sure. Let me check with Abe. Yeah. Abhishek will also join. Who else is joining in? Subnets from your side you are here. I think that's it Nadesh. I'm not sure he will be joining. Okay. And I think you mentioned Aksha will also be joining in. Okay, cool. To connect with you again. How are you? Yeah, we are good. I think you both didn't meet Abhishek. Abhishek, maybe it will be good if you can quickly give an introduction for yourself as well. You mean my introduction Mandrak? Yeah, I think we all have met in the office. Yeah. Hi abhiyan sabnesh. I'm Abhishek. I graduated from It correct for 2013 in Computer science and then worked in Facebook for around five and a half years. During her time at Facebook worked on different teams like the Distributed Caching System team was leading a mobile performance team and was eventually leading the business organization there. And then I did indicate my job came back to India and currently working on Trip Foundry basically where we tried to solve the ML deployment platform. Yeah we looked at the solution you guys are offering and building like last month awesome. Awareness of nestle the ML side of things has built out the ML platform. Aware was earlier on the data science side but then I think he realized that it will not work without the engineering and then in cars 24 and even before that he has been involved in a lot of work in building out the ML platform and overseeing it. So we had a good discussion. I think you have seen the notes from it. How would you want to take this call forward? I know last time you had questions, I do have notes around that. At the same time, maybe it will be good to kind of quickly recap your system once and we can again revisit the demo quickly and then go into the questions if that works or is there some other agenda you had in mind? You are in mute, sorry. So it is up to you, like not have any specific agenda. Last time we were discussing in general how we were working on it and I think you had few questions on that. We are happy to answer those. Yes. Cool. So let me give a quick context of whatever we know of the past 24th. I think the infrastructure is mainly GKE based. You are using Caseerv and Knative on that like using Feast as the feature store for things native. You use for the scale to zero things to make it easier and then I think you ensure that inside the code like people are not like things like Pandas et cetera so that the speed is faster and things like that. And monitoring you are using within Google itself, right? Like you're not using separate anything for monitoring. And then I think other context, I think you also have right. In terms of there were questions that had come around usage of I think triton or the flexibility that is there earlier, I think you are using GPUs, but now, from what I understand, most of it is CPU based. A lot of services are mostly real time. From what I understand. Again, correct me softness or anything is wrong there. So only models based on images are on GPU, Rest or Rest dollar CPU. Okay, got it. And there was one thing you had tried on the Cuban native log site, which was 30 days I think you are able to retrieve. But beyond that, I think. We realize we do not even need it. You do not, because most of our APIs are real time and if something has to go bad, it will go bad in 30 days. We are not maintaining audit logs as per se. Our APIs are stateless. All the audit logs are being maintained at check in. Our APIs are stateless. They are family for request response that says if some outlier comes in and the code is not supporting that, against that we will have some error. And we usually bind those deployments against alerts, again supported by GCP. So the respective teams get emails or we use custom Slack as SDK to shoot messages if they are in bulk. So we have some services running in Airflow there for a data engineering use case and they consume Gcpins in another way, but they send requests on Slack and then Slack adds acts as our so we have Slack Enterprise enabled in at cars, so we have no limit on history of messages. It works fine for us. Did you have any other questions? So, are you on the Airflow on Kubernetes? Not right now. It's on VM right now, but we don't see much of the traffic coming on Airflow VM. So right now it's working fine. But yeah, in the future if we get more Dags, then we'll have to move to Kubernetes because that also has to scale. I see. And so all jobs, if data center wants to run a training job or anything like that, it's completely on Airflow today. Or they run jobs. No, we don't use Airflow right now for those. Because these trainings, these data centers, they do by themselves, they have VMs specific teams. Every team has a VM, so they would do it on that. But at jobs they don't have any. So they basically get the data, they have notebooks, they run the training on that. They are different teams, basically. So when we started this practice internally one and a half years back, we decided categorically to focus on the ML Ops and Mln side of the thing, not the DS side of the ecosystem, because that was coming with too much baggage for us too. It requires everyone to move to something else and then that will have its own opinion architecture over and everyone will the backlash was too heavy and we are literally 2.52.25 bandwidth kind of the operator. So we are really lean. So we did not want to waste time on that. So Ashworth saying lately we have realized, okay, rather we have focused on how we can accommodate all those granularities on infrared side and scale that instead. You'Re saying training job locally they are running. So our model will save, they save it on a three key. Where do they GCs basically GCs. After they have done with the training, they approach us that we have to deploy this service, then we get them. So your service will either go on GKE or if it's like a normal slow or small service, then basically cloud function or cloud run. But your models would be stored on GCs rather than having it on the docker image itself. So we fetch it from GCs and during runtime it's loaded once like case of has this functionality. So in the load function it calls the GCs, downloads the model from GCs and it's loaded. So that way we also maintain history in GCs. Okay, I'm guessing you are also putting history on Kubernetes, right? Because you are using KSR. Yeah. So every deployment has a canary deployment. So we have the deployment numbers 1234. Every time we do, we redeploy things. We have logged for those also. Okay? And data scientists log, they ask for this thing like Immodel V one predictions can be right, it's being written somewhere. I can go and compare logging in case that comes with that logging. But is it flowing to some bucket? And they can analyze v one versus V two, maker difference and all that stuff. How do they know v two is greater than v one or equal to v one? So that is something as I said, our APIs are stateless, so they do not manage Kia haircani. So whenever the tech integration with our endpoint happens, it happens in between. There is an intermediary cloud function over the request forward case endpoint function throughput control kirpan for it to not choke like Kubernetes chokenake. We use Pub sub enabled cloud function jokey scale just to send requests to cases because we do not have API gateway in between. So rate limit, we do not have anything, but you have. Limiting. Understood. So basically request carcondra the clients, no client directly internal. We have all the requests that are internal. Directly exposing URL data science, your project zone, everything is driven by tech. Request goes to cloud function. And that cloud function will call the case of endpoint. Pub sub say actuate karna is not straightforward in at least GKE either way. For us, the easiest and cheapest way was 128 MB node kernel and eventually translated into this practice endpoints. And they are generic, they are efficient and they are generic. And our front ends which are cloud functions or cloud run, they are application specific. So this was again done to separate the Mln and Data DS responsibilities, cloud function. When they wait for the response to come back through the pub, they're also subscribing to the response channel, right? And post request for the slot. And we usually they respond in few milliseconds, so it has been working fine. Understood. Also we cache inferences, heavy inferences on cash. So we do not take same inference for same model on same image twice. Instead of running the model, just massive results. Distribution, all that stuff. Product development, more data. I also manage the data warehouse. The recency of data is more critical than archive. Drifting, model performance, downgrade horizon, all those things. Whenever a model is new after six, eight months, internal generated data. Okay, so data internally generated a vino key data and then zoomatrics. Usually business KPIs may be reflected. For example, there are KPIs that are tracked at business level as well. For example, if you go on our website, you will check for the price for web quotes for your car. Eventually appointment and final price quota error. Business KPI is monitor at a higher level. Higher level. Sometimes that delta is by design to increase footfold. Sometimes that delta is not expected. And that is where the DS monitors that API via Bi route at cars frontival DS is not working in Silos, it is always paired with the Bi team. Okay. Bi team is catering to KPIs, all the KPIs that is responsible for business. Understood. I think that answers all my questions. Or knight. Load pre process, predict post process, but at least navigation. Data scientists. Understood. We can show like it's actually very similar. We are also building on top of. CASER only take just few contexts while you are walking through, it will be good. I think awareness, softness has seen the platform at a high level, but I had shown them a little bit. But while you're walking through, if you can actually talk to the little bit on the architecture side as to how we have done it. And then there were a few questions that had come in last time. So I'll also interject you in between to answer both. But yeah, I mean, they have seen it once, but as you walk to talk more in depth rather than just the higher level. Okay, if you've already seen the platform, then let me know which parts are clear. It was at a very high level. It's good to walk through anyways. And I think it's been 30 days before us. I'm sure the pressure would help, but that time it was very high level. The more in depth things will help you. Basically, this is where you can add all the Kubernetes clusters in the company. You can add the cluster and you can add like which users have access to what cluster and all permission control ingress URL star API 24 com is cluster. So people can then start creating subdomains of that. So you can map keys cluster p added A and then you can also decide his cluster type a machine supported what are the instance that are supported on this thing? So every machine will create a different. Node pool PJ will create a different note pool. The kind of recommend using is auto node provisioning. It's both there on AWS and GCP mandoe note pool. Just because even if you mark it, it's not creating the note pool right now. It just means that somebody, the moment they try to deploy on C six, a node will be automatically be joined to the cluster. Dynamically configured but node pool cluster by default. You can add all the docker history that you're using ECR, GCs, Get, Repositories, add Cursor, standard stuff. Or Secret Manager. Secret Manager. We actually work with secret Manager. Also Google cloud secrets. The moment you add all the clusters, you can start seeing all the namespaces. Here. And then you can upload those cluster. That is the level at which you will operate right now. And then was there separation of access as well? But deployment last time. Basically namespace level between different teams and access teams data science could be around size limit so they can play around with it if they want to. Deploy something quickly and test out. And then you can also give them access to, like, service accounts, data science, three bucket access. There's no way they can screw up the other intro. And they can further limit instance type. On a namespace level, GPU machines, something along those lines. We also want to show the cost and all that. We have names level costing and all that things. And then on the deployment side, the deployment, we kind of try to make it very easy. PoeB Company Maker Paiga basically at least for testing out stuff and everything. Or you can also do the production deployment. So you just choose the workspace where you want to deploy on service. Java model is actually powered by Kserv. Underneath, I'll show you that eight questions last month. I just remember your workspaces in general. Like workspaces when you set the resource constraint, whether it's a soft or a hard limit, exactly what's coming in. It is a hard limit. It will not allow you to go beyond that other deploy carousel services. Requested. Xdiff or docker image already builder you can also give us a docker image uri public DNS you can do this and this is coming from the ingress URL we configured on the cluster you can do that. It'S not binding, right? This was also a question I think of where you had like binding or. Not binding binding. Basically it's a compulsion like is it binding or is it a non binding thing? So basically we use internal IPS so. Internal IP and cluster internal IPC cluster credentials on the IPI directly. Okay. Port 443 in a lot of cases below I automatically be able to environment variables. You can add the environment variables here. It is as simple as you can add it directly from here. But you don't get. Secret ad is secrets. I'll quickly show you basically secret. Config map or secrets config map or secret skype section. But the values just values google Secret Manager you don't rely on Kubernetes to store the secrets permanently. Secrets Storage so you can add the secret values. This is the key using which everybody can use the secret in their code. So whoever has access to use the secrets this value and your value after Google Secret Manager may store. We don't store the value also. It's actually going and creating in your Google Secret Manager behind that. Link. This is just the link request memory and all that stuff. Like standard stuff? You can directly just by default. If. You want to run it only on TC machine. It can also do that. A replicas, a service account add canon. Service account, constructive liveness probe, redditness probe, most of standard stuff. And the moment you click Submit here, it will automatically build the image and automatically deploy. And at the end you can see the service running, basically. And you can see all the previous versions of the service cupcap cupcup deploy and previous version revert back connected as simple as you just click here and revert. So basically hamari approach what we do is basically we reuse our hardware again and again. Last time I think maximum time we spent on multi model serving correct and that is basically we are reusing our hardware for different use cases the reason being every time we provision that hardware we incur cost and we have realized GCP pay on demand provisioning is not reliable at least for GPUs. So GCB asked us to reserve that hardware. Okay? The way out for us is basically to do multimodal survey where we consume same hardware. Last time, if I remember maximum triton, he discussed why we are using triton because it is the only open source multimodel server available, which is giving us efficient deployment plus inference graph. Recently OCR model deploy current like past three weeks. And we are working on that only to efficiently deploy OCR model on our infra. And we do not want to incur that extra GPU cost. Multi. Just be project. Understood dynamically the amount of memory you allocate to triton thing is to be some of memory of all the models. That you load, no right? It manages memory, but it manages it efficiently, pretty efficiently. We try to deploy these model in standard deployable formats onxia Tensor ID like we do not deploy them in native formats of any info. So OnX runtime efficiently down credit plus all these support FP 16 support. So basically mixed precision says further synthetically Ram double. Understood. So that's how we are managing it basically high availability not to just keep on deploying one model over another images model we do okay, understood only for images, not for tableau, but frankly Xgboos not for GPU, but CPU entrance and it was significantly faster as well. So basically the different workflows that we worked in our iterations may we looked at approach where if the model is deployed as a function at Kserv to scan her incrementally copy one three of it to multimodal server, which frighten and use GPU back end there. And XGBoost provides the inference file format, which is supported by triton. And recently OCR model without inference graph of triton with inference graph of trident and ten times OCR model requires multi people called. So every row is detected, then every row is detected for Yoski rotation. Every row then subsequently recognized gRPC request. So basically throughput backup, I think as the criticality of our services will increase, we will probably add another but recently on demand availability India region. I do not know if you guys face the same issue GCP early this year on demand GP availability. So we had to move it to Taiwan part of our code. Understood. So we have two type of jobs fewer real time and fewer batch for images. So when we were doing this that time, our primary use cases were batch oriented to Taiwan Petuladia. Then we realized my real time use cases we enough forget reserve. And then basically we are monitoring USA consumption gas and we are realized compute level pay. It is not even 50%, because I'm Q managed here. Understood. Basically the model deployment tenor. This is actually powered by CASER only the model deployment. Basically what we do is models the company model saved. You can pick anyone. So versions we automatically store of every model. So just say like fraud detection 6.76.6 Yovi version up to deploy Kanye. You can deploy CPU memory resources. Certain instance could target Kana. You can also do that. Those things will come. So that is something the moment you click behind the hood, it's creating an inference service and all those things the same thing that you're doing. Basically, you just enter the name URL of the hugging face model the pipeline name and then you can just deploy it model Java. This is how you basically show it. Data scientists basically trying to make it easy. Behind the hoodie we know the model framework. So TensorFlow server Python service model up TensorFlow server for every framework we choose the different this thing. We try to standardize the same inference protocol for everything. So we use the V two inference protocol. So by default we track to kind of make the choice like TensorFlow server Python. So all of that is by default. Taken care of how much is triton. And then probably TensorFlow. Shared memory. Understood? Okay, understood. This is very helpful. Because of this annotation is not present. Request knative skill to zero as a request during the day sometimes. So that scale to zero is activated knative we'll put scale to zero option, but you can run it in both options. Either you want scale to zero or you don't want scale to zero. Scale to zero. Knative knative functionality. So in our workflow, we end up spending time more time on figuring out how to take model into inference server rather than deployment. I have realized model go inference. So we were trying to deploy debugging that and taking that out of methods for our detectron model. Otherwise we'll convert. So there are a number of issues that I think we are stuck in. Ideally, you guys will never stuck in those because we want to GPU. Look to deserve it doesn't matter how. The reason I was not able to reply was beside the things that are getting into production basically repurposing models for different use cars and things cost wise automatically tightly binded. You can throw money at the problem. For example, detectron t four pay and media t four pay three requests per second. When it got converted to tensor RT, the request fitnama 24 requests per second. So latency we automatically resolve. How are you testing this thing? The thought process of Tech team was very different from it used to go in the red in this direction, correspondingly in parallel. These case projects came up as well. There is another good project that we will soon try. For similar thing but for wider use cases. Once we settle this case, we will start working on Ray as well. For similar use cases, the use cars, they are more broader. Like we can have more wider application again. Instance, graph is available there we can do scaled up data processing and things like that. This might reason why at the end of the day, it's both deploying docker images. From developer point of view, the thought process is very different like at least the tech we have at cost 24 days. We then resort to a post request request these are two ways they can directly communicate with us and we will reply to them with language established. For example, they do not get the idea of inference store, and we are using inference store for many things, inference. Understood. And then unparalleled my GCP ecosystem use fees, update karna so basically what people do is they put their data in BQ. They put their data in GCP GCs then GCs major data put author service trigger and then feast. Cars load update. For many workflows we are using Gcpk in built triggers. Feature storage copy used. So basically hamada and patio tech APIs with So tech do not does not share their these data endpoints live databases endpoints with DS the government request memo data, then data access connection postgres by security policy. But we can only access data warehouse which is snowflake in our case. So we can sync snowflake at 15 minutes frequency. So snowflake is more or less near updated data for us. And over that we build this. Basically. Just like thread layer. Over the data store, get that kind of thing. Fees was our replacement for Firebase. Basically. Initially we started with Firebase, but Firebase scale Fire, story of Firebase after a certain number of requests, then we realized, let us move. To this tech. So mostly codenam, static data by policy. Features, load functions. GCs server. So DVC we use for model versioning. Okay, understood. This is super helpful question. If put a platform, I think which parts together. Plus, I don't know if you would want to discuss the architecture from our systems. I'd also like a call. Or we can walk over our architecture and again upload already? But I have a coffee. Coaching, but still, if you want to try our system for deploying and seeing how it works would be happy to kind of expose the system to you. And we will try to make it easier for you to also update your cluster. I think we surely want to at least try if not consume it for our use case because these guys are moving to inference graphs which are even more custom thing we call those you guys will implement it for sure once you will integrate that feature in your ecosystem. So you will figure out a standardized way to connect different API endpoints for sure but we have tried it in a few use cases in QM there are a few use cases that we can easily build on those insurance graphs if you have looked at it. If it is possible to connect two clusters in different two different zones actually issue case of resources available I would want a particular service same case as the Du sole. Cluster. Level communication linked checks and balances build Nagaro the whole idea of using case internal language zone on a defined career. Checks and balances. We did not want a copy of the same service. It is not going to do inter. Cluster diagram 10,000 model plus problem. It is there. So I was assuming beyond cluster for simple reason document. Did you guys not try out Vertex? What was the reason this way and not doing that way? Main reason. That is what we are doing already. Triton recently. Because GCP is selling that 19 9% MSD and was 99% 400 MSP. Okay? For that small piece that we had cost multiplier as I said hardware across multiple deployment per deployment multiplier which was and then we realized even GPU availability to India. We thought but vertex India may be scaling or other underlined GPU available whole purpose got defeated. For us it is expensive. And even that is not scaling. Plus I frankly do not want to bind myself to any particular cloud as per se comfortable is liquor but other cost effective they come to the lambda. So Lambda cloud is really cost effective but management approval to go there. But what the cloud run is more efficient. Cloud run be equally inefficient. Cloud. Run is basically managed knative API application sometimes compute shared hardware. Basically routing routing. Specific payload specific. What we have done is we have minimized the generic. As we keep building. I would love to sync up with you if you have time and show you and get feedback. Yeah, sure. Actually it will be helpful for us as well. Mutually. So there are very few people who are actually working on the same problem. Actually startups of. But it has not been easy for us to work on this. Plus changes. As I said, Consumer. What is the right approach? T get in. Bye. Thank you so much. Bye.