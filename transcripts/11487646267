Thank you. Hello? Hi, can you hear me? Okay? Hello? Yeah, I cannot hear you. Oh, my bad. Are you on the road or something? Yeah. Okay. Okay. Did they confirm that he's joining? I mean, he responded on LinkedIn right now, so I guess he's joining. Yeah, he responded right now. Then obviously. Join tuning. Sometimes meat has a problem. Like some people who are used to zoom. Now let me just write I'm online. Hello? Abrianc? Hello? Jenkins? Hello? Can you guys hear me? Yes, we can. Can you hear us? Okay, yeah, I can hear you now. Sorry about that. No worries. Which time zone are you in Priyanka? Right now? Are you in India? I am in US PST time zone. Okay. Must be pretty late. Yeah. Wow, this seems like very late for you. I'm generally also based in the PSG time zone, but right now I'm traveling to India. Okay. Are you in the Seattle area? Yes, I am in redmond. Okay, nice. Got it. Awesome. Thanks a lot for taking the time. Were you saying something before that? Did I cut you off? Oh, no, I just thought I'd just quickly introduce yeah, you can go ahead. So Priyank, I am a part of the founder's office. I used to work with Nikonj and the other two co founders in the previous startup as well. There I used to again, it was in the HR tech space and started joining them on day one. Before that I've got about three or four years of operational experience across a couple of different startups. She has just a quick intro about me. Thanks. Akshay Priyanka, I can introduce myself and then we would love to learn a little bit about you as well. I come from machine learning background. I used to work at Facebook in the conversational AI team. And prior to that I got a chance to lead the machine learning team at a startup called Reflection based out of Bay Area. I moved to the US. Former master. So I went to UC Berkeley and prior to that I was in my undergrad at It. I met my now co founders in Wagon. Abhishek as well. Given that you are from It Priyanka, by any chance do you know folks like Ritikesh or Palak Bhushan? They are maybe like one or two years junior to you. Can you repeat the name? Palak bhushan ritikesh Kumar. Do any of those names ring a bell? They are from electrical engineering, computer science. No, I don't recall any of those names. Maybe if I look at this page, probably I can relate something. Actually at Berkeley, quite a few quote from actually shivam Sulphian, does that ring a bell? Yes, that rings of the name. Yes, I think so. Yes. But Shivan Sulcian I think is from the same bath. He came to Berkeley together with me as well. Okay, yeah. Nice. You moved to Canada for your masters. That's correct. I was on a business side when I started my career, was in consulting and then Deutsche Bank, and then decided to move to Tech. And that's when I did my masters at Ensure Toronto, and then I moved to Microsoft and then a small company startup called Zuli and have been building the machine learning platform here for the last three years or so. Nice. I'm very curious about especially your work with an ML platform engineer, I guess because that's super close to what we are building. Right. And then the second thing is it's amazing how coming from Microsoft, he obviously calls Zooleri as a startup, to us, as a startup, zuli is like a super large company. Right. Just a matter of perspective at this point. Yeah, obviously it's just a relative term, I guess, depending upon your perspective. Yes, that's true. Awesome. Technically I shouldn't say it as a starter, maybe a smaller company. That's the right word. Yeah. I'll give a little bit of a I think you have some context from the LinkedIn messages that we've sent out. Right. But maybe I'll spend like a couple of minutes to introduce through and the purpose of this call and then most of this call today will try to spend learning a little bit about your work at Vidi. So generally at Proof truefoundry, what we are trying to do is we had seen the ML platform at Facebook, right. Have you heard of Epiler by any chance? Yes, I have. Okay. Yeah. The idea here is that companies like Facebook or larger companies that have a lot of machine learning engineers, they have the bandwidth to hire a lot of ML platform team, can actually spend time building these platforms from scratch. But to most mid to up market companies it's a very expensive audio to take, basically. Right, so the idea here, Akshay truefoundry, is that we build out a platform that kind of acts as a base layer for other companies to build on top of. Our thesis is actually that we likely cannot build a platform that will work out of the box for every company out there, but hopefully we can build a lot of technology, a lot of it that people can build on top of and customize it to their own use cases. And in this regard we are fairly early in our journey, priyank, we are about almost a year into this journey and we are working with a few large enterprises to build a platform with. But obviously this is very important that we continue chatting with folks who are building such platforms or leading such efforts and learn more about the specific problems that they are trying to solve within the company. What kind of tooling tech stack and all that they're using so that we can always continue to build a platform customized to these variety of use cases that we discover. And that was primarily the purpose of this call, to just learn from what you're doing at Zillow. If you have any questions, I'm happy to answer that. Otherwise I have a few specific questions to kind of give some structure to our conversation as well. No, I'll let you drive the conversation that will make more sense helpful to you and I can contribute more if you issue driver probably sure. The first question is if we can set some context by training a couple of types of machine learning problems that you are trying to solve, right? And like the high level tech stack of model building and model deployment stage basically. Okay, so the business problem that we are trying to solve can be probably broadly classified into three categories. First is personalization and recommendation which can go through multiple channels which could be just personalizing your homepage, your recommendation over emails and multiple challenges. Basically the second one is primarily around forecasting the sales of a certain product based on the historical data that we have seen on our business side. So forecasting, demand, sales, those aspects. And the last one is on supply chain side, where basically we want to predict how much quantity should we hold in our warehouse and all those shelf capacities and all those things for products so that we can do. A better job at predicting when will we deliver and how much should we store in our warehouses and give a good estimate to our customers. Like it will take ten business days to deliver or five business days to deliver depending upon do we have the capacity, do we have the product on our side or not? In those kind of things. So that's at high level of the three broad business cases that we are trying to have established that we use our platform for, there are new use cases which are coming up which we basically are experimenting with, but they have not been established which are more around. Like if we can have a reverse image search platform on our solution where someone the user can click product image and search if they can find something similar and a little more about around NLP, where can we use some kind of similarity based similar products based using NLP and things like that. Those are the new use cases that are coming up but the first three are one which established and defined and are running and the two ones are upcoming so hoping we can make inroads to get that done as well. Right, and here actually a couple of questions. I had one more on the text back that I asked. But given that you're working on recommendations of personalization and you specified three different use cases here, I'm assuming the first one is more real time. Like the recommendations and personalization use cases more real time when the user lands. On the website to personalize the product recommendation is it. Even forecasting is real time because warehouses like engineers the managers over there use our model to decide how much product to order on daily basis like how to place orders and everything. Yes, even within recommendation there are some. Obviously the landing page is real time, but email and other channels is bad. Sure. Okay, understood. That makes sense. Okay, understood. And here in terms of the models, are you guys using more deep learning type of models for the recommendations or is it more like classic ML clustering type of model? Where do you see the more of the inclination of the team? I've seen things transition. Like initially it was more decision trees around our decision tree based models and now things definitely have started shifting towards deep learning models. So there was a transition and there are some use cases where certain things work better versus others depending upon complexity, data availability and all those that's data scientists decision. But as of now we have healthy mix of both. But I definitely have seen a transition in the last three years going from more previous models toward strict learning. Understood. And here these models that we're talking about, are they typically deployed as a service, like the model itself is deployed as a service or are they part of the overall application layer where the models are packaged together with the application? No, they are deployed separately as a service. That's the whole use case of the platform. Right. We want to keep it independent of the actual service and that platform becomes basically your backbone for deploying and iterating through different models. Got it. So on that note, when we are talking about deploying the model to the service, like what cloud are you guys using? And if you can also help me understand what is the deployment tech stack? Is it like open net? Is everything dockerized? That part of the thing. Correct. Zulu as a company I would say some more uniquely position that we live in two clouds, both AWS and GCP and the tech stack is everything is in Kubernetes. Understood. So what was the motivation to go in two different clouds? I think the decision probably was made prior to when I joined, but I guess they want to leverage best part of both the world that has to offer Google BigQuery and the data management side of things is really good and the solution around those are really useful for all the analysts and data scientists and everyone. AWS is really good on productionizing. The models like real time production services which basically makes it easier for us to support production ready customer services, customer facing services. So that's why probably we live in two clouds and that's how our workload is to be fitted. I see. Okay, so both machine learning is basically machine learning itself is also done on both the cloud. For example, would model training typically happen on GCP and then serving typically happen on AWS? Is that the kind of split that you're seeing? Correct. Okay, I see. Interesting. And I'm assuming that given that everything is on Kubernetes, you're not leveraging GCP's vertex AI or AWS megabytes. We evaluated them as a potential candidate, but we decided to build our own platform. We are using multiple open source technologies. Basically, we took the source code and modified it wherever needed and build our own customs. We are using Fees for feature store, we are using QFlow for our training platform. And then we have something custom for our server. Interesting. Nice. Do you also end up using ML Flow for tracking. ML Flow databricks? One yeah. No, we don't use ML flow. No, we kept tracking your experiments on Kubeflow itself directly because you flow. No, we use a different tool called Model DB. We use that. Model DB. I've not heard of it. I'll check it out after the call. Yeah, understood. I see. And all of these things are maintained in house, basically like the hosted version of Feast, the hosted version of Kubeflow, all of this, you guys are maintaining it yourself. Understood. One more question is what was the reason to not choose something like Vertex, AI or Sage Maker? We did the feasibility analysis three years ago and at that time Vertex what's doing? Mature. Yeah, like lagged a few features that we wanted, essentially needed for our use case. It has matured a lot since then. So we probably can do an analysis again that whether this is more feasible now. But at this point we are already invested in our platform and it is cheaper. Sure, makes sense. And I'm assuming you but you don't use anything like EKS or GKE currently, right? We do use EKS GKE. Definitely we don't, we don't manage. EKS. And how is your experience running Kubeflow like that like managing Cubeflow? I've heard very mixed opinions from different platform team folks about some people are like Kubernetes, too complex to manage. Some people are like set it up, no problem. Parts which work well are something which the center really like essentially that you can have pipelines to find the Kubeflow pipelines where you can stitch components together. In terms of maintaining, we had definitely challenges because like certain features were not available. There were some bugs where you had to go and fix it. There are some features which were not directly available, like if you want to run a Spark job or something. So we added custom support for having Spark operator in there and all those things. So that helped make it more usable and user friendly for data scientists. And thankfully it was not too tough to manage, but definitely initial learning curve. But once you go through that, it was okay. I see. So besides the Spark job, not being able to run the Spark job, what are some of the other features that you wanted to have but not supporting? Cubeflow is heavily inclined on supporting TensorFlow as a training platform, but decision trees really TensorFlow doesn't have really good support for training GBA decision trees and those there are other libraries which do better. So how do I bring those training frameworks inside Kubeflow? It's just one example. Interesting. I see. That's a unique insight. I can have never heard that as a problem that people called out about setting up different other library. That framework is very different in Kubeflow. From what I understand, it's possible to do it right. Are you saying it's difficult or it's. Like you lose it's not supportive of the bat. Basically, you have the good thing about Kubeflow is they build their core infrastructure, like cloud platform in a way that you can add application on top of, which makes it easier to bring in any new application over the framework and support that. I see. Okay. Interesting. I would love to do a deeper dive on this at some point, but I had a few more questions that I wanted to discuss with you before I get into some of the details of this. I also want to understand how is the team structured and how are the responsibilities of managing the clusters, building out the platform, building out the models, and deploying the models? These, like, four things are separated in the same team or across teams. Basically. There's a lot of background noise. Actually, can you repeat the four things that you're talking about? Which team manages the clusters? The Kubernetes clusters, basically like the intra layer. Right. How is the ML platform team interfacing with that team? Who are the people building the models, actually training the models, figuring out the best type of parameters, and who are the people deploying the model? Okay. The first two is the platform thing. Like, we manage our infrastructure, all the cluster and everything ourselves, and then all the platform application on top of it. In terms of data scientists are a big use case that we support for training the model. And then there are other applied MLE who basically help out, who work closely with data scientists. And there are different teams they work on who are responsible for using the platform and basically bridging the gap between data science work and putting the model in production in our platform. So basically, I got three ways, like the imper and the platform. There is the same team, and then your data scientists are different, who are building out the models. And then Emma engineers kind of bridge the gap between the platform and this thing. That's correct. Yes. Can you also help me understand. What. Is the interface between data scientists and the ML engineers? Basically, like, at what point would the data scientists send over stuff to an ML engineer? That has changed. It has progressed over time. As part of their more mature, they needed less and less help from MLS to put their models into production. But I guess the first part was capacity estimation for real time features. What back end infrastructure would it need? How to provision those resources if it can't be supported directly through some standard use case. So those are the things that definitely engineers were involved in productionizing their data job. They needed more help at the beginning, but as more matured data scientists could do that on their own until there's something complex which they would need help, but that become less and less where. But I think so. The hard part was the serving layer, the API changes and making sure that the platform will support the resources needed for their model, and if they need to provision more resources, how they can do it. And those interactions were definitely governed by Emily. I see. Okay. So, for example, creating docker containers has always been basically dockerizing their models, like managing the model versions. All of that has always been the data scientists within the data science purview. Not really the dockrising part. Their part was more around, how can I get the binary once I have the binary available, we basically expose the service which they can use to basically dockerize their models. But they are less concerned with that because if they run into any shorter areas, they will reach out to Emily's. But their task was more to make sure that once I have the data job that I need, which will put models in production, which will put features in real time features in production, are those correct features? Can I look at those real time features or material jobs working correctly? Those are more of their concerns to make sure that what features they use to train the model are something which are exactly being used for real time feature use cases. I see. Okay. What's the server do you guys use fast API or something? Or like TF dot serve, case serve? No, we basically use directly whatever the model like the framework offers. So for TensorFlow, we have TF serving for different frameworks. Whatever the framework directly provides, we don't use something generic which serves all type of models we want it to be closer to. That helps us iterate faster on different versions of except for serving and all those features that are available. I see. Okay. And for the three based models, is. There any framework is H two O? The framework is HTO that we use. Oh, you guys use H two o. Okay, understood. You guys also end up using a bunch of AutoML? Not really. That's more of data scientists question. Probably, but they don't use AutoML a lot. I don't know why, but maybe it's a good place to start off the model building. But definitely they need to do more to make it more optimal. Probably, but that's more I see. And do you use something like a delta lake or snowplake or something for a data warehouse? No. Understood. I see. So in this stack overall, actually, one more question before I get there, which is how long has the Platform Team existed and how many people are there actually, for that matter? I would love to know the team distribution across all these three verticals that you mentioned. How many data scientists, how many MLE and how many people in the Platform Team? I think it is hard to put down a number because people have left and joined, but close to six people I would say on platform side, on application side, I don't know really. There are sentence for some new team who gets on board and more decided to join some leave, but on the platform side probably close roughly to six. Oh, interesting. I think you mentioned an interesting point here. So are the data scientists typically embedded within like business verticals or is there like a centralized data science team that builds models for all the different products? Both there is a central team but within that team there are different squad seat and say so each squad has its own business problem to work on. Understood. I wanted to understand, I know that we are a little bit over time as well, maybe like I wanted to take like five, seven minutes to understand in this entire chat from building to deploying model. What are the problems that you guys are facing, what are the current challenges that you guys are trying to solve? I think so in terms of challenges, definitely how do we onboard data scientists into our platform? A new data scientist is something which we have hard time convincing. It's not directly very easy for them to say that I want to use they're more used to vertex AI or let's say a different platform, not really queueflow. How can we basically make it easy for them to use QFlow and how can we make interaction with MLE as little as possible and make it easier for them to deploy a model in production with less involvement at MLE? We probably are nibbling away with small utilities and small capabilities wherever we can. The third thing is probably is in real time, how do we provision resources and make it seamless for MLE's to provision resources depending upon their use case? Like if their use case involves maybe hundreds of features for millions of customers, then what kind of resources do they need? How can they do a better estimation themselves based on correspondingly what resources would they need to serve those? And all those information is something which has to be we have to be involved closely with them. We have been trying to make this more standardized so that they can go through some kind of playbook and come to the right estimation numbers which we have been kind of basically working on to make sure that we can standardize those posts. So, I think these are three things which we are working on. Like platform in itself is matured enough to serve most of the use case. It's just that making it last mile kind of thing which makes it really comfortable for everyone to use. Understood. There are few important points that I'm hearing. Number one, which is like the infrastructure provisioning. Number two, which is the data scientist experience of using Kubeflow as a platform today. So generally the friction of onboarding data scientists, and number three, that there were a few features that are missing on Kubeflow that you mentioned initially that I wanted to discuss. These are the three things that I would love to dive deeper into prior. And one other area where I would love to seek some help from you is some parts of what you are talking about. We are also trying to build in our platform as well. So, for example, all these things right, like the data scientist experience, the Provost and all these are some of the things that we're trying to encode in our platform as well, primarily based on the learning from Apple, learner itself. Right. Sorry, go ahead. There is a talk which one of our engineers gave in coordination with Feast. Maybe I can share that and that's public information and that will help you understand more about how at least on the feature source side and ML engineering and platform side, how things are. That should give you a good idea as well. Absolutely. If you can share that talk, I would love to see the talk before I come to the call or something so that I'm also prepared and I'm not asking any information that's already available in public. Was this talk with Willem Bosna from Peace? Willem Penar. Sorry, good friends with Willem as well. We were initially in touch with him and he did those calls on when we were trying to adopt Peace. He has moved to Tecton now. Yeah, I was in touch with Jillam as well. I met him after he joined Tectonic, but the piece kind of got acquired. Yeah, we were engaged with him when he was in Singapore. A batch of mine was at Kojak and we basically got in touch and then that's how we practiced our own fees. Oh, super nice, super nice, super nice. Actually, BoJack's CTO X CTO now, Ajay Gode is actually one of our advisers, so we get to learn a lot from AJ as well about the platform that they had built out piece and General, also other ML platform infrastructure provisioning. And also we get to learn a lot from A J as well. Okay, that's super nice. Definitely you have good mind helping you out, so that's good. Yeah. Actually, before, we have one other question. Are you guys not using anything for monitoring currently? Like model monitoring. Operationally? We are monitoring through not qualitative monitoring, as that's not really yes, we have some version of it, but it's not really matured, which is primarily like we just collect data and expose it to a data scientist and they're responsible for closing the feedback loop, essentially. But we don't offer anything on the platform as something as a standard use case. So we basically expose what was the actual model it was called what was the actual feature that was being used for the call. We log all those and then it's up to data scientists to basically use the data how they want to use it as so they can build tools on top of it to monitor and see what was the model score was and feature drift and everything. Whatever they want, they can use it for. But as a platform, we didn't go into exposing more ML capabilities in there, but more around, like, how can we make it easier for you to leverage the data and everything so that then you can go and work with how you want to use that data? Understood. Okay, got it. This is very helpful. Piano. Thank you so much for giving me this context. Overall, as I mentioned, that there are three, four things that I like to learn from the call that I would love to dive deeper into and also spend some time showing you the platform that we have built out and gathering some feedback here. Are we headed in the right direction? Do you have any suggestions on how things would have been interesting given the use cases that you are seeing internally? And before that, I would ensure to see the video that you're referring to so you can share the link. That would be helpful, but we can Google it. On the I dropped on the track itself. Yeah. So you should have it. Okay, sounds good. Yeah. All right. In case you have some availability for the next week, practically next year, that would be great if you can share some availability. Sure. Let's connect on LinkedIn and we can okay, sounds good. We can follow up soon. Okay, sounds good. We'll follow up and then set up a follow up call. Okay. Thank you so much. I hope you had a good Christmas and Happy New Year, like, in advance to you. Talk to you soon. Thank you, Ranger. It was a good talking to you guys, and have a great holidays. Thank you. Thanks, Bran. Bye.