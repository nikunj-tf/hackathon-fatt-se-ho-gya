Email responsible then I thought it was for us is a very. 1 second. Hey. Hi, how are you? I am good, how are you? All good. Great. Sorry, where are you based? Are you in Canada? Yeah, I'm in Canada right now. Okay. And how long have you been there? Like six years almost. Yeah. Okay, very interesting. Cool. So just quick context. Like what we wanted to do in this call was you are a part of the ML ops team at Son of C. So I would love to kind of take this call to understand a little bit more about the company data science initiatives, use cases and how the structure is. What are the tools you end up using, what are some of the key priorities and any challenges that you are also seeing? So that was the goal. Before we get started, maybe we can quickly introduce ourselves. I can give a quick overview of two foundry and the team and love to hear as to which part of this tag do you generally look at as well after that. Does that sound good? Yeah. Okay, cool. So I'm on Rogue, I'm one of the co founder at Truefoundry. We started Truefoundry around one and a half years ago. The goal was to kind of help companies in their journey of machine learning operationalization. So what I mean is anytime any company is kind of trying to build models, they have to go through a process where they have to need infrastructure for training their models if they are doing virtual training, and from their infrastructure for quickly testing out these models and deploying them to a broad environment with the best practices. So my teammates Abishek and Nikonju are my co founders as well. They used to be at Facebook, wherein they saw a really good system that Facebook had built internally called Bill on Earth, which allowed the ML developers to be quite independent. And it abstracted out a lot of pieces of infrastructure for them, so that even with less knowledge, they can still operate a system at a very good scale and with the right cost effectiveness and so on. So we started with that team, like if we can bring the power of an Epiler like platform to other companies around the globe and make it more abstracted out so that people with different skill set levels, knowledge levels, can still go ahead, test out models, deploy it to production and do it with the right engineering practices. That was the goal. So we have been building for one and a half years almost. The product is built out, we are deployed in a few enterprises where they are using us for their entire deployment and operationalization part of machine learning. And we are trying to talk to a few more companies where we can understand the challenges, understand the use cases, see if there is a potential way in which what we are doing could align with their challenges and then accordingly take it forward. If not, like, it helps us understand the things and it allows us to build the product better as we kind of are in the product building chain. So that's like a brief. Wewake is a part of the founders office, and he helps us in the product side as well as in helping talk to the customers, potential customers and so on. I think. Deep. You spoke to Abhishek? He probably may not be able to join today, but nonetheless, we both will be taking the call forward. Yeah. And just brief about myself. I graduated from IIT Correct School in India in 2013 and then post that. Spent six years working with the hedge fund, where I was using a lot of data to build trading strategies. So data there and then post that. Used to do portfolio management for worldwide. Used to be based in US and then in Singapore, and also a member of the CEO office. Looked at various strategic initiatives. Also in that time invested in a few startups out of interest and then finally decided to build something myself. So prior to Foundry, we built another startup in the Talend Domain, and we sold it to Info Age in India, and then the same team continued to Foundry. And right now we are a 20 member team. Yeah. So that's like a very quick picture. Yeah. That's really impressive. Thanks. When I saw Vishik's Profile IIT Crept, that's when I knew it has to be a good team. And then now I realize, obviously you guys are from IIT Corruptor as well. I guess all of them. Makes sense that you guys are going to have really good products for sure. Thanks. We still kind of that background is okay, but we still like the ML offices much. We still need to learn a lot from the industry. So this is like an attempt to do that this time. Yeah, for sure. I can give you some brief about what I do and what kind of tools we use. Interestingly. Okay, let's talk that in the end. I was going to tell you something about because I saw your product, Nikunj Truefoundry.com. I visited the website, and it's very similar to something that we just onboarded. And so if you had met me, like, two months back, I would have actually recommended your company's name as well because I'm one of the people I was involved in the onboarding of that tool. So it's called stealth. I don't know if you know, we just signed the NDA last month, and we started the conversation in probably December. Before Christmas. Just before Christmas, we started the conversation with them. Just like you guys, they approached us, and then we were actually looking for a model serving and model monitoring tool specifically for data drift and drift monitoring and model serving. We just onboarded. Anyway, I can still give you, like, a little bit of brief so you guys know maybe what kind of tool stack companies use? I don't know if that's going to be useful for you. Probably we are a bit late, but nevertheless, it kind of is helpful to know the stack. What are the kind of things, what led you to kind of looking at Selden, how did they approach, what went through? And that will be really helpful from the perspective. So maybe we can start once with the ML team as to how it's structured and take it from there. For sure, Sanofi didn't have a very good digital data team. More than very good, like very systematically separated out digital data team. Everything was like, there's a data analyst and data scientist working in a team and then they're just managing somehow. So what happened is, I think 2022 they started this team, and I think I was at 13th, higher, and probably their second ML Ops engineer. So basically the point was to build an ecosystem to help the data scientists and machine learning engineers build their machine learning models independently. And after I was hired, there were a lot of other hires. So now the team has grown to almost 350 people, 300 and 350 people. And amongst which most of them, or maybe a huge chunk of them, are data scientists who are working on building models. We have like four different business domains, like four different verticals that we have data scientists allocated in. And they're working on a lot of different projects, R and D. Then there's RWD, there's also advertising and promotion, a lot of different projects. Rnd is very interesting because they have really high level, cutting edge tech that they're using right now. So they just started using Metaflow as well. And I know Metaflow will be used by a lot of other team members as well. And I have some hands on experience with Metaflow, but not a lot yet for me, I'm mostly working towards the platform engineering side. So for now, there's one project where I'm working in the NLP side of things. So I'm actually working with a data scientist with making a Learn to Rank model for our search engine that we have, we have an internal search engine to search documents, data and whatnot. So for that we want to set up a Learn to Rank. So we have this tool called One AI right now, where basically what happens is we created a platform which helps the data scientists start to spin up their own small workbench, sort of a thing where they can work. I think a lot of companies are doing this these days. It's basically based on microservices platform. So we use Eks for that to deploy. That's where we a lot of Kubernetes and Terraform for deploying other resources like Clusters and stuff, and just a lot of resources on AWS, basically that's a bit more towards DevOps. So I don't work with Terraform that much, but I do work with a lot of Kubernetes. I work with building a lot of CI CD pipelines for the machine learning models that they make. We use a lot of Githubs, so we use argument and all that. I think a lot of use Airflow, even Sanofi used Airflow before and then they migrated to Argos now. Okay, do you end up using Argo workflows as well? Yeah, it depends on the kind of deployment that we have. Like if it's an API based deployment or if it's just a web app or API that's encapsulated in like a docker container that is deployed on Argo CD. But if it's like a scheduled job, like a cron job or something, that obviously then yeah, we have a lot of other a lot of the teams have their own tech sometimes. Like there's one team I know they're using cabanas or something, I'm not sure what they're doing. The cabana and elasticsearch they're using that. So that's basically it. And then I already explained about one. AI, but yeah, this one AI is the internal platform. Maybe I'll try to explain and please try to correct me in. So basically this data scientist, they would be running their own training jobs for training their models and all. So for that they will be needing resources. So this resources they are provisioning using the one AI. Right? And these resources are then provisioned like as a Kubernetes cluster over Eks. I'm guessing. We can auto scale it. Yeah, so you can auto scale. And who manages this resources? Like suppose some researcher requires like a GPU versus someone who needs a CPU and given so many data scientists are there, there might be cost challenges as well. So how do you manage and control that? Like who is basically the admin who kind of approves or is there a structure when there's a base limit of configuration that up to this much configuration anyone can provision without asking and beyond this it goes to approval. Like what is the workflow there? So we have a lot of taints that we set in our terraform script so they cannot go over like 100 CPU or all those teams and what is it called? What's the technical term? I forgot about ten cents and something for ETS. Anyway, so they're basically like thresholds that we've set in Terraform that we also have. We've also given admins the power to admin basically the project owners. So we've given them the power to set up their boundaries and now I think they're starting to onboard a lot of phenops people as well. So we have cost under check, so we don't want to go over our thresholds. Okay, suppose they provision these instances where do they kind of run their training? Is it done on Jupiter notebooks or is it done do you have a sense there of do data scientists run their workflow? I think data scientists normally choose to work on Jupiter notebooks but there are a lot of people I've seen who also use Vs code a lot like a small minority that uses our probably two out of ten or something like that. One to two out of ten people there. But mainly from what I've seen, it's duplicated notebook and Vs code are people who are very good with programming languages and all the heavy duty stuff and they use BS code from what I've seen. Got it. And these jupyter notebooks, do you have hosted instances of this running internally or do the people end up using the local systems itself? So the one AI that I just explained has this feature like jupyter notebook, Vs code and all of that is in build so the data scientists can actually work on it's like a link on your browser. Oh wow, that's nice. Okay. And then all the code from here is checked in generally to GitHub. Is it? What do you use for your GitHub? Okay, that's great. So now suppose a data scientist runs a model. Where is the data store? Do you use Snowflake data bricks? Everything is in Snowflakes for storing experimentation and mostly experimentation stuff. We use data bricks. I have not actually used data bricks that much. It's been a very long time since I used data bricks, but it's a ton of fi. I haven't used it that much, but I know there are teams that use it for experimentation, for storing that experiment. Yeah, that is all I know with data bricks. But. Snowflake is the data layer from there. Like people kind of run this on the notebook. Do you know if they have like do you end up using GPUs in your training? It depends on the project. Of course, a lot of the GPUs are used in R and D because they need to process a lot of the data about molecules and a lot of the microbiology stuff. So for that they need like heavy competition power. I see. So now they'll use that, they'll provision. Now if they're running their training, they will push the metric. Some teams will push it to ML flow, I'm guessing, which you mentioned. What about other teams? Like where do other teams store their metrics? Does one AI itself has a way to store metrics and all? No, one AI doesn't have to use graphon and promises for that. But that will be for your system metrics and all. But when you are, say, training, say ten different versions of models with different hyper parameters and you want to compare one versus the other, right? Like logging the model metrics, I don't mean like the CPU consumption and all of that. So ML flow, I understand, like some teams will be using, but what about others? That's why we onboarded Selden. Okay, so Selden, you kind of it. Has these features, I'm pretty sure like they just showed us demo and actually we started onboarding, I just started working on like a demo project on stemson. So their UI is pretty good from all the servings especially. Okay, this section that you just said, but probably I will, but I know they have that. And suppose now the models are trained. Now you have compared them. Now you want to take a version and you actually want to put it into deployment. What is the process there? Like do the data scientists themselves do it or do they give it to your team, which is the ML engineering team and you are responsible for finally deploying it? We do it. Like for example, we have this one project, it's called Turing. So they have a lot of these model versions for different countries and different drugs and for different brands of the drugs. So they have the models that they make the models. And I have to basically take that and then deploy it because the deployment is a little challenging because it has a lot of configuration changes. A lot of the Tweaks we need to do for every single version that they make. Basically that's what we do. And again, like I said, a lot of that involves also making changes in CI CD because we use GitHub Actions for we have runners, we have our own runners that we use. And again, the runners are also on Eks. Basically. That's the thing. The whole deployment part of the machine learning models that the data scientists create is what I'm more involved with. Okay, so you will then take these models, you will deploy it and you will put the CI CD through the GitHub Actions. This deployment generally deployment I'm guessing happens on the CPU instances or do you need GPU instances for deployment as well? We have only CPU. Right. And do you maintain separate like within Kubernetes, like Eks, do you maintain separate namespaces or different projects or something? Like what is the structure there? So again, it's like clusters. Inside that we have namespaces, we have deployment and in deployment we have deployment pods in that we have our container images where people work, basically. Okay, and does one name space map to a particular project deployment or is it independent? Not necessarily, it depends. Sometimes there are projects which are very similar and they just end up working in the same namespace. That's more of the decision from the management. I have nothing you don't have. But at the old level, the access control is maintained. Your access controls will be data centers. Do they get access to this production namespaces? No. So the production namespaces are completely when. Is a production namespace? What do you mean? Basically anything like basically, let's say production service instead of production namespace, let's say the services that are deployed in production, they will be sitting under some namespace. Do data scientists have any access to those or those accesses generally controlled or maintained by you? Yeah, they do have access, but it's again, their own namespace that they're working on. All of it that they are working. They are assigned to only that they have access to the obviously they don't have access to the back end of one area. They don't have that. Okay. And once the model is deployed, like the model metrics, I'm guessing before, were you logging your own metrics for measuring, say, data drift or anything like that, or was that not happening? Actually, we didn't have any drift monitoring, so that's why we onboard. Okay, and why did you onboard? One reason I understand the drift monitoring, but the survey, I think based on what you described, it seems you already have a good infrastructure built outside, like. In terms of I know, I told him the same thing, so so there are five machine learning engineers, including me. We actually spoke about this, and then we pitched this to product owner, who's the one AI product owner. So I asked the same question, why do we need model serving when we can have fast API that we deploy? And that does the job? But then they said that it's not just model serving. One benefit is model serving, of course, but then it's also data drift, drift monitoring and all of that concept drift. And there are a lot of other features of Selden that I haven't yet looked at, but there were a lot of features that they don't the problem was I went to India in December, so I wasn't in a lot of their introduction meetings. That's why I missed a lot of the introduction meetings. But after I came back, I asked them these questions, and they said that because they have a lot of the features. So actually right now, I'm in that zone where I'm actually exploring Selden, exploring how it works, because we just got the NDS signed, right? We just got access to the UI system. Understood you have some questions. Yes, I remember you did mention there are four views, right? And each be kind of sits. Your team kind of looks after all the different use cases or deployment process from each of these be used. So can you tell me how did this process of initiation of a new software procurement started? Like, where did it start? How does the decision making process happen? If you have any insights on that. As machine learning engineers, I was the first one who said that we need a drift monitoring system because it's something that I've worked on in my previous company. And I spoke to a couple of my coworkers, couple of my machine learning engineer buddies, and then I spoke to them. I spoke to our manager. I spoke to the product owner as well. And then I spoke to the data scientists. Mainly. We involved a lot of the data scientists in the meetings, and then we asked them if they actually had something in place, and none of them had model monitoring because they were so busy in actually developing models, they didn't have any time to actually work on model monitoring. My previous company was an insurance company and they had everything built in house, like the whole drift monitoring and concept drift, data drift and concept drift was built in house. So I had a fair share of idea about drift monitoring and I knew how important it was because what happened there was as soon as the drift got recognized, there was automatic retraining that happened, which enabled us to have our machine learning models very well up to date basically with the latest data. So that was pretty beneficial. And then that's why I pitched this idea and everyone seemed to like it. And the data scientists, the lead data scientists that we approached, they seemed to like it as well. So yeah, that's how it went. Decision making. If I understood you correctly, you or someone from your team kind of proposed this idea of having a modern monitoring, then you pitched it to data science team and then how did they go about, did it involve any leadership? After we proposed this to product owner, he took it till the CDO got their sign because I'm pretty sure it was a pretty expensive deal that they signed. I think it was a contract of some sort. I'm not sure about the details. So yeah, basically it went till the higher up and then finally we started it, it went to the higher up. Now again, we're taking over and we're actually trying to explore the tool. Yeah, got it. And it is already finalized. Yeah, I'm telling you, we just signed the NDA and we have KT sessions as well with them and they are actually they are developers, so they have a team of developers as well and they are helping us with the process of onboarding. So we have a POC that we're working on there's. One of the use cases that we have, we're trying to implement selden into that POC because that's one of the most complicated projects that we have in digital data themes. That's why we chose that as Plc. For that. Cool. I think overall I understand the workflow pretty well actually the like and to be honest, like I'll tell you a little bit about in the interest of time and also like to discuss a few next things. So basically the way we have built our system is actually before that, one question. Do you use any particular cloud? Like you mentioned, Eks, I'm guessing you're using AWS, but is there like multi cloud system that you use or primarily. For any group management, but that's only. Eddie group management, just for login and all of this. Okay, so basically SSO is on SEO, but the rest of the things is primarily AWS. Okay, got it. Cool, understood. Basically the system that we have been building, the platform, it's actually very similar to what you all have started on building out. So it's actually built on top of Kubernetes, it supports all the three clouds. So you can have clusters on Eks or say AK or Gke, whatever you want. And then within that, like, we have the concept of workspaces as well, similar to your workbench, but the workspaces extends to both the training side as well as the production side. So the way it works is, for example, a data science team can have an access to a workspace and within that they can run their training jobs and everything. And then the same workspaces can be used for production. Or you can have separate environments like a dev workspace and a production workspace and things like that. And for the data scientists, then what happens is they kind of are able to run their training jobs and then they are able to log their metrics, even the offline metrics. So even if they are running over, say, your Jupyter notebooks, hosted Jupyter notebooks, et cetera, they can still import their API keys there and all the metrics from that they can log into our system. And suppose they have 20 versions of the model, they are able to compare those logs and see, okay, this is the model I really want to finally log into production or move to a test environment. Then they can choose a version, they can corresponding to that run and move to a test environment. Then they can integrate CI CD there and accordingly when promoted to a fraud environment and all of that thing is auto scale with the gRPC authentication and everything. And then as it goes to production, you can set up the monitoring. Like the monitoring, you can log your actual predictions and it automatically triggers monitoring and you can compare drift as well. And the entire system uses the same stack, like it uses Argo CD for the normal deployments for the jobs and other things behind who do we use case of we also model servers. So if you want to kind of optimize for performance, et cetera, those things are there. And then the monitoring is basically the drift monitoring in general. So the stack is also pretty much the same. So actually what I was thinking is the following and let me know if this is something that will be feasible. One would love to schedule one call to show you a demo of the general platform that will be really nice to do. And then second is one to understand, like I understand it generally in such big companies, what happens is even when they onboard a vendor, as you said, there will be a POC phase. So what I'm expecting is the Selden would be going through that POC phase right now, right? Yeah. So do you think there might still be a possibility to kind of check if they'll be okay to do, say, a POC on another tool as well for another project or something and really assess because. No, continue, sorry, I was going to say I don't think it's possible. But continuing, the reason why I was. Asking this is because in bigger companies, what I've actually seen is generally before the final the contract is signed, generally they like to evaluate two or three vendors because the purchase and PO team generally asks for the vendor evaluation. So that is one thing that obviously that depends on whether you like the platform, et cetera. But that is one thing that if. Possible and one more, I need to check the name. I forgot the name of the tool. There was a competitor of Selden. Let me find that, that was also London based, I think maybe I'm not sure if it was London based, but give me 1 second. Evidently yeah, evidently. Okay. There's another company called evidently we actually evaluated Selden versus Evidently when you type model monitoring on Google, those were the two that I and my coworkers saw. And then we listed these two for evaluation and we actually evaluated both on high level. And then I went to India and they did a lot of the oh, I see both the companies. Okay, so from what I understand, the seller and this one primarily you evaluated for the drift monitoring part. So that is the primary reason for onboarding. Yeah. I think that's fine. What I would love to do is maybe schedule another call where we can go through the demo. I would love to hear your feedback if nothing and maybe just learn from you basically based on the internal system that you have built as well. If that is fine, then sure, for sure. No problem. Let me know maybe 45 minutes also that we can deep dive into the demo in more detail. I would like to learn from you guys too. You guys are from IIT corrupt. So. That is always like. I was just telling my fiance that I'm having a call with these bunch of smart dudes who are starting up and go from IIT Caribbean, which is like the same university where Sundar Pichao and it's pretty cool. Thanks man. Thanks so much. Either this week or towards the end of this week, we can do a. Demo called Perfect for me if you guys are okay with this time some other day because morning is very busy for me. Honestly. I have back to back meetings. Like for 4 hours straight I have meetings. The way it works is that we have so many teams, so many projects going on and there are only five machine learning engineers and we are kind of doing the work of consultants and helping out all the projects. So it gets very busy sometimes. Yeah, most of the day. Got it. Have you worked very deeply on the workflows? Do you have an understanding of both Airflow versus Argo workflow? Basically, you know, Humlow Jobs, Airflow 2.0, Explorer, Gadget and Argo workflows explore Gadget and you are confused whether to use Airflow 2.0. Or Argo workflows, because Argo workflows is obviously more suited towards the ML use cases. But Airflow is a much bigger community. No worries. I think few questions around that. Also, we would also love to ask, but yeah, I'm sure Keith with the call will also be able to help in a few things that you might be working on. Yeah, for sure. Let me know 100%. Cool. Maybe Friday this time or Thursday this time. Friday I may not be around, but Thursday is fine. Thursday or early next week. One of the two days. Awesome. Could you also drop your number? Sure. Someone actually messaged me. Already has it. What I'll do is I'll ask Abhishek. To create a group of WhatsApp? You, me and all of us. Using your number. And then we can coordinate there. That would be way easier, right? Would that be okay? Yeah, that's fine. I'm just worried. Hope like, no one from my team knows that I'm telling so much about I know that doesn't matter, but I'm just scared. Sounds good. Anyway, I understand that is an important point. So just be sure that from our side, whatever is there is between us, there's nothing outside of your knowledge that we reach out, et cetera, for specific. Thank you. Yeah, for sure. Thanks a lot. Thank you. Bye.