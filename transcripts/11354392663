Hello? Hey. Hi Vivek. I'm just logging in from my mobile actually. We are in this search event and somehow I'm not able to connect to internet. But are you able to hear me now? I can hear you. I can hear you fine. I should be able to take it mobile then. Sure, let me ping him. If it does not join it will be a delete for me. That's why I was not pushing for it by pinging him, I think. Hey guy. Hi, how are you? Hello. Can you hear me? Yes. Can you hear me? I can hear you well and good. Thank you so much for taking out time I have with me Anurag. He was there in one of the Sekoya event and unfortunately due to some internet issue he's not able to connect with laptop but he has joined in with mobile. He is the co founder of the company and I'm also trying to see if Abhishek can join in. But nonetheless Anural has joined. Would love to continue with the call. Hey. Hi guy, how are you? I'm good, how are you? I'm good. Thank you so much for making time and I'm really sorry I'm joining in from our phone we had like a Sequoia who is one of our investors. We had an event and we were in a place where some of the internet is very flaky. So I'm not able to connect by a laptop. But anyways, no problem. I'm sorry for being so difficult to schedule. I've had few international trips so I was very available. I can give you 30 seconds on my background. That is great. Originally from Israel, I have advanced degrees in mathematics. I started my career doing cyber security for the Israel NSA. I then had my own Algorithrading startup. I moved to the US and did a PhD in computational biology in Columbia University. And now I've been in Starter on Your Health for four years and I'm leading our largest machine learning team. We have a group of couple teams doing the actual prototyping infrastructure. Awesome, got it. Cool guy. I'll give a quick introduction for myself as well as True Foundry overall. So I'm one of the co founders at Truefoundry. I did my engineering from Indian Institute of Technology in India. So after that I went to like I worked with a hedge fund called WorldPoint. So I was actually doing trading just like you at so we used to build trading strategies across global markets, primarily doing equities and then we used to do a little bit of currencies as well. So I spent like three years doing that in Mumbai and then like four years between us and Singapore. And during that time I was a part of the Sea Office and actually WorldPoint is a big office in Israel. We used to love Israel as all the folks we used to hire. Awesome people. So yeah, during that time we expanded from seven countries to 22 countries. As a part of the CEO office and as a part of the portfolio management team, I was managing 600 million assets for them. I also started angel investing into startups myself during that time in 2018. And then as we move forward, like me and my friends Avishaik and Nikonj who were at Facebook, we kind of almost wanted to build something together. And that is where we all came together in 2020. Build our first startup in the talent space, sold it to Info Edge and over the last one year and little more than that, we have been building True Foundry. The goal is really to make it easy for data scientists and ML engineers to take their machine learning models to production. So if as a company you are doing ML and you want to kind of build a platform that can make it easy for you to reduce the time to value of ML models, we try to enable that. The inspiration really comes from what we have seen at Facebook and other companies like Google, et cetera. So at Facebook and Nikon Janavishik were there. Facebook used to have an internal platform called FBI Learner which enabled the high productivity of the Dev team. So we wanted to bring that to every single company. And with that mindset we started Proof Foundry. Right now we are a team of around 20 folks, mostly engineering, and we are talking to companies like yours. And the goal is to really understand how the ML pipeline works, both from the modeling side as well as from the infrastructure. What are the challenges you are facing and if it could be something that we can potentially help with. We'd love to kind of partner with companies, understand their needs, work closely with them to add value while we are in this initial stages, and even build something that is customizable and more usable by them. So that's like the overall background. My folks, like my other co founders, Abishek and Nikon, they spent majority of their time in software engineering and ML engineering. So Abishek was a senior staff software engineer at Facebook. Nikkunj was a lead ML engineer at Facebook. And yeah, that's pretty much the goal. And thank you so much. I really appreciate you taking the time to speak to us. So if you could guide me on what would be most useful. I can tell you a bit about the company at large. I can tell you about our machine learning operations, the business side. I'm a bit trusted on the infra because I started on this. We had the more ML software engineering mix where I was either directly contributing or supervising some of the info work. But it's been a long time since I've done that and now I'm very focused on the business and photo diving side. Okay. We've had like major updates to Emily and everything is in the air and I'm not on top of the entire story there. Got it. No, that is fine. I can ask you a guy a few questions and that will really help me get a sense of things. So first thing that will be helpful is understanding at what scale platinum help does ML and AI and what is the size of the team. And maybe if you can break down that size of the team into the people who are doing this prototyping and the modeling part and the people who are doing the intra part would love to kind of get a sense there and then I'll ask follow up questions from there. All right, so Flatiron is take a note, thousand people company at this point. So it's pretty large operation out of that. Data science team is not that large. We have between 40 and 50 data scientists. We also have other data, all sorts of different and more researchy. So we have epidemiologists and people who sort of do research on the data set and publish it and whatnot. But the core data science team is like 40 to 50 people. Out of those, I would say 15 or 16 are doing machine learning prototyping full time on the machine. We have those 15 people. And additionally we have product managers, maybe three or four, while focusing on machine learning specifically. And we have sorts like scientists and clinicians who supplement the team. So when we prototype, we actually work with oncologists and epidemiologists on the prototype. So you could add around like five or six people supplementing the machine learning team with their abilities, but they don't do like popular and machine learning themselves. Then we have more in house capabilities that goes into label generation. I think it's actually pretty unique. We do all of the label generation in house. Is it more like manual annotation? Is it more like manual annotation? Is it like folks who are using models to actually automate the generation of these labels? No, we have manual annotations for labels. So, Flatiron, as a company, has a ridiculously large operation of extracting and delivering data manually without machine learning. Oh, wow. And we are talking about thousands of millions of dollars in direct salaries for the annotator of the year. So it's not a small operation. And so we are sort of like piggybacking all of the infrastructure for that the workforce to also generate machine learning labels. The reason that it's really in house is that we already have this insane machinery in place, a lot of employees, it's a pretty big operation. And then we have an ML infrastructure team, which I would say is between five and ten people. Okay. So few questions before I ask follow up questions. One thing I would love to know is what are some of the use cases that you generally solve for guy, like as a part of the ML team, both in terms of prototyping research and what the core data scientists are working on? Okay, so Platinum is a tech company in Zoomcology space. We really do two different activities. So one, we develop an EHR system, if you know what that means. So that the software that we have oncology clinics around the US. Use to manage everything, right? So patient care, communication, billing, et cetera, it's part of that. We maintain leaks and tribes to the patient's data. So that means that we're allowed to de, identify, curate anonymous data set and then delivers those to Pharma clients. Most of the companies revenue and innovation comes from the Pharma side of the business. Got it. And then the kind of use cases, if you want to have like an example in mind to say you already have an approved therapy for some indication, but the Pharma client is considering extending the indication to a different disease. So say they have a drug for specific mutations for breast cancer. They suspect it might actually work for other types of cancer. In practice, you already have patients who are taking it off label, and we would be able to give them that information. Like, what kind of patients are even tested for this mutation in other diseases, how many patients taking the drug and how they're responding to it. And that could direct them on making a decision on which populations to make a trial on. Maybe we should focus on this disease, not the other. In extreme cases, Pharma clients have been able to skip the trial and use our data to receive immediate approval from the FDA, which obviously is a very big deal. And so that's how they're using data sets. Got it. I think that gives a good context. One question here, when you are kind of giving these predictions to the clients, are you using the data that is provided by them? Basically, do these models sit on their infrastructure or do these models sit on your infrastructure? We basically sell like a million dollar CSV is like the business model. We sell them data sets. Okay, I see. And they do the research on the data set. And then maybe to be a bit more explicit about the role of ML. So the main thing that we do is machine learning. There are like, side things that we also do is we extract information from the patient chart. So a patient chart is a pretty messy set of documents, lab reports, billing, whatever, and it wasn't generated with the research question in mind. So, for example, if you have like, a mutation that's not indicative of care, it would be documented almost as like an after fact, like some once on the report. And so we would extract the information from the chart. We're not in the business of making future predictions. Like, this patient is going to respond in this way or that way to treatment that's going to survive or not survive. We do NLT to extract specific data points from massive clinical documents so that's the main use case and then we have a lot of different variables, subtleties, whatever, but that covers I would say 85% of what we do with the machine learning. Got it. Understood. If I may ask guy, just if you can give me an overview of the overall pipeline. Okay, so these data scientists build the models, do they use something for experimentation or tracking? Which model is better then how do they kind of deploy it? Where is the system deployed? Do you do some sort of monitoring on top of it? Just an overview and if there is any tools you are using along the pipeline, would love to kind of get a sense of that. Right? So we usually start with label generations if we need to. And again that's using like a pretty insane internal system that we have and everything is in house. Sometimes we already have existing labels, so it depends. And then we would go through prototyping phases where in terms of the infrastructure we work on like a large jupyter hub cluster using AWS. The tools that we use, there's obviously a lot of third party libraries where we're not in the business of really inventing new ML tools. Like. We use XGBoost. We use tightorch. All of our stack is Python. We do have a few machine learning tools that are slightly typical. So we have a publication around clinical vertical. There are ways we apply either pretrain or apply an existing tool like that in a way that actually works better for clinical tasks. And sometimes we take the XLR for it to publish those. But most will vary applied so we will not want to develop something like wear it from scratch. It's like completely algorithms. Got it. One question in this part and then we'll go to the other part is when you kind of spin this Jupiter hubs, do you kind of use it in local environment or is it like hosted Jupiter Hubs or are you using something like a data bricks hub or a Sage maker? Like Sage maker notebooks. Do you end up using GPUs for training these models or is it like it's fine if you're done on CPUs? No, we have to use GPU instances. I think we are seeing our own machine on AWS and doing a pilot with databricks at the moment. Okay, it's happening on AWS but I think we're considering data bricks and I don't know if it's still the decision though. Got it. And yeah, after that, once this prototyping is happening after that I would love to know the flow. Sorry for interrupting in earlier. In terms of we don't really have extensive feature stores so we load the text either directly from Elastic. Yeah. That'S like the main thing we do is just those documents. We do aero analysis usually with an oncologist in the loop. So that's also a process where we really do like very detailed AO analysis because there's a lot of clinical edge cases that we typically need a lot of and also to prioritize that is important or not because the definition of the data is a bit messy and whatnot. So we usually have Denissions in the prototyping, which I think in terms of conceptual validation, sometimes we go way above and beyond what's typically done. So we really do all kinds of bias analysis and data analysis where we try to stratify sub populations, the model underperforms and whatnot depending on how much power we have. Sometimes we have existing data sets actually pretty have a statistical power to do that kind of analysis, like actual statisticians to play with machine learning predictions and validate them. I think we'll be becoming a bit more lax about it, but we still do what would be completely unhaird of outside of Hillstick, I think validating by us. It's probably a bit too rupological even for our field. So there's a big operation around that. We have different ways of deploying the models and using them in production. The consumption of like predictions for these models is done internally for generating data sets. The size of the data sets can vary, so sometimes we have very small deliveries, but then we also generate ML predictions for the entire network of patients, which is millions and millions of patients until it's documented those variations there in scale. And we have an internal monitoring platform where for models that are regularly used in production, we maintain like a representative test set that we actually continue to curate with time. So as patients leave and join the network, we would automatically send like the sub portion of them to human curation. We would have like a label data set that's up to date. And we would on a monthly basis make sure that the model performance hasn't dropped, and if it drops, we'll make sure it affects it. So it's quite a process. We have automatic validation for our models going all the way from keeping the labels up to date to measuring performance, placing alerts on that. And then try use for deployment because you mentioned AWS, do you deploy on the EC two machines or do you use Sage Maker or something like that? I don't think we use Sage Maker. We're using internal tools. It's the best two in house. Okay. Are you aware by chance guy, like if you end up using EKS, like basically Kubernetes for the infrared, do they build their infrared Kubernetes or do they use the normal ECS and the serverless and EC two instances? Do you have a sense of that? I think we switched the Jupiter to instances to Kubernetes, but I don't know if the other ones okay, so they. End up using EKS, but probably not for everything, if I understand. Okay, got it. Understood. This deployments, finally, do the data scientists do these deployments. Do you know if you dockerize the models before deploying? Or is it like you provide a pickle file and is it directly hosted? How does this work, like from your side or from the data scientist side? Or is the pipeline more like you have made the model and then you hand it over to the ML engineers and they pick it up from there and do their part. So we used to do something much more like we used to just hand out pickled models. Okay. I say pickle. It's slightly different even on that front, but I don't think those details are important. So we could say pickle for the most part. I think also during the switch to localize the model, but I don't think it happened yet. And I'm not sure I don't have the high level context for that. No problem. I think I just wanted to get a sense. Okay. In this entire pipeline, these models that you build, are they real time hosted as endpoints because they work on data sets once in a while? Is it like batch? Like, I'm guessing a lot of these are batch predictions that you run, but just correct me if I'm wrong. It's a good question. So in practice, we have the service. It provides all like, live predictions on request. But it's a bit of an overkill for use cases because, as you say, these data sets don't actually require that. So it's more of like how the system is set up for the larger cleaning processes. And we've been moving away from that, so we still have some connection stuff served like that. We feel like it's not a requirement for us and we're moving to something that's much more laid back. Okay, understood. And if I may ask, what is if in this entire pipeline and you working with the ML team, is there something that has been a blocker or a challenge or one thing that you feel could be improved based on the current state you are in? And you would want to say, make this part better or something which you are exploding because like you said, you are exploring safe data bricks for the Jupitery hubs. But anything else that from a problem perspective that would be really good for you or your team to solve, that will make your lives easier or will kind of make this data science even more effective at Platinum? In all honesty, I feel like it's been a continuous challenge for the company and the details have been shifting around. So I don't know how to phrase it in a useful way. I think a year ago, we were at a point where we felt like the state of the prototyping instrument is so problematic, it's just so flaky that we're having a problem retaining people over it. Yeah, and we hired a few people, but the state of the solution there was very urgent. So we really wanted to put a plaster on it immediately rather than do some kind of long overhaul. And at this time, we're in a state where we are really scaling up the two to three minutes machine learning data sets that we deliver. And so we suddenly move from doing a data set of like 200,000 patients to maybe 15 times as much. And the processes don't scale very well for that. And we feel like the state of the API is actually critical for the business. If it is solved in the next year, it's going to have real business implications for the company, which we're trying to avoid. That's a very high level, non technical overview of the situation and bringing it down to solve like, what are the actual info challenges or problems that we are critical to solve? I feel like not something I can give an accurate description showing the top of my hand because I spend my days like, how do we apply Barrett and where should we like machine business? And I've drifted very far away from the info parts. Fair enough, no worries. So I think a guy like this is super helpful to understand the data science and how the data science functions and how the data scientists generally are working on the build out of the model. I do understand that probably the intra site is something you don't necessarily look at. I think you're on mute, I'm not able to hear you. Yeah, I don't think I should be proud of that, but that's currently the situation. I think if we spoke half a year or a year ago, I would have been more up to date. Okay, that's fine. That is not a problem at all. And whatever you have shared, that is already very helpful. One thing I would love to do, guy, is as a follow up call, I would love to kind of show you once the way we are building our platform and love to get some feedback from you. I'll just give you a quick overview. So what happens is a lot of data scientists generally do not have understanding of the interim. They know how to build a model, they know how to take a bird, retrain it for their own data and things like that, but they will not know how to host it in the right scalable way. So what we are doing is kind of building a platform to enable them to do that. When I say that, it means that both training at scale, hosting and then hosting, comparing the different versions of training and then picking a model out of that and then hosting it in either a test environment and putting it into production. So I'd love to show you the platform. Given you have had past work on intro, you'd definitely understand it. I'd love to see if there is a use case that is there for your prototyping teams where they can use the platform to kind of try out different host different test versions of the model and then give it to others to play with and at the same time, this is one. The second thing is if it is possible for you to have someone from the infrared team as well joining would love to kind of hear a little bit from them on the intra and maybe showcase them how we have built it out and if working with us or using our platform will help them solve the challenge of intra and might help accelerate the ML thing that you are looking at as well and remove some of the intro challenges. So these are two concrete actionables that I was thinking of, but let me know if this is something that will work from your perspective as well. I need to jump off soon to a different thing, but I can follow up with the guy who's running like the MLC infrastructure side of things and he's open to connecting. I don't know when he's back. I literally think he's in Antarctica now, which I assume doesn't come with a good connection, but it should be back next week actually, so I can check in with him when I have a chat. That will be great. So we'll send out an email, we'll kind of try to get a follow up call with you and if he's able to join, that's great, but otherwise, even if he's not, we'd love to definitely show you what we are building and take your feedback and see your take from there. Thank you so much and really appreciate you taking the time for the call and explaining the overall ML pipeline. This is super helpful for startups like us. All the help we get in this, it helps us understand better and really appreciate the same from your site. Thank you so much. Have a good weekend. Thank you. Bye.