CTO at the same time, so we can take investment from them. Right? Because hey. Hi, Bryan. You are you hey. Are you busy to hear us? I can hear you. Awesome. Where are you based? Are you in San Francisco? I'm a little South Palo Alto. Okay, got it. Cool. Awesome. Cool. First of all, thanks a lot for taking time for the call. It's really nice to kind of connect. I think we have been following back and forth, but somehow both of us miss this other, so it's good to finally connect. So thanks for that. What we can do in this call was I'd love to know a little bit about how your assistant with the Builders Fund is, and then we can talk a little bit about our backgrounds and what we are building. Would love to hear a bit more about you.com and how you have been listing in ML if we were talking about it. So I wanted to kind of get that as out as well in the call. And if you have anything else that you would like to cover in this call, I'm more than happy to do that. Yeah. Awesome. I just love to learn everything I can about you both and Truefoundry and Akhil, everything. I can just start out with a little bit of a background on me. I'll give you the total overview. I'm the CTO and cofounder@you.com right now. Started with my Cofounder, Richard, a couple of years ago. Been around for about two years now. We were at salesforce together. He was a chief scientist there. I was on his team doing research and AI for about four years or more. Transfer learning, multaraining, large language models. Went through chainer PyTorch alpha testing, all the different forms of GPUs and GPUs. And it was an early kind of salesforce software of GPUs and got them hooked on those. Yeah. And with you.com, we're about 30, 40 people now, probably 20 to 25 engineers. A lot of machine learning work happens, but also a lot of product engineering, stuff like that. Now, my connection to Builders Fund is well, I met our Pete when I was a salesforce and had worked with him for all those years while we were there and worked with them on a bunch of different projects because I was a lot of time working with different devices or kind of some of the experimental stuff or really large models at the time. We're working on contextualized word vectors when production was just using word vectors and stuff like that. We talked a lot about incoming work streams for the platform and infrastructure and everything. That's where he was. So we were talking a lot about kind of the recent advances and how we could build things, but also how we could also support research infrastructure and bring things from research to production more quickly. So, yeah, we had a really great relationship there. And then he's an advisor@you.com now. He's been kind of advisor on our infrastructure side as well. So pretty much since the beginning I relied on him a lot for how we be setting up even the basics arvind with Kubernetes and stuff like that. And he's built a lot of experience over the last couple of years as well since then. But we still meet with him every week. And so, yeah, we've been working together on things in various capacities and in various ways. Now Builders Fund is kind of like a more formal way of us trying to also continually do that with more teams and bring what we've learned over the last six, seven years and kind of answer questions, but also guide people through the inevitable process of continually evolving services and technology. And then, yeah, with you.com we have an ML team, we have a big platform team, so we're also potential customers in that way. And if that makes sense too, then I'll loop in some of our like, ML Ops people and platform people and then maybe some of our like, engineers that work on the modeling site too. But yeah, so lots of different avenues. Awesome. Hopefully that connects the dots a little bit. Yeah, it does, like, had some background already from them, but I think this kind of completes it. So thanks for that, Brent. Maybe we can go ahead and share a little bit of our background. So just starting like two foundry we are right now at Gmail 15 folks, as Arthur mentioned in the Gmail, we are building in the machine learning deployment space, trying to make it very simple to deploy models to production and scale it without going through the hassles of learning new things each time. So we'll dive into that a little bit background, like we Karen, three of us, as founders. Mia. We shake. And Anikunjunj is based in the Bay Area. We shaken myself, we are based in the Indian time zone right now in Bangalore. Just quick background about myself. I graduated from Indian Institute of Technology, Pin, India. Did my bachelor's in electrical engineering. After that, spent like around six years working with a hedge fund called World Fund, where I was first doing development of quantitative strategies using a lot of data, and then later on went on to be a portfolio manager, building and maintaining a portfolio of around 600 million at the same time. I was also a member of the CEO of this with Knock, various growth initiatives there. So a part of this tent was based in US and Singapore for almost three and a half years. Nikhunj, his background has been on the ML site primarily after India Kharagpur, he went to Berkeley for his masters, and then he went to join Reflection, which is a status in the recommendation space. He was the first ML engineer there, so he built the ML platform there from scratch along with the rest of the team. And then later on he joined Facebook as a lead ML engineer in the team called Portal which is like a competitor to Alexa. So he spent his time there. So had experience with the Apple on our platform or the internal platform of Facebook and brings the rest of his experience on the ML side. And Abhishek will probably tell about his background as well. We all left our jobs in 2020. We built our first company in the talent space and then we sold it Pin Forge which is one of the biggest players in talent space in India. The reason was we are facing some challenges in operationally scaling it and we felt that it was not involving a lot of tech like selling to the HR folks. We were not able to kind of get them acquainted with the technology and let them adopt it. So at that point as a team we thought that we are more from engineering background, we want to build something in the tech space and that is where we kind of thought about this idea of making machine learning much more simpler. So the journey connected in the way that when we were at building that talent platform which we used to sagar enthire we had also built like model for matching up the job recommendations with candidates and in there like while the local building was easy, like the productionization was really tough and we were discussing like it was so easy to productionize models in Facebook. And with that like we started thinking why is this so tough externally? And we thought that if we can bring something that Facebook has to everyone right from smaller startups to big companies, they don't have to do the heavy lifting of building a platform themselves, but they can actually take and use something from scratch and build on top of it. That was the idea that led to found it. But yeah I will go into more details there maybe I wish you can give a quick background. Hi Brian. I also graduated in 2013 Pin Computer science and then worked in Facebook for around five and a half years. Worked on different teams there. Like the Distributed Caching system team was leading the mobile performance team and was eventually leading the leaders organization there. And then early 20 to quit my job. And then the journey is same as Pin broadcast like we did the start up on the talent space and currently working on Profoundry. So that's roughly about myself. One connection is there like Richard, we have met Richard once, not me and Amish but Nikonje has met. So one of our investors Anthony Goldman is actually the founder of Cagele. He and Richard apparently run a fund right like together. Yeah. So I think connected lot of connections. I just remembered like now as you. Karen talking about it's funny, I feel like the more time goes by I feel like the more people are just kind of invested in each other and status are just customers of each other. I think in many ways it's good. It makes the whole network in some sense more robust, I guess. But yeah, it's funny, I feel like it's everywhere I look now. Do you do like a normal kind of intro to Truefoundry itself and kind of what you're doing and maybe you can just treat me like a you.com customer and then I'll see what my normal questions are and stuff that come up and we can see if there's any overlap there. And yeah, in the process I'll have the secondary in the back of my mind of builders fund and everything and what that means for you all and our pee and that group as well. Sure. So generally when it helps to understand a little bit more about the stack a bit because the overall introduction generally is qualified, but it'll help if you can give like a two minute overview of the you.com current state and then we can talk about Co Truefoundry, if that will be fine. Sure. Yeah. Well, which part of the stack do you care about? I guess the most. Starting from the data science, like the model building to the training side and where are you storing the models, how are you serving the models? Sure. So we're built on azure. We do everything. Pin azure. We use data bricks. So a lot of stuff gets done in data bricks. All the analysis, plotting and dashboarding, things like that. We have ML flow. Flow a lot. We use Kubernetes and pull images into machines and stuff like that for deploying ML services or like downloading models from wherever. I think right now, maybe the model files are in block storage, but she's basically Amazon s three. But I shouldn't say that because I'm entirely sure. I think we have some blob storage. I know we had some on like independent taxable volumes and stuff like that. Whatever we need to set up. We don't have a huge problem with starting new services up. Usually we're not like starting a lot of machines and tearing them down and stuff like that. We just roll over once a day or something like that. But the startup cost isn't too bad there. Let's see. Yeah, we mostly use PyTorch. I think we're in the process of trying to figure out nvidia triton stuff for model serving and things like that. But there's been some hiccups. Translate all our models into onix for serving. Between GPUs people use a combination of notebooks and pipelines. For pipelines you're using data breaks for pipelines? Yeah. Okay. You don't have anything like Airflow or anything like that? We don't use airflow. Yeah, we don't use airflow. Okay. We TerraForm all of our pipelines too. One quick question. Who does the deployment of the models? Like, basically do data scientists do the deployment themselves out there? ML engine to do the deployment? We have like one data scientist and they don't at this point work on like, modeling as well. They're mostly like just in the data and analytics and stuff like that and engineers. Okay. And the platform thing that is there Bryan you mentioned about. So they kind of build the platform for the general software engineering label and the same platform, like the ML platform also sits on top of it? Or are they currently separate? Like how is it. The platform is developing a lot of the patterns for the services and how they're deployed, but then like, an ML engineer would, like, actually do that. So yeah, like the set up for the Kubernetes clusters, how keys are handled, how models are handled, those patterns are all mapped out and set up by our ML Ops and DevOps people and some back end engineers. Everybody pretty much has some ML experience so they know what to look out for. Those people have also in their previous jobs just been ML engineers or specifically ML Ops people. And then an ML engineer will come through and have another model. And if they've learned the pattern, then they'll just do it themselves. They'll be like more self served, obviously with some reviews and stuff like that. But yeah, I mean, some people are new. Every once in a while they'll sit down with those, like, platform people and be like, how do I deploy a model here? It's mostly selfserved. Okay, understood. I think get at least a high level sense of the system. Maybe we can give a brief about two foundry and take it from there. And we'd like to kind of take your help also branding, figuring out like where there could be a mixed good fit in terms of you so close, that's fine. So I'll start and wayve I wish you can add so truefoundry are actually a path on top of Kubernetes specifically designed to support the ML developers in helping them deploy their models to production. So you can think of it in a way that as a developer, if you Karen writing your models, after that you just have to like add a few lines of code and post that. Everything from the deployment to the basic monitoring is taken care of by the platform. So it supports like, real time as well as batch case deployment of models. If you deploy any of the models, like if you deploy real time model, you'll be able to get like an end point. You get like service monitoring out of the box, locks out of the box, as well as you get basic monitoring for your models, which is like ML monitoring and basic drift characteristics out of the box. If you're deploying batch models, you get the scheduling and other things out of the box. The platform sits on top of either our public cluster or it can also connect with your own cluster. For the DevOps team. It allows them to easily provision and manage the resources across different teams. Suppose your teams are working on different types of problems, then you can have different instances set up and then within those you can have different workspaces created which can be allocated to specific teams. And for each workspace you'll be able to track how much resource are being utilized and how much cost is maximum. Team can enter and you can allocate it to a team. And then within that the teams can then start deploying services and so on. As you deploy service we allow like a fast API mode of deployment and then we are adding like a model server out wherein you can actually choose the right model server to kind of deploy. And within the workspace we also locate the kind of machines you want to deploy so that you can easily optimize your deployment for better, for a more optimal cost, et cetera. So the goal is really to kind of make it very easy for deployment. And if this deployment is supported both either from a UI or from a CLI or from like a Python wayve, so all the functionalities are supported. So even if say there are data scientists on the team who do not know engineering, they will be able to deploy it directly through their notebooks. In a simple way. It kind of enforces the best samira principles. It kind of automatically ensures that CI CD is done and it kind of enforces those kind of things. And if they are ML engine then they can use like the CLI or any other way of deployment. And then on top of this, what kind of the goal is sense the more complicated things that you might need as you scale for example traffic splitting or saddle trafficking, saturday traffic, those things are supported by default. So your learning curve for any of this is not very high. You can get started on the platform at a high level. That is the way we are building it. And which do you want to add? No, I think we can show you. I think it will become just by talking it will just become too much. Maybe we can show some of the the experiences that we Karen trying to give to our developers for deploying. That will be better. If you want I can show it to confess on my screen right now. It will not do. A license. Is Airflow, like you mentioned Airflow. What was the nearest kind of thing in the space that you feel people would be already using or you'd want to be switching from? Is that kind of where you imagine yourself plugging in? Like where people are using things like Airflow? I haven't used. Not airflow. Brian, what your platform team did now, right, bryan, you said that they're laying out the best practices and to make it selfserved it must have taken them one to two months, like a couple to three engineers. And these are like the best of engineers, probably you have brand@you.com. Many of the companies don't have the luxury of having engineers plan out and making itself. So it takes them like years to get to a platform stage. So we are trying to give this to companies from day one that they don't have to build this. Again, every company is getting it to a stage where they make the covenant stand up and try to make itself serve for ML engineers and data scientists. And still the experience is not that great because they constantly have to maintain it, something gets upgraded and things like that. So we kind of try to take over those things. And I can show you what I mean by that. Just like then we'll not do a live demo because that will go a lot into code and also we'll do like a high level walk through and maybe that will give you a sense of what the platform is. And based on that, you can also think of some of the use cases and then we can have another discussion and maybe get some of your other members of the team as well. Take a look at it if you feel that it could be worth. Yes. So basically this is our platform. And you can connect like your existing Kubernetes clusters to it. So you can just bring any cluster and just add the cluster here. And once you have the cluster, then what you can do is basically the India team or the platform team or whoever that can just allocate different parts of the cluster, we call them workspaces. So different parts of the cluster, Karen, different teams. And you can also limit like, okay, this workspace. Karen limited to four CP and Hgbm. So there is no way that somebody who has allotted this workspace too can spend more than like $50 a month. So even if the code screws up, even if they mess up, there's no way they can screw up the rest of the infrastructure or spend a ship ton of money, basically. So basically this basically allows you to provision the cluster and divide it among multiple teams or people just to play around with. And this is your kind of the admins view of who is using what. We also plan to show a cost estimate right here that this workspace isn't quite this many cost later. We also want to give cost insights and things like that. That probably the service you can do this week to make the cost go down by 20% to 30%. So that is on a more admin level. And for our developer, the view is going to be as simple as if they want to deploy something. First of all, you can get a service catalog of all the micro services that are currently deployed in your company, like all models, everything. So as of now, you can see services, jobs, and there's one more thing which is coming, which is the models. So jobs can be like your training. Jobs can be you bash conference link. You can trigger them right away from the Uri right here so you can trigger the job. It is currently suspended. You can also make a crown job that runs every Sunday or whatever frequency you want to run them at. You can also see the logs and metrics of the services right here. So some of the services will have logs, some of the services will not have logs. So you can see the logs and metrics of the services right here and you can go inside also to see how many versions of the service have been deployed, what are the environment variables, the secrets that the service is using and then basically to create a new service or a job. It's as simple as you go here as a developer you'll see which workspaces you have access to and you'll just click here and then you can just give the name of the service like OSBC and then you can just choose the data repository from where you want to deploy and this will automatically figure out okay. The repository already has a docker file so everything is set and all you do is just click on submit and it will automatically go ahead and deploy the service. So this is initializing, it will actually build the docker image for you and then it will automatically deploy the service. This is kind of a self service. Platform that and then it supports basically like stored by a docker but if you don't have a docker find then it will automatically docketize and create it for you and so on. It supports all the frameworks like Psychedel and Python and all of those things automatically just like GitHub, it has support for Bitbucket and then we can add GitLab depending on if there's a new customer who has the requirement. Right now the pipeline thing is not there in the platform but that is something that will come in where you can deploy like a pipeline as well and run a pipeline as well. And then a few other things that we want to be able to do is like the model server thing is right or not there but that will come and you'll be able to choose like you want to deploy via model server so you can actually optimize your deployment for a certain case. Then GPU will be something that will be supported and then machines, like the choice of machines and all that you want to have in a workspace or while deploying will be coming as well. So one more thing we are working on right now Brian is to add a model thing here and you can just select the model and you can decide you want to deploy by trite and you want to deploy OnX like two or three different strategies and then you can just run a load test on them quickly to see which one is performing the best. And you can also choose the machine types here. So for example usually in case of if you're doing performance optimizations and all the type of CPU and the type of memory, the process makes a huge difference. So we also allow you to quickly choose like whether you want to deploy it on intel processor, on AMD processor or things like that. So you can just choose like I only want to go for a Zone processor. At the end of the day it's the same self sense platform kind of thing that you're building internally. Yeah. And then few other things is like we have our model registry on model registry but you can also bring you own model registries and connect to it or the Doctor registries etcetera. And then the authentication and secrets management is also there as a part of the platform. So you can either use your own secret store or you can use our own secret store. So those are some of the things that are there. Brand so at this stage we karen still very early working with a few companies and we wanted to work with slightly more advanced tech companies to be able to kind of mature the platform further and be able to support their needs and in the meanwhile like add value to them and maybe actually improve the platform further. Yeah, I mean that was the UI way of deploying brand that I showed you can do the same thing by Python code. So this is the Python code that data scientists have to write and then they can just judge to job deploy and automatically deploy the job and they'll get to see on the Uri like it's healthy and all. And this was the service that I just deployed right now. So hello SPC and this is already live and there is an end point that is automatically generated where you can go and play around with the thing. I think it'd be great. Yeah, I feel like it'd be great for you to chat with kind of like the person who leads our main engineering and infrastructure development and basically like our lead ML ups DevOps person. Do you mind if I set something up to follow up with them about this time? Yeah, that will be great. That will be great man. I think we wayve something we have. Something. On Tuesday already set. We did have something on Tuesday but I received a mail from Freder that she just canceled it given it was set for today but I can just request her to maybe reset that meeting. Do you remember what time it was? It was ten ups. What do you think Brian? Like something like this and obviously the platform is in a certain way, but it can evolve depending on the requirements and needs. So do you think there could be a way for a need case or a use case for you.com to try this out. That will be great if you love to hear that. Yeah. At least initially here. It's definitely the kind of things that we've used and it makes sense to me. I think that big. The initial questions that come to my mind that I want to see from my team is office how big of a pain point is this solving right now? How much effort and or time will it take for them to switch to it if they think it is a good thing worth trying out? And maybe three is like is this the right thing to is this the right time to try something out and experiment with something new versus using what we have right now? We try to focus more on down our product market fit or like our revenue models and stuff like that. So I think it's a matter of kind of understanding the resource, the resourcing, time commitment included pin resourcing and kind of the timing on that. Like if I was just wearing my CTO hat that's probably the next question I would be thinking about. I'll ask them do they think this would make their life better and if so, where do they think it fits in the queue on making our product better versus making ups more productive? Yeah, that's how I kind of evaluate it while looking at it. Makes sense. I think we'd also love to learn a little bit more a few more questions to understand if there is some more top of the mind pain points that might be there for the ML team so that will help us even present this to them in a better way. That would be great. I'm going to add you to the meeting now. Let me grab the other one. Do you want all three? Yeah, I will just send out the emails. Cool. Well, this would be great for me, the rest of the team and you can see if this is a good fit for you right now. Like it awesome. Thanks a lot. Van helpful and thanks for being so helpful as well. Sense you next week. Bye. Bye. Let's see prompt and you sense the difference in experience that you have internally versus this. Actually. I know when they use the product maybe they will ask the moment he takes the Credential. They have very high bar of reliability and everything. Any good person will understand really really fast. The team goes some point but at some point they will reject because of stability and too early and all those reasons. But his company is not the right customer. Bye.