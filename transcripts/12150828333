Should I get back so early? How come? I'll be taking a trip in March. Not now. Okay. You started dustin very differently. So I start officially full time for December 7. Okay, got it. We are trying to build something like a YouTube for comics. The idea is to make comics creations using generative AI. Eventually we get into Ocean Graphics Comics and US company. You already have a little bit of context on 200. Sriragh is one of the ML engineers with us. So would love to know in terms of the setup of the pipeline and. Then according to it I think subset which is I was looking at 200 also is on the pipeline of the pura process. Right. So what we often have to do is take a modern could saw parameter, saw different data parameter for example. I can give you an example into ease one is to one comparison and this is probably true for any generative startup in the image space, not just us. Because everyone is running a lot of experiments because no one really knows what is working and what is not working easily. Because it's a subjective thing, right? Is a very subjective thing. Like if I'm looking for let's say I cannot be as good as I would say s***** as lens because for a production quality content that people will consume I need to create production quality images. For that I have to be very diligent on the quality. Which is why my pipeline is far fairly detailed for me to even take a call model actually very sorry, checkboxes aligned. I'm trying to build that piece by piece to reduce the effort to manually parameter parameter and that's literally what I'm doing. That's all in the next six months. Is literally a Python script that you run that access. Before hundreds start running and the more painful effort is painful production push is fairly easy except for the versioning part. But it is an easy problem to solve for me. Basically. At Kono like this models who are the models? Is it mostly like individual users? Yes, the goal is key and who basically so you can write a prompt and you can create panels of a comic and all of that and then do minor editing on them and all of that. So you need that pretty much as a core part of your tool. Even for 20 users you feel the testing is important. Even if it reaches 50, that will be important. Yes. Also because there is at least five people in house for me who are doing this right. In house hired mean literally because we have built something that figures out character consistency set of universal characters here for Dash two and you as a comic creator can use any one of them and the model will make sure when you say let's say Samirji as a character iterating over it in any way. I don't know. But I'm trying to think about what you said is better, but that is harder to implement because again, the way these models are designed as a time load to inference on a GPU, single data, single instance GPUs inference. So I have to figure this out. Ideally at some point we have a universe of models of around at least 40 to 50 models that together will serve the universe of comic creation. This is where we want to reach. Multiple. For example, I'm using one model for one use case. I am not able to do two models, one model for two use cases. Do you guys plan to allow the users to fine tune their own model? Yes. That is also something we want to give. By still figuring out the commercials. You. Feel it will be fairly consistent for the next few months. Just what is the guess? What is your best guess? What I think is coffee iteration. Legal use cases. Want to be on. I was also leading infrastructure in Odan and I have seen what tied up to one cloud does. So I am very damaged all my infrastructure overnight. Okay, cloud kubernetes. It's just that purely and completely scalable and cost effective. Multicloud Kubernetes. But that is something you have built there and it is stupid to not do that because we will be very cloud heavy knowing the GPUs. That is why I have multiple clouds and. We are putting multiple nodes from other clouds. Into one Kubernetes cluster. There is a data center, there is AWS, there is nodes together within a single Kubernetes cluster. And the power that it gave us, unbelievable. Because you can choose key by concept based on the pricing, then it's let me get. You can give an overview of whatever possible. I think you'll understand it really fast, the problems that might make it easier. So multi cloud birthday. We don't do multiple nodes from different clouds in one Kubernetes clusters. We do Kubernetes clusters across clouds in a single control plane kind of platform layer over Kubernetes similar. This is where you can basically attach Kubernetes clusters from different clouds. Control plane types. You can think of it like if you control plane or different different clusters can be different clouds can be on premise or anything. And these are like different clusters that we have added. Internal platform, we use our platform. You can add this cluster. Up to you. So these are all the components that you have installed on the cluster. Those things will come over time. Access control. I don't think it will be useful. Now this is a layer. This is the control plane layer. Over EKS. Not over EKS. It's a general control plane layer which applies to any Google displace control plane. And this talks to basically multiple Kubernetes clusters or Kubernetes clusters. Oh, okay, got it. So basically the control plan is like a co operator. You can call it like that. Agent basically helps you talk to the control pin and you connect with us the moment you install our agent in any kubernetes cluster cluster then you can also do things like cluster instance. Types so you can decide automatically get the instance type of filter namespace and all every single namespace. This might not. Be that useful right now. This is. Where you deploy workflow what you can do is basically to regular training job. And then you can trigger it with this. 100 set of arguments. It will spin up hundred jobs. It's fully auto scaling where the data is there Eurabia. One should have it. The first one. You'Ll get the data of all this metrics and all. Okay, module type metrics. If you know some measure of performance, you can definitely log it. Bakki what kind of auto logging metrics? Like for example I want to do this I have a script that has my 100 prompts I just want them running that's it. Then it's fine like you're just running that as a command in a job right? And then your output I want to pipe it to let's say my label studio or something else is this possible? It is possible, but without the exact command details. Not able to tell you how exactly, but it is possible. You can also do labelstudio.com. So in this case, what you can benefit a little bit from here is obviously going to click on December, something like that. In the Results tab there is images also. You can also log images so you can quickly preview the output of your. Images plots. We can walk over it just for my understanding also and then maybe other hand we can actually ship it here and you can see the data and everything cool. Actually technically there is someone meeting who will be running this mostly not me. So I will ask them to try things up and then. Pricing is less of you are also an early startup. At the end of the day by. The way, clarify. Now you can use. Our hosted control plane bother approaches. That. Also you can do directly from here. Scale available. Metrics would be like CPU. Have a model history where you can push the models oh nice. Just go to models and go to the source in deployment just go to models and just go right now. Just. Go back go to the source of. Metadata files. It will take you back to the actual source. Haven't built anything yet of this that's also an advantage. You can put them on. Feedback so I don't think through foundry can help me with rate or are you trying to. We can do that. What would be valuable is if truefoundry can allow me to bring up two services like a canary, right? Basically you will query one service behind the hood would randomly pass. Right? Yes. You can go and attach a candidate basically you can change any of the parameters change any of the parameters that you want Monday model FQ and you have to change. We launch the second service and you can decide the traffic percentage between the two that will come. Make sense. But it will be helpful. Sure. Also, will this need to be dedicated? One more question. Today you guys use GPU for insurance. So how big is each model? Does it take up the whole GPU at once or is it. The way I'm doing right now is there are three models and all three models are loaded in memory. So that takes up all the memory because I don't want to load them at one time. Right? So that is how I'm doing it right now. So that takes up all the memory. And all of those three models that are shifted onto the GPU at once or are they shifted into CPU, Ram. And GPU alternatively, so they are in GPU at once. Okay. So basically for their use case, the 16 GPU memory or 15 is being shared by three models. So technically they can do away with 00:33 for each model. Technically. We are also thinking about how to ship fractional GPU support. Because there are. Yeah, so there are two how many GPU you want? If you go to 800, then you get eight partitions of a single GPU and that access is how much memory you want. Just thinking about it. How is the data science use case? Is there a lot of data science use case? I used to be. Previously, a lot of the data science team is affected. Coffee. Four years. That would be really nice. But that is for a mid size company. Don't worry about it. Don't worry about anything. Like even for another startup. You don't. Have to worry about it. We can figure out how to use it and see if it's actually helpful. And I will try to set things up myself and then we will set up a call. But the data plan remains there. He's asking that set up. Data plan. We're.