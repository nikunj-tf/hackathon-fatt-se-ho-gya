Hello. Hello. Hey, Richard. How are you? Yeah, doing well. How are you? Sorry, I'm getting the pronunciation wrong. Anurag. Anurag is great. Yeah. You can also call me ANU, by the way. That's that's fine. All right. Nice to meet you, Anna. Yeah. Where are you based, Richard? Based in London. Okay, great. That's great to know. It should be pretty early or time around 10:00 or so. Is it 11:00 a.m now. I'm in the workday. Okay, awesome. Cool. So, Richard, first of all, thanks a lot for taking time to set up this call. A few things would love to kind of get through in this call. Karen, the following one would love to understand as to how you got to learn about Pro Foundry and what was the motivation behind that. We'd love to kind of do an intro as to what part of the product you are taking care of at Wayve. And then we can do a brief intro from our side like she has joined his co founder and the CTO and we love to dive a little bit into some of the use cases at Wayve in terms of the model development, the model improved and see as to how it aligned with what we are building and if it kind of suits well. Or we can go into a place where we can also showcase you a demo of what we are building and take it forward from there. Does that sound good? Okay, perfect. Do you want to start? I would love to hear a little bit. I know you've been working on the ML side of things for four years at Wayve. Would love to hear the journey and what you're looking forward to, what you're looking at in Wayve currently. Yeah, for sure. So maybe I'll start with, I guess, like Nikhil Truefoundry and my current responsibilities. So, yeah, kind in in in in D and Truefoundry through just launching investment news. So I think congrats on the latest founding funding. But yeah, just kind of heard you all through email newsletter alert. And I am generally responsible at Wayve for all our tooling around, I guess, all of ML ops. So my responsibility in products is just trying to figure out how we develop better workflows for our model developers so that they can consistently ship quality models with certain performance guarantees to improve with the actual driving quality on the vehicles. So that means that I kind of run the full gambit of tooling that we use away, both externally bought and internally developed for how we do anything pin the model lifecycle so responsible for our integration, like weights and biases and tooling for robot visualization, for debugging models, some tooling we have internally for ball testing and artifacts versioning and deployments and so forth. So anything that runs the full gambit them all. Lifecycle was interesting to your foundry because it looks like they'll have some interesting designs to how you do a lot of these components of the workflow, but I don't have enough clarity on exactly the implementation that you all done. So that's why I don't have exact requirements just yet. Okay, no, I totally understand, and thanks a lot. A few things like Richard would love to hear and know. If I understand correctly, you all built AI for the autonomous vehicle industry, right? And what sort of things do you help them kind of work with? Is it like a full suit that plugs into an autonomous vehicle and then kind of does everything around it? Or is it more like something on top of whatever the autonomous vehicles are doing and then some analytics on top of that and predictions on top of that? So we're building the full AB stack for autonomous vehicles. So our approach is focused on more entering machine learning for this problem area and developing off of internal fleets where we basically bought vehicles from various suppliers. So like Jaguar Ipace, for example, and then retrofitting it with our technology. Okay, understood. How many people are there in terms of the ML engineering team? A lot of work I'm guessing would be then very much AI first focused, like what percentage of the employee base or currently how many people are there who are like data scientists, ML engineers and so on? ML engineers. So we have a couple of different roles around ML developments. ML engineers, ML scientists, data scientists have a bunch of different definitions. The number of roles is probably about, let's say around a third of the company. So maybe around 60, 70 people. For a company of about 200. What is the workflow? Richard and you mentioned you are setting up this entire stack for ML. Are you doing the entire work of the setting of the platform in house? Are you using take a note of open resource or how are you thinking of this stack overall? Sorry, when you say stack, what part? So maybe starting from experimentation to development to deployment, to monitoring, how does that kind of damage look like for wayve? Yeah. So for wayve, most of that is internally developed because for the autonomous driving industry, there's not really many off the shelf products that work. For components of this workflow, the major ones that we would use externally would be probably waiting biases for experiment management and tracking experiments through training, but nothing really beyond that step in the life cycle. And then also Azure and Azure ML for training and almost just for training. So anything that's kind of the standard parts of experimentation workflows. In particular, we have external tooling to provide it. But then I think beyond that to actually bridge machine learning into more of the production setting and to deal with more mohan one engine workflow, then that's the scientist workflow that's mostly internally developed. So I think around deployments, around monitoring, around testing, even, that's also internal solutions. Okay, got it. For your deployment, do you end up using Kubernetes at all, or do you directly use as your full deployment? We have custom systems because we deployed to the vehicles, so it's not really kind of the null deployment setting. Right. Experience. There is nothing like you don't have, like, end points hosted or anything like that for these models. Entrypoints for running entrance to me? Yes. No, because it's directly on the vehicle itself. So we don't really want to expose entrypoints in any form beyond anything that would be talking with the model within the vehicle. Otherwise, then, yeah, it just comes to safety risk. Yeah. And do you export the data out, like, these vehicles, when they export, do you get the data back or do you get the data back from these vehicles for further retraining or analysis? Offline ingest, so we don't cut over the air data ingest. I see. Okay. And Azure you mostly use for training, so the most of the work is actually on the training side because deployment, like, you pretty much just package it with the software that goes on the car, on the vehicles, right? Yeah, pretty much. There's definitely some things we want to improve on the deployment and monitoring sites. So ultimately, yes, there comes what we need to do over the deployments and be able to handle that. But it's probably not like the similar problems and most employment problems you'll have. Pin traditional ML settings. Monitoring is still something that we're figuring out, to be honest. So, like, what that looks like in the setting. Real time monitoring is probably not necessarily something that we're interested in at this stage, but at least be able to have some better concepts of how you do performance monitoring and triaging from there. It's something that we're actively interested. Okay. And these models, could you give a sense of some of the models? And do you also care about the data drifts, etc. For pin these models? Richard. So data drift in the model is something that we do care about, but I think it's not necessarily in the setting of we need to actively monitor for tests. Okay. All the Mergens of the values in real time at this stage, we still have safe drivers monitoring the vehicles themselves. Like, that degree of detention of distributional drift is not really relevant to us. It's more so, like, major distribution shifts on anything that tells us that we're out of the operating domain that we're looking at. So it's something that we're actually looking to develop internally or buy office job solutions, if possible, to sense how we can best accelerate our development process when we can identify the major shifts in the data that we're seeing on Road and see how we can actually develop towards this new deployment domain. But, yeah, it's not like really real time trip monitoring that is probably we're. Working with images, right? Like images or video data? Both, yeah. I mean, not much text data or structured data. You don't have many use cases of that, right? No, not really. Yeah, cool. And what is the biggest, Richard, like time sync or something like that, that you're planning to anyways, that is one of the biggest things that you really want to improve in your pipeline today. So that we're planning to build or just in general planning to build. It generally either plan to build or buy or the major time sink. What is the thing that is missing pin the pipeline today that would drastically improve your speed? Basically, yeah. So I think anything around really tight pin the feedback loop for us of what exactly are the bugs we're experiencing at deployments and how do we actually identify what those bugs are and develop solutions to improve our learning performance on top of them. So anything around. I guess. Identifying domain ships at the moment to be able to identify what's maybe some data we need to collect further to be able to target the situations that we're seeing and so forth. Or being able to have more robust testing of our models to identify benchmark performance that you want to make sure we don't regress on or be able to better debug into the model and understand the bugs are occurring when we actually see performance issues where we can monitor on the road today. So, yeah, I guess I think that really makes model development much more of an engineering practice and a scientific one. So actually, as long as we do deployments onto the vehicle itself, what are the steps you need to do action and the visibility that you need as a mobile developer to actually consistently fix the issues that we see pin on road deployment right now? Because a lot of this tooling doesn't exist and we're trying to reactively Karen, trying to figure out what that might look like. The feedback loop from on road performance is mostly just based off of scientific intuition. I see it's a slightly different use case. In traditional systems, you can just upload the logs, like when it happens or the bugs or if it crashes happen and all you can upload it. But in this case, I guess the data is stored on the device and then once in a while you upload all the datta to the servers right. For this analysis and figuring it out. You don't get live updates from the model? Not yet, but I don't think that's necessarily the biggest issue for Ups. So the question of when we get the logs back isn't really the bottleneck for us. It's more so, like, what do you do with the logs? So we can have a lot of data labeled by safety drivers, like what were the bugs that we saw or like the performance you saw on road. But that doesn't necessarily tell me how do I fix that intervention or disengagement. So just because I know that the model maybe potentially didn't stop for this red traffic light and this weather setting in Jury London winter. That doesn't necessarily tell me, okay, how do I actually fix that? That's what we need to figure out how to do. Okay. Any other questions? So we should go on this. I was trying to think, Richard, what would be an ideal solution to the problem that you have? I mean, like, let's say all the logs come and then we have a system that will visualize all the logs for you, will show you the distribution of things. But since this images, data, what would you want to see in that case? I guess that's the question that we Karen, trying to see. We don't solve the problem exactly in the form that you have, but I feel like it can be addressed to some extent. So that's why I was trying to understand, like, is it fine if we show you where the model error and the images and the state of the world at that point of time? Will that help? Or do you want an overall distribution of these errors, Karen, happening during this time, like some sort of clustering or something like that? When you say, like, where the model error, what do you mean by that? So, for example, as I mentioned, right, let's say the vehicle is running, right? And the model is also running doing its predictions. But you said you kind of know when the driver said that the model made an error. Like, you get that input right there, right? The model did the opposite of that. So that means the model made an error or something, and you'll probably have this data saved on the device. And then once in a while you get all these logs back on the server and then you'd want to see all the cases, the state of the world when the model made an error. Right, and then do retraining on that. Yeah, something along those lines. Yeah. I mean, we have a way where you can give us a bunch of logs, will show the visualization, will show the cases where it is, but it won't work in the current system that we have. It will need to be modified a little bit according to your use cases, which we can do. But I just wanted to get an idea about that part. We actually deal with mostly deal with real time in France, to be honest, it does help in real time. We can show you the demo of the platform, like what we do, and in scheduling training jobs and making it very easy for data scientists to run jobs and services. So that's where we are main value proposition. And we also help in monitoring of the models. But currently the monitoring is mostly done for structured data. But we do have a system for visualizing a bunch of images, like where it headed out and things like that. It's still in the work but two. As of now, the mode of engagement resource that we have is we work with a few companies closely. We don't work with a lot of customers because we are very early. So we work with like five to six companies closely and have them get to a state where we are solving a few of the problems and along the way we kind of build our product also. So that's the kind of mode that we operate in right now. So since your use cases, like we can show you the platform, but it sounds like something that we can also help you build since you'll be building it internally anyways, like we can work with you to build it out. We'd love to understand the use case a bit more and things like that. If that sounds interesting to you, like Richard, we wayve can collaborate along those lines. Yeah, it's potentially interesting. I think for us, the thing is there's always a complexity around for our specific use case. We've already thought pretty deeply about what exactly this might look like and if not actually implementing the projects themselves and designing the potential systems around it, and particularly think about how they integrate with all of our more fairly complex internal ecosystem of systems and tooling that Wayve already experienced, it's pretty hard to integrate with a lot of off the shelf systems. So, Canada, I think we've done similar projects in the past of code development. A lot of these tooling with other suppliers for some of the tooling we make sure we didn't want to have to build ourselves. But if it's a situation where this is not clear what additional clarity on what the product might look like is provided by the other team, then there's not really a compelling reason for us to invest in these code development projects. In many cases we find that that's more of a distraction and actually helpful. Yeah, of course. I think Richard, if it is interesting from both perspective, we can definitely try to provide the understanding of what could the product look like based on your use case. And then if it feels aligned, then we can then take it forward. But before that, I think it might be useful for you to see as to what we are currently building. And then we might have a few questions around your use case for this. And based on that, I think we can then think about the codevelopment project. Does that make sense? Okay, yeah, that seems pretty fair. So just I think I'll give a two minutes overview and then Abhishek can directly instead of going through sites. I think you are well aware of this space, so there's no point trying to dive into that. I'll just give an overview. The goal really Richard, for a set proof of this, to make it very easy for data scientists and ML developers in general to be able to test out and deploy their models into production. So right now whenever people are building models, ultimately they have to depend on ML engineers or DevOps team to be able to take their models into production and even hosting like testing entrypoints or testing use cases which they can expose for their members pin the team. So the reason for this is that data scientists and ML engineers like ML developers, a lot of times they do not have that core engineering skill set and therefore working with infra becomes challenging for them. So what we try to do is we abstract away the enthire infrastructure for them and expose the ways to kind of do and run this using simple lines of code, boilerplate codes which they can easily understand. And we do this in both Python way, YAML way as well as through the UI. And we enable you to then take your models to Arnaud. And these models could be of different types, it could be a real time model or it could be a batch inferencing model because it's batch you can also do a training job run into it or you can do like a production run into it. And once we deploy, our goals is to kind of be able to kind of provide you with insights that allow you to track as to how the deployment is going, which is basically we set up the monitoring for you out of the box. The monitoring includes system level monitoring as well as includes ML level monitoring. Things like what you mentioned, like the performance criteria, the benchmarks with respect to different types of department and so on. So that's what pretty much we are building. Initially when we started we were thinking of building the whole platform, but we realized that this part of the platform is where a lot of challenges are pin and therefore we decided to focus on this cost. Does that give you an overview of at a high level what we are trying to do as a product? Yeah, I think at a high label for sure. I think there are a number of similar products in the market. So I'm always just curious to get into the actual nitty gritty of the ten code details. Yeah, a few things before we go into that demo we have natively chosen Kubernetes as our place where we deploy and because we wayve chosen Kubernetes by designers cloud native. So you can deploy on AWS or GCP or any other cloud for that matter. And we try to make the system in a way that the entire access, control and authentication system works properly across the use cases and then it integrates with your workflows. Like for example if you are doing a CI CD, it will integrate with the CICD. If you are doing things like traffic shipping, there is a functionality to do traffic shipping or if you want to do, there is a functionality traffic shipping as well. And similarly other use cases, for example, use of GPUs, use of model servers versus normal models, et cetera. So that's at a very high level. Happy to answer questions. Otherwise, will you kind of go a walkthrough of the platform under the other things you want to do? Yeah, I can do that. Any questions you have, Richard, before? I not one. Okay, I'll just share my screen. So. Basically I'll show you first the way to deploy model. Like either you can deploy training job or you can deploy a model as a service. And there are multiple ways to do it. You can do it by Python code, you can do it by ML, and you can do it by UI, UI. It's as simple as if you want to deploy something, you can just choose where you want to deploy. And then it's as simple as you just give us your GitHub link wherever your service is hosted, you give us the branch which you want to deploy. And if you haven't written a docker file, then you can give us a docker file. If you don't have a docker file, we can automatically generate one docker file for you and you just give us the version that you want to deploy. How many replicas supports and the environment variables pin the CPM memory usage and the moment you click submit. Basically this will create a service which is shown here. Like for example, it will generate an end pin for you. So the service you can open and you can test it out. So for example, the service has one inference entrypointtype, video batch inference endpoint. This is regarding a service. You can also deploy a training job if you want to deploy a job. Same process goes here. Again, you choose your GitHub repository and deploy. And for the job, you can either have a job that you can trigger manually using the UI or Python code, or you can make it run on a schedule. For example, you can choose the schedule so it runs every 30 minutes or something like that. And a job is pretty much just a Python script. You can give us any Python script, any piece of Python code and just put it here and the job will continue to run. So for example, some of these jobs, like teachers job, that is training xg two models and it's running every 12 hours. So we kind of make it very easy for data scientists. I can show you the Python code also that runs this thing, which is our this is basically the Python code I'll show you. So this is our Train PY, the standard train training script that you have. And to deploy this thing using Python, what I showed you was the UI way. And if you want to deploy it using Python, you pretty much just need to write this much code to make the deployment. And you call your job to deploy and your training job will start running on the cloud. And at this point, any questions? Richard, is it clear this part? So for the actual image building and interacting with Truefoundry, all I need to do is specify a command to run and then some just requirements for dependencies. Yes, exactly. We'll automatically build the docker image and we'll automatically deploy it for you. Got it. And you can come and integrate your own docker register. It's not like our docker register. So you can integrate your own docker. It can be clarification question. So that means that theoretically, AI transcription can be any arbitrary logic inside it. It's just that needs to be like no yes or no. Yes, it can be arbitrary. This is where you can integrate your GitHub bit bucket or get lab repositories. If you have your own Kubernetes clusters, you can also integrate your own clusters here. So basically, it will kind of work with everything in your cloud, and you can just use it that way. Sorry, you go back to the last page, this one. So I get like, why they should be integrated in some ways. I'm just trying to understand from this view, what is that design for, how to like, how does it interact with this integration? It's just like a record of what integrations are available. Like, if I'm going to the git page, what else does it do there? Yeah, so once you integrate your GitHub repositories, that's when you can whatever is integrated here is what options you will get to choose when you deploy. So when I made the deployment and the list of repositories was coming, this is coming from the list of integrations, the repositories that have already been integrated. Okay, got it. And regarding clusters, it's more like in a cluster, what we do is we kind of try to give every data scientist or team a safe space to play around with, where they can do their own deployment, the rest of the system. So I can basically what I can do is I can create a workspace. I can give it out to a team, like, let's say hello, world space. And I can add, like, editor, viewer, admin, like different team members. And I can limit the size of this workspace. Like, I give out, let's say, four CPU and a GB to certain teams, and they can pair up with this. They can do all their prototyping development, even production rate stuff in this, but I know that they can never screw up the rest of the system or exceed beyond that. So the cost is you kind of curtain the cost, and you give autonomy to all data scientists and engineers to do to carry out their own things. And this is where they can do all the deployments. Once you run a job and all, we also have a component where just very similar to I think you're already using weights and biases. So very similar to, like, you can track all the runs, the metrics and everything and the system metrics and the artifacts that you have logged. So this probably already have weights and biases for it. And then if you want to do data monitoring, like so let's say you deployed your model and is making inferences. So this is where you can see the model, how many predictions it made, what was the actual value, the loss that has been logged. You can also see the data distribution and these things work well. As of now they work for structured data. We karen trying to extend support for nonstructured like images and text through some proxy metrics that you have for each image. So this is where you can just see the distribution. You can compare it like let's say model version three is running this month and I want to compare it with the same time last month. I want to compare this day to so this is where you can see the model, the different distribution of different features that happened between yesterday and day before yesterday, and the actors and the predictions. And you can also browse through the raw datta of the model, all the features that the model was getting and what was the actual value that got on the predicted value. Good. So sorry guys, I got to actually run to my next meeting. So this is always interesting. It might be worth continuing the conversation later. It looks to me like two founders were a nice way of integrating a lot different tooling and components for both the ML scientists and on your workflows and nice abstraction layer on top of the deployment problems you would have with Kubernetes deployment. That sounds pretty interesting, but to be honest, I'm not quite sure what will fit into our systems right now. But maybe it's just a conversation because I really have to run. Yeah, sure. We can continue the conversation over email, which thank you so much for putting out the time to speak. Cool. Alright, thanks. Bye.