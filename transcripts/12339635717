Hello. Hi, Nikkum. Hi Chandram. How are you? I'm good. Did I pronounce your name right? Yes. Very nice to meet you. Chandram. You're based in Hyderabad? Yeah. Okay, nice. Whereabouts in Hyderabad? My hometown is in West Bengalur. I am working here since six years now. So I'm in Pachibali. Okay. Pachiboli is a very nice area in I don't live in Hathrap, but I visit the city often. Very nice. And where? About in Bangalore? Sorry? In West Bengal is your hometown? You have heard about hookly. Hookly? Yeah. So by the way, I'm born and brought up in Calcutta, so I know West Bengal area recently. State of Westport recently. Okay, awesome. So thank you so much Chandim, again for joining for the call. I really appreciate you taking this time. As I mentioned to you over my LinkedIn message that I am right now working on a thesis and I'm trying to validate the thesis by talking to some practitioners. And that's why I reached out to you. Now let me start by introducing myself and then I would love to learn a little bit about you as well. So Chandima, I come from a machine learning background. I used to work at Facebook where I was building a lot of conversational AI models for them so working on their virtual assistant product called Portal. And prior to Facebook, I was at a startup called Reflection where I built out a lot of recommended systems and personalization algorithms for the company. We were serving the ecommerce industry. And there besides building out these models for the first couple of years, I got a chance to work and build out a horizontal platform, horizontal machine learning platform for the company. And my educational background, I went for my masters at UC Berkeley and my bachelor's at IIT Crackpur. So that's a little bit about my background. Before starting True Foundry, I did another startup in the talent space and that got acquired by Info H, which is the Pen and company of Nokle.com and now Nikunj Truefoundry.com, the one of the co founders and the CEO. So that's a little bit about myself. I would love to learn about you first. Yeah, sure. So I did my CD in Computer science and Engineering from Kiait University. And then I had found Iidius as a software engineer. And there I have founded the data science team. And prior to that I had internship experience from IIT product on machine learning and data sciences. So in I did yes, I was working on problems related to accounts receivables, problems related to accounts receivables on collections and deductions. I'll talk about Hidden, it's fintech company on the B to B space and it deals with the finances related to receivables and payables. So I had worked on machine learning modeling. So it was around six years ago. And then we were not having any platform as like machine learning operations platform. So that time we were using a closed source library known as Shape Mml. It was predictive modeling Markup Language framework, which was used to create XML files from this psychic learn models. And then we using that framework back then. And then we had to move our portbased into Python since Majorly High Reduce was a Java based company at that time. Since then I've been working on machine learning platform engineering, creating pipelines, and on AWS cloud environments. And apart from that, I have done certifications from IIAC with Angler on computational data sciences and also did a program on Masters program that sort of told. Me if I understand correctly, Chandam, you are a data scientist at High Radius, but you also work on the ML Ops platform side of things. No, I used to be a data scientist, and then I am completely into software engineering related to machine learning operations. I see, okay. And initially, when the company was completely Java based and you were using some of the machine learning, you basically had to convert the SQL model to PML format that you could use in Java. But later you got the machine learning portion of the company starting to use Python, and then you could skip this entire business of PML. Is that right? Right, understood. Okay. And you mentioned that you're on a multi cloud. Can you tell me which all cloud providers you end up using? Yeah, so like all the major cloud environments like AWS, Google Cloud platform, and also on premises as well. Okay, understood. And what is the main reason that you all are across so many clouds? Across all the clouds? Yeah. So the major reason being since we are in the fintech space, so certain clients would not like to host their solutions in certain environments, in cloud environments, say Walmart, who is a competitor of Amazon who will not trust AWS, so they would prefer it on Azure environment. And also we also have banking clients. So these banking clients would not prefer any cloud environment, so they would prefer to be hosted in an on premise. Okay, so if that is the case, then what are the companies that might have an issue, let's say with Gcp or even Azure? I understand AWS because of ecommerce companies have something against AWS, but I'm wondering, like, for example, why is, let's say, Gcp or Azure not a good option? Majorly. We are in AWS. The primary option would be AWS, and recently we have acquired Google as a client of IRAs, so they would prefer their solutions to be built on Google cloud platform. Recently also we added Google Cloud platform in our list of cloud environments. Oh, I see. Okay. Less percentage of clients are in other cloud web environments other than AWS measured clients at AWS. Understood. Okay, and with this, does it also mean that your model training and model inference are also across all the three clouds or your at least the machine learning portion is centralized? What do you mean when you say that machine learning portion is. I mean that do you actually train your models across all the three clouds also? Okay, you do. So majorly the issues with the data basically. Right. So if you have to train your models, you need to have access to the data sources in the respective cloud environments since the databases are enterprise grade and they are hosted in the respective cloud environments. I see. Yes. Okay. So one other question. Can you also model inference also running on all the three clouds? Yeah, understood. Okay. Do you end up using. Any of. The ML op solutions of the cloud providers like Sage Maker, Robotics, AI, Azure, ML, et cetera? Yeah, some of our data scientists use them for modeling which all frameworks you. End up using, which are platforms you. End up using frameworks have been major. Leaving Vertex type case. Right. So mostly it's Sales Maker and the libraries, like all standard machine learning libraries we use like Hyplearn and Pi dots and all those things. I see. But. No one is using Vertex, AI or Azure. ML. No. I see. So people who are building models on Vertex, AI, on Gcp or Azure, how do they end up building? Like where do they train and train their models? So mostly this modeling and Ed activity are done locally. So if they would move to cloud and gardens, if they have some requirements of some computational requirements oh, I see. Okay, got it. So when you say it's done locally, you mean like on Jupiter notebooks or something? Right. I see. And do you end up hosting your jupyter notebooks somewhere or is it actually like your local laptop? No. So notebooks are locally used and these notebook files are being converted into the scripts which are hosted in our repository that run our machine learning services. Oh, I see. Okay. And then these notebook scripts are run over on top of a VM or something. Like we said, this notebook scripts are run locally, so they are then converted to files like Pi files or scripts. And they are being used like we have frameworks, like the Flask or Training. We have frameworks, we have these containerized environments, it can be local as well. So they would run their flow to see that if the flow is fine, the flow is fine, they would move that script in the repository in the Sprint, like we have Sprints and releases, they would move these files into the release process. I see. Okay. So basically you start with jupyter notebook, then you convert that to a Python file, and then that Python file is checked into your GitHub repository, which is then run on, let's say, a virtual machine. And then this virtual machine itself can be running, can basically be on any of the cloud providers like AWS, Gcp or Azure. And is there like a CI CD process that is followed or it is do you all end up using like a Jenkins or Argu CD or something? Yeah, we use Jenkins and Ansible scripts as well. For these pipelines, we use Ancient scripts. And do you use GitHub actions? No. Okay, understood. One other question that so this is about machine learning, right? Like the other parts of your application deployment, do you all end up using Kubernetes at all? Initially we had plans, but we are fine with like orchestrations, like Swarm and ECS. So initially we use that, but in the long run, we plan to move to Cuban. It is. When do you plan to move to Kubernetes? In the longer run, like the or two, maybe. Okay, got it. Now I have another question. So you mentioned about developing models on all the Three Clouds, right. Then you mentioned that some people who use AWS also end up using Sage Maker, but most of the others end up using a Jupyter notebook. Right. Now, this Jupiter notebook is getting deployed on all the Three Clouds, correct? Yeah. You convert it to Python on all the Three Clouds. Right. Now, if the script is deployed on all the Three Clouds, then you also need to ensure that the dependencies of the script, like Python packages or whatever, like some other environment packages, et cetera, are also consistent across the Three Clouds. Right. So how do you manage that today? Yeah, mono repository, basically. Monolitho. We also have a problem now that we are seeing is that the scale of we have more than 100 scientists who work on our platform, and it's being a mono repository, so we have a single build, like, we have a single pocket file that has the 2% of all these libraries. And the size of the image is also quite huge. Wow. Like am I imaging it's multiple gigabytes? Yeah, it's around tenGB. Of course, because if you have 100 data scientists, then you have TensorFlow Pytorch. Do you also end up using GPU? No, we don't, but we think we have to. I see. I can imagine this getting out of hand very soon. No, I think it's manageable. It's not a problem. And like with mostly the images are cached like this. Libraries that we install, we rarely update libraries. Like, if we do, we have to run regulation, basically. That makes sense. How many models do you have in production? More than 500 models. Do you end up using a model registry? No, not really. We have a solution in build which acts as a model registry, like in house solution. I see. What is the inhouse solution? Basically, we version this model. Understood, then. Interesting. Okay. Very nice. What's your docker registry, by the way? In onpremises it is an excess, and if it is sorry. If it is AWS, we use ECR. So basically every model that you have, do you have separate micro services for each model or no, no. So micro services we are creating for the sake of reliability like the problems we face with ML inferences is reliable software. That is to say, like in terms of performance, we have problems. And the problem that we see is like if it is on a monolithic architecture, then if it has cross cutting impact, if a use case or model solution has a problem which leaks memory or something like that, then it impacts the entire architecture or the software. So we are segregating routes. URL routes. So that the perspective. Those solutions which are a bit problematic, we try to have a certain microservice out there. Understood. Okay, I see. I'm wondering why have you all not considered building out, let's say, a unified platform across the three clouds which basically just optimizing this entire flow so far using a unified platform? Yes, the platform, the code base is all same, only it's the same code that runs across all the cloud environments such that they are configurable across cloud. Got it. Okay. How big is your machine learning platform team. Of 15 or something? 15 people in the MLS platform team? Yes, sir. And number of data scientists? More than 100. I see. Okay, understood. What is the biggest challenge that you are facing in orchestrating these cross clouds models, architecture environments? Actually, I'm also curious to understand how do you allocate resources as well to the developers for running, let's say, larger models and stuff like that, access to different databases and all? Because actually that's the other problem that would happen because when you have the same code base actually, I don't understand how is this happening nicely that you have the same code base and this code base is reading data from, let's say, AWS S Three and Google Cloud Storage and Azure Storage as well. Right. How is the developer writing the code such that these things are abstracted away? Right, so basically we have a cloud native utilities here written, which are which like we follow certain conventions on file paths. Say, if it is Three file path or Azure file path or Google Cloud file path, we would have a naming convention basically. And from that the code is written in a way that all the cloud environments are abstracted. Say I have a function which says fetch file and then you send the file path to it. And then this file path, it starts with S Three. So then that function knows that it is trying to fetch a file from S Three. So we have created all the S Three buckets in that same naming convention so that we know that it is from S Three, or if it is starting from Azure container, it starts with AZ and so on. I see. How many of these models are real time versus how many of them are batch models? What do you mean by batch models. Where you offline store the inference in some database and then service from there. Right? Yeah. So certain models are real time, which requires high availability. Like, one model is there which would be inferred every two minutes, and it has to be available real time. So the count of such models is not that clear. I have to see those statistics. I see. Understood. Yeah. So I was asking you that what are the challenges that you are currently facing, the major challenges that you feel like you're facing as a platform team that you all are trying to resolve right now? What are you building right now? What's the plan for the next six months? Yeah, like you were saying, we are creating my growth challenges we are seeing is mostly performance related. So like, memory leakages in the endpoints, like the inference endpoints. And then there are certain use cases which uses huge model files, like around 600 MB size models. So these models has to be loaded and then has to return the predictions. So we are trying to segregate those separate services. These are the same challenges and organization wise, also we have challenges related to resources like Attrition and all these things that they have. One thing, we have a single like I said, we are using a mono repo. We started with 3.6. Now there are machine learning models which needs to be on 3.9. So the same code base is having both 3.6 and 3.9. And we have two separate buffer files. And we actually plan to migrate all 3.60 to 3.9. So that's a huge stress in terms of the amount of regression and the amount of trainings. And those retrained models has to be moved to the 3.9. So that's a huge activity, I think that we will have to do. I see. Okay, understood. By the way, is this machine learning platform, like the machine learning 100 people? Do they all belong to the same business unit or are there like different business units under which the team is structured? Yeah, so basically, High Radius has various products. So these products are under various units. And we have a central data science team. We have various managers of various products who would manage the team of data scientists. I see. Okay, got it. Okay, so different products have different data science teams and the ML platform team is central. Yes. Okay, understood. And the other question is how is the team structured geographically? Like, do you have most people in the data science team in India? In the US? Yeah. So major development activities. We are in India only. So we have to like three office. We have four offices in Adrabad angler. I see. And even the ML platform team is mostly in India. Yeah, it's in adrabad itself. Who leads the ML platform team? For example, who is your manager and who's your manager's manager? They are also in Hadrabado. Understood. I see. Okay, understood. So both your manager and manager's manager, both are in Hyderabad. Yeah. Got it. Okay, understood. I think this is very, very helpful. Thank you. So much for taking the time to help me understand these challenges. I think some of the challenges it would be I know that we are already over time, so that's why I don't want to drag the call too long. But what I would love to do with you is set up a follow up call with my co founder Abhishek, who's also the CTO in the company and he also was at Facebook where he led some of the distributed platform teams within Facebook, et cetera. So it would be nice for him to also understand some further. How are you orchestrating the infrastructure and everything details around that. So if you are available, maybe you can set up a call 30 minutes between the three of us. Yeah. So what would be the agenda or the end objective that we are yeah. Right now the goal for me is primarily to learn on what are the I'll tell you basically we are building a thesis that companies that are cross cloud might have more challenges in building and orchestrating their machine learning models than companies that are on one cloud. Okay, that's our thesis. We are trying to validate that by talking to people who are on cross cloud. Now it turns out that there aren't many companies that are crosscloud. So that's why every time we find someone like you who is actually building and managing a platform across cloud, it's like a rare resource that we have found that we want to learn the maximum things from. So I come from a machine learning background and I already got some machine learning related perspective from you. Abhishek comes from an infrastructure background and he would love to understand the infrastructure perspective that what are the opportunities and challenges in that area. Now from our perspective, once we have gathered some of this information from different people building cross cloud and different solutions they are adopting, we will build out a more core thesis on what problems we can solve as a startup. So to be honest, at this point, you are just helping us in being able to build a thesis. That's what you are doing. And I would really appreciate if you can spare 30 more minutes of your time with me. Sure. Amazing. What I will do is I will reach out to you separately. Can I also get your WhatsApp number or something so that we can coordinate this call quickly? On WhatsApp? Yeah, I think I have already sent my number to LinkedIn. Okay, sorry, I might have missed. Okay, all right, I'll set up something with you together with Abhishek. Yeah. So from your present study that you are doing, what are the challenges you see from your perspective? Some of the challenges that we are seeing here is how do you basically get the visibility of all these models that are running across clouds? Because with one cloud you have one dashboard where you can see stuff with multiple clouds. You have everything going to disreferent places and you don't have any visibility. If something is breaking, then you have to build out the entire instrumentation pipeline. So those are the challenges that we have heard from some people, which seems like a major challenge to me. It is true. It is true. And also the developers from a developer standpoint, also, you have to be very careful. Basically, the speed of execution also takes a hit because you have to be careful about a lot of things that otherwise are typically abstracted away, basically. So those are some challenges that we hear from people. But your challenges could be unique. Every company's challenges are unique. So that's why it's just best for us to learn from everyone. Sure. Hello? Yes. Okay. Thank you so much for taking the time. We'll reach out to you and set up another call. Okay, sure. Thank you. All right, take care. Bye.