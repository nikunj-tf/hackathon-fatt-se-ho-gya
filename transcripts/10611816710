Abu. How are you? I'm doing great. How are you? Doing well. IBM doing well. Where are you based now? So I'm from Hydrawat. I'm staying in Hydrawad. Okay. What about you guys? Where are you joining from? I am in San Francisco. Cool. Right? And IBM reserving crot. Cool. There's some note taker. Yes. Recently we got introduced to this fireplace note taker that I really like because often I'm giving demos on a screen share and I can't take notes. And this really helps. It has some AI driven notes that I don't think is very helpful, but at least the transcripts are very helpful. Awesome. Thanks a lot for taking time to speak with me. I was actually quite excited about this call because given all these different places that you have worked at and how I rarely get to speak with people who have worked on such relevant stuff, basically. So I was like, I was quite excited. CTO Keep, I learned from you your experience at Lyft, Google and then Fiddler, and now basically driving ML of itself. Right. So all these were very interesting. So I was looking forward CTO this call. Primarily, I'll just give you a little bit of a background about myself and set some context here. So I come from a machine learning background myself, have been working in Recommended Systems Conversational AI at Reflection and Facebook. And right now, the company Truefoundry All Hands, we are building. The inspiration comes from a couple of different experiences in life. Like at Reflection, I had built out a horizontal AI platform for the company. And then at Facebook, I realized that how much more could we have built that we did not end up building a reflection. So I think those two things really helped in shaping what we're building now. And as a team, like, my co founders, Sync, Bishop and Rag are also from It, correct. So we are batchmates from there, and we built out one other company before starting Truefoundry All Hands got acquired by Info H. And in this particular call, my primary goal is just to learn that, like, what's your point of view on the ML of Space? What are some of the interesting problems that you're trying to solve at Fiddler? What are you trying to solve here at Juo? Is that how you pronounce it? Sumit. Yeah, junior is right. Okay, so what are you trying to solve at Sumit? And that's pretty much it. Just literally just learning throughout. And by the way, Anohau is a member of recently joined maybe three months, and he's a member of Founder's Office Parth. Truefoundry. Awesome. Yeah. Thanks for the introduction. Like, you're right. So I've spent some time at Google. My background is more in product infra and tech infra, like, building back end systems which can really scaler or reliable and performant. So I wouldn't like to think of myself as being an expert in ML Op space. In fact, when I got out of Google, I was like, kind of overwhelmed with all that's out there in the open source world, right? Like the public cloud. So the thing about Google is, like, they have their own internal stack on which folks work. So you are pretty much live in that bubble, right? So when I came out, it felt like there was a lot to learn. And I think last few years I've mostly been consulting with these different organizations, primarily focused on setting up their data platforms and machine learning platforms and helping them get started, right? So that's where I think I'm really passionate about understanding why they want to build this platform and then just sort of advising them and helping them build it up and yeah, there were like, different things to learn and all these different companies that you mentioned. Right. So I think Fiddler is very relevant to what you're building at True Foundry, right? So they started with model explainability. Right? And I think they were doing pretty well, but I think they pivoted to doing model monitoring because I think that's what most of our customers were asking. So for me, I think the biggest learning there was working with the founder, like Krishna Kade. He's also a Facebook guy, by the way. He worked on the Feed team. He was the manager on the newsfeed team, and he's very charismatic. Right. And I've been on sales call with him and he just has this deepak understanding of customers and their problems and he can understand the stuff really well and sell his products really well. So I think that's what got me hooked onto Fiddler, right. And yes, there was kind of a steep learning curve. I think Model Explainability is real cool, right? Like, what the stuff what they were doing with shapely values. Right? So shapely values, if you know much about them, it's like computationally very expensive. Right? So there was a lot of work going on how to make Shapy values, like, more performance, right? So Fiddler actually did some research and something called Fiddler Shop, which was like much more performance. They even have papers out there if you're interested in reading about this stuff. Then they also did a lot of work on integrated gradients. Right? So I think for all the deep learning models, right, those integrated gradients is what gives you sort of a better signal of the model state. And like so they use those to sort of explain deep learning models. And they did a lot of work in that area, right? Especially some of the models are very challenging, especially models, which sort of takes when it's like normal tabular data, it's all good, right. You have standard tools to sort of deal with stuff, but when you have more complex models, like text models or image models, things start to get more challenging. It's more art than science. It's like how you really model. So there's no clear ways on how to sort of explain those models or how to even monitor those models, right? And a lot of models are getting that way now because very hardly do you see customers which have simple sort of tabular inputs going into models, right? Most customers will have some sort of complex inputs, right? Like trying to condense the inputs into some dense vectors. And how do you make sense of those inputs and explain those in terms of the original human readable sort of inputs, right? So I think there's some interesting sort of challenges in that area, which is what Frederick did really well, right. I think where they sort of like pivoted, right, at some point was they thought that this was not really selling for them. There was not enough market for Explainability, right. Because most customers who are buying the products were not like those tech savvy companies, which you might think of, right? These were like big banks and insurance companies which were buying the products, right? And for them, things like monitoring was like paramount, right? This is what they wanted, right? They wanted monitoring. They wanted ability to detect bias and fairness and models. So that's what they wanted. Right. So fiddler really pivoted from explainability. Now I think they are the leader in model monitoring space. I think they kind of nailed it. Like most of the deals that happened after Series A, right, were like, mostly in model monitoring space. And I think one thing which really helped them was I feel that they built some really nice sort of intuitive UIs around the product, right? UI, which, like, you know, which, like, you know, a regular sort of, like, person could like, just use, right. It wasn't like complex and like, you know, you didn't have to do a lot of digging into. It was very intuitive and very nicely done. And I felt like it was responsive and I don't know. Have you seen Friddlers Demo? I have, yes. I closely follow them. So I think that really worked for them. And I was involved with that part of the product, right. I helped them sort of build their sort of monitoring tools and added support for that and the product. And I worked with a lot of different customers trying to understand their monitoring needs. And really so I think one of the key things which customers were interested in knowing, of course, whether their models were drifting, right? Like how their models were performing in production. Right. And some of the big challenges there were, like, unlike test data, which is like neatly labeled, right? And you can clearly tell the performance, right? Like the model is breaking or it's drifting. It's hard to tell for models, like, in production because labels are not available right, at production time, right. The labels are only known much later when you get humans in the chain. So I think being able to detect drift, right, in real time without knowing the labels was a very big challenge we were trying to solve. Right, and this would really help customers if there was some way to sort of figure that out, right, so they don't want to know like three months down the line how their model was doing. Yeah, exactly, that's not useful. Just want to know now, Kiara. And then the second thing they want to know out of this whole thing is like when it's time to retrain. Right? So training models is expensive, right, and it costs expensive GPUs and there are customers who have hundreds of models, right. Imagine training hundreds of models on a daily basis just eats up into your budget, right? So if you had like a neat monitoring solution which would somehow help you decide when it's time to sort of retrain and not only decide, it could tell you subsequent of the data which are underrepresented in training and help you build sort of new training sets on which you could train the models. That is like just the icing on the cake, right? Absolutely. I have a couple of follow up questions on that. This particular topic that you brought up is actually very interesting to me also. So I guess here data drift, right? So data drift itself can be computed based on some simple metrics like, okay, divergence, divergence, etcetera. Right. By the way, is that what people mostly end up using today? Like which is at the feature level KL divergence and all? Or are there other techniques that people are starting to come up with? Number one, the second question is that is data drift truly computed in real time when the query comes or the common pattern is that you would actually compute the drift at certain cadence, let's say once every hour, once every day or whatever, and store the number with respect to certain reference sets. How does this thing work realistically? Right, so first of all, you're right, like you can compute like data drift like on the features, right, feature drift. Because like the features are known. Right. But that really doesn't give you a true sort of indication of your model performance, right, for finding what you want to find is drift in the model output. Right. That is why I was saying you need to know the predictions and the labels, like the correct labels for those predictions right. In order to compute the model drifts correctly. Right. So you are right, there is a feature drift but there is also a drift how accurate your models are. Right. So I think that is one of the signals which is more important to know how well your model is performing data drift. It's a good sort of proxy, right? Like if your data is drifting, it's likely that your model might be like the performance might be going down. Right. But I think one of the key challenges there is to understand is the model output drifting or not, right? So say you did some tests, right? And you have some performance numbers. Like this is how well your model performed, because in those tests, the sort of labels are already known, right? And you can calculate the model performance based on those model output and this is the label. So you can compute the outputs, but in production time the labels are not known, right? So you have to figure out ways to compute the performance without the labels being known. So I think that's the challenge I was talking about. How do you go about doing that? And thing which you mentioned about the cadence, you are right. Most customers actually don't look at real time signal, right? In fact, they are mostly good with daily or hourly sort of numbers. But there exists like models in which people want more real time. So, for example, there are like in the publisher world, right? So what happens is in publisher world, you have these trends which go up and down very fast. So we need to be able to detect those trends. So they have like very sort of detective things that are going up or down in a much more real time fashion. So they even look at signals at smaller buckets, like 1 minute, five minutes, how they want to be able to, like I think you were talking about recommendations, right? So I think a lot of publishers use recommendations and they would want to put their highest bids on articles which are most popular, right? Right. So I think that's where these sort of more models which can compute sort of ROI in real time help. So there are customers, even ecommerce customers, right, because they also have all these trends and all. Sure there were customers who asked for this, but I think not all of them have trendy data, right? So most of the banks and insurance companies are completely fine with one day off. So I guess one question that I have in this context is from a product perspective, right now, you can compute data drift. For computing data drift, you need basically one reference data set, one current data set, right? Now, if your reference data set itself is a training data set and then your current data set is your inference data set in certain time windows, basically, I can imagine that you compute the drift with respect to your training data set for all these different time windows that okay. Reference training data set compared to day one of prediction, training data set compared to day two of prediction, and so on so forth. I can imagine that setting up a process for that. Okay? But also sometimes people are interested in seeing the drift for your itself compared to three weeks ago to now. Right? How is my system doing from three weeks ago versus now? Right? And now, Chris, time frame itself is usually configurable in products that you can select that reference data set is June 1 to June 7 and then my current data set is June 20. CTO June 27. Right, but you can tweak that. Okay. My reference is June 2 to June 8 and my current datta set remains. So basically the number of possible combinations of this like two data sets is actually infinite. You can have however many and that kind of a setting. You can't actually pre compute the drift and store it, right? Correct. So do people put like product constraints that users can't do stuff like this? Or do people actually compute this in real time once they know exactly what are the two data sets? How do people solve this product related, right? So at the time, Fiddler did not have this and there were customers asking for this, right. They wanted to be able to compare production data sets against each other. Right, exactly. Like how you're saying how is this week's data comparing to last week? Right. So you are right. It's very hard to sort of precompute distribution numbers and like, you know, run them against each other. And this would require it's computationally very expensive, right. So I think that's where you need CTO invest in tools which could sort of help you sort of overcome this. Some of the sort of analysis which we did, we saw that instead of really using the whole data set right, and like computing distribution numbers on the entire data set, we could actually take a sample of it and still have pretty accurate numbers come up, right. So I think sampling was something which worked really well for us throughout the product. So there was like I remember there was these algorithms for computing feature importance, right? Like random feature ablation, which is also something which is very computationally expensive. Right. So we found that we could tweak the sample size and have significant less sort of CPU going at it and roughly have the same quality of numbers. So I think you need to be able to run experiments to figure out how much data you need to use and what's the kind of quality of answers you can produce with that sample. So if you have something so what happens is if your data is pretty much stable, right? So you can actually sample very aggressively, but if it is like it's not very stable and it's very spiky going up and down kind of time series, I think that makes sampling very hard. Right, and you won't get good quality results also. So I think it depends on the nature of the data or the time series for which you are trying to compute these numbers. But I think sampling is something which really worked for us. And by the way, this is something which customers asked for, right. So I think disabling this in the product is not a good idea, right? I mean, maybe making it like having customers so that they don't consume all of your CPU resources, right? Maybe giving them some sort of credits or something, like you have these many like five free credits or something. To do this kind of thing and to do more deep analysis, you have to take the enterprise version or something. So I think that works. I think that technique worked for us in Google Analytics too, right? Right. So about four years ago in Google Analytics, we pivoted to google analytics has been a free product since forever, but I think four years ago we decided to monetize it and how we did that was it for free customers. The queries which they make are on the sample data sep, right? And the quality really suffers, right? So if you're a big customer with a billion rows, right, running analysis on 10 million doesn't help. So to run analysis on 1 billion, you need to pay, right? Sales sense. So you always have to monetize on your sort of compute, right? They come up CTO service for free. Makes sense. And then it fits into that completely makes sense. You pretty much don't have any other way at this point. If you expose that function to the end customer, basically arbitrary, basically in that. You can actually have limitations. You don't make it like they can select per minute buckets, right? You make it so that it makes more sense. Currently a computer, you can cash it, right? Then you can serve it from the cache. Actually, I think I have a question. Yahap is not necessarily about per minute or per hour, right? I think it's about the range. Even if it's like per, whatever, like per hour, the combinations are many, right? Even per hour. Actually, if you think about June 1, June 8 compared to June 20 june June 27, right? Now you can restart and end dates and kind of get almost many permutations, right? June 2, June 9 comparison like that in that data changes, right? And you can't potentially pre compute it. Maybe you can even cache it if the user wants to use the same thing. But you can't potentially precompute it at that point, right? Correct. Like some sense. I think it makes sense. As you see, we used to compute only at hourly buckets, always gong to be from twelve to one, right? Never from time series. So you can cash it, right? So you only need to compute it for one CTO two and then I further aggregate it with twelve to one, right? But I get other distributions computer like what you're saying would make sense, captain twelve to one, the distributions computer but other drift computer actually, let me show this to you, maybe this might become clear. What is it that I'm referring to? If I selected Is and if I go here, data distribution ticket. So we allow something like for example, if I go and select this time frame, let's say, and then I say add comparison, right? So imagine current data timeframe. The issue is the user can technically select otherwise sell the product, expose the user can technically select whatever time ranges that they prefer. Right. Prediction distribution in this time frame, when I'm comparing against this time frame, then there will be a drift number that I can compute, right? Key distribution by the sales divergence is zero nine or whatever, right. Does that make sense? Yes. And the same thing for your actual same thing for all your features and all like the map feature, distribution of balance the carnot, for example. Right. If I want to introduce like a drift column here, let's say, which will just take you like this feature in between these two ranges is so much drifted that itself I cannot possibly pre compute that. Okay, like I'm missing something here. Correct? Yeah, I think drift you're right. I think you're right to skill it, you need the entire sort of data set to compute. Right. So you can't write. I think at the time we didn't run into this problem in Fiddler because we did not expose like lot of free form sort of selection updates. But I think this is something which people were asking for. Right, okay. So how expensive do you think it is? Right? It really depends on your data set, right? That's what's important, right? Correct. Yeah. If you sample and try to do it in real time, I can imagine a solution working in real time. And real time does not necessarily in this case have to be like 2030 milliseconds. It can go to a few seconds right. At the end of the day, it's an analytics tool, basically. Correct. Like some of the customers which you've shown it like they are okay, giving you their data running on your cloud. We get both basically. Like some customers are like we will use the sales version where we just get an API key and the thing runs on our cloud and we will do this thing. Some people are like, our data can't leave our cloud. So we actually have a deployable version of the entire product that runs on their cloud. No data ever comes out of their cloud, basically. So we do both actually. Right, so if it is on their cloud, then it's because you would just need the compute power, right? Because then they will be paying for those resources. Right. So you could actually use some sort of like fancy sort of distribution and compute it really fast if needed. So you are trying to optimize for. The CPU cost, the CPU cost here and also just the infrared itself at some point if the product we give some flexibility and that reduces the experience, makes experience of the user bad. Like it becomes really, really slow. If you're trying to compute it in real time, then that's not a good thing. Right. Like from a product perspective. So I'm trying to understand that where to draw the boundary of giving flexibility to the user and ensuring good performance, I guess. Got it. At least two feature may drift. Those could be computed one at a time, right? Like that. You don't have to compute all in one shot. Right. I think features have to be expanded. How does that work? When you make a request. The way we are doing this is we will actually have these parallel computations that we spin up a pool, a gig feature. We are able to compute the distributions of each feature and store that and then a gig feature, we just send out computation in parallel and aggregate the data and send it on the front end, basically. But we haven't tested with really, really large data sets right now. So just sampling by the concept that we have not built in yet and we know that it's going to be a problem. So it's like an understanding problem hoga. But I guess we're trying to prepare. Once we start doing this with large customers, how do we solve for this problem? We're trying to figure out that's what I was right. Sampling was something which we did. We actually use sampling all across the product. Not just like for monitoring, but even for like explainability maybe like I was telling you about future importance and all. I think sampling was key to. I think also it boils down to how you sample, right? You just like do random sampling. What happens is if there is something known about the data, right? So, for example, a lot of customers will have user data, right? So instead of doing some purely sort of random sampling, you try to include few users journey all the way, right. So that you can have more sort of richer insights on the user journeys clearly captured in your sales. It depends on the kind of insights you're trying to show. Right. Makes sense. Yeah. Understood. Yeah. I think that overall suggestion that you have I think is very helpful. I think I'll try that out, actually, to see how nicely we can do this in real time with a proper sampling technique. So let's let me actually try that out. This is very, very helpful, actually. I had a few more things that I wanted to discuss with you on this topic itself. But before we dive deeper into that, two questions. Number one, that we are kind of on time, like, not least the blocked time that we had, is it okay if you go 1015 minutes over? Yeah, sure. So I was done for the day, so let's do that. Nice. The second thing is, I'm also curious to understand what's the charter for Junior? What are the type of problems that you're trying to solve at Jumu as well? Right? So Junior is actually a much bigger company, right? I think they are like 200 million arr and they are like 300 engineers worldwide. So it's a much bigger company. So they are into ID verification and identity verification, right. So US banks and all these people who want to sort of do KYC on their users, right? I know that it's a real user. They use junior services, so their users will upload like the documents, right? This is my passport, or this is my selfie. I am the person in this. And then Sumit will say yes. So traditionally in Kakia PUDA back office, Mi Otata, they had a back office in Jaipur, right. So all these requests would go to Jaipur and some human person would say yes, no, or Sich Amar. But now the company has grown, right. They get more than like a million sort of verifications per day, right? And it's not possible to scale these sort of back office. So they are investing in AI ML solutions to automate these pipelines, right? Whether it's usable, right? Mostly image based models is matching together. So all this can be automated, right? And they have like 100 different models for doing all of this, right? Many. Everybody simplify, but believe it or not, there are like 100 different models doing 100 different checks. They have like, I think, around 40 data scientists building these models for them. So I think there's a couple of challenges at Sumit, right? So all across the model lifecycle, right? So one of the key challenges hard for me to tell you all the problems, but my charter is EndToEnd infrastructure support, ML AI team support, end to end. So I'll be the tech lead building that support. So I actually have more than 20 engineers on my team based out of Bangalore and also based out of Vienna. So half the engineers are in Vienna. So the idea is there are different phases, right? So, for example, the developer experience is one big thing, right? So what happens is they are very uptight about privacy. Developers should not be able to see production data, right? Logunki photo logan phone number. So they want to expose smaller subsets of data to these data scientists and make it easy for them to run experiments on the bigger data set without really exposing that data to them. Right. So, for example, Python code TensorFlow model and then the same thing, you can run your evaluation cycle on the large data set, right? Right. One of the challenges is how to build a developer experience key from whatever notebook stuff they are writing. And so it can be very easily be translated into sage maker pipelines, right? And Arun on larger data sets. And we can tell Karee this was the evaluation results and developers can iterate faster. So this is one set of challenges, right. How do we go from a developer environment into a production environment and be able to iterate fast throughout because they have different access levels, right. Like I was saying, developer environment will have some test data, right, which might be anonymized or which might be a. Smaller subject and the production will be the entire thing. So we want to enable this CTO, develop hydrate quickly. So that is one of the problems we are trying to solve, how to improve their experience. So that's a broken experience, right? We want like that's minute may, a past minute May joking experiment. So that is one of the big problems we are solving. The second problem we want to solve is again, it's related to monitoring. It's very hard to predict when is the time. So first of all, it's all image models, right? So there is no off the shelf monitoring which works for them. It's very hard to say how is your image drifting? Fine. Metadata you can tell, but image itself cameras become like sharper or cameras become different. Very hard to detect. There is some research efforts along these parts so I'm not involved with the research, but IBM kind of leading the effort to automate this work. How can we incorporate such signals and automate our sort of retraining pipelines and help customers? How can this select training sets more effectively? So I think automating that like post deploy monitoring and selecting training sets is another sort of set of problems which we'll solve but we are not solving. Now what happens is like Junior May, it's a very old company so there are like six different teams like data scientists and all of them have their own sort of orgs almost right processes or they've all built their own feature stores, they've all built their own sort of training sort of pipeline. So I think the biggest challenge would be to get everyone on the same sort of platform, right? To convince them you can add value. I wouldn't call it a technical challenge, but I think it's more of organizational challenge. So I think that might be one of the first challenges I might have to solve how to convince people that will platformize all of this for you. Because what happens is when people get used to some way of doing it, right? So there are some people who are using Airflow to build these pipelines or there are some people who are using Sage makers now say if you come up with the platform which has some way of doing it, they'll find it extremely hard platform, I think even when you go and sell your sort of tools, right, the same resistance, right? So to go in into an established company logo, infrastructure built Kiawai palace, it's always gong to be hard, right? It's much easier trying to onboard like new sort of teams who are looking for infrastructure. But if they already have infrared you need to show how well your infra is compared to theirs, right? Yeah, I think those are at least some of the challenges. I think again, one of the other big challenges is the price, right? So they have like more than 100 models in production, right? And it's pretty expensive to sort of run them on GPUs, right? So how to sort of like optimize the models themselves, how to compile them to like better formats, right? So that they could run on more optimized hardware. I think there were people who are trying to run these models on Arm. Chris, could you speak armchair how to do that? Edge Devices up mobile Page there is a lot of there's one team which is completely focused on those optimizations. How to optimize your cost for running these models and then I think General Jovi monitoring related challenges have model analytics in one place, right? These are all systems are all separate, right? Your training systems are running in some other place or your sort of configuration model configuration is kept in a different place. Your monitoring data is being logged somewhere else. So how can you like build a model store, right? Or a model analytics store where you can get everything in one place and really derive those model lifecycle insights, model pay evaluation results. There is version pay is version pay. How is it comparing to that model? Like v one, v two. How to exactly get those insights, right? So that is one very important thing which data scientists struggle with all the time and there's no clear like ML flow. I think that would be one of the challenges. How to collect all these data points in one place and CTO scaler. I think it will be hard to convince people. I think as sort of engineers we'll have to go and instrument their pipelines and collect the required data and put it in whatever model store we settle for. So I think this is like a rough set of problems which I see already. We are starting with the developer experience. I think we'll move to production and optimizing introductions the cost for serving the models. One other question here is Apur. You mentioned about Sage Maker Pipelines, right. So is the company generally married to Sage Maker already? Are they using sage maker heavily. Yes, big time. So not Sage Maker pipelines in a big way, but Sage Maker operators. So what happens is you have all these operators like for stage Maker, like training operators, processing operators. So right now we have tools in which data scientists can directly notebook invoke this training operator, right? And run this job. So we use Sales Maker in a big way. But all of this is like spread all over. There is no pipeline stuff really in place infrastructure because you never want to do like data prep sep of model claims, evaluate. Most of these things are done one after the other, right? You always do some amount of data preparation or building your feature stores. Then you'll do some model training, then you most probably run like some evaluation after that and then once everything is there, you publish your models, CTO, some repository, right? So generally these steps always exist like one after the other. So we want to make sure we have this notion of a managed pipeline wherein they are forced to sort of go through with the entire pipeline. Right. And sure, if they want only one step, if they only want to do one thing, they can do that. But we are trying to make it more end to end complete. It's like spread all over the place. But Sage Maker is used very heavily across the company. It's like awa shop all the way. They use a lot of sage. Maker Understood. We Chad also mentioned about cost being like a big factor. And from what I understand by talking to other companies, like Sage Maker roughly charges like a 40% premium on the computer resources on an average, basically. So to run your ML pipelines and on. So that discussion is not like that cost itself is not too much important like the markup that they have or are there discussions around optimizing the cost? We are sage maker as well. Right. So I haven't had the opportunity to spend time it's not as if your compute resources are like it's only like when your prediction request comes, it's more like those lambda functions or something where you only pay for. So my understanding is that's how it works, right. And I think it's fair for them to charge a premium because what happens is say if you were to build this infrastructure yourself, you might have a sort of python wrapper running all the time. Introductions. I think they have some optimization baked into the. Got it. Okay. Understood. I think inference is one part, even training is the other part. So I think net of the way some companies think about this is like let's say I'm spending a million dollars per year on my ML infra cost, which is serving on Sage Maker. If I'm not on Sage Maker, that ML infra cost can be brought down to from a million dollars to let's say 700K. That's pretty much what the equation that they boil down to. And a lot of the inference, the overall Emmanuent cost not only come phone conference but actually also comes from training because training happens over multiple long hours. And if you end up paying that markup on the cloud cost, that becomes more expensive because that's where I think there is less value add from this entire serverless architecture kind of thing. And like the cost continues to scale. I think that's my understanding so far. Right. So, again, I haven't looked deepak into this, but I think the strategy which we will use is to reduce our sort of training cycles rather than do training every day. Right. Data set. We'll try to figure out like is it really required? Right. Once a weekly train. So I think we'll first try to optimize the training cadence, right. Or maybe find out more intelligent phase of figuring out when it's time to train and optimize there. And then maybe go with by the way, they've already built quite a lot of infrastructure. For example, they have something called they have built their own docker images in which it has all the required sort of job packages, use all the dependencies baked into it. And all that is required is deploying that image on some hardware and triggering the training. Right. So it's not really tied to a Sage Maker training. Sort of stephen we could do it. I think very sort of Sage Maker helps is like I think it has some amount of sort of better integration with the whole pipeline stuff and it makes the whole process more observable. Right. All the logs will be better integrated. Cloud watch like iOS. So it's all neatly observable. Right? So I think they kind of like that, having all their infra pieces in one place and just one thing to monitor. So I think that's one of the key things that they'll never move away from Sage Maker because all their pieces are running in that same infrastructure. Got it. So much very helpful to know. This entire conversation is just so helpful. I really, really appreciate you taking this time. I do want to do like a deeper dive. Like two things. There is there are things that we could not cover in our call today. Number one, some further discussions on the monitoring discussion that we are having and just honestly, some technical discussions, getting your opinion, insights on how to build certain parts of it. And the second thing is also kind of getting a little bit on feedback on how we are building out the product as well. Like, I showed you a very small, very, very small part of our product today. But actually, how do we do deployments? How do we do, like, monitoring and stuff like that? How does the entire system come together? We'd love to get some advice from you, given that you have been building in the space. Can we stay connected? Maybe set up another follow up call, GTM discussion? Raj this yeah, so let's do this. Maybe we can reconnect in a week's time or something. Maybe on a weekend, if you don't mind. Surge week. So that might work well, weekdays make it to bed and all that stuff is there. But yeah, I'm open to doing it. If you do have some recorded video of like a product demo, right, maybe you could send it to me and in fact, I could even show it to folks in the company, right? Like if something like this could add value to Sumit itself and maybe do something for sumit. If you have some recorded video, please do share it. I'll watch it and maybe I can like so that we can save that time in the college. Makes sense. So we will certainly share some videos with you. I think in terms of actually getting feedback from junior folks, we might be a little too early. Honestly, based on what you're describing, I feel like we should spend some time trying to understand the system a little bit better. And only if you realize that there's this one area where we could truly add value. I think basically opening up the entire platform to the team, it's very hard for them to make a decision that, okay, could it be easy or not? Like through GTM discussion, Raj, we find out a nice niche problem. Maybe we can send out that more than the better of the video to the team as well. I'm very interested in kind of just learning and picking your brain and kind of helping us build out the platform a little bit better. But meanwhile, I'll definitely send out the videos, number one and number two, if you want to do it either next week sometime, do you want to do like kind of 22 October, which is a weekend? Would that work for you? Yes, that would work. I don't have any plans. Okay, nice. Same time? Same time. Yeah, 22nd October, same time. Sure. We will send out an invite actually right away. Right. So I think what would really help is if you could figure out monitoring for image models. Right, yeah. I think that is something which Junior might be very interested in. Maybe build some sort of evaluation tools for how to evaluate image test sets. I think there is already a few companies in the marketplace who are starting to sort of I think even my fiddler block Patraka, they are also starting to work with like such data sets. But yeah, I think it might be so cool. Right. Image make portion of image. This is the image which is housing the anomaly. There is some fraud in this part of the image which is housing the model. To fail those sort of things would be like really cutting edge in next gen IBM. Not sure what those next gen of monitoring would look like, but yeah, I think that I think is something which even though like, Junior looks like a company which is like a bit sort of heavy, like it's not a very sort of company which is like they probably want to build all these things themselves. Right. Because they have large teams like, you know, they have like 50% data science team and 30 sort of ML engineers. You can imagine it's not a small company, but I think still building image sort of tools won't be very easy. Right, yeah, exactly. I think it's a very hard and almost at this point, it's actually a research economy also. Like if you're able to make any progress there, I can imagine that can be very attractive to companies like Juumio or just so many companies working in the computer vision space generally. Yeah, makes sense. Awesome. This is incredibly helpful. Thank you. Really, really, thank you for taking this time and helping us out. Yeah, not at all, actually. It also gives me an opportunity to collect my thoughts and get them sorted out. So thanks. I think next time I would also love to do a deep dive with the kind of use cases you are sort of optimizing for. I think you went deep into monitoring. Right? I think I have a sense of what you're trying to achieve there. I can try to recollect my thoughts also on that monitoring exactly how we sort of optimized. Can we also stay connected on WhatsApp or something? Just so like scheduled in. Right. I can share my phone number, although I'm not very on WhatsApp? It's kind of a bit disruptive. CTO me IBM, like very old school. Okay. So I prefer collaborating over Gmail and documents. I know it's like logo go especially remote worldmates. Like how does it work? But I think you have to balance right. How real time you want the communication and what quality of communication you want. Right. So you can't get very high quality and very real time. Right? That is true. Yeah. Okay. I think either way is fine. Take care of will do. These two things like we'll send out the follow up, we'll send out the video and look forward to speaking with you. Take care. Thanks guys. Thanks. And above. Thank you. Bye.