Hi Nigen. Hi Priyank. Hi Shaans. Hi Nicole. How are you doing? Very good. How are you both good. Nice. Been such a long time triangle, I think in my life I have not moved a meeting or like a meeting has not been moved more number of times that like we have, I think. Past few months some kind of crazy means for us. And last month I was off on vacation to that kind of had a problem. And then at the same time there were a few members of the team that we also wanted to be a part of this meeting. So that's like that happened. I know. Apologies from my side. And at the same time I think. It'S not just you, I've also done it. I was not hiding anything literally, like. Mutually something that's happening. Yeah. So I think in July and August we were experimenting with first of all models and wanted to get some flavor of it and then have a call because I was following up with your releases as well to see we are progressing in a very fast mode now, which I can see totally. So are you currently in India or in NSF? I'm in NSF. Oh, nice. I just wanted to know. We have Sivank in BIANC, so Bianca is also a data center within our team and Sivank just recently joined us. They are like primarily working alongside with me on some data science models that we are building and will be building within the next few months. So I wanted them to also have some flavor of the call this was about. Right. So basically you're giving us a demo of what troopont can do for data centers and how we can use it. Basically the USPA is suggesting that what you wanted to discuss today was kind of like get a demo of proof on somewhere. Like, how can we help? Essentially yes. In short, yes. Otherwise, whatever you want to drive, I'm fine with both. If you want to do it like sometime next week or if that works for you, then that's also great. Or otherwise you want to have whatever you want to do. Sure, sounds good. I think maybe a good start would be like a brief introduction between Priyank Savank and me. You and I have already met, so maybe I can introductions briefly, introduce myself. Priyanka and Sivank. I'm one of the co founders here at Truefoundry. I personally come from a machine learning background. I used to be at Facebook where I led one of their conversational AI efforts. And before that I was at a startup called Reflection where I got a chance to build out a horizontal AI platform for the company, kind of scaling it to roughly 600 million users. So that was a remarkable experience. I feel like it's one of the best experiences of my life because I really got to work on both the engineering side of machine learning and the model building itself. We were building a lot of personalizations and recommendation systems basically for the ecommerce industry. So I learned a tremendous amount while working at Reflection. And prior to that I was doing my Masters came to UC Berkeley here, and before that I was in India doing my undergrad. And undergrad is where I met my now co founders and wagon, Abbychecks, actually. The three of us are Bachmates from It, corrector. So glad that we're kind of building out the company together with them, basically. And personally, I grew up in Calcutta. My family originated from Rajasthan, so we're it about myself. Okay, Shaun, go ahead. I'm working as data scientist at Apnaklub. Before joining at Apnaklub, I was in algorithmic trading. So I did not like that job. So I switched through data science and more machine learning space. I graduated in 2020 from Tal itself. Like, I'm a junior of science. You can see in life. Okay, this is a brief introvert. Nice. Awesome. Which algo trading company that you're working. With, I was in which one? Trexpond. I haven't heard of them. Trexpond? I think it's more like an India based algorithm ICS based itself. Okay. But it's 2019, so it started I. Think there is recently started coming to IIT many times. I didn't see any of those coming in. It was more or less biggermencies. Okay, I see. Nice. Welcome to the bright side of the corporate life. France, the startup life from the but yes, go ahead. Simon would love to know a little bit about you. Are we losing Sri Lank? Or is this my Internet? I think it says. Okay, well maybe Srivanka is coming back. So do you want to briefly introduce yourself? Yeah, sure. Hi. IBM currently working as a data engineer here. It's been more than one year, so I started here as a back end developer and now it's been three years here. Oh, nice. Awesome. Very good sumit, Shivan. Yeah, so sorry, my inderjeet connected. So I was saying, like, I recently joined a club, like two months ago previously. Before that I was working in a market research form. Currently Chris was focusing on the political domain. Like we used to study about the elections going on, predict their results and all of that. And I graduated in education and have a very brief interest in this particular field of machine learning. And IBM currently living here. A great experience in. Awesome. Sians market. Sri Lanka is very specific. I'm currently loving it here. So he's in present in that route. I hope it's not a fast night. Nice. Awesome. Thanks everyone for joining him for the call here. Do you all have any background? Already have had a discussion from Srians about Trufaunrian, Shivank, and I think it. Would be great if you can provide us a brief background to them as well because compared to me, I think you are like a better verse with Trufanti. Sure, I can certainly give a little bit of a background. But in today's call triangle team. What I would like to do is instead of directly giving a demo of the platform. It generally works best for us to understand what are the problems that you are trying to solve and figure out if this is even a good time for us to potentially like in a showcase what we are working on and how can like what are the areas that we are helping basically because if the problems itself are not relevant. The demo is generally never useful basically. So I think I would love to spend more time just understanding the problems itself and then after that we can actually set up a follow up call where we come with a little bit more personalized demo as well. If I notice that, okay, there are a couple of problems that you are trying to solve that true Poland has the capability to solve. I'll show you the demo of that part instead of giving you like a full dump of the entire platform basically. Yeah, I can start off. So there are few models that we are currently working on and we have already worked on few of them. So one of them is what we have worked on was on lead scoring. So where we predicted probabilities of person getting converted onto a platform. So if I want to talk about the engineering side of things so it's totally deplied on Google. So it has like Google BigQuery as the data warehouse, it has Google Airflow as the data engineering pipeline and the model is based on XC boost pipeline. What we do is we create probabilities, then we integrate probabilities to event based systems and we drive events onto web engage which is like our marketing automation tool. Using marketing automation tools we drive like the strategies, right? So either it could be a product nut, it could be a marketing nudge, it could be also we are calling, you know, like the calling sales things that we have or could be like the actual offline sales. Right? So there are like four types of end touch points. So that is the whole cycle of one of the models that is in production right now where we take the app data. Massage it with the other kinds of data that we have. Use GCP as a full running background and then provide results to an automation tool where they strategize on how to use the probabilities and figure out what is the best way forward to continue with the engagement with the person. So this is primarily on activation. Priyank and Sri Lank, they are currently working on retention scoring where we want to prioritize ordering or at the same time basically prioritize touch points, communication touch points. Right? So suppose if the priority of order of a person is very high then we basically bifurcate them into two parts. One is on the product side which is like match them on product again and the second is the offline and online. So the inversion of this probability, like a high probability would mean the conversion is high, the low probability would mean the conversion is low. The inversion of this will give us the churn side of things which is like we are solving for and there is a churn as well. The third thing which currently is not a data science model but it's a data engineering and analytics solution which we are currently using is on the which more in detail, which is on the supply chain optimization where we are trying to predict what to procure, when to procure and where to place. So we do the demand planning and then we do the demand distribution within our warehouses. So last time when we had a chat like we had only few warehouses, Nicoons. Now we have since 20 warehouses, two mother hubs, right? And that's why the network planning problem becomes a big issue for us where we want to forecast demand at the same time distribute demand in a way where the overall aging of the warehouse and utilization of the warehouse is very high. Fourth thing that I am currently working with Sudhir here is on the product recommendations where we are trying to build not implicit recommendation which is like the Sudbus algorithm figures out the features by themselves. However, what we are doing is trying to turn it into a regression problem and then solving it because that will help us solve for GMB, we want to optimize on GMB 1st. 2nd is we can play around with the optimization function. Third is we want to solve for cold start as well in the first version itself, so that the dynamic pages and the app is more dynamic, right rather than static. Fifth is where we want to have it more explainable and that's why we wanted to use some kind of parametric model. So right now it's in data engineering phase. However, what we have scoped for the model and the features is to be explainable so that the end users are comfortable, right? So for product we don't need to provide any kind of explanations, but like we want to drive the same model through our sales agents offline online and also through our communication platform which one of the team members who is on off right now is currently working on WhatsApp based smart communications which like last time I talked with you, we were like only in ideation phases. We have started implementing it right now. It's an analytical solution where we figure out okay, this is a product which a person would be interested in. Then we send out a WhatsApp message which is optimized to the time that he is most active. At the same time the session time of that person is highest. So we look for model R. We look for session time and then do an optimization on time itself to increase the Chris right that is all driven through right now it's on edit and the process automation workflow but it will be moved to Google Airflow system together with web engage so that the marketing can work together with product to apply throttling limits on WhatsApp second is we can do some kind of journeys and integrate the information into the journey and third is it gives us a playground of flexibility right? So that is on the communication side of things. Apart from that there are many internal areas and use cases that we are currently exploring which is on internal gamifications of system. So we have like the upnake club app which is the customer app then we have the delivery app where we have our own delivery person's delivery pendo products so there we will be solving for operations and optimizational operations it is like route planning and as well as together with CTO prediction. So if the CTO is high then those kind of models will be built into those operation system. The second app that we are building within the product is the SOS app which is turns feet on street app right? So these are like SOS where you know like in tier two and three cities the app usage is increasing but it's a gradual change. I see a technical shift in how the app usage would increase and we'll be driving it not us, startups like us right that's why we need a combination of product which is more suitable to these people at the same time a combination of people who go and do like a hands on onboarding of these guys. So these app will also be optimized for an So to just look like here suppose Chris user I have, what kind of products would you recommend? So we want to link our recommendations platform together with communications platform with the supply chain that we have all three of them together to reduce our inventory aging which is like the inventory turns would increase and the margin should increase. The second that we want to optimize for is like availability at the right time at the right space with a proper person who knows how to pitch, when to pitch and what to pitch. Right, so that is like a full scale like the model that you're building but talking about four motors that you're currently working on. One is supply chain demand distribution, third is product recommendations and fourth is retention. Probability scoring so supply chain demand distribution. Recommendation and lead scoring but in retention so probability of ordering I would say. Okay, I see, understood, sounds good. So this is very helpful backgrounds and basically all these four problems you are currently actively working on basically yes. Okay. One of the problem is already in production which is leavespurring the supply chain and demand distribution is also in production and currently working. Third is the retention probability scoring and fifth is recommendations which is currently in Datta engineering phases. The supply chain you're working on currently supply chain is also in production. You're saying version one, obviously we haven't solved the problem, but obviously there's like a version one and then there's like the version N which is quite far into the future. Makes sense. Yeah. Okay, understood. Awesome. And in terms of the team, is the four of you working on data science problems? Yeah, so there are two other people as well which are missing. So Rohan, there's another guy who is working on the communications platform where I talked about the optimization of time and the WhatsApp and the what to send and when to send. So he's currently on the Valley holidays. There's another person was Kayhan, she's a data engineer together with Sudhir. So Sudhir is taking care of our all order management system pipeline and new integrations with the warehouse management systems. Right. So we are moving towards the ERP to an ERP system which is called ERP Next, which will take care of all our centralized ordering in one place. So he's taking care of that together with helping other create data engineering pipelines for other models which I just talked about. Understood. Okay. Makes sense. The other question is that in terms of the engineering support, do you have like a separate team kind of who helps take the models to production or is it like just you all on your own trying to take it into pendo? Yes and no. In cases of all product driven changes we provide the data and then the back end engineers use that data to production. But just like lead scoring is something that we completed end to end, supply chain is something that we completed end to end. So it's like a hybrid. Depends upon where we need product help, where we don't need product help and where we can build our solutions ourselves and drive it forward. The idea since like it's a startup, right? So whoever has more bandwidth. Simple. Makes sense. Okay, that completely makes sense. Alright. And then in this entire pipeline are most of these models like bad scoring type models where your model makes predictions, you sales that save the results in a database and then use it somewhere? Yes, most of them will be like that for like next six months at least. Okay, so most models are bad scoring models and you are completely on GCP but you're not using Vertex AI. No, but we'll have to start using it for retention, probability scoring and sickness product recommendation. The data size is quite huge and we can't build it on our own systems. I see. Okay. So what's the point of using Vertex in that case? I think so we wanted to use Manage notebooks which have a good connectivity with Google warehouse. At the same time we can deploy it in a faster way compared to doing our system then creating a report deployed. Makes sense. Okay, understood. And then if you think about the entire workflow of machine learning and in this case others can also jump in like Priyank Simon Sumit. If you want to share from your experience, what does that workflow today look like basically? How frequently do we get the data, how frequently do we train the model, how frequently do you run the inference job? When do you update the model? All of that workflow basically or when you plan CTO update the model. If like this entire effort is fairly decent, maybe you have not updated the model already. But how do you plan to do that? So we would love to understand that. Yes. So I'll talk about lead scoring. I think Prion can talk about one of the models that is in production and I forgot to tell which is like an associated rule mining model where we do people who bought this also bought this section within our something of that sort. So I'll talk about lead scoring. So for lead scoring we have built the model, we have integrated with our communications pipeline and then there is a program manager which is using this probability to drive the communications forward. How do we track if the model is working fine is through amplitude. So what we do is like we send events against the probabilities that we have bipartited the users. Then we do a sectioning of that probability greater than zero nine and amplitude has this cool features which tells you what is the conversion this time compared to last month. If the conversions is around the same number or around one standard deviation, then less than the same number, we don't calibrate. We calibrate if the conversions have drastically been changed or our marketing methodology has been drastically changed. Right. So that is one. So for this model we calibrated last on September, like I think first week of September and still working fine and we'll probably calibrate it in first week of November again and then they do it on a monthly exercise. However, I think Priyank can tell more about the ARL model that he worked on. For the social rule mining we have taken historical transaction data set that we had for all our orders that would ben then it was like standard machine learning process, like splitting the data into training and validation using a priority algorithm to mine association rules from both the sets and validating it on the validation data set. If it seems that the support and the confidence of those rules are high then we are adding the predicted rules onto our data set and this repetting of the model happens weekly and on a daily level. We check all the predictions that were made whether the products are available in our inventory or not. Based on that, the people also bought section is filled on the app. I see. Okay. So if these models are kind of getting updated daily or weekly, priyanka, where is this job running like the model training job itself running. Is this on a collab notebook? Is this on a Jupyter instance? It's on instance and GCP. Okay, so you have both AWS and GCP. Yeah. Interesting. So right now your team is actually using both the clouds. We are by migrating from AWS to GCP. So one of the model data creation pipeline would be on AWS which is still running and we'll have to migrate. But the model which calibrates is done on Airflow every week. So you have an airflow job which is done every day which predicts and populates the data into the database. But then there's a weekly calibration exercise that we do on airflow. So there are like two jobs, weekly job and a daily job. Understood. Okay, understood. And in this part so you said that this runs on like an easy two machine or something. So is this EC two machine like always up and running? Yeah. I see. And this is used only for back pendo jobs? Yes. Got it. I see. Do you know if typically how much percentage of the time these jobs running today? What happens is we have like few jobs which are running on the same instance but at a different time intervals. So I would say we run around ten to twelve times in a day. But that jobs only run for like five to ten minutes. So I would say around 2 hours is the running time of instance over a period of 24 hours, which is not a great utilization. But these are all like micro to medium instances. I see. Okay. So the data sep itself is not very large. Yes. For the data that we basically populate into the data, it's just like a one day data. So it doesn't take a lot of space on that easy CTO machine. So we are using right now small and medium instances, but known that instances on that front. I see. Okay, understood. And by the way, one more question I guess. So you mentioned that there is no Real time API deployment currently, right? Yes. And there's no plan to do that in the immediate future. Basically that's not the concern. Yeah. Right now the problem is that even if we do it and the product is not ready to be able to configure to be configured, this is real time API will be deployed on recommendations and as well as some other product based models. Right. For like probability scoring. We are just thinking of bad prediction every day. But like real time is more on recommendations and as well as the real time. If we were to go and delivery expected models expected to deliver, then in that case but we use product kind of reading. So what we have decided right as of now is like one day is enough for them to easily use it. Okay. Other question is Shayanski monitoring or experiment tracking tools? Experiment tracking. We use amplitude so, as Piano mentioned, so there are like two metrics. One metric is that data datta science metric that we look and there is a business metric, right? So the business metric for any section is RBB able to increase the AOE and the SKU bit of that particular section as well. That is done on the that is done implicitly and explicitly. On amplitude, we track conversion compared to some other version of the model, more or less like Harika models are almost events and trackers. And then business matrix we are able to track on amplitude. But data science matrix is something that we do it on jupiter notebooks. Take care. Got it. And now in this entire pipeline of training retraining like amplitude based tracking, jupiter notebook based analysis, where are you all seeing any pain points? I would love to understand that in this entire process. Also one pain point is your interpretation. So I'm getting this number, right? So how do I inderjeet inderjeet it like what actionables do I come think about to improve them? So that is 1 second is how do you basically mix two metrics, business metrics with a data science metric? If both of them are going on opposite direction, there will be few cases when it go opposite direction where datta center because app user base or some other behavior that you have not accounted for. So that is 1 second is like I think tandem. Basically, if you are able to figure out things in one place, that is like great, right? Figure out what kind of things in. One place they are like two metrics, right? So business scale is a business matrix important, right? And the DS model should be considered to play around with business. But at the same time for me, the DS matrix is more important. I want CTO reduce the variance, I want to increase the classification. Now what happens is these are all separate areas. How do you exempt together? That is 1 second is. However, it depends on person to person and company to company, right? What we have done is we knew amplitude is a great tool for product analytics and product management and we wanted to use it for both DS based algorithms and we are able to do that and say like rough tracking, which is necessary for right now for the data science team that we have, which is like on an average, are we doing good or bad? And if we are doing good, then we don't calibrate and don't go into the details of the S, right? But that's not the ideal way of doing things. The ideal way of doing things is like looking all of them together, then taking a call. Okay, information and what do I make out to make out of this information? Got it? Okay, so muthu again, anything that you. All would like to add here? Simon Priyanka, Sudhir, in terms of challenges that you are facing. I think they. Solved most of the challenges that we face apart from it in the ERL model. So the challenge is more on the implementation side on GCP, where the code that we have requires a lot of memory usage. Optimization is required for that. I see. Which might be true with a lot of the modeling work that you're doing, like more CPU or whatever, more memory usage, essentially, depending on the type of model that you are building. I see. Okay. Yeah. Understood. And how are these machines allocated to you all? Like, the EC, two machines that you mentioned where the things are running. Is there a DevOps team that allocates certain machines? What happens if you need to increase the size of the machine? Decrease the size of the team machine? How does that all that happen? We are only the DevOps people. We see that after three A and between office B will have a DevOps team which will be dedicated and at the same time optimizing on the resources that we are using. However, right now what we are doing is like figuring it ourselves and doing it ourselves. That is something that or taking the help of existing developers from the tech team. Got it. I see. So at a high level, what are you all trying to optimize for? Like, are you all trying to optimize for different teams? CTO optimize different things, right? Like Amar HotTime could block an Gmail and I want to change that. Some people are like, all I care about is my model should be more and more accurate. Some people are like, oh, I want to cover more am I use sales, but I'm not able to cover that. What is it that you are optimizing for? I think third use cases are like so we are not trying to optimize for accuracy right now because existing so whatever we will be building will be helping. Based on version A, what difference are there? Because data science and without data science, you know, there's a considerable amount of datta difference, but version one and version 2 may incremental changes, means maybe like 10% or 20%, but not like 100%. So we are solving for that zero to one journey right now for each and every use case, rather than solving for one to ten. And I see. So what limits you from being able to try out more use cases, I guess. Where are you ending up spending a lot of your time that was saved? You'd have more spoke to do stuff, basically. What are you working on that you don't want to work on? I think that's the main question. I'll shy away from this because Mira calendar actually meetings, so I only get like night time. But I think Priang and Simon might have something to talk about. You're both on mute, though. Yeah. So I think one of the major problems that we have seen, it's not a data center, it's not. The problem that you would have. How do you make it easy for the end teams to adopt? You adopt your models. That is first and second is negotiation, right? That's like a handshake right now that handshake could be driven through dialogue and communication. But then third is like how do you make sure that you want a model output up? You are able to make the other person understand that this is the change that I want to bring in. So that is where a lot of engineering and mind focus is output account so that the business is optimized and takes care of your inputs. So that is where a lot of time goes. However, where it not goes right now is on the engineering side of things. Right now the engineering deploy is something that we do in tandem. So our microscomptounces then we ask like we do paralyze of pluralization of tasks, this integration of tasks. But on the that is where like a major chunk of our time goes which is like how do you explain a particular model to a person which has no information, no background and at the same time able to provide a usability and you can forecast the business value out of it model? That's a major question right now. Sorry, go ahead. You were saying now how do you. Disintegrate it like you can tie data science matrix to the actual revenue outputs and I think that is like hard for everyone to what did you do? I think Facebook you have to use AI and ML, right? But in traditional businesses it's still a quantum leap where you want to drive that forward. At the same time it's like comfort space which you have to move the other person from. Makes sense actually. Like I've experienced this problem as a reflection. Also key data science team is trying to track whatever log loss or Ndcg metrics and stuff like that. And the business team, they are like I care about my business metrics, what's the click through rate on my product, what's the conversion rate on my product and stuff like that basically. So we used to have this constant battery like what is it that we should really be tracking and stuff so completely relate with the product problem. So basically I'm gathering key problem around metric tracking amplitude and jupyter notebook. That's where a little bit of a manual work is there. And what's the problem around how do. You make. The data science and the business metrics kind of cohabit basically co exist in some way, right? And these I think will come after April is like how do you optimize on the cloud cost, right? We were a part of GCP Explorator program CTO kind of be Arun free trial for the next one year. So we can focus near, right? Once we start billing, TCP starts billing us. Everybody's focus would be on how do you optimize on each and every resources that we use on CCP. I see, understood. Okay, got it. So I'll give you a little bit of a background. This is very helpful, by the way, Sharla, thanks a lot for sharing this context. I'll give a little bit of a background and I'll tell you where I think we could IBM hearing where you could potentially help. So number one, that actually most of the problems that you described are not within the scope of proof truefoundry current insurance. So that's like the short answer. One thing that is within the scope is the cost that you mentioned. Because I can see that currently the way you described that you have these two machines running and you have relatively less utilization, less fraction utilization of the machine. Those are things that like a troponic can really help because the way we design our jobs is, let's say if you have a two minute job, it literally spins up a machine, runs the job for two minutes and then kills the machine. Basically. That's how our jobs work. So you can truly optimize for that cost. And you could technically be like spinning up a job directly from your local machine on a remote cluster. It will spin up the cluster on the job, kill the cluster. And this also becomes very interesting if you want to do something in parallel. So for example, if you were doing for experiments, you want to do hyperparameter tuning, right? Instead of running the 16 experiments that you want sequentially, actually just launch out 16 containers, get the results and get done with it, basically. So there's a lot of implications in this design that we're doing. And that's potentially one area that we could help. But to be honest, like you're saying, it's an April problem, it's not a July problem. Last July. So then it's not top of mind. I guess right now I'll describe a little bit cloud Truefoundry, especially for the sake of Priyank, Shivank and Sumit to just give you a context. Cloud truefoundry. But before that, do you all have any questions for me for what I talked about a little bit earlier? No? Okay, sounds good. So Truefoundry health IQ motivation for building True Foundry, by the way, was when we were ourselves at a very early stage startup like, we're building a company called Entire in the HR tech space. We're deploying models and we realized that this is not as simple to configure everything as we had at Facebook. Basically, like with all those tools, right? That was the motivation. But then we also realized that we actually don't need all the things that we had at Facebook. I just care about being able to deploy my model. And maybe best case, just see that my monitor that my model is doing fine, right. I'm also looking CTO spend the manual effort in the early sales to just go check out the dashboard, look at the deployment, try things out. That also works out. Like I don't even need a proper alerting system in the super early stage. So, what we realized is that this use case as a startup is typically super simple when people get started. But as you mature, as your system matures, you always keep on getting more and more complex use cases that oh, now I also want to be able to try a B testing or now I also want to be able to track all my models in one place. It sales like your use cases keep evolving essentially, it's never constant and you should not be needing to switch your entire toolset from use case to use case essentially, right? It's just too much effort to find yet another tool, switch to tool price, negotiate and stuff like that basically. Right, so the way we are designing True Foundry, the reason we are building out Giri Truefoundry is like we help early stage startups to get started on the platform really quickly, like few API calls and it satisfies your most important use cases and we are done, right? But as you're ready to mature like trufanti scales with you that you're like oh yeah, now I'm ready to start tracking all my machine learning experiments in a proper experiment tracking system. Right now I want to be able to do like this batch deployment and save my cloud first. I'm also ready to do like this real time deployment and actually have phone conference API latency constraints basically as your use case is mature and then at some point you'll be like oh yeah, my model is not performing well, I want to track whether the model is drifted, what's happening with the model and stuff like that. So whenever people get these use cases, the platform kind of evolves with them. That's how we design the platform. And our one line pitch to folks is that we are as easy as heroku to get started but as scalable as AWS when you need it. Essentially that's like our one line pitch the way we talk to people. Again, I'll take a pause to see if you all have any follow up questions. No, I completely understand the value that it offers and where are we in terms of within your product market fit? Right, so I think Torah earlier but we will be looking into few things which is the APIs in few months. Second is like really looking into the optimization of cost which you mentioned, right? Like spinning up the container and then stopping it altogether. That is like one or two use cases that we see where we will take a note of value coming from because then we go into detail and then we see the costs are not last week only we saw that it's like costing us around 300,000 per month where we could have like the Google GCP like just for our use case like BigQuery and Airflow and Rupees. Rupees, many rupees. Not in dollars yet. Okay, yes. Even that is a lot, right? If you are able to optimize on that, we would be able to that is currently right now, but in six months now it's increasing month Qovery month by 50% or 40%. That's where we focus. A lot of focus will be on optimizing all our jobs as well as instances that we use. Second is how do we make sure we don't think about the engineering part of things. We think more about the problem solving where the two countries can help us in terms of the APIs and stuff. So Nicole, what would make sense? Should we have this call again when we are moving towards streaming and second is on the GCP optimization or do you suggest to have a background of the product in one week or two and then also have another call in few months? Yeah, so I think what we can do is just so you all have context about the product and how we are building out the product, where are we adding value? We could do a demo of the platform, but this demo as after hearing on this call, I think it's going to be I'll give you a full demo of the platform. Basically all the capabilities, what we have, what we don't have, what are we planning to build and stuff like that. And that's literally just for you all to keep at the back of your mind. Or maybe with the demo like you realize that this one thing that we chad not thought about, but this is already immediately adding value. If you find something like that, if you basically find a very concrete specific use case that if you like Akshay truefoundry able to add value only then I think there is any point of trying to do a pilot POC or whatever. Otherwise if you're not actually solving immediate burning pain points, there's actually like no point spending your time trying out a new platform, basically. So what we can do is and that I think is fairly straight forward from my side to do like giving a demo of the platform itself. So if you guys want, we can set up something early next week also or if you want to do it like in a couple of weeks. I'm pretty flexible basically on that demo. Cool. So what I'll do is I'll set up a call and then you can give us a demo within the time frame that you have mentioned. Does this time seem suitable to you? Yeah, it does. Okay, works. Okay, sounds good then. All right, so she's looking forward to the, to the calendar invite, depending on whatever dates work out better for you guys. And then we'll do a deep dive of the platform. Of course I would love to hear feedback anyways on what are the features that you would like to like to have in the platform when we do the demo. Okay, surge week. Great. I think next week some of us will be on leave due to the Valley. But I'll set up, according to all the team members are there because then we can invite opinions or two more people, which is like, you know, effectively the whole brains of our team. So that would be great. Apart from that, Happy Valley. And enjoy it. Yeah. Happy the Valley to all of you. They have good fun. I'm also looking forward to celebrating the Valley here. My parents are visiting, actually, so this. Is oh, that's great. Yeah. Thanks, Nicole, for your day. Bye. Bye. Thanks. Hold on.