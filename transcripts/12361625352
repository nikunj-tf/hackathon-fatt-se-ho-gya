Hello. Hello, good morning. Hi Pratheek, nice to meet you. Good morning. Just starting the day. Patika. Can you give me 1 second? I can think I'm having sure. Hi, can you hear me better? Yeah, I'm able to hear you, yes. Okay, sorry about that. First of all, thanks again for taking time for the call. Starting the day. Just already had a meeting before this. Can you hear me? Yeah. Hello? Am I audible? Okay, wait, I'll rejoin maybe. Yeah, no, you're audible to me and Rob, I think. Can you check your I'm not able. To hear you, so let me I'm not sure what yeah, I'll rejoin. Thank you. Hello? Hi, can you hear me? Yeah, can you? Yeah, I think so. Now. Okay, finally. Yeah. Was there like a speaker problem or something? Yeah, probably. I don't use Google Meet often, so I think it didn't work with my work with my headphones, so I'm just. What do you use internally? Zoom or something? Mostly zoom and sometimes teams. Okay, got it. Cool. Thanks Patrick. Thanks a lot again for taking out the time. I think just wanted to do this as a follow up of the last call and the intention was to cover the following things. First is like some of the issues that you had mentioned your team was facing in respect to unifying all the different workflows and in general migrating from one state to another for the overall model management part. So had few questions on that that we can cover and then wanted to kind of go over the demo of the platform so that you also have context on what we are building and potentially we can install on potential ways in which this could help in the unifying of workflows for the developers as well that you have brought. And then little bit questions around security. I think we just were touching on security towards the last time at the end of the call. So wanted to get some few more insights there as well. Sure. Does that sound good? Like any other things that you wanted to cover? That sounds good. Okay, cool. I'll just briefly introduce myself. Hi Patrick. This is Cinema, I'm graduating from It and for that I was working with McKenzie as a management consultant. Joined to Found about six months back. Work here in the founder's office. Look at mostly product customer development. Nice to meet you. Nice to meet you. Happy to introduce just quickly prettika groal, I'm leading data sciences and digital innovation at Medtronic, part of diabetes organization. Nice. Diving into from what we remember last time and you mentioned that earlier you were team was using ML Flow and then there were challenges and then now you are trying to use Sage maker in some way to kind of see if it simplifies things internally. And then there is another team like which you all acquired which was also using Gcp. And while the overall longer term intent was to move towards Cuban. It is for scalability and everything right now for simplicity. You have kept it independent and there are teams using different workflows. So I wanted to get a sense of what are the different kind of workflows that people are doing and where do you kind of see some of the gaps, challenges or issues that come internally. And also you had mentioned that there is some complexity that has been introduced because of these different tools being used in Silos. Like what kind of friction are the teams currently facing? Is there some usability friction? Is there some friction in onboarding the services from Gcp to Sage? If you could just shed some light on that. Okay. Quite a few questions. So tell me where to start. You can narrow it down. We can start with maybe the unification of the workflow part and we can take it from there. Okay. What is the question there in terms of unification of workflow? What are the different kind of workflows that people are having in the team currently and where do you see the challenges? Basically. Yeah, I think mostly. I'm just. Framing up my thoughts in terms of how I should describe it. So give me 1 second. Okay, so challenges are one, the process itself comes in very misaligned and so broadly, I think ML Ops, everyone kind of does similar things, but the differences are how you integrate it into your development cycle and what are the checks and kind of balances that people have across the board. So there are small differences between different teams that can lead to major kind of problems with how everyone aligns and what tool sets people use and things like those just really aligning people between all of them sometimes between different teams, sometimes is a challenging problem. More concretely, initially at the ingestion side of things, I think a lot of deviation comes in where people often are when they're developing a model, they're developing tons of additional features, right? So people have lots of differences around how to go about validating a feature versus really kind of just putting into development and validating it at a deployment stage, kind of things like those. And so depending on the type of model, people might have different trigger that is put in the input variables before it goes to the engineering teams to deploy it. So really kind of making sure that we are doing the right sanity testing and doing the right interpretation on the features that are being developed is one of the fine tuning that we are trying to do with the team between the different processes. So that's one. The second is being able to. But. The second difference is being able to do the scaling and deployment of the model in a cost effective manner and trying to figure out how do we deploy the model and run it at a speed which is kind of required for the business. People can often end up running the model on an ongoing basis when the results are only required monthly. And we end up wasting a lot of these resources that could have been saved. And so maybe can be solved with ML Ops, maybe not. But that's kind of one of the things in terms of really what we need to incorporate early on as an input to the model. So that's second and then the third is looking to minimize the effort that is required and monitoring for the model while really taking the right action in terms of fine tuning and in terms of retraining. And I'll go into details of it a little bit. And so for healthcare industry, especially the models that might be used in a therapy setting in an FDA approved manner, we have to be very diligent about what can be changed and what cannot be changed and how far we can go with online learning. There is very fixed guidelines that have to we have to pre, prescribe to FDA and give out documentation that this is the way the model would change and this is the way the model would not change. And so making sure that we set up a pipeline which can follow that very strictly is one of the challenges for us. And then finally, the one that comes up often is the privacy one. And so within the pipeline, we want to create the identified views of data for the researchers and for analysts. And eventually when it gets deployed, we. Want to kind of continue to deploy. It in a larger production environment which has the data. And so being able to kind of go through that pipeline and kind of manage the data de identification is kind of a challenge, which is more of an infrastructure and kind of data management challenge or data governance challenge, more than ML Ops, but it's a challenge nonetheless. Understood. So this is data. The identification is primarily so that the researchers and analysts who are working on that, they don't get access to the Pii data in some way. Right? Yeah. Okay, got it. And how do you do it currently? Is there like a separate team that is kind of unbundling the tagging or doing the data and then that is separately fed into the training pipeline? Correct. I think it's between access control, separation of duties and kind of encryptions and things like those and masking kind of right. So between all of them, we are really just there's a separate admin team that is responsible for the infrastructure that is separating out the Pii data against the rest of the data and creating an access for the analysts and researchers to have limited views of the data. And does this also kind of mean that once a model is in production, because real data is flowing in that time, at that time someone is like you have created, say, monitoring view. So that probably is not being looked at by the person who built out the model. Is there like a Chinese wall type of thing between the developers who are building out models versus people who are monitoring the real like the final model that is deployed or that is more from a perspective of just generic training type where you want it anonymized completely. Mixed up both so mix of both. So for the most part, most models are still running on a deidentified data set on a population level. There are a couple of instances where we have had to go into patient level deployment and kind of model that personalizes to the patient itself. And in those instances there is some sort of a Chinese wall where the results kind of the de identified views get shared with the developers and the scientists. But the management and administration of the monitoring dashboards happen through engineers. And I know for a set of models that actually impact like a patient or go directly into the product you were mentioning, that goes through a degree of pipeline in which I'm guessing explainability also comes into the picture. So who kind of does this explainability right now? Is there a way you do it currently or just using SAP values or how do you do it currently? Yeah, most of it relies on shoplots. That's not entirely sufficient. I would say we are looking to augment that more. Right now it's between kind of most clear on shop blocks and team have kind of explored few other kind of methods of doing it. Okay. But they do it individually for each model. Is there like a system that they run a script and it automatically just gets it done? No good system. No good system. Okay, got it understood. Yeah. I think you also mentioned around monitoring and FDA regulations and to simplify that. So just trying to understand like what kind of so are there like a fixed test, sets of tests that you need to do and how do they get done right now? Is there a team which iteratively goes into does this test, how much human in the loop is there in this entire process? In the monitoring you mentioned? Right, you have to create some reports for FDA, for the direct customer facing models and you would want to automate that using some pipelines about this part. No, not the report. So essentially the process that happens the way FDA is so managing a model that is changing or that is learning on an ongoing basis is very new to FDA. Right. So they are not really it's a very weird or foreign concept. It was a very foreign concept to them where they say for them software is you tell me the requirements and it should always perform that way. Whereas an AI kind of probabilistic model, so they think in very deterministic sense and probabilistic model is very new to them right now. It creates a challenge for them in terms of how to go on that. And so one of the guidances that they have provided is essentially they put the owners on the device manufacturer to say tell us beforehand how the model would change and what would be the limits of it and we have to talk to them and make sure that justify how that creates a safe environment. But essentially the conditions that you define there are plenty of conditions I'm simplifying it kind of grossly but I think essentially you define here the variables that will go into the model. If we add any other variables then we'll make an amendment to the submission. Here are all of the performance criteria that we measure. This is why it matters to the end performance of the feature of the product itself. And here are the performance criteria for the sense for the model that will measure and here are the distribution bounds or any limits, control limits for how the performance would be for the model. And so it might vary between these limits. Usually it can also have one limit because have better performance, can go up to 100%. But essentially you would kind of define those limits in terms of how the model would be performing and if it changes then you'd have to do a manual intervention. But between that you can set up a pipeline that does it automatically, that continues to learn and can update automatically. Understood. The pipeline itself, what it is doing is it that the limits that you have given to update, monitoring whether it's within that limit and taking manual overrides when it's going beyond that, is that understanding correct? Yeah. So it's not so much of manual override. So we have to come up with limits where we will start our manual training process and kind of replacement process kind of beforehand because they'll take a set of documentation for us to submit it. But the limits that we define are where the models would be with the introduction of new data, the models would kind of retune themselves kind of within that limits automatically. And one more point. So models would retrain then how are you able to quote to the FBI after retraining as well? This would be like the boundary military output. Yeah, that's what I was going to say. And so essentially what we have is a fallback mechanism where it goes back to the previous version. Right? So if it doesn't kind of meet that then it goes back to the previously trained version. Understood. And since these models are direct customer facing, are these models on the hardware? How do you update? Are they periodically synced with the cloud? Today they are processing, we are exploring kind of the edge computing kind of framework to kind of get on the mobile phones and devices but today they are all on the cloud. Understood. Are there connection issues with these models? If they are on the cloud? How do you monitor them when they are devices and get the data afterwards? How is the monitoring done for this? Yeah, so you'll have to divide up the features, right? So for the most part. You can. Apply two or three kind of approaches to kind of deal with it. One approach is you run some part of the algorithm, or some part of the feature, I would say, on the devices. And when you don't have connectivity, or what you're using cloud for is learning and optimizing the parameters that go into the algorithm, that go into a heuristic algorithm that sits on the devices. Understood? I'll give you an example if you want. So we have a technology called Flu, which is meal detection technology. So essentially a person is wearing an activity monitor. This technology would automatically detect based on the gestures when a patient is eating. Very cool little technology which reduces burden quite a bit for the users in terms of understanding how much meal they're eating. And so they don't have to log day in, day out, but we can still provide a lot of value to them, right? So in this case, what we would do, or what we do kind of in the background is we have a heuristic based algorithm and I'll kind of simplify and give you a very simple kind of method. So essentially when a person is taking a bite, right, we want to remove false positives. And so they can be picking their nose, they can be scratching their head, they can be doing all sorts of things. And we want to kind of improve the accuracy of detecting when a person is eating. One of the things that can be important is a number of bites that patients are taking in a minute. And that is different for different patients. Some patients eat slowly and they take two bites a minute, some patients eat faster, they take four bytes a minute and things like those. And that can be optimized, right? So it can be a very simple heuristic algorithm to detect the pattern, but the number of bytes can be optimized. And that can be a learning algorithm that's running on the cloud. It kind of connectivity goes for some time because you're not going to be training it all the time as well. So that's one mechanism. The other mechanisms that you do is you always have a fallback algorithm on the device on the phone, which does some basement performance for the patient. And so when there is connectivity on the cloud, then you kind of rely on and that can be a simple, again, heuristic algorithm, not a very complicated algorithm, but that has a baseline performance. And whenever there is connectivity, you kind of fall on the better performance for the model that is on the cloud, otherwise you fall back on the default performance. And so that's another mechanism. And so we have kind of applied different mechanisms but if you guys are edge computing and being able to deploy the models on the mobile devices and manage the monitoring and performance kind of those is also a problem that we're actively looking to solve. That might be very interesting as well, because all of these are just workarounds, not a very workaround to kind of enable our solution in the market, but not end solutions that everyone would be happy with. And so there's kind of work to be done there. Yeah. So on the edge computing side, I think right now the support is for cloud, but over a period of time, the goal is to also go towards edge, because right now, the combination, as you said, is becoming very common, especially a lot of companies that are serving customers in those cases. But right now, the focus has been on cloud. Maybe if there's a way we can kind of engage on the cloud side and then later on, as your needs come on the edge, we can potentially work with you as a design partner to build that out. So that is something that is possible. Just a question. So you mentioned that there are separate models also for different clients. Right. And if they are separate models on the cloud, how do you end up monitoring? Because this would become, like, a very complex problem, like, so many different patients. And then if they are, like, personalized models for each patient, is the monitoring for all this currently automated? How do you manage this part? Yeah, we don't have separate models for each patient, yet there are clusters of patients. And so we do have monitoring on all of those clusters, but not we haven't gone to a patient level. Okay, understood. That makes sense. Then you also mentioned another point about scaling the model effectively and, like, cost effective. And you said that some people are, like, doing the influences multiple times. So where is the model currently? Like, do you have a VM or EC two, which is continuously spinning and people go and trigger the output? How do they take these outputs currently? Like, what is the workflow? Who actually does this? Is this the business? Or they ask a data scientist to run whatever model this is. Mostly data scientists. So I'll answer your last question. Data science, not necessarily just data scientists, but the technology team that is kind of deploying the data science algorithms. And your previous question was where are we deploying it? So the ones that we are doing right now, we have migrated to sage makers, mostly through sage makers. Okay. And is this like a backdrop or because you said it's being run multiple times. I'm trying to understand why are people running it multiple times to the data center, get a request regularly from business or what's the reason for that? I'll explain you with an example. Right. So one of the things that we help with, help within the business is trying to understand. This is very common, not a big surprise, but we try to understand patient churn and try and understand what are the factors that might kind of reduce patient churn and improve their user experience. Right, so as we get more device data, as we get more sales data, shipment data, complain data, we want to kind of really update the model to understand how our risks on patient churn is changing with all of this data. And so we would want to run it on a periodic basis to take actions based on that. But in certain cases, our ability to take an action might only be on a monthly basis. Whereas if the model is running on a daily basis or even kind of early basis and that kind of is just wasted resources. Understood. That makes sense. At one point there, I think sometimes what happens is the end model might be still running on a monthly basis but ends up happening. Is there's a confusion around or what gets complicated is some of the underlying models, sub models, kind of whose output is going to be kind of utilized in different use cases. That gets kind of out of hand where people can be running it, people can default to running it on a more frequent basis without actually kind of knowing how frequently it's going to be used by the rest of. The models, whether it's kind of a code that develops a feature, whether it develops an inference, results that will be used by another model, things like this. So RT is two different models, the subnesting model than the actual model which is giving the output. Are these managed by different teams? Why is there like a lack of visibility for this feature update or any other process for that matter? Trying to understand that different teams, they. Might still be within my organization, but there might be different operating teams or working teams or data scientists who have developed them. Understood. No, that makes a lot of sense. And then you also mentioned so about standardization part, right. What are the avenues where people like typically collaborate? You mentioned ingestion. Are models not managed and to end by one team or is there like different stakeholders also like combining across different functions in like a single model management and deployment? Yeah. If I had to tell you. Broadly and simply, my team has two kinds of responsibilities. One is developing algorithms that go into the product and therapy and there is the team that develops algorithms for supporting business. So I just gave you two examples, which is one is on the mobile application, detecting meals or managing glucose or the other is kind of churn model. There are different requirements, regulatory requirements that come in for both. There are different kind of rigor that needs to happen on both. There's different kind of development cycle that happens on both. And these teams can be different and their processes can be different. Those are just two examples and they can be other teams in the organization. No, that makes a lot of sense. One other question, just PATHIK was around the part of Sage Maker and then people were using Gcp because some of them kind of got acquired. So before using Sage Maker, I believe you are still deploying on the Raw machines or the Kubernetes or something. So what happens to those models right now? Like the team that is doing the final deployment, they are managing those models from earlier and then the newer models also they are managing because then it would be quite difficult. So I just want to understand once. It'S a different tool chain, right? So we are reducing kind of the effort on the whole kind of docker and Kubernetes kind of cluster and monitoring there and we're trying to migrate them into on Sage Maker. So most of the newer ones can arm on Sage Maker. Okay, and what about the older ones? Like when you have to retrain an older one and then redeploy it. So do you kind of separate from the Kubernetes docker containerized person to Sage Maker or do you kind of continue to use the old version? No, mostly we are trying to migrate them and it says we are requiring to change them. Okay, got it. In many other questions you had, otherwise I can maybe in the interest of time. No, I think those are most of the questions. Thanks a lot Patrick. Answering that what I'll try and so is maybe a generic overview of the platform which focus on some of the parts that at least I've heard from you. But we can also do a more deep down specific demo for any specific part of the problem. So the product is consolidated of different parts and I'll show it to you as a workflow, but you can also continue and use just one part of it, if you will. So let me start by sharing my screen and please feel free to ask questions, et cetera as well. So just from a perspective of the product that we have built and what we are building, the goal is for helping companies make their Mls'very simple and where ML of starts, for us it starts from the moment you start training your models to the point you actually deploy and monitor. And for all of this, the infrastructure resources that you need to provision and manage that is also kind of a part of the platform. So what will showcase in the demo is like how even if you have multiple teams doing multiple things, how you can or even on multiple cloud for example, how you can manage that very easily. In order to unify flows between teams, you would want that they use a configuration, a common Git versioning system or so on. We'll see how that works. We'll show how you can divide it into different accesses for the team, especially in healthcare, you will want to have different access controls at a fine end manner. So how that works. And then after that I'll go into maybe a diabetes regulation model and try and show you how you will train out that model, how you will log that to the registry, how you will kind of deploy it as a service, and then you can even go into the monitoring side of it as well. Okay, let me start with this. So here, this is the place first where the system first of all runs on your cloud. So what we do is we ship this entire system to your cloud and within your cloud we can connect. We just need like say, for example, in case of AWS, which is the primary cloud you are using, we'll need just like an Eks instance for your team to provision. And on top of that we will install our helmstrong and then it will connect and create a cluster where we install some of these applications. Some of the applications are things we need and some of these are depending on your use cases. So we can also remove them. So we reconnect and we allow access control where we kind of give it access to a few collaborators. At any point you can have someone running an AWS instance as well as a GCP instance on top of the same thing and you can manage it through the same. That's good. And just to add to that, the multicloud, how it works is even if you have like a local on prem Kubernetes or across the cloud, for a developer, the experience of shifting from one to another is just like the name of which cluster you're using. Right? There's just a reference to which cluster you're using. So in terms of workflows, nothing changes at all between any of the clouds. Yeah, one question I wanted was to see is it fine if I record this meeting as well? I can send it over to you later on if you want. Sure. Okay. This is where you can think of it like just being able to connect quickly to your cloud in a very easy way. And we integrate with whatever tools you are using so we don't ask you to replace anything. So you might be using something as a docker registry. So we'll just kind of integrate here. You can even add multiple registries from across cloud accounts. Then similarly we integrate with whatever versioning system you are using here. So here, like, we have connected GitHub and Bit bucket, but Gitlab, some organizations, we haven't had a customer that uses Gitlab, so that's not connected, but we can also kind of link it and basically you can connect to your GitHub account, even secrets if people are using that. Actually, sorry. We use Gitlab. Oh, you use gitlab. Okay, great. I mean, we can easily connect to Bitlab, so it's just about. An organization. So we take like generally two to three days to integrate it. So this too we integrated, but this one we can integrate if that is the requirement from your site stay in case. Now one thing is within a cluster, I'll show you a demo account within a cluster what happens is there are workspaces and you can think of workspaces as different environments. So imagine that within the entire diabetes or there's one team that is working on certain types of models and they are the research team. And then there is another environment you can create which is like a deployment environment. So you create a dev environment and you create a deployment environment and in that you kind of fix certain resources, et cetera. And you create those workspaces. So that way and for each workspace you can give access to different folks as well. So for example, let me take an example of workspace. Each workspace comes with certain resources limit. So this way what happens is you ensure that at any point the resources limit in that workspace never exceeds a certain CPU memory requirement, et cetera. You can also kind of support instance families. Like for example, you are saying in training you primarily end up using CPUs and you don't use GPUs. So all the GPU machines you can disable. So that way what will happen is there is no way a researcher who is running on a developer cluster can run or use GPU which ensures that your cost is constrained. And this limits ensure that at any point, if the resource limits are hit, there is a trigger sent or alert sent. What's the configuration of workspaces? So it's kind of are you familiar with Kubernetes in general? For the broadly, yes. So in Kubernetes there is something called namespaces. So Workspace in general maps to a namespace. In our language, workspace is basically a collection of a set of resources which kind of sets the limit on the maximum amount of resources that a team can use. Sure, yeah, and it's flexible in that sense. So you have a minimum set of resources that you can request to the workspace. But let's say there is a certain spike or something that a researcher needs. You can also configure that you can give them like a limit. So let's say you can use 16 GB of Ram of CPU, but even if they want like 24, they can do that, but not beyond 24 because you don't want them consuming a lot of resources. So in that sense it puts a constraint, puts like a boundary around the kind of resources that you can access. So is this feature replacing Kubernetes? Is it using Kubernetes in the background? How is it this uses Kubernetes behind the hood? Okay, yeah, the entire platform we built on top of Kubernetes. So we built a lot of abstraction which help you manage Kubernetes through the UI or our API. But maybe a base of Kubernetes. And so with this tool, would we still have to configure and manage the Kubernetes clusters? That would go away and you just manage this? Yeah, you would typically not want to do that because we understand take a note of management overhead. So typically when we work with clients, we use like Amazon Eks or they're unlocking Gcp or Azure operational overhead such as managed a lot by the platform itself. You wouldn't have to do anything and the developers don't have to do anything about managing Kubernetes or the DevOps team, they just have to interact with the platform. Okay. Yeah. So we can also create this access controls easily. So imagine like there was a developer team, you kind of name it as a dev cluster and you give access to all your developers and there was a production cluster. And in that you can give access to the people who are deploying and you can set different kind of roles like admin, editor, viewer, depending on who is kind of managing that. So this is like a one time thing. There's also a way if you wanted to kind of test out a new project, you can do it in a new environment rather than using an old environment. So that way your things are maintained at one place. Also suppose there are two teams that are working on different tools like as you mentioned, Gcp AWS, you can again allocate here. So for the Gcp one, you can allocate them in the Gcp cluster. Like this one has one cluster, but you can create a workspace for them in the Gcp cluster. And then basically you can see everything that is going in your at one place. Right, okay. And this is purely about resource management, not really the access management for the underlying data itself that happens or can. You do that in the workspace for that? What you can do here is that you can attach service accounts that it looks so wherever you define the access on the data, let's say it's sitting in your S three or some other bucket you could directly attach. Let's say you want a certain project to have access to a certain data. You can just directly do that in workspaces. So it also helps you in some way constraint all of the data access and resource access. But yeah, the authentication itself would happen on wherever you put in the data. So you also get to see, okay, what is the cost that is incurred for workspace and so on and what resources have been consumed and on. So this basically allows you to kind of manage it at a high level. Then we'll go into the part around other workflow of a developer. So a developer like who is kind of running I'll just start with maybe here. So let me take an example like the diabetes one, they will generally start with training their jobs. Something that we've tried to maintain across the platform is that whatever code that you're using currently we don't meddle around anything with that. So you can literally migrate your entire code and we don't enter any parts of the actual code itself. We are situated in a separate file which you would just write like five to ten lines for the deployment itself, choosing the config, whatever you need. So we are like agnostic of the architecture that you use, the type of model that you use, whatever complexity or scale you want, everything is supported with the same workflow. Yeah, so I'm just showing you the kind of a notebook example. So here, these are our libraries that you use. So you install the libraries and then basically the workspace in which you kind of want to run your training. You give that the name of the workspace and your training script utilized as it is, just like there's no change to the script. You don't have to change the code in a certain style or whatever you are doing, you'll run. And we have certain things that allow you to log things like log parameters, log metrics, log plot and so on, log model. So once you do that and your training job is complete and you kind of just kind of run this job deploy, what it will do is it will start posting this training jobs within this job section. So this is where you can run your training jobs, you can run your inference jobs. This is the part where you can deploy your service and this is the place where you can deploy a model using a model server route. So for example, in case of diabetes train like when you are running your training jobs here kind of we logged two versions so you can see all those versions run with what is the run duration. And then because we logged the metrics, you can actually go and compare your metrics as well. So here you can see the metrics that were logged with the jobs here like it's showing one because both the metrics like we ran the same model, you can see the kind of metrics as well. And then you can draw log your plots. So even your plots with both like can be logged here. So this way you can actually go and select that. Okay, this is the model run. So here you have the run details in this. Again, within a model. If you wanted that this run, then you can select that. Okay, this is the model run that I really want to kind of host. So by using log model, you have also logged this model and you have a model FQM which you see here. Yeah, so it's like a model directly with going back from there if you wanted, anytime later, you could compare the starts or just directly do a one click deployment directly from there, whether it's for demo or testing consumption or anything. Once you have that model, the model action allows you to deploy at one click. Now, suppose you wanted to deploy a version of the model. You just go here, you select the model, you select the workspace. Suppose this was a dev workplace in which you wanted to deploy, because right now you don't want to host it in production. You kind of give the name, say whatever, ABCD, you put your model SQL and then you can just click Submit and it will deploy. If you wanted GPUs, that's not needed. So you can put it as none. If you wanted GRP, you can do that. And then once you click Submit, the model is deployed here in this format. So I'll give you an example of, say, a model like this. Before we go to that, could you also show the Cron jobs at the thing? I think that would be useful. Just 1 second. So I just wanted to show that this is where the logging happens. So this is basically the model registry. So for any model like this, you can log the Schema metadata, et cetera. This model does not have a lot of things logged, but basically you will start seeing everything here and you also can track models to the corresponding source. So the point that Chinma was mentioning about, say, the Cron jobs, I just will show that. So this model, when it was running, the same thing that I saw through the code, you can do through the UI as well. So you can import code from your git directly. Or if you have a docker image, you can do that. Here, it's importing from take a note URL, as you can see. And here, if it is a Cron job, right, you can also schedule it like I want to run it every day at certain time and so on. And then what it will do is at that time it will turn on the resources it will run and then it will shut down and then it will log the corresponding models here. So let's say the use case that you mentioned, right, that the reports of Churn are required every month. So you can automate this one. And these jobs and services, they can also talk to each other or trigger each other. Everything is connected through the platform. So what you can also do is if there is like a final output job which you've set up in Cron, you can directly use that to trigger, let's say, data processing job that you have also on the platform, right? So you kind of create those pipelines and automate the process instead of someone having to do that. And then the dashboard and everything, it gets generated. Or if you wanted to log model somewhere, that also is possible. So this is the same model which once trained, once the model has been selected, it's now running as a batch inference. So if you see it swing every ten minutes. It will run every ten minutes it runs, it logs the predictions here and then you can see the metrics of this. Predictions? Yeah, I think this one is job. If you go to service you'll be. Able to see just 1 second. Here. There'll be a lot of versions of runs like it's taking time to load but basically it runs every ten minutes and all those metrics along with showing the status finish, how much time it takes to run and so on, it will run, it will close it and so on. Right, let's go back to the deployment. So here in the code I will just see. Now suppose this is the job that deployed like it's deployed using it, then you can trigger a jobs run every ten minutes or so. Now, the same model, if you wanted to deploy as a service, as a fast API, then here is the same model I'll just show what happens in deployment. You basically select the source code. You can also kind of suppose if you have integrated with your git, it will show the corresponding git and you can then select it and give it. If you have some other source, you kind of use that which is the case if you had a docker image already, you can directly give the link to the docker image. And then here kind of you give the command for running the Python code. You can give the part to the requirements if it's there, otherwise it will automatically put requirements or Txt. You can expose it at a custom URL or you can put custom security rules in terms of so that if you are exposing this, no one else can express it apart from say a developer X who knows this password, right? And then you can put your environment variables if you want, if not you can disable it, you can put your resources and so on. And then once you click Submit, you also have additional fields here in case a DevOps like finally wants to use it. And once you click Submit, it basically deploys the job to the corresponding workspace that you select. Suppose this workspace was a demo workspace. Now once you deploy, your developer will also kind of see your logs and metrics. So your logs, your metrics that are there, they will also see it, right? Now this service is not running so it's showing a slide, but otherwise it will show the corresponding metrics and then sorry yeah. On this part we also have like auto scaling. So let's say you are not serving the model at all points of time, right? So the service can automatically scale down free of the resources and whenever there is traffic coming in, it can scale up based on the length of the queue of request. The cost optimization actually can happen. So you can enable auto scaling so that way it will automatically scale up, scale down so you don't have to worry about closing the service. So the cost optimization automatically is taken care of. And suppose now this was in a demo environment and now you feel that, okay, the metrics you are seeing are good. Then you can want to kind of promote it to a production environment. So from here you will select the new workspace which is in a production environment and then you will select the corresponding application and then you click Next. And then it will move that to the new environment. You can also clone it from here, like say you clone and you kind of deploy it to the new workspace which is a prod workspace and you click Next step and it will kind of take the same service and deploy it to a prod. In this we have not yet done the A B testing part, but the A B testing is coming soon. There you will be able to do Canary. And also the part you were saying last time after the wherein you wanted to kind of put constraints. Okay. Data scientists can only take a model and go up to deployment in, say, a demo environment. And then a ML engineer or someone else, a data engineer, comes in who can take it from a demo environment and then move it to prod and so on. For everyone has to put code into the same model repository and this is the format of the code and so on. Those things you can control directly and you can ensure that everyone is following the same practices. Okay. And at the same time with the ability. Okay, is there a way or configuration for adding checks on the outputs and the metrics of the model itself before it gets promoted to a certain level? Yeah. So that is something we are doing right now that is not there right now, but basically the auto promotion thing, right? Like in some way that okay, if the model meets certain requirements with the characteristics, you ought to promote it. That is why no matter what happens, if you are promoting this model, if it fails three things, no matter what happens, you never promote. Right, I was talking from that checks and balance perspective, but yes, both would be valuable. How you can configure that right now would be something like you can set up a CI CD pipeline so we have an integration with that. So let's say the developer is like logging the model. You have a CI CD pipeline that automatically triggers and that logs the flag whether it's passing the required test or not. And then you can have the pipeline promoted if it's passing the test or it's not. However, like the UI part of that we have not built. But if you want to support that through a CI CD pipeline, that's something that you can already do. Okay, yeah, we open the Yaml specs. So this is what then goes into feeding into the Csv pipeline thing ultimately. But yeah, this is like generated automatically. So the developers don't have to write this. So the DevOps, if the DevOps is writing the pipeline, they can be directly take the CML or otherwise like the developers themselves, they also can directly integrate it. I also wanted to I sold this model as a service. This is the same model that is deployed. I also sort the batch prediction job. Let me kind of show you once the monitoring site. So in your batch job, if you kind of go to this, let's see the batch inference. If you see this example in the batch inference you have written this line lock predictions, right? So this allows you to log your predictions that are coming from the model and allow you to do monitoring on top of that. This is the only line you need to add and once you do that for your model, your monitoring dashboards are automatically generated. So for example, if I go just back so it basically starts logging some of the basic metrics like predictions, actual savage manner. You can also have custom metrics that are added here. You just add and it will start logging. Then you can go and see the data distribution in terms of between the actuals and the predictions because you had logged the model training as well and now you are logging the model final output as well. If you wanted to kind of check between two different ranges, like what is happening between one range versus other, you can then see as to where you are seeing certain changes in distribution for the data. You also kind of are able to monitor the drift for that. And if you want you can set up alerts to kind of send okay, if the drift is greater than this then send me an alert on the slack or something. You can go also to the level of raw data in terms of analyzing as to which parts of the data is actually leading to the drip. So this monitoring dashboard automatically it is generated. You don't have to do much. The alert part is right now quite manual. But this is something we are thinking of improving upon further as well. What is the model summary showing? Can you go back to that? The model summary is just a basic set of metrics. The total predictions, total actual, Savagement square, precision recall, all of those things for this, like we have logged this three. So it shows that. These parameters that need to be added through the code or you have pre configured it or how does that happen? So for some of the models it's pre configured like some of the basic metrics. But depending on the new type of models that our clients are using then we kind of add specific customer metrics that they might need and we have the flexibility that your team can also add those metrics okay. Yeah. If they write like a python function that could directly be fed into the monitoring dashboard and create this metric, that's something that we've been building. We just need to define the function. That'S roughly at a high level. The demo. I just wanted to show you the different parts. We can dive into any specific part as well on another call because I know we are also out of time and more time than needed. But I wanted to hear from you, like your thoughts on which parts do you think could be useful. Do you see a utility at the current stage? How should we think of this? And we are also looking for early customers. So it will be great to kind of partner and we are happy to also build certain things as per your needs. Yeah. How do you look at connecting to Sage Maker? So, Sage Maker right now the integration is primarily one thing that is there on the monitoring site. So if you have a Sage Maker deployed job, we can actually take that and you can use our monitoring dashboard from that. So that is the part that we do. But apart from that, like if you are already using Sage Maker for your training and so on and deploying, then it's then not very useful because Sage Maker will do a lot of it. So our goal there is to make it even more easier than Sage Maker and be able to provide you the flexibility which your team uses can manage. So we are trying to also provide most of the functionality that you find that you could also make use of here and over Sage Maker. There are some advantages that you get from using the platform, which is like cost optimization. Like we're typically 30% to 40% cheaper than Sage Maker. No cost overhead added because of that. But yeah, if we talk about the model, like the integration itself, I think it's kind of like a parallel product if you have to think about it. Okay. One thing that I've seen a few companies do is they've also taken Sage Maker because it's very complex. They kind of build layers on top of Sage Maker and expose simple APIs for the developer. So that is like in case Sage Maker is something you really want to go ahead and consider, then that is one possibility that is there. We are happy to help in any way there. Right now we haven't done that, but we are happy to look into it and help along the way. But that will be more in terms of helping out. Probably that doesn't fit directly aligned with this product. But maybe that also helps us to build a layer on top of Sage Maker that other clients can also find useful. Okay. All right, thanks both. This was very useful. Yeah, I can totally see the user case. Plenty of use cases kind of for what we're doing here. So I'll talk to the team. I might even have might request you to repeat some of this for the team members kind of for team members as we kind of do a demo and engage them as well. I'd probably ask you, I'll request you to do that. And so useful. Let me engage more teams. And we'll have to carve out like I said, we're going on the Sage Maker journey. So I'll have to talk to the team on kind of how much we can carve out and do separately. Got it. That's fine. It would be great to take your help. And what I would love is instead of doing a very detailed demo, which sometimes can confuse folks also if you tell us these are the three things, if you demonstrate, then we can actually prepare a more customized demo for them so that they are able to delay them and they don't get confused as well. Okay, yeah, that works. One question. I think we discussed it briefly, but I don't exactly remember what is your financial or commercial model right now? Are you guys kind of mixed of service and licensing? Yeah, so I think we mentioned last time it's basically licensing based on number of users. And then there is an additional service cost, which is basically support in which we kind of help build custom features and support the teams as well. How much is your license cost? The license is generally $300 per user per month, and we have a minimum of 20 users, which includes data scientists, Tamil engineers, DevOps, anyone who has access to the platform. So if you would look at an annual level, it'll probably be roughly in the range of. Okay. So that is one model. There's also like few companies who have asked us for usagebased, so they are like we can also think of a usagebased model. That's not something we have done any like because metrics tracking at your end becomes complicated but happy to kind of work around what might work best for your use case. Okay. It's minimum, right? Yes. So that's basically with 20 users, I'm hoping like that is at least the number of users that will end up using. If it turns out to be very less users at your end, then we can have something that basically scales us for the number. Got it. Okay. One thing that will be useful is that you can leave your number. I can coordinate with you offline over WhatsApp or something as well. Sure. I'll email it to you. Okay, got it. Great. Thanks a lot, Bridge. Thanks a lot for the time. Absolutely. Very interesting work. So thank you. Thanks for connecting and doing a demo. Thanks a lot. Thank you.