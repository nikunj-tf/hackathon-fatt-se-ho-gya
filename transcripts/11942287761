Hey, can you hear me? Hi, George. Yes, I can hear you. Can you hear me? Okay, I can hear you. So just 1 second. I'm just trying to join my laptop for the purpose of the camera. Okay, yeah, for sure. Okay, so now you should be able to see me and hear me, I think. Yes, it works perfectly. I did not know about this companion mode. I will go look at that later. Yeah, I am just learned about it too because my audio is always better through my phone, so I figured I would try to join both. Excellent. Okay, well, great to meet you. My name is George. You figured that out. I think you know a little bit about Soroco from maybe reading about us and I can share a little bit more with what we do with you and then we can just sort of hear a little bit more about your product and see whether it can help us. Sure thing. Absolutely. Yeah. So maybe like George, what we can do is we can start with a brief introduction between the two of us and then we can spend the first, what, about ten minutes, ten minutes in the call, just understanding a little bit more about Soroco and like the state of machine learning, like opportunities, challenges, priorities, et cetera. And then we can use the last ten minutes of the call where I can give you an overview of what troop onto is doing, where we are, and then what are the next steps we can decide. Is that okay? That sounds great. That sounds great. Sounds good. All right, maybe I'll introduce myself first and then we can learn a little bit about you as well, George. First of all, I come from a machine learning background myself. I used to be at Facebook where I was building a lot of conversational AI models for their virtual assistant product called Portal. Have you heard of it? I have not, no. Okay, so Portal is like the Alexa or Google, home of the world, and that's where I was driving some of the proactive assistant effort. Okay. Prior to Facebook, I was at a startup called Reflection where we built out recommended systems for the ecommerce industry. So built out a lot of personalization algorithms and then also spent about a year and a half building out a horizontal machine learning platform because we had like by the time we had like five or six different teams building out machine learning models, so it kind of made sense to operationalize these models better across the company. Between quitting Facebook and starting True Foundry, I did spend about a year building another startup in the talent space, and that startup got selected at YC and got acquired by one of the largest players in the world called InfoAge. And we realized that there is very little tech in HR tech, and it was practically becoming an operations only business, which is something that we as founders did not want to run. So that's why we thought that acquisition was a better route and decided to build two Truefoundry. And I moved to the US from a Master, went to UC Berkeley there, and before that I was in India doing my undergrad. And that's where I met my now co founders and Ragnabi Shake. Like, we have batchmates from undergrad, so that helps building a company with them. I will talk about Truefoundry later, but we'd love to learn a little bit about you, George. Sure, sounds good. So, thanks for the introduction. Just as we have background prior to Soroco, I would say it's easiest to describe my entire career as student. So I did my undergrad Master's PhD in Computer Science and finished the PhD from Carnegie Mellon around, like, 2013 or 2014. And during my time there, I had met my colleague Rohan, and Rohan was a PhD student at Harvard and we got together over the idea of Soroco and then co founded it together in 2014, I think it was. And ever since then, we've been very much focused on helping teams and organizations understand how work gets done globally. That for us is done by collecting hundreds of millions of data points across individual desktops in an environment. So what you can imagine is, let's say that there is a department that might have tens of people in it that is within a particular region or geography. We install our product on the machines of the individual workers. But then what we do is we collect digital interactions that happen between them and business applications. So when they might be working with SAP or Salesforce or even a homegrown application, we will observe and capture all of the individual interactions that they take with these applications. So we would know something like the users were submitting a purchase order form and these are the individual steps that those users took. And so for every digital interaction they take, we record a piece of information, we stream that to a server that's hosted in Soroko's cloud or the customers cloud environment. And every individual who's engaged with the product generates about 140 interactions per day. And so all of those interactions spanning multiple users across multiple days, weeks, months, et cetera, you end up with hundreds of millions of data points or more. That what we're then doing is we're analyzing via different techniques to identify patterns in the data. So what we're doing is we're trying to find patterns in the work and align them to business processes by saying something like this user was conducting a purchase order process and it was between these particular times and it had to deal with this particular purchase order. And then what we would also do is identify other users performing that same activity, even if they did it very differently. So they could be performing the steps differently. But all of our learning is to basically say even though they took different actions, they're still fundamentally doing the same process. And then what we do is we build like a map of how the processes get done internally in the organization. So that's our goal. And where machine learning comes in for us is a few different places. All of that information that is gathered from the end users desktops when they conduct digital interactions, filling out documents, we capture things like what was the text and the drop down that they selected? What was the text at the top of the screen when they were interacting with it. All of that is natural language. So the information that we're dealing with is the same information that the users are dealing with. It's all the captions, it's all of the text and the documents and so on. And all of that is natural language. So the first part where machine learning plays a role in our product is your typical steps and vectorization where we want to take all of this natural language, use different embedding models and vectorize that data. And then once we do that, then we apply different models to then learn what are patterns in the data and those are often given as like samples. So what will happen is a user will say something like this is an example of the process, almost like labeling it and all of those interactions and we build classifiers so that we can take unlabeled data and say what process was this? Take another batch of unlabeled data and say what process was this? And try to build a set of classifiers from there. So that's a really high level overview, but that's a little bit more about me and then what our goal is as an organization. Got it? Understood. This is a very helpful background, George. I do have a couple of follow up questions. So number one, just for me to understand like mind map the business a bit better, can you give me an area of impact where such analysis can improve or did improve some business process in the past or something? What's the pitch to an organization buying so Roku, I guess that's what I'm trying to understand. First of all, to build a mental model. Yeah, so what the organizations are typically trying to do is understand how processes are done so that they can make them more efficient in some way. So a very real use case is when we're working with a large supply chain customer and they say, I want to improve how our purchasing process works for getting materials to then be able to manufacture the goods that they eventually sell. And they're trying to understand where the inefficiencies are in that entire process of identifying what goods do they need to purchase from where, and then effectively make sure that that information gets conveyed into the manufacturing process and so on. And they are able to use high level systems to estimate things like delay and EndToEnd time, like, you know, roughly when they need to order all of these things, and when the manufacturing process should ideally start once they receive them, because they have high level records and systems and they have project plans and so on. But how does the actual team go through and perform all of these activities using such a diverse set of applications that they might have in house? They might have to use SAP, they might have to use a homegrown. App. Then they have to go to a mainframe, which is, like, decades old, and at the end of all of this, they get material. But what they don't understand is what happened across all of those applications for them to decide what they might need to buy. And on top of that, all of these individual team members might even have built their own little ecosystem of how they work, like their own custom Excel sheets and trackers. And maybe in those Excel sheets they have all sorts of formulas that help them make decisions. But no one knows this. There's no understanding of how the work gets done. In the end, they only see outcomes like there was a delay, right, or the materials got in on time. But what happened in between, no one understands. Now, if you were to understand that, then you can say something like, look, all of the users have these custom Excel trackers by which they're trying to figure out how the work gets done, what they should order. And maybe if we built a system around that, or we incorporated that into our SAP implementation, or we incorporated that into whatever homegrown app they built, then they can remove all of that manual effort and make the process more efficient. So there's a lot of digital experiences that the teams are going through that they just don't know how to make a process more efficient without understanding how it actually works. And unfortunately, most systems today only gives them a very high level view. Got it understood. No, I think now I understand. I have a good understanding of where this would impact. Cool. Now quickly jumping into like, we'll spend like maybe three, four minutes just trying to understand a little bit about the machine learning models and the infrastructure that you have currently. Right. So George, like, typically, like, do you already have a bunch of ML models in production? That's number one. Number two, are you using any cloud platforms like Sage Maker or something to build and train and deploy these models? And number three, how have you structured your team, like, in terms of people who build models, deploy models, and manager? If you can give me a context of these three things. Yeah, so the short answer to all of them is in infancy stage. So we have a very small number of models today that we do try to iterate on and we do attempt to version and so on. But we're not at a stage where we have tens or hundreds of models that we're trying to decide upon an AB test and all of these kinds of things. We have a few models that we're typically testing and then shipped to production, and then we might a B test two in production at a given time and so on. We don't have any system to track these as of today. So we basically build our models, have them as some sort of binary blob of some form, and then we might keep them in some sort of Blob storage where they're fetched, or we might version them somewhere. But we don't have any system specific to our machine learning models, so we don't use Sage Maker or anything like that as of today. It's something we've periodically looked into and said, hey, when is it worth bringing in this level of complexity to our management? And so on, and hadn't found it yet. And then finally, how are the team structured? Most of the team, it's a very small team, so it's a young company. The total team size that might work on something like this is like five to ten at any given time. And their responsibility is to build those models, understand how they're deployed, but ultimately give a standard operating procedure for the product to fetch the models and to basically run them when needed. And so they are responsible for making sure that those models get deployed to production environments. And the important thing to understand about Soroco finally, again, is, like, we're not a single SaaS product. There's no one place where all of the models get deployed to. They might be deployed on our cloud, on the customers cloud. They might be deployed in test environments and in many different places. Right, yeah. I think that makes the model management actually quite hard, that kind of deployment pattern. So today when people are deploying these models, what does it mean? Is there like an API endpoint that's getting hosted somewhere, or is the model getting shipped somewhere? What does the deployment mean? The deployment means for us, typically, we take the model and we put it into a Blob store location, and then when the product gets deployed, it's typically fetching those models from that Blob storage location, and then they're fetched and retrieved locally, by which they are then loaded by the product. Understood? Okay, I see. So in that case, George, one of one question here is that the model file itself, I understand, is probably stored in some Blob storage, like an S three or something. But when you load the model, there are pre processing and post processing functions that you need to be able to accurately run and predict the model and stuff like that. Right. And that usually needs to be in sync. Like, for example, the order in which you pass the features to the model usually needs to be in sync with your training code as well, which I'm assuming now is living separately from your actual application code. So how do you ensure the consistency between your training and serving code? Like the preprocessing, post processing logic, et cetera? Yeah, I think for us, we are not changing these very rapidly or agile or anything like that. What we're mostly doing is we are shipping new versions of our products every few months and in that time period we are ensuring the version of the product and the pipeline that basically takes that data, preprocesses, et cetera, is all lined up with the model that will ship in production. Understood. Yeah. So I think it's very basic in that. I see. Okay, that makes sense. And are you a multi cloud given that your product is deployed on the customers cloud as well? We are, we are Azure and AWS and we are deprecating any sort of on prem cloud style deployments. Okay. And then do you also plan to migrate to enable GCP as well at some point? Probably not or no time soon. We just really don't see any customers on it. Okay, understood. Got it. And one last thing is do you all use Kubernetes at all? We do. So that's something that we've been building up and most things are migrated to it. If not, everything will be in probably a few months from now. Okay, understood. So I think that's all the question that I had in terms of the infrastructure and the last question that I will ask you before I get to describing what Truefoundry is in your mind in terms of building, deploying, scaling models, et cetera. Is there any particular area where you feel like the company is putting an effort or should be putting an effort? Any pain points that you have identified in this process? I am not the one who will typically experience the pain. So I would say it's probably best for us to figure that out with one of the engineers who builds it and deploys it and so on. I would say they'll know it, I pass the pain on to them. So we should figure that out from them. Okay, understood. I think we can probably set up a follow up call with them, but let me take a few minutes and describe what we are building. George, what's the intent, what's the problem that Truefoundry is trying to solve. Okay, just so you have some context as well. Yeah, I have a hard stop in ten minutes just to make sure that. I will not take any longer than that. So first of all, I'll describe you like the motivation of why we are building through foundry. So I realized that when I was at Reflection, we had like five different teams building out machine learning models and we ended up building a horizontal machine learning platform for ourselves. Right. Which is like just operationalizing models. How do you create features? How do you deploy models, how do you do batch and printing, how do you do monitor models, et cetera. And then I realized that when I joined at Facebook, they had every learner predicted type of platforms that made it really easy for us to build more models, cover more use cases, deploy things fast, monitor things fast, et cetera. Then when I came to building out my first startup, we were again building out machine learning models and I noticed that the same problem that I faced at reflection I was facing here again. So the realization was that almost everyone who ends up building models, at some point when they scale beyond a certain level, they end up in a similar set of challenges that, hey, how do I dockerize my model? How do I engineer containerized my model? Quickly get an API endpoint out of it. Like, figure out the server, like, boilerplate code that I'm writing for servers and stuff like that. How do I make sure that my data is not drifting? How do I make sure that my model performance is not degrading and stuff like that. Right? All of these challenges people end up running into sooner or later in their machine learning journey, basically. And of course there's all these more advanced things like how do you version all your models, how do you version all your services, how do you ensure that the versions are in sync? If you have to revert, how do you revert those things? Basically CITD of models, et cetera. All these advanced concepts are always there that people run into challenges. Now, there are a lot of platforms that are trying to solve this problem, right? There's like a sage maker that's trying to solve this problem, vertexure ML that are trying to solve these problems. We are clearly not the first ones to identify this as a painful. Now, the focus of what we are building on top of George is that none of these platforms out of the box can serve use cases of every company out there. They will always need to build out some thin layer of customization for themselves and that's where a lot of the existing platforms break. That's the core focus area for Truefoundry where we are building out a platform that enables companies to build on top of so they can use the vanilla features out of the box, no problem. But if they wanted to customize something, for example, if they are using their own authentication layer and they wanted to integrate that in our platform, they could do that if they are already using a dag orchestration service. So, let's say if you're using Airflow and our platform is designed on arbor workflows, you could use your Airflow basically to orchestrate the services. So basically play nice with your existing setup as opposed to asking you to build your models in a certain way or use certain tools in a certain way. Basically, that's our core value proposition. For why we are building out this platform. Okay now in terms of first of all, do you have any questions, follow up questions from that? No able to follow so far. Okay now in terms of the focus area as a startup you can't be solving all the parts of machine learning pipeline. So we are actually narrowing down our focus basically. So if you think about an ML development workflow from your data feature engineering left to right essentially data feature engineering, hyper parameter tuning, model training, deployment monitoring, et cetera. We actually come at the model stage that you could be launching your model training jobs on our platform. You could be deploying that for batch inferencing async inferencing real time, inferencing basically any of that on the platform very easily and then close the loop we are monitoring. So if your model performance degrades you can pick up free training loops like basically just generally have like a dashboard where you can monitor the model performance, data drift, et cetera. That's the part that we are focusing on now in this journey. We are building it in a cloud agnostic fashion. So we are actually doing everything, infrastructure as a code, everything. We are trying to use open source technology like Kubernetes which will run on every cloud, right? So that if people want to build out models and ship it to their customers across different clouds they don't have to work with multiple cloud providers. They have a single pane of management. They can optimize costs across clouds and stuff like that. That's the approach that we have taken to build out the the company in terms of the maturity of the product. By the way, we are a fairly early stage company still like about a year and a half an hour journey in terms of the maturity of the product. We have built out the core parts of the platform such that it's already in production in a couple of enterprises, very large enterprises, like $50 billion plus enterprises. So we have gone through their security reviews, they have deployed it, they have started deploying services in production. But the way we are working with them is what we call as a design partner mode. I'm sure you might have worked with other companies in design partner mode as well where they use your product to write feedback and then you build out more integrations for them essentially. So that's where we are. Our goal is to continue working with a handful of companies for the next nine months or so where we really handhold them, make sure that anything that they need we are kind of going out of our way to solve for their problems. And then eventually, once we are a bit more mature, we go general availability where these nice cities would obviously drop when we start scaling essentially in terms of our funding and all. We are funded by Sequoia and we have gotten some very key strategic angels who are helping us in this journey. So like ML platform builders at Facebook, Snowplay, Cooper, Google, etc. Have joined us as angels and advisors. So they just ensure that we are not taking any wrong design decisions, basically. And also some folks who help us on the business development front as well. So that I think, is the overall view of where we are as a company, what problem we are trying to solve. I'll take a pause here to see if you have any follow up questions here. Got it? Yeah. So what is like the basic interface or usage to the product? Is this something that is mostly like CLI? Is it something that is like a user interface? How does it basically integrate with everything else? Yeah, for sure. We basically have three modes in which developers interact with the product. One is the CLI. So you have UI actually get like a command line library that you can install and then you can start invoking like completely. You can play around with the platform, with the CLI. The second is a Python library. Given that we are machine learning focused company, python is the most common language of choice. So we built out an SDK in Python that you can install and get going, basically. And the third is the GUI. So the platform itself has a web based UI that you can use to deploy manage monitor models. Of course, the dashboards and all are all on the account when you log in display. In terms of typically how people get started, there are two modes in which people use our platform depending on their level of comfort. One is what we call as our public cloud mode. So like our SaaS mode in some way where you basically get an API key on our platform, you sign up on the platform, get an API key, and then immediately you can install the library, start pushing in metrics, deploying models super quick, right, to get started, basically. And the second mode in which people use our platform is take the entire platform and deploy on their cloud. Basically, at least the data plane gets deployed on your cloud. So that way you ensure that none of your data ever leaves your cloud. That's the most important part, basically. Right? And if you had some cloud deals or something that you wanted to make use of, you could do that, basically. So that's kind of how the platform gets deployed today. Got it. Okay. No, makes sense. So here's what would be helpful would be helpful is if you have any material that you can send me that I can circulate to the team that works on this. I think what we need to do is we need to identify our ML. Aspects of the product are continually growing. We're also trying to consolidate how they're happening across multiple teams. So there's sort of like pods within circle who might be focused on different models or different solutions. And part of bringing those together is something that we definitely care about. And what I'd like to be able to do is just kind of like circulate this throughout the teams, have a conversation with them, and sometimes starting these things earlier before you desperately need them is better than waiting until the point where you desperately need them. Right. So even though we're at maybe like a stage where we're not feeling the strain or stress about it doesn't mean it's not helpful for us to put those practices in place early. So that's what I'd love to see and pass it through the team, get a discussion with them. And then if this is something that makes sense, I'd like to bring someone else from the team on who, like you were inquiring about, experiences the pain that they probably shield me from, so that way we can see how the product can help them as opposed to me. Sure, absolutely. I will definitely after the call, I will share some material with you over an email, like a small writer, maybe a document which describes what we are building and probably even a demo video or something. Right. So I'll share those with you. And I also understand that this is not a critical pain point that you're trying to solve right now, George. And honestly, I'm also in no rush to move this immediately forward so we can take our time and figure out when the need is, when we have a use case where it feels like, okay, the need is urgent. That's when we can start, like, more serious engagement. But meanwhile, we would love to keep in touch, understand where the pain points are looking like, and basically if there is even a venue where we can help and avenue where we can help. Right. So I would love to understand that. And in that regard, one thing that could be helpful, regardless of the fact whether the team thinks that it is immediately useful or not, it's just like as a saver that maybe you can help me out here, is set up. Like one follow up call with someone from the team who has enough visibility into building and deploying models where I can just get a sense of where the team is, what are the pain points, and we can always take everything else from that point onwards. Sounds good. Okay, that sounds great. Perfect. So let's do that. We'll do these two follow up items. Sorry, go ahead. Yeah, so just follow up with those and then I'll discuss with the team and then we can schedule something again. Sounds good. Awesome. This is great. Thank you so much, George. Yeah, great to meet you as well. Take care. Nice to meet you. Have a good one. Bye.