Hi. Hello. Hello. Hey, Julian. Is it Julian or Julian? Julian. Yeah. Is it badal? Yeah, badal. Oh, call today? Yeah, it's anuraag anuraag. Anurag. Okay. It's not anurag. It's automated note taking. Let me take this out. Oh, this is a note taker. ENIAC is joining now. All right. Hi ENIAC. Hello. Hi. Are you able to hear me? Yeah, we're able to hear you. Hey. Hi, Julian. How are you? Hi, I'm great, I'm great. Hey. Hi, Ivan. How are you? Hi, nice to meet you. Yeah, thank you so much, Jebien and Julian, for taking time for the call. Really appreciate the same. What we can do in this call, since we have 45 minutes, we can spend first five minutes just for a quick introduction for our side and also love to know a little bit about what you and Julian look at in Eniacventures as a part of the platform. And then we'd love to understand a little bit about your machine learning use case and the kind of info you have built at your company and what are some of the challenges that you still see and post that we can go into a little bit about how we are thinking of building and what we are building. Plus if time permits, we can do upwork with them. Otherwise we can pendo you the follow up call as for the need. Does that make sense? Okay, sounds good. Okay, great. Could I record this call if that's fine with you all. If you recall, I may want to blow my screen then. Okay, I will just skip the recording in that case. That's fine. No problem. We'll just take some notes. That's fine. Cool. So before we get started, like just a quick introductions, bother and myself, we are both from the founding team at Truefoundry. My background is more in portfolio management. I used to work with H fund called World Fund, initially in the US and then in Singapore where I was looking after their portfolio management arm, managing around 600 million assets for them and then build my first startup in the Talend domain. We sold it to Info and then we have been building Truefoundry over the last one plus year parth Truefoundry. We are right now a team of around 16 fulltime members, most of them engineers, primarily based in India, but we have folks in the US and I'm right now in Singapore and then we have a few folks in Paris as well. So pretty much a remote team. The goal is really to make it easier for companies to do their ML deployment. So companies pendo a lot of time in building a platform for getting their machine learning workflows up and running in terms of the deployment of their models and monitoring. And we want to be able to build a platform that enables them to get started faster. And the background comes from our experiences that companies like Facebook and Amazon where we have seen internal platform teams being built and we want to provide a similar platform to other companies so that they don't have to rebuild it themselves. Like even I saw you were at Uber so you might have seen Uber's own internal platform, Michael Angelo or you might have experience with it so you can think of it like what Uber's internal platform was doing. It was obviously more customized to Uber but trying to build a more generic platform that is applicable to other companies that sep a very high level inderjeet so that you have the context with the call but maybe it will be great if you can also quickly introduce your background. You are on mute. Yeah. Hello everyone. I graduated like eleven years ago. I've been working as a software engineer ever since before Intro Nikunj Truefoundry I was with a company called Phone Pay where I was managing petabyte scale of data warehouse for machine learning and other work and before that I was part of AWS where I worked on different like AWS services like Sage maker Comprehend and Code Deploy and others. So yeah, that is me. Daniel is trying to join do you guys have to let him in? Anil okay, I think he's here. Hey. Hi Daniel. Oh, hi. We were just doing a round of introductions so me and buddy from our side, we introduced. We'd love to know a little bit about your team as well, Danielle, even in Chirginga. Yeah, sure. So we are from Nexus. Nexus is part of the SD eniacventures arm of Standard, just a bank. You can imagine Nexus is pretty much operating. What we are trying to build is pretty much what the digital bank offers. So we are banking as a service platform. It's been announced in the news that we have partnered with Boklaba. So imagine Boca Japan as an ecommerce player. They want to become a bank. What they can do is that they can partner with Nexus and then using the Nexus stack upper can offer Casa accounts, we can offer credit card debit cards and no products for example. So in a nutshell, this is what Nexus is trying to do is banking as a service platform within Nexus. We are the data science team. We are not a big team, there's just three of us in the moment. So I'm the data science lead, anil is the senior data scientist and Julian is the data science analyst. And how big is the team at Nexus if I may ask you? Sorry, say it again, how big is the overall? Yeah, currently it's around $150. Okay. And is Nexus like a pretty old company or is it like a separate company that Sep Ventures acquired or is incubated by Sep Ventures? Yeah. So nexus was started within Se eniacventures. We started 2019. So we are still a very young company and in fact we operate pretty much like a startup. Okay. We are part of SCP. Okay. That's great. And you are based in Singapore, I'm guessing. Yes, that's right. The central team is based in Singapore. We have a bunch of developers based in India, Poland, but the main team is in Singapore. Got it? Cool. Understood. And if someone from the team can throw some light on the use cases for, you know, when you started this data science team of members, like, what is the plan for further growth of the data science team? And also what stage are you, what kind of use cases are you currently looking to solve within the data science team? Right, so in terms of the growth currency, we have just launched our product in the public domain. In fact, it's just launched this week. So we are getting the users to sign up and then we are collecting the data from all these users. With the data collector, we can of course use them to build further data products. Some of the data products we have looked at, for example, Churn prediction, for example, for user, given his transactions, how likely will he be churning from our platform? If he decided to churn in the next two weeks, we want to target him with some incentives or some notifications to ensure that he sticks to our platform. Other products that we have looked at also include dashboards. So we, Shane, built a number of dashboards to monitor the metrics for the business and then also to do all this to schedule all the jobs. We use Airflow and for the dashboard, we use Superset as the tools. Okay, for the dashboard, what would it be to you? Okay. And right now, these models that you are building, either for Churn prediction, etc. Are they already in production or are they running more locally in a test environment? So for the Gong, we have tested it in a local environment and we are planning to put into production. Okay, got it. And are these models I'm guessing this model will be more offline use cases, batch kind of use cases rather than real time or how do you kind of think of that? No, this model should be in real time. Okay. This model has a micro service, and once it is called, you perform Chris function and return the results. Okay. And if you can throw some light on the stack in terms of what is the deployment stack you use, currently you have not deployed, you will be deployed. That I understand that. But in general, what is the stack you are looking to use to kind of deploy these models? What is the current stack within Nexus? Yeah, for the stack, maybe we San Jose in the data engineers CTO explain in more detail, but in a nutshell, well, our data leak is on S three, and then we use the Kubernetes platform to deploy our microservices. Okay. So I think that's why we are interested in your solution to understand a little better of how your solution can complement the current platform that we have. I have a question. Have you worked with bank before? We haven't worked with banks. Like just very quickly I'll mention Danny, we are also pretty young company, we have worked with a few health care companies, but we haven't really worked with banks. But in the France that we are in, we have built a platform in a way that it has the security requirements that is mostly needed and we are happy to work closely with you in case it fits your requirements to ensure that we are able to deliver a faster experience for you. If that question was intended towards more trying to understand how we work. Yeah, the reason why I asked is because I've been in banks and in other environments as well, startups, and I see that they are actually quite different in terms of what you can do in the banks. It's not as easy as what you can do in in the startup where all these things that you can easily do in a normal startup. You probably find that it is going to be difficult, very difficult in fact, when you want to do it in the brains. No, totally understand that. And that's why anil trying to get more context on this call to see if there is a possibility where what we are building aligns with what you need. And if you would want to explore this further, if it turns out to be in a place where it is not aligned, then obviously it will not work. But the goal is to try and understand and get to a better understanding there first. Got it. Cool. So understood even I think one more question is in terms of this deployment, do you foresee data scientists team itself doing this deployment or do you actually think that once you build the model you will hand it over to someone else like data engineer or develop person and they will be doing that? So currently we built the microservice in the local environment. Once we have tested it in the local, we pass it over to the DevOps the production line in the cloud. Eventually of course, if there's a tool available to allow us to easily deploy into the cloud, we will be interested as well. Okay, got it. Understood. Cool. And for monitoring dashboards, are the dashboards that are specific to these models or the general business dashboards as well that you are kind of building there? So for the dashboard, it's a general business dashboard to monitor the business metrics. Okay. And then for your training of models, are you kind of doing it locally even or are you using some sort of hosted notebooks et cetera, or virtual machines? Okay, sorry, even I lost you. It's done locally. The training is done locally. Locally. Okay. Are there other questions you have here? Just the framework, like which framework do you use for your models? Is it cyclic? Learn yes, correctly. I presume that you don't need to use GPU because it's only cycling currently. Not at the moment. Okay. And are there other use cases, like future use cases which will come for the machine learning? So we are very interested in customer lifecycle management. So Churn prediction is one model inside this whole management framework there could be other models, for example propensity to buy models. How likely a user will purchase our Nexus product and then gong the lifecycle? How likely would this user transition from a high value to a low value customer? If he's going to withdraw all his half of his deposits in the next two weeks, definitely we want to know in advance so that we can give him some kind of incentive to keep his deposits with us. Other than that, we're also looking at for example, servicing use cases. So given the user when he uses our app to look for particular article such as how to update your mobile number, whether we can locate the correct article for him given out of the 100 FAQ documents that we have. Got it. One question here is like in this process of building models and then actually testing them locally and then finally taking them to production. Is there a major pinpoint or a major challenge that your team actually kind of is facing with respect to any capability that you specifically need and is currently not there with respect to maybe the time that is going around something would like to understanding the shane? Yeah, I think so. We are looking at for example for data science we focus mainly on the modeling aspect. Then eventually when we want to deploy the models, it has to be handled over to the DevOps or the data engineers. So given this requirement, the DevOps or Datta engineers could be tied up with various other tasks. So we are looking for a tool whether it is possible such that the data scientists can use and immediately deploy into the production environment. Like you mentioned. Uber. Is Mike Angelo to do this? So is there something similar that we can use off the shelf? Fair enough. Understood. I think that's exactly the kind of problem statement that we are trying to solve at two even. Very excited to hear this, that it aligns with what you're looking forward to. What I can do is I have a presentation which gives you an overview of what we are doing. So I think I can walk you through that. It will give you some context of which parts of the pipeline we come into the place and how we are thinking of doing or how we are doing some of the things. And then around that like happy to answer questions before so that at least we are all on the same page. Will that be okay? Yup, sounds good. Let me shane my screen. You can see my screen? Yes. So I'll just go ahead and basically our goal is CTO enable data scientists to be able to take model to production in a faster way and do it in a way that is scalable, cost effective and with some basic monitoring that is in built so that they don't have to worry about failure of modules that are installed. Like I'll skip this, but basically what I'm trying to say here is if you look at the machine learning like cycle, it consists of Chris seven stages and this part rely on the data engineers at a lot of times. Obviously data scientists do some feature engineering in some place, but not always. And then after this, like the pipeline which is model training, serving up the model monitoring and ensuring that if you have to retrain models that compute, automation is present. It's something on the data science team. We come into the picture here, like in the model serving kind of stage which is basically the deployment piece of the pipeline. And as you mentioned, this is a time taking process, a lot of time. Data scientists and other team members, if they have to deploy, they have to build the infrastructure and building the infrastructure to take it to production takes a lot of time, like in some cases it might take a few months. Whereas if you saw it Uber it was very simple and you can just quickly productionize something we have built. So we want to be able to solve for that problem. Second is we want to ensure that the developer experience your team is very easy. If you are starting on deployment tool today, it should not involve a big learning curve. You should be able to just lead through the lines and you should be able to execute it and it should happen within 30 minutes. And finally, we want to ensure that the models that you deploy don't break going forward. So right from day zero you are using scalable engineering practices. So examples like companies like Facebook, Google, Uber et cetera have all invested in ML platform teams. But building this ML platform team from scratch takes a lot of time. Just to give you an estimate, it takes roughly four ML engineers a year of time to be able to build something decently good, that can scaler. So now that's a huge investment for a company and instead of trying to do that and waiting for that entire time, we kind of get you started in a simple way. A brief background about the team I already talked about myself. Bathal. Again, Abhishek and Nikkun co founders like Nikhunja and Aviate, both of them were at Facebook. They have seen the system at Facebook and then the rest of the team also comes from companies like Gojek, Amazon Reserve Postman where we have seen how they have built their internal platforms and we want to bring that to other companies. We are also advised by some good advisers like Anthony who is the founder of Kegle and Abyssia, was actually the first creator of the FP Learner platform within Facebook for productionizing of models. So yeah. Basically the goal is to enable your teams to kind of get models to production faster so that the time to value for them is reduced and you can focus your time more on creative parts which is actually trying to build a different kind of models rather than worrying about the infrastructure and spending time coordinating with different teams for operationalizing the ML processes and the way the platform is built. The simple practice we have followed is it's easy for you to get started like Hiroko, but you can scale and the platform scaler just like any other bigger platform, like an AWS system, et cetera. I will skip this once. Basically we try and cover different parts of the pipeline like training, serving and monitoring, but the major focus is still on the serving piece and that is what I will try to COVID in today's presentation. Design principles. This is just to kind of ensure that right from day zero we are not building things in an ad hoc manner, but building it in a way that is adhering to poor design principles that apply to larger organizations as well. And it makes it easier for you because for example, everything is fully API driven, everything is treated as GitHub in terms of the configuration as the tops. We kind of make it self serve for you to do most of things. There's complete access control on your hands so that you can control who views what and you ensure that there's no flow of data anywhere else. We provide you with cost insight so that you have visibility into the channel and what is being used, to what extent and so on and everything is reproducible. So from a perspective of being able to use the system, we make it easier for you to adopt and use. Now, coming to the two foundries system like I will talk about this deployment piece. So our central infrastructure is the deployment infrastructure. Just like you team. We are also built on top of Kubernetes and we integrate very easily with your own Kubernetes clusters. So we have both cloud offering as well as an On Prem offering behind the hood. We use systems like TerraForm for infrastructure as a code. We integrate very easily with your CI CD pipelines and we also expose the layers for DevOps teams to manage their info. The deployment infrastructure you can run from your Jupyter notebooks if you want, or you can run it from your CLI, also from your UI. It supports both the YAML as well as the Pythonic way of deployment. The simple way it works is you just bring your model to it either as a predict function or you can load the model from our own model registry and once you do that, you can run a command and you can deploy your model over our over our infrastructure. It integrates very easily with your infrastructure so that everything can decide on your end and there's no issues of data privacy and security as well. Once you deploy your model, we also provide a system wherein you can actually track the monitoring of your models like how they are working. And we also kind of provide ML monitoring. Like if you want to actually track the accuracy and performance of your model, including the feature help, you are able to do that. So the code system is built around enabling you to do deployment faster through whatever mechanism you prefer YAML Python and through different things like CLI Jupiter notebooks or like UI interface and enabling quick monitoring for you so that you can actually track your models all at once. Any questions so far? Otherwise, I'll go. Chad. No. Can you explain a bit more on the experiment tracking box? Sure. So this is one part of the system that is there wherein if you are running different experiments for your models, we integrate with other experiment tracking tools that are available in the market like an ML flow order, weights and biases, et cetera. But at the same time we have a support for a basic experiment tracking system in our side as well. So if you want to ben able to track all the different versions of the model you are trying, you can do that in our interface and then once you kind of track and log into our interface from there itself, you can deploy. But just I want to be clear like this is not necessarily a differentiated offering, this is something that is just market standard. So if you have used other experiment tracking solutions in the past, this is kind of similar to that. But yeah, it allows you to just integrate seamlessly with our deployment platform and so on. Can I ask, when you mentioned the experiment, do you mean the AV testing of customers or is this just about the different models that data scientists develop? Yeah, this is more the offline experimentation. So this is when data scientists are developing and they want to log their models and be able to track that at one place. This refers to that we do have ability for traffic splitting and shadow testing in our deployment infrastructure, which is the online experimentation platform, but that is in itself a part of our deployment infrastructure. So you can do that. Any questions? Which part is more interesting? Like just I want to double click here before I go ahead. From what I understood, it seems like this part where you can easily deploy a model and test it out and even release to production is something that is more important for you, right? Yes. For doing the experiment you mentioned about ML flow on the user Pendo. Is it in the form of Jupiter notebook or is it a development environment? Where we can maybe access the CLI or anything else. Okay, so for this, basically say you don't have to change your current workload. Suppose you are working today on jupyter notebooks. Then if you wanted to access this, all you need to do is just install our package and then you enter a few lines in your jupyter notebook where you are working. And then you get access to a UI where you can go and track everything. Right, okay, got it. All the system works from wherever you are operating, but at the same time there is a UI on top of it which you can go and track things, log things where you can have access control or et cetera. I think it would be much more clear if we saw the demo. Yeah, Surge week can actually address some of this when we actually do a more deeper demo as well. Okay, cool. I will go ahead. Once like the workflow of a user using two Truefoundry. First part that you as a data scientist do is training and tracking your models. So here you can continue to train your models in the way you do it. Like you write your training code, you can run your training job on a laptop or on your cloud. And you can actually, if you are deploying your training jobs, you can also do that using a simple command like this, where it's simply like SFY, deploy, job, train and so on. If you wanted to kind of actually log your models, you kind of log your metrics params and models using simple commands like x, dot, log, model, run, log, params and so on. But again, this is only if you want to kind of track and log you models. And once you log, you have a dashboard where you are able to go and track all of these things at one place. So this is the UI part where you are able to kind of sep the structures like projects, and within projects you have several runs and you are able to compare between them. If you don't want to use this, that's completely fine. We go to the deployment wherein whatever model you have, whether you are running today in you jupyter notebooks, whether it's in your CI, CD pipelines, all you need, CTO need to do is just import your model into our UI or kind of write the predict function and decorate it with this decorator. And then after that you run a command like two foundry deploy and it goes and deploys to our workspace. It deploys to a place which is actually your own cluster, or it could be our cluster, depending on what you want to do. It allows you to auto scale on GPU as well as CPU. You can track everything in a dashboard. Like this is the concept we have. Like suppose you have two teams and you kind of create a test environment and a production environment and in the test environment you are just doing service endpoint creation and basically using it for testing and exposing it for the team members. And when you are sure you kind of move it to say a production environment and that deploys on your own cluster, then you can do that. You can use this in different ways. And for any service that any model you are deploying, you are able to see everything like your endpoint, how much memory and other things it's consuming, the metrics and logs. Then this is the part which you are talking about, which is online experimentation. You can actually do shadow testing and traffic splitting easily for your models by kind of adding a few lines to your code as well as there will be a UI thing that supports it as well. If you want, you can deploy your model in a serverless mode as well. Then as you deploy, you get this system monitoring by default over the pana. So you don't have to create this. It automatically comes at this list like the metrics that if you click it will lead you to this system monitoring. And then you get some insights on cost. Okay, for each model, how much cost is going on and you can actually track it for the team. I'll just show you one more thing here this entire thing, everything is available both as YAML configuration as well as a Python configuration. So you can just write simple fast API code and deploy both by YAML and Python. These are some of the use cases within deployment, like deploying as a fast API, deploying a training job, deploying a batch in France, deploying a model with artifacts, deploying a model as multiple microservices, then doing traffic saving, which I was discussing about and how it works in different slightly more detailed here. Then we have system metrics which I sold and then if you want to retrain, that part is also there. So just kind of giving you an overview of the system at a high level. Once this deployment is done, you can actually also monitor your model. So in your model you'll log a specific line of code like log prediction and then you are able to actually get access to monitoring metrics like your model health if you want to do it. If you don't want to use model monitoring, that's completely fine. You can just simply use deployment and that's fine. This is just additional on top of that. Yeah, I will kind of go into a few more parts. As I said, we kind of deploy on both your own. Like you can use our cloud or you can deploy on your private cloud. We make it very easy for you to deploy on your private cloud as we give a health chad and that can be easily run by your DevOps team and it will just connect with your cloud. We provide full support here so that anything. If you need any help, our entire team is available to you. There are a few things around privacy and security that we also do which is like we ensure that data never vivek your cloud environment. Everything can be put behind a VPN and any security measures that are taken on the service on your private site can be there. We have the infrastructure that was reviewed by an external system as well. So the security testing has happened. Everything is encrypted as Rest services. Communication is done on Https so that way it's secured and there can be storage policies etcetera. That your DevOps can also kind of easily customize. Yeah, I mean I just wanted to give you a high level overview. I'll stop here once just to see if there are any questions that we can answer before going further. I think you showed the API usage for the machine learning model. So what about we wanted to run, let's see on a scheduled basis. For example, we want to get the Chad predictions of our customers daily at 05:00 p.m.. Can we do Chris scheduling? Yes, but I don't can you answer this? Sorry, actually I missed the question. So you're saying that you want to do like you want to schedule a Churn prediction for a particular customer or you want to schedule a Churn prediction for a batch of customer? I want to schedule a Chris prediction for the entire Nexus customers. Okay. So at this point we have this. In our beta product which will release in one week. So we will make it production ready in another week. So in that product we allow you to schedule any batch job training job or anything like that. Got it? Yes. So that is actually supported as a part of the system, it's just not released publicly even. But we have been like there are a few others that chad this requirements as well. So we have actually kind of built it in our interface and will release it. We are doing some testing with a few customers as well and we should be able to provide you that functionality. To further even questions. Right, let's say we have this scenario earlier, even say that the Churn prediction is supposed to be accessed in real time. So there is some model actually running ready to receive the request. Then we also want to update the model, right? The model gets overtime. So let's say we have a once a week schedule to update the model and then after we finish the training, normally we need to compare the model first before we say okay, this is good. Or we can also automate that criteria whether it is good or not. Then we deploy the new model to the production if it is good. So what kind of workflow do you have for this kind of scenario? Okay. So do you want to retrain periodically or do you want to retrain when there is a data drift or loss in the performance. IBM fine with either one, but I think the easiest one will be the scheduled one. Right? Because the other one is you need to calculate some drift or you need to calculate something. When it happens, it triggers the retraining. Yeah, so we actually have both. But the second one is our beta product, which we are working on. We can discuss it later. But for the first one, which you mentioned that you want to schedule a training job. So yes, you can schedule a training job on our platform and if you're using experiment tracking, either ML Flow or our experiment tracking, then when this training happens, you can log all the important thing on the experiment tracking and you can log the model at the end. And then if you want to do it manually, somebody can just come and check on the experiment tracking UI, which more like the retrain, whether the retrain model loss or logistic loss is up to the mark or not. And then based on that, you can choose to decide to deploy that or not. And we are also working on that is, again, our product is still in our beta. We will be working on that. So that is building a complete approval loop. So what we are trying to build is the complete workflow where you can define, let's say you have one manual step where you want to send an email to a team of developers and then they can come to the platform and just click yes. And then the next workflow which is to deploy the model will trigger. So Unrak has seen the example of that, has shown the example in the PPT, but that is still in our beta. So we are working on like more, more and more ways for people like more and more ways in which people retrain their machine learning model automatically retain their machine learning model. So we are building those capabilities. Got it. So let's say once I see the result is good, then I want to deploy the new one. I believe the old one is still running, right? What is the schedule to actually ramp down the old one and then ramp up the new one? Yeah, I presume you would be using if you want to do it using the standard procedure, you will do it using CI CD. Right? We have support for that, which means that in your GitHub so in your GitHub, like, we have examples in our documentation how to add GitHub action. So in using that GitHub action, anytime you post a new commit, it will go to the production. So you can set up all of that. And as soon as, let's say if you're using Model Registry, then you will go to your code and update the model ID which you're using. And as soon as you commit your changes, it will automatically take care of the deployment. So this is when you want to keep track of keep track of the production using GitHub. Right. But we also provide other ways where you can just go to the UI, click a model and just say deploy. But here you will not have a GitHub commitment, things like that. So it is up to you. Like we provide different ways to deploy the model. Got it. So when the model is actually running into production right, do you actually take a note of which model is actually running that we can actually track? Because I believe some user may still be served by the old model. Not yet. By the new model, for example. Okay, so the deployment, like we support different deployment. The standard deployment which we support is, let's say your model is deployed on, let's say, ten instances. Then we will deploy it one instance at a time. So let's say if you have ten instances, first your new code will get deployed on the first instance. Then if months succeed, then the second and third, then fourth, and within half minutes, all your instances will get updated. It's very fast. I see. But you need to make sure the Gracious except for the old model rate. Sorry, the Gracious, you mean. The stopping of the old model? Because sometimes the old model is still serving some customers. So we don't want the connection to be just deleted or stopped. Probably. Okay. So there are two ways to handle that. One is like adding a hook in the code, sep down hook in the code, which means that we will not force like we are using Kubernetes. Our system is built on top of. Kubernetes. And you have to just handle that sigma and complete all your existing machine learning inferences and then stop the process. So I can give you example for that, like how to properly attach sick term and things like that. But we are built on top of Kubernetes, so you don't have to worry about that. It would not be a four skill. It is going to be a Gracious skill. And your model will have like your service will have time to stop itself. Got it. Okay. The other question for me is do you support multi multi Kubernetes? It could be private cloud, but on a different location. Yes. So our system is like completely built in a way that you can have as many Kubernetes cluster as you like, and you can have Kubernetes cluster from different cloud provider. So let's say you want to use AWS for some part, you want to use GCP for some part. You want to use San Jose. So you can have like three Kubernetes cluster and integrate all of them into our platform. And then you can just deploy your service on any one of those clusters. These are, by the way, really good questions. Actually, it's exciting to hear this because it also kind of reflects on how you are kind of building your system in a nice way. So thanks Daniel and William for asking these questions. Any other questions? Otherwise maybe what I can do is since I'm guessing we are actually at the end of the time, would you have some time in the coming week any day that works for you where we can actually do an overview of the platform including a live demo? Yeah, I think that sounds great. Maybe also you can send us your slides and then we can forward it to the data engineers so that we can get their thoughts as well. And then if there's also interest from them, we will get them into the call as well. Just one more question I have regarding you tool. So there are various competing tools out there that serve this AI ML platform function within the bank. What the bank has been using is Data IQ and Datta robot. Also there are other tools such as H Two o can you share a bit what differentiates you CTO from these other tools? Are they in a similar space and what is the advantage of your tool versus the rest? Yeah. For H Two, H Two was more like an AutoML platform and then they added the capability of deployment for their AutoML generated models and they also like nowadays they allow a lot of different things. So the way we are like there are a few parameters on which we have different. So one is that we are solely focused on that deployment side and we are making our deployment system as generic as possible. Which means even if you don't use H Two, no matter which platform do you use to train your model or which framework you use to train you model, we will help you deploy that. And the second thing is that we are built on top of Kubernetes which means we are Cloud Agnostic. By cloud Agnostic meaning you can move your service from one Kubernetes to other Kubernetes service. If you have multicloud strategy then it's going to be a very big win for you. And the third is that our whole system, we have health care companies as our clients, right? In Europe Healthcare has a customer requirements for data as banks too, right? We deploy our complete system on your on prem account as well for the customer who are like where they have mandate to not leave the data out of the system at any point. So for that we will deploy the complete system on your cloud and the benefit is that when we deploy it on your cloud, you can deploy it in you VPC if you are in AWS or you can deploy any virtual private cloud and inside that you control what traffic goes into your virtual private cloud and what traffic goes out of your VPC, right? So that gives you very good control on what leaves your system and what doesn't leave your system. So that's like very big for a company who has to like banks and health care. Sorry we didn't have banks client but we have healthcare clients so we understand we have clients with PII data which cannot leave that system and things like that. I'll add a few more things even in terms of the differentiation like these platforms, they are like black boxes. If you look at a data report or http like they are black boxes and while you can use them if you want to get out of the system it becomes very difficult. Whereas it our system, the way we have built it and the way we are building further features into it, it makes you independent and then tomorrow like you want to get out of it, there is no vendor locking you can get out of it and all the things you have done will still stay with you. We can even provide you access to the cube CTL if need be which allows you to have full control of things. So it's not like a black box system. That is one major point. And then we try to optimize for developer experience. Our goal is to kind of enable data scientists to be able to do things in a scalable way so everything is done in a way that there are simple lines of code and you can actually run the system. The other thing that is different is these are all mature platforms and we on the other shane are comparatively younger, very young like we are starting CTO have a few customers but we are working with all of these customers hands on. You have seen the team and you can be rest assured that everything will be there for you to kind of help you within hours and get your things upwork. You can think of it not like a vendor but like a partner and an extended member of your team enable you to get to your output in a better way. So that is the approach. I would say you should think of us like we are early and we want to work with you wherein we can help you. You know your requirements, you tell us, we will get them for you and if there are specific requirements here we will get those sep within a week or two rather than you kind of wondering if what you need is there in the platform because with the customers we are working with we are actually getting feedback and we are improving our platform on the go. It's not like we have this feature set and that's it. What do you need? You are our kind of people will help us go ahead and we will do everything CTO ensure that you pendo goals are. But you can think of it in that amar from technical differences and other things. I think that's one major point of difference. Got it? Yeah. Surge week. You can send us your slides then we will connect with our data engineers and then we will get back CTO you. That's fine. Definitely. I will send out the slide and some notes from the meeting. One quick thing is could we put a tentative date on the calendar for a follow up call? Just because of line? Sometimes communication becomes difficult. I just want to kind of see like suppose even if other members in the team are not available, at least your team who is here, they can be available. And if sometime next week, whatever is suitable for you, you can let me know. I can at least put something on the calendar. Yeah, I think I have to go back and check the calendars as well. Also get the data engineers in. So I will have to go back and check the calendar to find a suitable time. That is fine. I will follow up then. But I wanted to understand, is this something that you feel could be useful for you or is this completely not what you are interested in? For sure. We are looking at ML platform, as you said, that you help us to improve our deployment experience to make it easier and to shane off the time required to get an ML model into production. So definitely this is one of the tools that we are interested in. But I want to make sure there's enough buy in from our data engineering team as well because they ultimately have to help us to deploy this solution. So I need to get their thoughts as well. Sure, absolutely. Cool. We'll look forward to it. Doing a demo and if you want to try it out also we'll kind of give you something that you can try out easily and then build phone conference rather than anything else. So we'll set up something. We'll call up and set up something becomes available. Sure, sounds good. Cool. Thanks a lot. Even Danielle and Julian, thank you so much for taking all the time. Thanks. Thank you. Thank you. Thank you.