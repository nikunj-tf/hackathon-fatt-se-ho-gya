Hi Nikonj, how are you doing? I am doing great. How are you? I'm doing fine as well. Thank you so much for joining in and taking time, especially late in the night. No problem. I was just working, wrapping things up. Thought why not? It's actually a great time for me. It's like 10:17 a.m. Right now, but I know it's like late as for you. Yeah, cool. Awesome. So Nishan again, this is incredibly helpful. I think at this point, at this stage of company building, I feel like these calls is pretty much like the most important thing that I spend my time on. So thank you so much. I'll briefly introduce myself. I grew up in Calgary. My family roots are in Rajasthan. Spent all the way until Undergrad in India. I was at Krakpur for Undergrad and after that I moved to the US for my master. Since then I've been working here in the valley, in the machine learning space. I worked at an early stage startup called Reflection where I was one of the early engineers, like six engineer in the company and got a chance to work with an amazing team there, scale up the system to about 600 million monthly active users and serve hundreds of ecommerce customers. With our recommendation models and all, we got exposed to both model building and ML engineering. Kind of the intro aspects of things there, which is one of the motivations for building what I'm building right now. At Giri Truefoundry and post Reflection, I worked at Facebook where I was leading one of their conversational AI efforts and I saw the platform that Facebook has internally built out. It's called fi learner. I don't know if you have heard of it. It's like the internal ML platform for Facebook, basically. Okay. For deploying models? For deploying models, yes. And just with deployment it does a lot of other things like making sure that things are monitored and access control and version and scale and stuff like that. So a lot of things it seems like you're just deploying model but realistically end up doing a lot of other things. And that was almost the second trigger for me that while what we had built out at Reflection was already super useful, when I saw what we have. At Facebook, I was like oh s***. This is like 100 x better than what we had. It's not like five x ten x better then while building through Foundry, when we spoke to more than 150 companies understanding their pain points and how they're solving their problems, we realized that what we had at Facebook, it's like almost no company has built out anything similar. So kind of that gave us the confidence of building what we are building at Truefoundry and I'll tell you more about it as we move forward, but we'd love to spend some time and understand what your role at Branch as well. Yeah, thanks for the introduction. I don't know if you've heard of Branch or not. Branch is also a value based company. Like we found it there. Obviously it does not have operations in the US. It has operations in African countries and in India, I mean creating revenue generation. So it definitely has headquarters in Valley, but it does not have a revenue generator. And in terms of business we primarily do lending. So it's micro lending, app based lending and in India it's only lending. But in African countries they also have different products on our platform which is wallets, which you can think of like all the things that PTM did in early phase is what we're trying to build in Kenya. So we are a bank in Kenya. We have applied for, I think applied for a banking license in Nigeria. We are slowly going to get it and we are slowly expanding to other countries. So that is like a brief overview of the company. Now coming back to how you use machine learning, branch is like we receive applications for loans and we use our machine learning models to evaluate those loans and we generate a bunch of credit scores not only for loan applications, but elsewhere is also to generate offers to things like that. So machine learning is like at the heart of the product advance. Right? And my role here is I work in the lending team and I am an ML engineer here. So we are like I think four ML engineers here at Branch we take care of stuff, all the ML related infrastructure, engineering, the pipeline, the data pipeline and the models, obviously. Yes. So that is introduction. Any questions you have? Yes, I do. This is great context to begin with. So ML engineers, we are poor ML engineers and is there like a separate data science team? No, I would say what do you mean exactly by data science team? Basically are there the four of you build models and deploy models and maintain the infra or there's like a separate team who kind of builds out the models and you guys manage more of the infra side of things? No, so we do end to end stuff. We build the models. It's the feature creation, feature analysis, everything is done by us. I see. Okay, so what are the machine learning team side? The branches is poor essentially. That's fair to say. I would say yes. But then obviously you have people understanding SQL and analytics. So I mean that helps to just outsource most of the analysis. Or if you have like if you just want to analyze model results then obviously it can be outsourced to teams who are more into analytics. Got it. So I would not like just have this boundary around data science but it's like a mixture of all the things like for example, you have a model performing then some teams or the policy making team because it's a lending company. So you have like a policy making team. So policymaking team could take over some of the results that the model is producing. Might not understand what the model is doing but have like the results. And we have obviously built out a lot of dashboards to help the company understand what exactly entails into ML models. Like using shop values, using a lot of explainability into the models. Wow. So just between the four of you, you guys have done a lot of things it seems like, in the company, like to enable other teams like analytics and the operations team, policy making team as well. Yeah, I would say it's been like evolutionary effort. I would not say it's like we started when I joined the company about 1.5 years back and at least in India it was pretty small. We were about to grow it's more than 1.5, I would say maybe like quarter to two years. We were still deploying models on Sage Maker. Then it was a pretty bad, like infra and a lot of manual steps. And then we moved on to this German startup which is I don't know whether you've heard of it, which is into a very similar lines what you were doing, which is like we use their services for model deployment. What is it called? Tactile. Tactile? I have not heard of that. Okay, yeah, it's also a start up. It's a German based company. Okay. When you move to that. I think it was before me. So maybe like a couple of years or more than that I would say 2.5 years back. No, you said that the Sage Maker based deployments was really bad and the company that moved to that time. Do you know the story of what was the problem that you guys were facing and why did you move to Tactile? I know a bit of it not to complete. Like when I came into Branch, the migration was already in place. So I helped a bit on the migration front, but I don't know the complete story. If you want I can connect you with folks if that really helps your company. But I think the problem was there was a lot of manual effort. I guess the advantage of Tactile is you could use just the functionalities of git to enable deployments. You could use like commit hash, you could use the GitHub actions to automate a lot of stuff. What is the spelling of Tactile? I'm not able to look them up. I'll just send you the link on that. And yeah, Nishan, I think it would be very helpful if you connected with someone who did this early member of the team who kind of took this decision of first building out models on Sage Maker and then changing it. I think we also get this. The reason this is very important for us is as a startup, we also get this a lot that we are already using Sage Maker and these are some of the problems that we are facing. I want to understand that what was the state of the world like a couple of years ago where you guys decided to do this? It will really help me understand a little bit more. So post this call. If you can make some introduction there that would be very helpful. Yeah, sure. Let me talk to someone. I think there will only be like because in the startup team just not very stable people come and go. So I think I'll be able to find just one or two folks who would be there. Generally as long as we have one person who out of the decision making at all, I think that's good enough. But the gist of the problem is there were a couple of manual efforts and it was not an automated deployment. Like you could have model artifacts, model artifacts residing and then you change the link to model artifacts which is a manual effort. We wanted a more like, I would say customer development, more of like and at branch the machine learning ecosystem is pretty sophisticated so we don't want to spend effort where it's not worth it. You train a model and then. We. Would rather have manual efforts where it is like a part of policymaking or decisionmaking process rather than be a servant of like infrastructure. So that is what Tactile helped us with which is to really I was not a part of that decision making whether what other vendors were evaluated when we made a decision to move to Tactile. But I think with Tactile at least our deployments are like sophisticated now in terms of it is just like commit hash to deploy a model. Got it understood. Okay. Yeah, I would love to do a deep dive onto this thing. It sounds very interesting to me. One other question that I have Nishant is like how many models roughly do you have in production currently? Like what are some of the use cases? So obviously the loan approval process you mentioned is like one use case. What are some of the other models that you have in production? So the models vary by product and at firstly the models vary by country. So we have operations, like major operations in four countries. So we have models for four countries and then we have models for different borrower types which is to just to bifurcate a different customer segment because one customer segment has a different set of features and the other customer segment has a different feature. So at a given point in time we'll roughly have about maybe ten models running in production. Ten to twelve depending on we have also had prototypes so I would not call them as those increase the numbers but yeah, sorry for the word prototype but it's more like POCs. Like for example, we want to deploy say a classifier for our internal use case but then the output of the classifier goes into the final production. So things like that are called POCs, and then some are not really machine learning models, but they go into the pipeline are what we call heuristics, wherein you don't necessarily need some like a machine learning model. And I would say they are part of the pipeline, but they're not deployed on Tactile means. I see. So one interesting thing here is in terms of the POCs that you were talking about, do you guys also deploy these POC models also in Tactile or what's the workflow for that? Okay, yeah, good question. So I would say it's mixed wherein depends on because our infrastructure is set up that way that a machine learning model created just is easier to deploy. And Tactile, because we have the infrastructure set up today and the codes all written to deploy a model on either Sage Maker, even like a POC is easier if you train the model on a Sage Maker notebook. Then if you use our pipeline or our infrastructure to train the model, it's better to deploy. I see. Okay. So you guys still are using Sage Maker for some other use cases. I would say no. The answer would be no. There it's like very unique use cases that we have. Say we want to test something out which is only available in the library of Sage Maker. So we can use Sage Maker, but no, we are not using Sage Maker. And then one other question here is cloudy and all completely on. AWS, yeah. Okay. Got it. And do you use Kubernetes internally at all? Yeah, for deployment. For deployment. The way Tactile works is they allow you to deploy your models on your own Kubernetes cluster. Is that how it works? No. Then they have their own clusters. Why do you deploy it on their clusters? Yeah, it's like close access. Yes. Okay, I see. Interesting. Okay. So these are like two different things, the training and the deployment. The training happens on our clusters. The deployment is on there. Interesting. And deployment is basically each model is an endpoint in itself. Is it? Yeah. Okay, so a model is an endpoint, and a model has like you could create endpoints. It's there like the scripting language, I think in your video. Also you showed you could add like log lines, like a predict function in your introductory video. So similarly you could create like multiple endpoints for top 100 features, say, or just like false positives or things like that. Basically we can create endpoints. So we write all the codes to create it's. Not one endpoint. You could create like multiple endpoints. And we do have multiple endpoints. We have multiple use cases where we need not only the output from the model, we also need certain other data from the model. Got it. Okay. So actually this is such an exciting like this is the first time I'm hearing a stack like this. Would love to do a deep dive into this year. What does your current training to deployment workflow look like? In that case, you're training on your own machines. Where does that actually even on your own machines, where does the training happen? Right, okay, so all model training happens on Sage Maker right now? Yeah, I mean, that is configurable. We could use any other it's just like the data resides on S three, so it's easier to train on Sage Maker. I see. Okay, got it. So when you're doing the training, do you use any kind of like model version tracking? Any model registry stuff as well? Yeah. So you have a model. Okay, let me explain it from the basics. A model is basically a repository and each branch is a version of the model. So each branch is a different model. Okay, understood. That is how we do like a tracking. So the main branch is the currently deployed in production and everything. Like you create a feature branch that is the next version of the model that you're trying to train. And once that we have certain evaluation matrices set up and once that approval process goes, everything for the model looks good. We move into main and that becomes the new production model. Interesting. So this concept that we just discussed, terminology that you used to describe model registry is actually not like the most common one that I hear. Is this like a Tactile terminology that you guys use? No. So you mentioned that a model is a repository and each branch is basically a version of the model. Right. So do you actually use git as your model tracking system as well? Yeah. And that's the format that is enabled by Tactile? Yeah, basically Tactile works like that. Okay, understood. Nice. Interesting. Okay. Yeah, I think a better way than all the other ways that I've seen. It's easier to track that way. You could have like PR descriptions about modules you could just build. We have a lot of things in pipeline as to what we want to build there, but yeah, no, this definitely makes sense. Like I know that a lot of companies are now using the same git style for ML artifact tracking. Like for example, DVC, I don't know if you guys use it, but DBC does the same thing for data, right? Data version control, essentially. So maybe one other question here is like with model, you have a lot of other things that come in handy, right? Like for example, your model performance, model metrics, training metrics, hyper parameters, validation, inference metrics, and a lot of other things. Right. The model files, maybe you have some supporting files like Word to index and stuff like that. So do you track all of that together on git or do you have like any other tools like ML flow and all that you use for that part? No, everything is in git. So how it works is once you start a model training. It runs via GitHub actions. And the GitHub actions does commits to that branch. And that branch has like a new analysis files. We have this folder called Analysis, which contains a bunch of analysis files, markdown files, which are then displayed on the Tactile platform. Oh, I see. So the dashboard is the Tactile platform? Basically, yeah. But essentially like anybody, not all the members in the team have access to Tactile. So for us, it's a good, like a UI to use. But you could also use the GitHub to just render the markdown files. You could basically just point into any rendering tool and give access to the GitHub files that will be your because inside the company, everybody has access to the GitHub to the GitHub repository. So it's easier just to share the analysis files there. I see. Okay, got it. Okay, I get it. Now your model is stored and by the way, are you actually storing the model file itself on GitHub? Or is it like a pointer to a model file where the model itself is stored on S three or something? No, the model is also stored on git. Okay. So like, sometimes the model can get large, right? Do you guys have like multiple hundreds of megabytes? We use Git LFS okay, understood. Sounds good. Okay, so this is helpful. And then after the model is put on GitHub, how does the actual deployment workflow happen after that? So, from our side, it's only like a commit hash that we do. So on the back end, tactile basically spins up a port for that branch. And all the endpoints that we have defined, all the configurations, all the memory configurations that we have defined in the git, in the git config file, that configuration is used. All the environment variables, everything is now available there on the pod. So that is how it takes place on the back end. So the pod maintenance is not our headache now, but we do have access to the pod. So you can look at logs of the pod, any errors that are happening, and there are various dashboards to just track everything there. I see. Interesting. Take a note of times with model deployments. Like, just the model file itself is not sufficient. Right. You sometimes have this pre processing logic, post processing logic from the model inference itself. Right. Where does that sit in that case? Everything in the so we have something called assets for a model, which is like all the files that you're talking about, like test data, evaluation data, maybe attributes of the training data, something you want to show on the analysis files. These are internally we called model assets and they also reside in the branch that you do. So essentially everything about the model is in the branch. They obviously linked to S Three. So since the training is happening on S Three, the assets are built there first, but then they're copied into the branch via GitHub actions. I sep so even the code like the preprocessing post processing code is also put there basically. Yeah. So it depends how you want to we have structured in such a way that the same branch contains the training code and the pre processing the post processing code so that it is just easier to run. And as soon as you spin a new branch you can just leverage that code to just run a training. So it is essentially the training code is available there and we also have like an internal python repository that enables us to be python, I would say library, like a branch internal library and that enables us to do a lot of stuff. We have just abstracted a lot of stuff there. So all the training scripts also the model repository does not contain most of these. They are just pulled from that library is installed and then the abstract functions are used. Interesting. So I sep the reason I'm a little bit confused is how are you able to maintain the consistency between your training features and your serving features in that case? Because at some point you mentioned that your training is happening on top of your S Three data. Right? So the data preprocessing logic during training time is working off of the data warehouse that you're using. Whereas during serving time I assume that your data is coming in some real time, not from S three. Right. Serving in fact time is not coming from S three. So you're doing some other pre processing logic on that and I would imagine that's a separate code. A little bit of a separate code. So how are you maintaining this consistency between the feature generation? Yeah, so what I've talked currently does not include anything about features, how we generate features, how does our feature pipeline looks like this does not talk about anything. So imagine like say we have to start a training so there is a database repository which contains all the data. Now that training needs like a training data. So firstly your training data needs to be pulled. So firstly the training data is pulled stored in S Three and then all the pre processing happens which is part of the code is written in repository and then the training happens and all the pre processing logic is stored as I would say, what do you call it? Like pickled into certain objects, pre processing objects which then converts basically you could write an endpoint to convert, take that pickle file convert, pull the real time raw features, pre processes and then send it to the model endpoint. I see. Okay, got it. Okay. So in our case the transformers are such that they themselves can be pickled. I'm assuming then you use models like whatever, like Xg Boost, killer and those are the common type of models that you're using. Yeah. Okay, understood. Got it. Okay. And you may even be using pipeline in that case. Yes. Got it. Okay, that makes sense. Understood. What are some of the challenges that you're trying to solve, Michant, currently as part of this entire thing? And it could be challenges in whatever, like your model maintenance, deployments, batch inferences, timing, monitoring, anything along those lines or something completely outside. Yeah, there are lot of challenges. I would say like build system. We are still solving a lot of challenges. But I think one of the major challenges is not on the model deployment front, it's mostly on the features front. It's like when do you this is the thing that we have struggled with is when do you decide or how do I put it in words, how do you decide when to create like a new version of a feature? We're trying to solve it using some having distributions of future distributions over like the training data and then the inference data. But I don't think we have still come up with a good heuristic to when to do this. What does it mean by featured version? Nishant and I'm sorry, by the way, if I sound like an ML loop asking these questions, but I want to. Make sure how I'm speaking is some of these internally, how we talk at branch, which is more familiar since you're not familiar with our infrastructure. No problem, you can ask a lot of questions. Okay, what do you mean by so consider a model? Okay, so essentially your entire machine learning pipeline will have some features coming in and that feature code is written somewhere like just to create that feature. Your database will not be structured for machine learning. It is structured as AI transcription, like a postgres data or something. So you have to do joins and stuff to get that data. So now say you trained a model and somehow and put that in production. Now you know somehow that the feature is somehow misbehaving or not performing, which is a difficult, like, questionable thing to know. How would you know that? You want to create like a new create a new version of the features. Somehow you know that the feature is not performed. That is the version I'm talking about, wherein you cannot just edit the current code because that will effectively what will do is it will hamper the feature flow that is going into the current model. You'll have to create like a new version of the feature and then train a new model via that feature. Like, for example, let's say you use something like, for example, I am say, a food delivery service. And I want to create a bunch of features on drivers, like GPS data. So one of the features that I created from the shop, what is the average time the driver takes? Just through. So this is one version of each. I have code already written for it. Now I'm calculating the mean here, say average. Average now the next I think Mean is not doing a good job. So now your model is already trained on Mean and it's already serving production. So the next feature version you'll have and you think like a median would do a good job. So you will not change the production code that is already running. If you change it to medium, what will happen is like the input to the model will now have medium and that will skew your results. So you'll create a different version of the feature, which is like an entire stack. Or how we do ML codes at how we maintain ML codes at Branch is like an entire different, I would say, topic. And so that is the version I'm talking about. So from Mean you want to change it to medium. When do you want to do that? What are some of the matrix that you look at? So this is the challenge. I think I've read a lot of threads or community posts about it, but I still not have a good heuristic as to when do you want to create or maybe like putting it simply, when do you think like one feature is going stale and your model, like your inference is very different from what you train the model on. Okay, I sep I see. So here, like, basically you're saying that nothing necessarily is broken in your data pipeline. You're still computing Mean as you were supposed to create compute mean. It's not like suddenly something is broken and things are not working. You're saying that while things are working, like for some reason that feature is not effective anymore or the data has drifted or there are some more like. Or one thing is you would want to just test out medium. Yes. Okay. You want to test out a new feature altogether. Okay, got it. And today, how do you do that? Do you have some kind of traffic splitting A B testing for your models currently built out? Yeah, we have the AB testing framework is outside Tactile. So Tactile just supports the deployment. So internally we have this experiment framework at Branch which we use to just splitting out the traffic and then sending it to different endpoints on Tactile. I see, okay, so you manage that internally? Yeah, the splitting happens internally. Is that trivial? Is that hard? How is your experience? I would again say over the period of time, like, we have built a sophisticated experiment framework wherein it's become easier to test out experiments now. And we would not want to because the experiment is not necessarily only like a model deployment thing. So even if we outsource it to Tactile, it would not make sense because it's not the model that is only going into experiment. It's like a bunch of other artifacts in the pipeline that are going into experiment. Like you could have like a different screen altogether, android screen. You could have different APIs back. End APIs. So I guess I don't know whether that answered your question. No, it doesn't. So if you have that maybe like I'm just trying to understand Nishant then doesn't that solve for the problem that you mentioned? Because I'm imagining like basically what you would do is you would create a new data pipeline where now you're computing the median as well as a feature, either as well or replace median with mean. And that basically builds out a new model version. Right. And then you're saying that you already have an A B testing set up so that version also gets deployed to production and then you split your traffic and you figure out which of the model versions are performing better and then you choose that basically. Yeah. Okay, good point. And you don't necessarily need like an AB test framework. What you could do is we have this test data, how we evaluate model, also do it offline, take a note on GitHub and just do that. The problem there is you do a new feature version, you have to go through the entire process of model deployment, get the data for a bunch of users and then train the model, which again takes like a couple of hours or maybe three to 4 hours depending on how much data you have. And then fast experiments don't work there. I guess it's all about statistics. If you could just have a quick statistic on like you create a new feature. How effective is this feature on the new model? I guess this is more like a machine learning question rather than a deployment question. So you're saying that I really want to be able to figure out whether this feature will be important or not without actually training the model? Yeah, that is what I'm saying. Because the whole process of training the model and then because there are a bunch of maybe we need to solve something on our pipeline front and that could become easier, like training a model. I would say that piece is still not 100% aligned because what happens is you could only train models from a production feature code. It's not like we have like something is deployed on staging, essentially. As the. Example I gave, if you calculate medium, you would only push that code to production once you know that features working. Otherwise it will all be like you have a bunch of ideas. You don't push that to production. So I guess if that code is in production, you cannot or at least our pipeline is set up so that it is difficult to train a model. Interesting. Yeah. To me it actually sounds like more of a research question at this point than even like either a model or deployment question because it seems like a hard thing, right? Like how will you test out a feature without training a model? But it sounds like a useful thing. It's a hard and a useful thing. So something that I could also think a little bit about nishantiya, I could not finish all the things that I wanted to learn. This seems like a very unique way of taking. I told you, right? I've taken more than 150 calls and this is probably the first time I'm hearing this kind of a pipeline for building and deploying models. It's very unique, actually. And trust me, it's not like I go to all these 150 calls and say the same thing. It is truly unique. So I would love to do, like, a deeper dive with you on this thing just to ask a few more questions. Number one, and number two, I would love to get an introduction so that I can understand the Tactile migration violations. So if we can do these two follow up action items, that would be great here. Yeah, I don't know if I will be able to do connect, but what I can do is I can search through because all the discussions are slack. So I'm sure I'll be able to find or I can maybe point you to some like I can share some of the decision making. I would say what do you call it? Like boxes that check boxes that we take to just move from Sage Maker to Tactile or what are the vendors if an introduction is not possible. But I guess that should not be difficult. Okay, sounds good. Should we set up like a follow up call where I can do a deeper dive into this thing? Yeah, sure. Maybe if we actually already know, like, a good time. Do you have some time tomorrow? Like tomorrow night? Let me see. Tomorrow is Friday I generally don't do. And tomorrow is India's. Match. Okay, then do you want to do like on. Monday or Tuesday I could. Do okay, let's actually do Tuesday. Monday actually, I have some calls. Does Tuesday at 09:30 p.m work for you? Yeah. Okay. Sounds good. Okay, I will send out an invite to you then. Take care. Yeah, cool. Okay, perfect. Thanks, Rodriguez. I've actually run a startup, like, for one year, for 1.5 years, I would say. That is why I know it's the pain of building products. And how helpful is it if somebody helps you? Yeah, this is of course I would love to know a little bit more about your startup as well in our next call. Yeah, I did not come to fruition, so maybe I don't have much to share, but yes, it is generally helpful to have like a deep dive or just understand so. I think I also read some of the posts about you. Raise some funding. We recently announced our CD round of funding. Nice. Yeah, congrats on that. Thank you. What exactly are you trying to build? Is there one statement where you could say, like, this is what we are trying to build, or you're still figuring out or researching what areas you could help with on the machine learning plant. Yeah, making model deployment faster and more mature. That's what we're doing. So here faster means once you built out the model, like deploying is like a few minutes worth of work and mature means that once you have deployed it like things like versioning auto, scalability monitoring dashboards, all those things come for free and they are not an afterthought that needs to be built out later. Okay, great. I don't know how you're thinking about your model deployment, but I think like as I mentioned, if somebody is managing repository on git, a git commit hash is generally a good way just to maybe like deploy models. You could just do, I don't know, hashtag deploy or hashtag. I see, okay, got it. So just lean back on the get ecosystem for doing that. Basically that is what we have internally, also not related to tactile. Internally we have like deployment staging. So that deploys to staging deploy, you have this or you could have Circle CI builds that has access to your deployment. So you could do that. Basically that is just a click. So you have all the model artifacts in a branch. If you are planning to use git, I don't know how if you're not planning to use grid, we are fairly. Integrated with the overall git ecosystem. Like everything that we're doing is we are get off only. So I think what you're saying will actually tie back very nicely, but I don't think we do that similar. I've also done that tagging the branch and deploying from there directly in my last job at Reflection. I don't think we're doing that specifically right now, but like something to address, I guess. Yeah, I mean one of the ways there could be like multiple ways. This is one of the weaker and cleaner ways that I've seen, but you could have like other ways also. Yeah, I would love to connect on WhatsApp as well. Maybe either I can bring you my number or you can send me yours and we can connect as well. I'll just put my number in chat sep. I'll bring you on WhatsApp then. Yeah. Okay, cool. I will send out a WhatsApp note to you and then we'll connect and talk. Sounds good. Bye. Bye. Bye. Nice talking to you. Bye.