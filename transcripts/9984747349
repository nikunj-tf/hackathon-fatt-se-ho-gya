To hear me clearly. Yeah, IBM able to hear you. IBM audible. Yeah, you are audible. Thank you so much for taking all the time to speak here. Simon is actually in the hospital because for something related to his baby for checkup treatment. So he's not able to john, but I'll take this call on behalf of him. Okay, cool. So may I just give a very quick background. So IBM on Rock, one of the co founders at two countries, graduated from Idaho 2013 batch, and after that spent six years working in the hedge fund industry. At Fault Point. I was initially in Mumbai and then in US and Singapore doing portfolio management for them. At the same time, like my colleagues, we Shane and my batch Mats from Karakur, they were at Facebook on software engineering and the machine learning side of things. And in 2020 we left our job built up for startup Netaling and solely to encourage and then we are now building two foundry with them of making it very simple for data scientists and ML engineers to deploy models to products in a faster way. And a lot of it is based on the experiences of what we have seen in terms of platforms at Facebook and trying to bring a similar kind of experience for everyone when in smaller teams and mixed teams. So we are still in early stage of our journey and we are trying to talk to companies to understand their MLM pro and pipeline as well as trying to see if other use case would actually be something where we can help add value in terms of solving any of the pinpoints. So with regards to that, I wanted to chat with you a little bit to understand Commerce IQ, ML stack and the kind of problems you face on a day to day basis and where the challenges lie. And based on that we can think about like I can tell you brief about our platform as well, but I mainly wanted to understand a little bit more from your context first. So nice to meet you. Thank you so much. So at Commerce IQ, the solutions that we're building, they are mostly based on sage maker and we're automating on those pipelines. We've created our own CLI and our own environment to cater to our own files. So the issue with setting up a very automated pipeline is that we don't have direct models. There are a lot of things happening here and there with the models itself. So it is a collection of multiple models inside that and then we overlay some other effects or other signals to essentially cater for it. And that is the basic reason why we did not think of going directly to a pipeline sep up like Azure or something like that. But for now, the Sage maker instances are doing pretty well for us. My manager would be a better person to answer. Why would we want to ship to any other kind of environment. But for now our tech stack is mainly Python. For our queries we go to Snowflake and then our whole environment is setting and for data procedure. Okay. And just one ning I'm just trying to take is it fine if I record Chris meeting for internal purposes? Yeah. Moya, what are some of the use cases that you see internally for commerce? How big is the ML team there? So IBM in the DS sales team we manage our own product called Market Shane. And there is another product that we're working on. The other team is DS advertising. They work on the advertising side, share of voice and all that. So that's entirely a different product and they use the same tech stock. It's just that their product is much more mature. Our product is still I mean the clients are happy with our product, but still we're trying to scale it further. And for that only, I've been working continuously on some ML models. Okay, and how big is the team for data science? Data science? We have one data scientist, three. I am the one who is basically being supervised by him for the next solution that we're building the classification part of it. I am the complete owner of that for the automation purposes. There is one other guy to manage deliveries. There is another data scientist. So we have total of three data scientists, one. And we have two interns, two product analysts and one data center. Three and our data center. Okay. Small team, not a very big team. Okay, got it. And how long ago did you start using them? And love to understand a little bit more about this. You may mention you're using Sage maker endpoints. Where do you train your models? Do you train it on local machines? Okay, so the the training purposes, our job is majority since Amazon is so dynamic and the data set we're working on, it's too huge to actually train a supervised model and then understand its results. Where majority going with unsupervised models and the models which are very quick to train like approximate nearest neighbors or the other ones which are pretty standard and can be trained on a sales maker notebook instance. Okay. Those are the ones we're currently working on. Because training is entirely a supervised classification model. That would be futile for us given the dynamic data that we have. We would be poor on coverage and we would also have the issues of scalability as soon as more data comes in. So that's the reason we're not going into training of the models. Again and again, not supervised specifically. We were majority using statistical measures and some unsupervised models plus weighing. Okay, got it. Do you use kubernetes internally? The tech stock that the UI team works on? I don't have an idea about it. They are basically uploading the data that we crunch the whole classification sep or the market share set up that is prepared, that is given to them and the end point is hosted on their UI. And now as far as I know, it must be either a JavaScript spring boot thing or some such kind of rest API where it is being hosted on. But the whole UI, if it is in docker containers or it is utilizing gate clusters, I frankly don't have an idea about it. But personally, I don't feel that would ben the requirement for them, I don't think because there is not so much data incoming or so much requests being done on the UI platform. It is just like a dashboard that you get to see, you have results, you have all those things. So it is not like you'll have to go and ping out it again and again request. There would be multiple participants requesting it. Each client has a dedicated other endpoint. So I don't think there is any requirement of gate plus there. Just 1 second. Got it. Understood. Cool. One more thing that I wanted to understand is, okay, so when you build the models, like you build the models and do you actually build this dockerize them and create these endpoints or you don't do that. So for me, I create the models, I store them in S Three and I work on the data. I do not create the endpoints of docker. Okay, and when you store them on S three, how do you store them? The communication of Sage maker to S Three is pretty fast. Even if the data is in GB, it gets transferred in seconds. I see. So then someone picks up this model from Chris Three storage, the product ops. Team that we have. Right, okay. They are taught how to use the model. How to use the model is not fully automated. We do some kind of text tagging at some places. There is a lot of manual feedback that is given to the model. And then with multiple iterations the model improves and then we give the file output. But the sales estimation is different. That one is already prepared and done. And frankly, I don't have an idea about sales estimate. Okay, got it. So your pipeline basically pendo the moment you build the model, you just host it. You just kind of dump it to someone else. Has the function to actually host the model and create an end point out of it and everything. Right. So basically what happens is the model entirely is not directly the output of the model is not directly given to it. There are multiple layers, after which one model is classification model, the other model is our sales estimates model. Once the classification comes up, then the sales estimates comes up, then we combine that data with the help of other auxiliary notebooks. And currently we were using notebooks. Now we've automated to a CLI. So then the final script is prepared using both the data, then that data is then directly uploaded to you. It's like that. And is any of these models being used or called in real time or is it mostly like a batch kind of influencing? No, we do a batch processing. Okay. And from what I'm guessing, it's not like a lot of models. It's mainly two of these models. It sales that for each client the model changes. Okay, and then do you kind of post separate models for each client? Yeah. So the pipeline works like this, that once a client is set up, the whole automation is set up for that. And now the product ops team takes on the data for the new dates, then does the forecasting classification and all that. Whatever is given to them. Right. And then they just bring out the output. So the main effort goes in the very first run of classification and then estimation and then checking if the outputs are correct or not doing a sanity check. Once we are finalized with the model, then it is just an automated piece of job. Only when some client feedback comes in, we try to change the one that's call today. Got it. For this. When you are building models, if you had to kind of give it like do you have a use case where you have to kind of expose it to the product teams for texting? Or is it like how do you kind of expose models to the product teams to kind of help them actually see if the model is right or not? Okay, so a very asif version of the model is given to them at the starting of it. Then based on the inferences that the model gives them, there are multiple iterations which help improve the model and that is done by product of team. They just have to run a few commands. The model is, you know, it updates itself, iteratively. How does this automation kind of work from you? Basically when you say that they just run a few steps. Are they doing hyperparameters? It's not hyperparameter ning, it's majorly. The development of key text tagging parts that we would want to include or not include to refine our full classification setup. Model gives out a base classification and on top of the classification we would want to change it a little bit. So we give some manual inputs and that is done. In iterations there are clashes between the classification and that is how the model in some iterations it improves. And then when we have a final model, then it is updated to the automation pipelines and then it is just a matter of running commands. Since the model is already prepared, the deliveries are automated. Got it. And is there any concern internally or for your team when it comes to how much resources are being used for saving these models? So the resource allocation thing actually that is where we kind of, I mean, currently we were struggling but as the Amazon Sage Maker team only to help us with the cost optimization of that. So our Sage Maker cost is going quite high. That was the reason as them to intervene and help us out on how to utilize the sales maker and the other things Amazon is offering us more efficient. Okay, and could you give an estimate as to how much like do you have an estimate of how sales make the prices itself and how these costs were going? Oct I actually don't have a quantitative number. It was just some meetings that was scheduled with us telling us that the cost is going CTO shane, that kind of cost is not exposed to us. Okay. And that cost is mainly for training of the models and listening. Not just training but the iterations that we run on Sage Maker instances, the runtime of the notebooks and all that, the uptime of the Sage Maker instances, whatever that is the amount of space that is consuming in s three. Okay all that. Okay. And the choice of Sage Maker is it because your system is mostly AWS days, that's why you Shane chosen Sage Maker. And how convenient do you find using Sage Maker versus say using a tool that is muthu easier to get started on or something like that? Myself also from an engineering background, for me it was pretty easy. I was already using jupyter notebooks and NDA environment so it came pretty easy to in two clicks here on the sales machine. Basically the Jupiter environment with Jupiter lab. Okay, that is pretty easy. Okay. And everyone uses the hostedglab environment of Shane management. Got it? Cool. Understood. This gives good understanding if you have to kind of run these batches at a particular period of time like you kind of do that settings on Sage Maker itself or is that done or managed by the product of things that. You basically automated the whole set up. I mean we've given and the commands to the product ops team and we've optimized the time that it takes. I mean, there is some optimization still left that I need to do but other than that for the weekly data and the weekly updates that we have, it's pretty smooth. Once the model is prepared, it is just a set of commands that has to be done and they run seamlessly and each of the file is prepared in a manner that is independent so each command can run individually on other. Course so it does not affect okay, got it. One other question around this is like when you are kind of training these models yourself or building this model, do you kind of maintain a tracker? Okay, I shane tried many different models and this model is good. Like how do you even compare different types of models that you are trying CTO? Or is that not something that has come up as a problem at all? Okay. So as I told you, our sales estimation model is pretty much mature, right? That was mature even before I joined commerce. The classification model, I'm still making version two of it in that I might france such problems where I need to keep a track of the models and keep a track of all the Efficiencies and all chad. Blunt, for the existing model it is much more based on clustering and word tagging. So there is not much to keep track. I see just that when we get the outputs the product of team makes the required Pivots or data comes in, we make the Pivots even for them. They just analyze, change the word tagging things and all that and with a few iterations it is up and running. Got it? Understood? Cool. So that is helpful. Amoy, thank you so much. I'll give you a quick context about what we are doing at Truefoundry. Sure. And love to see if there's any of this that could be useful for you to try out. Even not necessarily internally, but even just for experimenting purpose or something so that we can also get feedback. So basically right now, as you said, like you know, you are housing Jupyter notebooks and you are using a list statement environment to host these Jupiter notebooks. Once you build a model, you are actually dumping this to a database like an S three and then from there it's being picked up by some other team. So the way our platform we are building is you have built a model. After that you can actually deploy that model as a job which can run on fixed durations of time or you can deploy it as an endpoint or a service basically where it's completely dockerized and an end point is created which can then be called by a UI engineer or you can also create a pipeline on top of it. So pretty much anymore that you want your model to deploy, the entire deployment can happen from the comfort of your jupyter notebooks or if you want to use a CLI, it can happen from there. So basically both the Pythonic and the YML way supported and the entire tracking of resources and allocation of resources can happen to you. So for example, if you are three member team you can have workspaces wherein you are allocating resources like okay, XGB of whatever M and so on. And within that whenever you are building a model, you will be able to track how much cost that model is incurring. So that way it will never exceed the resources that have been allocated to you and you will be able to manage the cost and as the model is being called multiple times, if the model has to serve larger traffic, the service auto scaler based on its own. So basically you don't have CTO worry about additional costs being incurred for keeping the service running and so on. So that's how the platform has been built, we have made it in a way that it's very easy for a data scientist or an ML engineer to also operate the entire platform in terms of deployment of a model. Internally we use Kubernetes to kind of deploy on our cluster, but we also ship it to any other clients clusters. So if you are using Kubernetes then we can add to your cluster or if you're using A plus we can deploy on your AWS cloud then that infrastructure. But the goal is to make it easier for ML teams to deploy models like in a seamless way without depending on other teams. And even if you want to create quick demos out of your models. Like for example the web app. If a model is taking in towards the inputs and you want to kind of showcase that to someone in the team. Then you can easily create a web app for the model wherein if you kind of plug in the inputs. You'll see the output and then you can share that with someone else in the team. So that's roughly how we are thinking of how we have built the platform and how other things in the platform are built. Basically wanted to also understand, do you do any monitoring currently for any of the models? Like what is output, what is it compared to say the training and so on? You're talking about the score accuracy scores or no? Yeah, accuracy scoring or data driven. Our use case is not particularly dependent on the accuracy score. To be honest. Our use case is much more dependent on the classification of the groups right that we have for each of the Amazon essence and then the sales estimates for that. Now, if we are doing bad in the sales estimates, that is something we would like to monitor. But as per the classification, only when we are sure about the classification, then only push to automate classification is something that we would not be tracking. But sales estimates model is something that we might track. But as of now, I guess it is pretty much mature. It is not mature for some Amazon categories, but it's mature for pretty much all of them. There are some still pending for which we're still trying. That is a use case that could help us. But other than that, for all the other categories, since the sales estimate model is going on from last one and a half years, it's pretty much mature. Okay, got it. One other question is like your model output ultimately say it depends on a few other models that you are calling and so on. Who sets up this pipeline or do you run this using some sort of a scheduler or do you kind of like it's just a number of models built within each of them. Basically, if I were to summarize it, it's a collection of initially we were doing it on notebooks, let's say six or seven set of notebooks which used to run sequentially. The outputs of one would give the CTO the next one right that way. But now we've converted all of them to Python scripts and then these we use simple commands to run those particular scripts and that is how it is automated. Okay. Basically the first there are some pre processing and then the reprocessor took up further and then got it. But there is no there's no schedule that is running the set of scripts that are running like. Manually. For now, scheduler could be a use case. That's fine whenever, but it is kind of I mean we have to see if it has completed the execution then only we'll jump on CTO the second one. But as far as I think as soon as even if I enter five commands directly on CLI right. Only once the first one is completed, the second one is going CTO run. So if I enter all the five months, it is automatically scheduled. Yeah. Okay, got it. Understood. Cool. I think I get a sense of the overall thing maybe once we'll just discuss internally and it will be good to kind of showcase you the demo of the platform as to how we have built like a live use case demo. Will that ben something that you'll be open to and maybe giving feedback and even trying it out for your own use cases? I can certainly try on my own other projects. Do you see that internally there is any need to adopt any external platform or people are comfortable with Sage Maker? And I'll tell you, the issue with adopting other platforms is that the people that we're hiring are majorly from engineering background and they pretty much have coding experience then only we're hiring them. So almost everyone is comfortable with Sage Maker. So I don't see a very dire need of switching to other platform unless anil the models and all that, the team expands and it gets too complicated. Right. The team is small now and I guess the job is manageable. That is the reason we're not even thinking about switching to other things. Got it. Understood. Cool. I think this is helpful. Thank you so much. Maybe what I'll do is since I have your number, I will schedule one demo with you. I will set up a time and we'll schedule a demo. We'll probably be able to show you how a batch use case deployment can be done either from a notebook or from a UI or from an existing code base as well. I would love to kind of see your feedback and also real time model service deployment. And it'll be good for you to try it out maybe first. And maybe once you look at the platform and based on the context you have internally, it will be good to also see if there is any use case you potentially see that could be useful internally. We are looking to work with companies that are in the early two next shane of their journey so that we can also learn from them as we are building the product. Great. I love to see the demo. Okay, cool. I'll set up some time with the engineering team along with you. Okay. Thank you so much. Thanks. Bye.