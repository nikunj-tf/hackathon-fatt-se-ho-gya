Hello. Hi. Hi. Soma is that how you pronounce your name? Yeah. Nice to meet you. Sama. Where are you based? I'm in Bangkok. Oh, nice. Right now I'm visiting Bangalore. Usually I'm based in San Francisco. So at a point, like, while I'm here, we should probably try to catch up in person. Certainly. You were also in the US at some point, right? Yeah, long time back. About eight years ago. Yes. Okay. Where all did you spend time in? Yeah, I was in Ames, Iowa, for two years. Then College Station, close to seven, maybe seven years. Then about two years in Indiana for this. Spent a lot of time. Yeah. But mostly in school. Okay, understood. Like your master's PhD and stuff. Yeah, got it. Okay. Did you always intend to come back to India after? Yeah. There was never a conscious decision to stay or stay put then. Nothing of its sort. So I took the decision in like a matter of weeks and then packed my backs and flew to India. Okay. No strong derivations. Understood. Understood. Okay, understood. So take it here. Right. Go by the flow. Flow by the flow. It's not the best thing to always, but that has been my life so far. It works. I think a lot of thinking overhead goes away. Plus I think at this point for the space that we are all working and I feel like India and the US both present a lot of opportunities. But it's not like India opportunities are less anymore. I think that level playing field is getting normalized. Also, I think in that context, I think India has more challenging problems. There's a need, there is a demand maybe for India. I think it's a leap problem not to absorb this technology and particularly for the space we are working with. Right. The AI for social impact. It's a pretty interesting space. For sure. For sure. Absolutely. Completely mixed. Soma Wadhwaniai is something that we are decently aware of, familiar with, et cetera. We have some understanding, but this model is of course, one of the more unique models in industry. Right. Almost. There's very few companies that are able to pull off a model like this. Basically, I almost like to think open AI is very similar to that's. How I feel like a mental model. Right. After we spend maybe some time introducing ourselves, given that we are all meeting for the first time, we should also love to hear a couple of minutes from you directly. How is this structured? Just to understand where does the deal inflow come from, how does the funding come from? That set up a little bit, because it but maybe we can start with a brief introduction so I can actually go ahead and introduce myself first and then we can do a round table, basically. Right? My name is Nikkon, co founder and CEO here. Chinmay. Truefoundry. I come from a machine learning background, so mind. Yush. I used to be at Facebook, where I led one of their conversational AI teams. So if you have heard of their product called Portal, which is the equivalent of Alex or Google, home of the world, that's where I was driving the proactive assistant effort. And prior to joining Facebook, I was leading the machine learning team at a startup called Reflection, where we built out a lot of recommended systems and personalization algorithms for the ecommerce industry. And there initially one and a half years I got a chance to build out these algorithms and move the metric, the CTR conversion rate metrics for our e commerce customers. But the last year and a half, two years, I actually built out a horizontal ML platform for the company because we had like by the time we had like 516 using machine learning, building machine learning models, so it made sense to kind of standardize the operationalization elements of it, basically. And we did get a lot of productivity games, to be honest, for developers. But only after joining Facebook I realized that all the things that could have been built that we did not end up building a great learning experience like building something from scratch and then seeing like the state of the art systems at Facebook between Facebook and starting True, Foundry, Abishekandra and I, the three co founders here. Also another startup in the talent space called Entire and that got acquired by Info Edge, which is the parent company of Nokry.com. We realized that there were a lot of operational challenges and there's a lot of HR in HR tech and very little tech. So we thought that acquisition was a better out there. And now it's been about a year and a half that we are building True Foundry, and we'll tell you more details about True Foundry, but that's roughly the journey that we have. That's pretty good to hear. Thanks a lot. So much. Like Shinmay, if you want to introduce yourself. Hi. Soma this is Chinmay. I graduated from 21 from It Karakur in chemical engineering. I was actually Bathmed Sudyash, and after that I joined McKinsey as a management consultant. Previous two joining McKinsey, I worked mostly in NLP. I worked with Apple in the CD division. And now at Two Truefoundry, I'm working in the founder's office and looking at product, customer development, et cetera. Okay. Yes, you're in the same class. I graduated from It for 2013, computer science, and then worked in Facebook for around five and a half years. So I've done different things like the Distributed Caching system team was leading the mobile performance team and was eventually leading the business organization at Facebook. And then Audio 28, quitting my job, came back to India, worked on entirely the same journey as Nikkuns, currently working on two founders. So, Nikon, you are in us, right? Yeah, so I'm generally based in the US. Chinma and Abhishek are based in Bangalore. Right now I am visiting bangladesh to spend time with the team, basically. So you have basically presence in Bangalore too? Yes, we have our team primarily based in Bangalore. So anybody else from no, I think. That'S pretty much it. If you all can also just briefly. Introduce yes, reintroduce yourself to your friends. Yes, she's there, I think. Yes, you're on mute in case you're. Saying something. Maybe you can go first. Soma yeah, sure. So my name is soma my background is in engineering, no statistics. And then since maybe since seven, eight years, I'm operating the space of no air Force or Good within the education sector for close to four to six years. Contributed to a platform called Diksha, which is kind of like a learning money system that's by the central government. That's the chance occurrence that happened here. And I've been at the Orban Institute for AI for Social Impact since last two years, where we work on basically various problems concerned with public health, education and agriculture. So largely I work on various problems with him. I'm more like research manager. Not really, I don't do like a coding these days. But that's what my background nice. Great to learn about your background, Suma, and if you can help us understand a little bit about the organization, that would be very helpful. Sure, yeah. So I'm looking to say that no, we develop ML, but we are not Facebook, we are not Google. We write software, but we are not like the TCS hurting forces in the Indian context. And we are not for profit. But you're not like a typical not for profit. It's a very niche tech space that we operate in. And we work with most of our not most, actually all of our customers or eventual customers, or basically you can call like one third of the permit. But our primary users are some government machinery. If you take, let's say, public health to space, it is typically the Asham workers of the world, or the physicians, depending on which industry we are talking about. And in agricultural space, we work with the other non for profit organizations like Kambuza Singhman Foundation, for example. So instead of directly reaching out to the farmers, we work with extension workers where they educate the farmers on using this technology or solving their problems. In the education space, we don't have anything concrete, but we are working with the Gujarat government. I think Gujarat is very rich in terms of more data. Every student gets tracked with unique ideas and so forth. In terms of our work, we operate every problem that we pick up. It's as if it's like a micro startup. We have to align the government, for example, we need to solicit problems from the government, what problems are in our interest to them, because our constraint is that we will not have a lot of ground presence. In some sense, we are like a leveraged organization, even though we are kind of big. For a non profit sense, we are kind of big, about 120 plus people. But for the impact we like to create, this 180 people is not sufficient. So we need to have a very large ground presence. So consequently we will not be able to grow at that particular scale. So we only use technology as a lever. But to make this technology nowhere the robot needs to meet the road is when you need to have this large ground presence, which is where we ride with government partnerships. It could be state health programs or central health programs or in the case of education, secondary. You actually work with Minister of Education, both central and state departments. It was an agricultural kind of scenario. We work with various state agricultural units, so on and so forth. That's how we get the scale. So both the ends now the problem solicitation and eventual deployment or the is where we have to work with governments or allied partners in that space. Of course, no, we don't seek any funding from the government. So we go to organizations like for example, just like academic institutions, we write funding proposals. Okay? For example, with the Google for India, we got about a million dollar funding from Google last year, November of last year. Likewise. We work with Melinda Gays Foundation. Of course, they have a large presence in the public health space. Likewise USA, which is the USA funding arm that funds various programs, mostly in public health in the developing countries. India is still not like a developing country, right? Likewise we work with the counterpart of US state in the German setting called GIC. Likewise, so these are primarily very large funding agencies. We don't solicit typical CSR funding like $20,000 because that's operationally pretty difficult for us. So we source we have something called as a technical support units which sit in the government. We see problems. From there we write the funding proposals. If there is an alignment, there's a funding, there's a donor, there's a taker. That's when we pick up the problems. Obviously, ML and AI has to be a company. We are not a typical software company in that sense. From ML angles, we are about 20 to 25 people like Yash. We have a very bright young team, about 20 plus and few senior leadership team on the side. And we have funding, program solutions, communications design, allied disciplines. We have subject matter types like physicians, doctors, technical experts, entomologists to validate to bring that kind of domain knowledge and ground knowledge. Yeah, that's lovely organization. Lovely. Very nice to get this background overall. So basically one of the things that I'm hearing is like the project specific. There is not like a transactional model that for this project will deliver Blah and will charge like $10 million. It's more like the overall organization gets the funding and fund all the projects with that money. Basically no. In some cases you can actually have a suite of problems for one grant, but typically Google grant, right. With Google grant we are solving four problems. Okay. Generally it happens both ways. That one seed grant to fund, let's say many subproblems with them, or one fund for one project depends on the donor and the kind of problem we pick up. It can be either ways. Got it? Understood. Sounds good. Take care. This is very helpful. Zuma. I think today one of the things that we would love to do a deep dive into is what is the current state of this evaluation that you all are doing with respect to different machine learning platforms. Where do you think you're facing the maximum challenges and what are some of the immediate next steps that you are intending to take? Basically, we would love to get that background. Of course, all of us here understand the space very well. So if we directly start talking the problems and the space, I think we'll hit the ground running, basically. Yeah. So I'll be happy to share a couple of documents. We're also like you mentioned, right, particularly, we are not very big on the next set of things. What I meant to say is that we are not like a core engineering department. We don't have a very notep expertise in the ML platforms. It's a decent as far as the subway service as well as far as front end and back end is concerned. But at this time, every problem almost like it's figured out on its own, right. We don't have a consistent this is my ML stack, this is my ML platform, this is how we deploy. There's nothing of the sort as of now. That's what we are trying to figure out. Maybe I'll mention a couple of things, a couple of overarching objectives while we are solving from an air for social impact perspective, right. The other thing you want to do is to open up our innovation process in the sense that if you equate, let's say the Cadillac of the worlds of the web dot here is the world, right? You put a public data set or the reference implementation and let the community build on the top. Okay. In that sense, no. Even if you have Namal platform, can you with a flip of a switch, you open up your internal process, basically exercise into the process. Consequently, maybe students and undergraduate students in various institutions across India, instead of solving the same amnesty kind of hollow world problems, right. Can they not pick up one problem that you already worked on? Make improvements along the entire ML cycle? Not necessarily just the model building, basically. So the abstraction I like to think about, ML model is just a compensation with that. So as long as you open up your dash, people can make modifications anywhere, including no data preparation, data augmentation, better data, more data fitting, better models, better architecture, better evaluation, right? Anywhere. So there's a scope to improve any and all of the individual pieces in the puzzle. So in that sense, the platform that you like to think about should also support this aspect, which I call Open Source open Sync model. Okay? Open. Open. Sync. Open Sync. Yeah. This is a backward integration, right? I mean, if you look at the open source world, you have four integrations possible. So you can download like a model from hugging Face and you are open your options running. Right? But consequently, we also want to give a tangible proof of work for people to contribute. For example, I want to be able to say that this particular model got integrated and that actually is actually doing automated decisions on light progressive automation. Or. It'S better than what you build. Suppose you take chest X rays. Let's say we are ready to say 95% PPV for all the classes in chest X ray. Maybe some students can figure out a better tomorrow, right? It actually happened, by the way. We already gave problems to students. For example, we ran one computation, we outsourced two problems to Harvard students. Of course they did better and we are trying to think about how to integrate, right? This is kind of underneath kind of a lens we look at, okay? It's not a closed platform, but wherever and whenever we can, we won't expose these pipelines for others to contribute subject to data governance, privacy, so on, so forth, wherever possible. So this is like one kind of overarching principle because there's one way we can scale up our basically the question is how do you do more with less? Is a question. Yes. Okay, so I'll share I'm pretty sure that I mean, what I'm going to say is not like revolutionary ideas or revolutionary pain points obviously must have gone through when you think about obviously you think about some pain points. I will certainly go through one, maybe if I want to talk about two, three aspects which are very particular, which are non negotiable for us, or that we should not be locked in to any particular platform. Okay? So that's 1 second, we want to have an absolute decoupling from the so called training to inference because we work with government agencies, so we don't control where these models get deployed. For example, in one particular application, it could be the CDAC. I'm sure you heard about CDAC, right? In some case CDAC builds the application. We need to ship our models to CDAC water environment. So CDC wants to wherever it wants to host, or it wants to they should have the flexibility. Like in some other case, it could be the National Security Center. Nic or even government is thinking about coming with an alternative nic, I think it's called Digital something, I don't recall the name. But the point is that we can't control the deployment environment, right? That's number one, even if it's a cloud, we can't control it maybe the second point is that in some cases, like if I take an example of the test management example, where these need to sit on Android phones, again, operationally, how do you do it? Right? We need to go through these compression cycles, making sure that these models work on these low end denominator phones, right? That's one things like that, yeah. Effectively, we don't control the environment. It could be mobile or it could be cloud. In some cases, we can control our own deployment, but that's an exception more than a rule. Okay, so just a quick follow up on that. Like no vendor lock in and a good separation between training and inference, basically. Training and inference in some cases. Right. What do I also mean to say? Vendor lock in, right? In some case, let's say it is a Google.org, right? So Google gives us, let's say some credits must be free GCP credits, like whereas AWS gives us free credits. Okay? So we want to exploit both of that. Makes sense. I also explain a little bit about our current ongoing efforts in coming with a reference ML platform. When I say ML platform, I don't mean to say that we do the core engineering stuff. In some sense, we are looking for a base of integrations. And look at I can tell about maybe 1015 aspects of quality tributes that I'm looking for. In other words, when you develop, let's say, a particular model or a computational lab using a suite of frameworks, what is that I want to guarantee my customer with, right? So you can actually separate them at different layers. You can separate them at a data plane, you can separate them at the science level, you can separate them at the application level. For example, at the very dry level. Can I ensure reproducibility, for example, these predictions were made at this particular time? Can you actually rerun this entire pipeline, even if two years later, can you reproduce it? Right. Reproducibility is a very probably one of the most enablers of the reminding ones. Like, for example, auditability. Right. Today there are not many laws governing governing aim. Right. Responsibly framework is kind of decent, particularly in the Indian context. But we are very aware that lack of regulations or lack of compliance is not a thing for us to think about those situations maybe years down the line when we deploy, let's say, a mission critical chest Xray model that we already have. So somebody could say, why was this prediction made? The government really cares about those things. So at which point we need to know what went into making those predictions reproducibility so that it can actually support auditability. Moral observability. These are fairly kind of engineering heavy, not very science heavy kind of things. I'm expecting that these are the concern that data science should not care about. This is something that a platform should offer the other aspects or model observability tracking, I mean, a lot of tools exist out there, but generally speaking, these are taken care of by the frameworks. If you look, for example, we use weights and biases like essentially for experiment tracking and metric logging. Right. And we don't have any framework today on AB testing. Of course, it's fairly advanced use case or advanced requirement to run AB experiments. But it's good to think about them, right? Things like that. So in terms of where we are, we're evaluating metal flow just a second. Sure. Yeah. Sorry, I think that time you were a little bit on and off so we could not get a quick introduction. We're not able to talk to you properly. Great to meet you here. Hello. Am I doing? Yeah. Yes, sir. I'll give a brief from ID for a friend of Chennai I've been working. With. Close to two years now. 1.5 years and few months as an intern working as an associate machine learning scientist primarily on the Newborn Entropy project. Awesome. Yeah. Thanks a lot. Thanks a lot for the intro. I'll just say we just wrote about ten points you would address. For example, you want to solve these pain points by this key end. Suddenly this ML plot from Platform or ML locks will be one key requirement for us. It just be a second. These are pretty high level statements. Okay. For example, in a data side, I don't know whether it's your concern, but we are like looking at no data sets are not discoverable across projects, right? We don't have something like a data like architecture. And all those people collaborating with same projects should be able to discover either primary or raw data sets or the intermediate prepared data sets. Okay? On the compute side, obviously, we have one on Prem resource which we fight. It's a kind of friendly fight. I mean, we do negotiate, right? But there is no system like a queueing kind of a concept. But what we like to have, which I think Metaflow largely solves, is that can we get spot instances with very little, for example, Metaflow with the decorators you can ask kind of compute that you need best declarative on demand paper use, which is flooded plastic that is largely taken care by Metaflow. So testing some kind of testing framework, something like maybe evidently or Pi test. I mean, it's responsible to greater designs to write the test cases. But nevertheless, how can we integrate them with something like Theater of Actions so that after the tests are passed, the model should go to, let's say, a pre prod environment which we intendly want to let's call it like a demo environment. In other words, we want to have all the data scientists to think about going from end to end to end. I'm done, I wrote my model or model, I'm done. That should not be the case. Right? So this kind of a CI CD framework shall be baked in into the very process. So I likewise ability to roll back models, happiest health sensors, plant them that concern with the model logging, right? Maybe based on some alerts you want to flack off or roll back models. No, things like that. And in a live concept is the AB testing. This ABB testing CI CD, spin up a demo server. All projects will have a consistent demonstration platform where we can showcase the solutions when we go to funding. Right? This is important. When you go to funders or when we talk to governments, we need to go with demos, not just concepts or ideas overarching idea. Is that how do we reduce this time from ideation to deployment? Okay, we should not basically get into this more of a two language problem or rewriting the code for the sake of deployment. That should not happen. Obviously there are some challenges with it, right. Particularly when we want to deploy into the models. And a lot of your appeal processing happens in Python and you don't have any good run time in Python, how you handle those scenarios. Right. There are some blind spots like that, but that's the space we need to be. And reliability is not constant with Paris engineering, but it's mostly about the science of the things. For example, you are developing a library called Effective Machine Learning where we want models no explainability robustness and quantification to all shall be bagged in. Now, regardless of what kind of problem you work with, it could be computer vision, speech or NLP and some standards, right? As a responsibility, we internally are developing responsibility framework and model cards and data cards. Consequently, every project comes with a model card. It comes with the data card. Obviously, if you delegate this responsibility to data scientists, the metrics can become scale. They don't get updated. So can we not think about these model card themselves? Or like a data driven or even for that matter, data cards, schema validations, self consistent checks about the data or data quality issues, all of them can become part of the data cards. They can be, they should. In other words, these things should be leaving documents, not something they need to be got updated. And the responsibility can be delegated to tools. Right. So here, if I divide the kind of responsibilities the platform is looking for, you can divide them into data compute, testing, department observability, reliability standards, standards. I'm going to say that can we not even have conventions as far as coding is concerned? Like for example, we piloted Kedro as a platform, I mean as a framework. But Kedro comes with opinions on how to structure your code. So it's an opinionated structured code structure along with all the things that I was referring to here. Right. There's something that we are looking for. I understand that not everything may not be possible, but can have. Every internal thing that we have is to let's pick up a particular tool, like a framework that handles one particular response very well and integrate all of them. That's what we buy platform. Now, I want to understand what Trufantre does a little bit about your company. For example, how many people are you? What is your vision, what's your roadmap? This is a very competitive space, right? For example, every big player like Databricks, Google, IBM, you name all the I don't know about Meta, though, but they all have their own platforms, right? So it's always very difficult actually ask for somebody like us, for example, to try, let's say, a new entrant, right? I mean, I see you like a startup and you might be solving problem very differently, but what is your differentiator, what's your roadmap general concern? I'm pretty sure that it's the same question that you get asked right now when I can go with all the big vendors, why should they come to. You. Especially your roadmap, in terms of the stability of the company? Maybe you can talk about those things. Sure. Thanks a lot so much for giving that background. And we will certainly talk about True Foundry, our vision, and why do we think we should build what we're building, right? So we'll answer that question. Before that, I just had a couple of other clarifying questions for you. Number one, that given that you mentioned that you want to build out vendor lock in, right? You want to be able to leverage your AWS GCP, Azure credit, et cetera. Do you want to build out, for example, do you see a world where you could be using Page Maker, a Vertex, and an Azure ML and leverage the credits across the three clouds? Or would you rather have something more standardized works across all the clouds and just leverage credits as you go? Yeah, I think the utopian world will be like, I don't care about whether it's that it's A or whatever. I should be able to, for example, how Metaflow does, right. Basically, it's already providing those abstractions. My code is not changing. In other words, I don't want my data scientist to write a new code that's specifically targeted with XCI, something like that. But that's the ideal case. But in case I don't know how the features are going to look like, particularly, let's say no, Google offers a platform, maybe we'll be trying that particular platform. It depends on the mostly the constraint is not so much on us, but mostly on the client side. For example, let's say in some cases, right, the government actually has a contract with Azure. Yes. In which case it makes sense to have the deployment environment on Azure because that's ecosystem that government already paid for. Right. So at least on the training side, we want to be Plaid Agnostic. On the client side, I think it's largely a matter of the government's view on what that should be, which is where we need more flexibility. But going. Back to this particular side of the story. As we go out and reduce credits, we like to leverage that cost advantage. Understood. Okay. Got it. So this is very helpful to know. The second thing is you talked about some of the demos and deployments as a use case, right? Like in your wonder sheet that you had. Right? Yeah. So when you think about demos, is it basically like you build out a proof of concept that an end client can quickly try out or see some of the results before you actually get to building like a full projecting that's what you mean by that? Exactly. Right. For example. It's like MVP. You can think of like NDP even before I think one particular stand that we take is we don't go with ideas to any of our customers. You build a prototype, however minimal, that is then show to be shown. Right now, we struggle a little bit. There is no consistent way to spin up these demos just as a part of your product experience. Right. It has to be like an afterthought. Okay. I have a model. Let me write a streamlining application model. Let me write an application in Grado. Right. At least the API should be spin up automatically. Then the front end can be very minimal in Grado or streaming. Right. That's what I mean by basically give the APIs. As soon as you go through, you have a basic model out. It passes your minimal kind of test set. Then the API should be available with the consistent naming conventions or names. Namespace or Blobs. Right. So by project by name, whatever that is. Yeah. I see. And the third question was, like when you said that actual deployments and CICD pipelines basically integrating with your GitHub actions, writing tests, et cetera, those things, again, does your team actually deploy the model Endpoints and manage it end to end for your customers? Or typically you see that you build out the models and the deployment and management of endpoints is done by them. Yeah. So right now, or at least the one couple of models that are deployed, we are managing that. Okay. But few we will not be able to do it or we will not do it. Okay. Both cases exist. Understood. Got it. Because we manage the CI CD model monitoring and in some cases we ship the model. But the challenge here is that that's a difficult spot to be in. Right. The maintainers of the models may not have the knowledge to fix the models. It's a loose kind of a decoupling. We still want an ability to ship models, which is where observability becomes very critical, even if the hosting environment is not in our control. But we need to have the ability to observe, ability to ship in a guarded environment. Understood. Sounds good. Very helpful context. Soma, just a quick thing, abhishek, Chinmay, do you guys have any quick follow up questions? Before we give background about truefoundry on the stuff that someone just asked. No, I just have one question. Somal like, when you hand it over to government agencies, what is the variation of the deployment info that you have seen? Is it all three clouds? Is it Kubernetes? What are the different options that you have seen so far? Yeah, cobalted is a big thing. Okay. I don't think it's a beast. Okay, so right now, let's say if it comes to and we can say I need an easy to with the certain GPUs, right? That's as far as it can go. At least based on my conversation, that's the case. I'm not seeing a future where we can ask for these self managed clusters, et cetera. Pretty much there's like an old school tech stack, not a modern EKS type of stuff. So they give you one machine, right? So you get access to that machine or what access you get? We might get access to just like as a BM for us because of the credentials. But I don't think we will get these no scale deployments, no cluster configurations, cluster management, it's mostly like no instances, one VM, whatever that you can ask in AWS. It's more or less similar to that. Even if Ni has a cluster, right, I'm pretty sure it will be like old school. They may not have any Kubernetes clusters. Okay, understood. Got it. Cool. Google. Awesome. Sorry, go ahead. You had something? Yeah. So I just want to ask how are the development environments provisioned currently for the machine learning scientists? Like do they spend up easy two instances or do they work on the on prem device? Do you have like a custom manager? How does it work? I think Yash might be able to talk about the on prem, but for others we take a dedicated for a project, we ask for what compute do we need and you spin up that particular and you fold it for its lifetime. And which we certainly want to change, which is where for example, metaphor is coming. And yes, you can talk about the on prem. For on Prem we have like reject machines. There's a certain set of GPUs, that's eight GPUs and 80 course. And we have internally set up something like this, that if I'm using GPU number one, then I'll be using the CPU course ten to 19. There's no kind of scheduling or anything that we are using right now, but that's just something we are exploring, having some kind of slump scheduler in our onprem. But we generally we face trouble in finding the GPU because as the number of projects are increasing, number of books are increasing, the GPU concept is getting more and more. That's why we are moving some of. The projects to the cloud. Makes sense. Cool. I think in the interest of time. We'Ll move forward to explaining a little bit about what we are doing now. Soma and yash. So I'll first explain, why do we think we should be building what we're building, right? Why do we exist? What's our main focus in the market? Right? And then I'll try to explain, based on what I heard from you about the use cases where we might fit in. And then we of course will have to take a follow up call to dive deeper into things, right? But the very specific problem that we are trying to solve as a company is we see that machine learning, like for large companies, usually they have all these different cloud providers that are building out the ML platforms sage Maker, Vertex, et cetera. Very frequently companies care about being vendor agnostic, cloud agnostic. They have either a hybrid cloud setup or a multi cloud setup and or a multi cloud setup, right? Now that typically rules out that they cannot use like a Sage Maker and a Vertexure ML and build something in house on Kubeflow for their Onprem setup, right? It's just too much maintenance. So they ideally need one platform to be able to build, train, deploy, monitor machine learning models across different clouds, and ideally be able to optimize the cost across clouds. Basically that okay, this type of machine, this type of training is cheaper on AWS versus a sure or whatever. They can do stuff like that, leverage their credits, right? So that's a segment, ideally, that we want to focus on. Now, the part of the machine learning stack that we go super deep into is deployment. And now when I say deployment, deployment itself is a pretty broad term. So deployment could be you have a model, you quickly get an API endpoint out of it. Running on a model server, you have a function, you get an API endpoint out of that function, just any arbitrary python function. You have a model that you want to train on a schedule. Basically, that every day at 09:00 A.m., this model triggers because I get some new data set. So all of these fall within the scope of deployment. Right? Now the area where we build out the maximum IP, where we contribute the most in this entire setup is two fold, okay? First fold is we do the very complicated plumbing layer across different clouds that make it super easy for people to manage. Like all these best practices, like how do you do CI CD across clouds, how do you integrate with all these different GitLabs and GitHub actions, et cetera. How do you integrate with AWS, GCP, all the secret managers? How do you integrate with all the monitoring systems? Basically, the super complicated plumbing layer is what we handle that's under the hood. Okay? And the second major contribution that we make as a platform is a very slick developer layer, basically the UI layer for a developer that the developer can do practically whatever they want, but they really don't have to understand the complexity of this plumbing layer that we have built out to them. They are writing a Python function. They click a button, they could deploy it on AWS, GCP, Azure, do a CI CD on GitHub, actually GitLab, whatever that they want, basically. So we try to make that problem really simple for a developer that's in a nutshell the area where we focus and where do we contribute. I'll take a pause here to see if you have questions. That looks good. Okay, go ahead. In your documentation you were mentioning I did not read fully. I was just letting her part that it works with the Metaflow, right? How does it work with Metaflow? I wish you want to take that? Yeah. So Metaflow is a pipeline orchestration. Summer we don't handle the pipeline organization part yet. Metaflow, you can schedule a training pipeline and everything. We are more on the model deployment side. So when you want to host your models or services as APIs, so that's where we come in. We do plan to integrate something like a pipeline solution like Metaflow, but we haven't really decided on that front yet either. Airflow or there is arguably that is Kubernetes native. That is something that we plan to integrate, but ours is more like if you want to deploy quick demos, metaflow will not give you an API layer. That service is running 24/7 as an API. So almost imagine. Soma what will end up happening in New York is, as you mentioned, that you want to separate out your training and inference layer, right? Basically. So what would end up happening in like the kind of integration that true fondly build out with Metaflow and to us like building out an integration with Metaflow, AWS step functions, Airflow, etc. Will all fall in the same league. What will happen is you will run your Dag orchestration on Metaflow. Output of your training layer would be a model. Right. We have our own model registry. So you can publish that model to our model registry and then we can take over the deployment piece of it. Then if you want to run a batch inferencing, if you wanted to run an async deployment of that model, if you wanted to run an API layer out of it, you can do all of that with almost like a couple of API calls that we have on the platform, what the integration would look like. To also answer your question, remember I told that I first will explain what are we doing as a platform and where would be written in the use case that you described. Right. So a couple of areas in your ten pointer list where we think we already have a very core strength in is your testing, demos and deployment. Basically testing, demo and deployment is the area that we already solve for very strongly. I'm almost seeing a cut, right. For example. So we onboarded couple of projects on the Metaflow. It seems to be working well for us. Right. Basically, if you can see the separation between, let's say training plus inference, I mean, that includes deploy and inference, right? That's where it seems like you could help. Exactly. Yes. How are you using metaphor, actually trauma like you're currently using it on AWS? Yes. When you go to GCP, have you tried Metaphor and GCP? Yeah, I think we haven't tried it but based on of course, we are in constant touch with CEO which is managing this. So he said basically you just need to provide a different cloud formation template. Basically fill up your confirmation, whatever that is. It could be three confirms, one for AWS, one for GCP and one for resort. And basically you can just mention where you want to run this orchestration file. So the code did not change at all. So for example, you say Metaflow run, you can say it's like AWS, then it will pick up the AWS configuration. So that's how you get this cloud. Agnostic with Metaflow, I think what Metaflow? I'm just confirming once I think Metaflow will run on top of Kubernetes. That's one option. I think that's another. It's a managed service by Metaflow themselves. So it can work with AWS batches functions or it can work with EKS. In fact, this morning William offered their commercial platform for free for us, where they host this EKS later with auto flows. So they take care of the maintenance process. Sorry mentioned he offered their commercial platform right. To try for us to try out their commercial platform. Okay. Also comes with the CI CD. Avin is a close friend I don't know if you have met with. Yeah, we always speak in constant touch with Seven. Yeah, same like Seven is generally a close friend. We hang out with each other in the Bay Area. You know each other? Yeah, we know each other pretty well. Yeah. Both of them are very supportive. It's a fantastic book. They are metaphor builders from Netflix. Very good team, very good people for sure. I wish you were asking something. Yeah. So our stuff is actually very similar to metadata flow. We also do that same Kubernetes thing like the spin up on whichever cloud you go, we automatically spin up the Kubernetes. So it's all managed by us and then the motors are deployed on that, the auto scaling and everything is handled by default basically. So fundamental architecture is very similar. Okay. And in fact, by the way, one of the things that I want to mention that we have not done this officially yet, but I did talk to Savin as well about this integration with Metaflow, like official integration of true fundamentally metaphor as well sometimes that they handle the orchestration orchestration layer and then kind of plug truefoundry on the model deployment and maintenance layer as well. We have also talked about that as well and that I think is probably the most ideal in the describing. Generally matter of substitute is free, right? I mean, so for the source and they also offer it a commercial platform. We haven't decided yet whether to try it or not, but at least what's interesting working well for us is the inference part. Okay. We are wondering, for example, on the deployment stage you would try Sage Maker. We haven't figured out. I think that's the learning phase for us, right. So how ready are you at what stage are you in? Yeah, so let me give that background. So as I mentioned that we are designing the platform usually for more enterprise customers, right? Because the target market that we are looking at, like multi cloud, hybrid cloud, usually comes when a company goes beyond a certain scale. So in that aspect, we are already deployed on multiple enterprise clouds who are using our platform and their ML models are already in production on top of our platform. So that's the stage that we have reached. Now that said, obviously it does not mean that our product is fully ready. Every single creature is working, etcetera. Core components are working. For the things that you described, right? Your model deployments, demos and testing layer, these are already working parts that we are still building out, which are very strong in our roadmap are things like A, B, testing and then creating our own standardization layer about how do you write code. Those things are some that are still in the works. Basically, we already work with all the three cloud providers, AWS, GCP, assured we rate with a lot of major, basically CI, CD frameworks managers, et cetera. So those are some of the things that we already do. That's where we are in terms of you also asked about the team size. We are roughly a team of 20 people, 15 full time and five contractors and interns that are working with us. Most of the team, of course, is technical because this product requires a lot of technical effort. We largely hire people from companies that have already built out these ML platforms and bring in that experience so that we learn from the best practices of these large companies. The other thing that we have done in this aspect that I think is helping us a lot and we have gotten the architects of AI platforms of most top companies and advisers with us. So like, we have people from Google, Facebook, Snowflake, Uber already working with us who are advising us so that we don't take the wrong design decisions. Yeah. And what is your business model? Do you have like a free tag versus commercial tag or something like that? That's number one. Number two, how do you basically sustain yourself? I mean, this is very aggressive space and getting customers is always a hard problem in pretext, but selling is not exactly. So how do you address these two? So in terms of the tiers we of course will handle, we will provide a free tier for a certain period of time that you can try out the platform, et cetera. And after that, if you want, you can get converted to like a paid tier, of course. Right. So that's how most of our companies do it. Usually the set up, what we have seen work best is when people are trying things out. They use more public data sets or less sensitive data and try it on a public cloud so that they can get the full developer experience of how do you deploy models, how to deploy functions, etc. And once they are fully ready that okay, yes, we want to move ahead. They do deployments on their own cloud as well. So we support both versions like public cloud and private cloud. So that's the usual setup that we have seen in terms of sustaining ourselves as a startup. It's always a hard problem, but we have picked the niche of this hybrid cloud multi cloud thing, right? So that way we already get past the competition with the Sage makers and vertex AIS of the world, right? And realistically, like in terms of being enterprise ready, when I say enterprise ready, the entire infrastructure allocation, cross cloud, Kubernetes management, secret layer management, SSO integration, et cetera. There are actually not a lot of players that do that have built out the product that much in depth. From model registry to deployment to monitoring basically like we have done for enterprises. Essentially that part of the system is not as crowded as you would see the traditional ML ops. Basically you always as a starter have to focus on a niche and then expand from there. Our niche is what I just described to you. Okay? And of course we have support from our VCs like we are funded by Sequoia. So that helps. We do have a decent runway already for us working in our favorite favor. That said, you know that we are not for profit, right? Sure. So I think our space is very different, the kind of scale, et cetera. See I mean at some point we might be scaling, right? Right now we'll accept 20 projects with not a whole lot of throughput. Okay? But things might change. We are in for the long haul. So certainly if you want to think about, let's say this is a partnership, let's say, right, it requires time, there will be bucks, there will be features, not everything that anybody wants to offer may not fit well. Right. So that needs to take place. How do you work with these rent prices? I mean, do you dedicate, let's say one relation manager or somebody like that? Then you evaluate more like solution architect, right? Sure. This is how you promote, this is. How you do it. Yeah. To be honest, we are a small enough company right now that for our customers currently that we are working with, we are the solution architects. If we decide to like, for example, if you work Yash Wadhwaniai, you will probably be constantly in touch with me generally about what are the features that we need to build out. And we appreciate the fact that we are not fully ready. People might need more integrations, and that's how the platform is designed. We have built out the plumbing layer now, for example, we already work with GitHub actions and GitLab. If you come and tell us GitHub actions and GitLab, let's say, okay, come and tell me that. Okay. I want to work with a third thing. We will build out that integration because that's how the system is designed. We will actually go and build out these integrations to make sure that our early customers are happy. Obviously, interests are very aligned because it helps us expand our platform and prove the use case as well. Right. That's what we will do very actively with you. All right. And of course, given the nonprofit nature, soma wadhwaniai things around, commercials are all we will also keep that flexible because, again, something that we will adapt to, the nature of the businesses that we are working with. It's not like things are not set in stone for us as a startup. Right. So we will make those things happen as well, assuming that we find out a good use case to work with. One of the things that we are very particular about soma is like we don't want to get into places where we think we cannot add a ton of value because we have resource constraints and we want to make sure that every resource that we have is really getting optimally utilized. So before we decide to work together, I think the evaluation would be mutual. Basically, we would also want to really understand that your use cases fit into the problems that we are trying to solve when there's a long term alignment. And I'm sure you would also want to understand that local it seems like a good fit as well. So we'll have to do a deeper dive into that to make sure that there's a good fit. Once that happens, we are ready to go at any length to make things work neatly for people that we are working with initially. Okay. Anything else more to us? Yes, so the thing that I would love to do is potentially do a deeper dive into understanding the document that you have. By the way, if it's okay with you, if you can share the document with us, that would be helpful. I would love to do a deeper dive to a show you the platform about how we think some of the how we design the platform, how would it potentially fit into the use cases that you described. And we would love to understand more in detail about one or two use cases so that we have a mental model of okay. Like what we are imagining with a short call. Right. Now will actually work out in the longer term. So if we can do a deeper dive there, I think that would be a very good next step from your. So basically we can have two sessions, right? One you give, let's say demo of the group on actual application. Not like features bells and whistles but something in the pudding. The third, if you consider this as the first 3rd, we can actually show maybe Ash himself can show his project, right? He can walk you through the existing project and see putting it CACD I think is making a lot of sense to me. Also I want to know the pricing model. I don't know, I can't conceptualize how would you price things like this? When you say management, do you bear the cost? Or suppose I get, let's say free AWS credits. I can actually lend you my platform, my kind of intro. You simply become let's just the plumbing layer. I'm not clear on what the more of the pricing is. Let me give a perspective already. Actually it's a very simple question to answer quickly. Basically you can have proof on to deploy it on your AWS account where you leverage your entire AWS credits for managing the infrastructure. Basically your cloud overhead is exactly zero. Like you can completely leverage your AWS credit. Where you would typically pay is for the platform usage fee. And at that point you are not paying us by the how much are you using? Like you could be using it for running a billion queries every second. It doesn't matter, right? You would be using it simply by the number of developers that we are helping at that point. Ten developers using the platform developers. So it's a very simple pricing model that way. Okay, that's realistically. Like all there. Right now is it too early to ask any sentence on that the seat how much is. I can tell you the pricing, but I want to make sure that you understand that if the use cases are good, surely the enterprises that we work with, usually we charge roughly $5,000 per developer per year. So if you're using whatever like five developer seats or something, you have like a contract of $25,000. We do have certain minimums there typically. But we'll work out these things. We can figure that out. I mean, if there is a mutual benefit to both and that depending on the number of use cases, blah, blah, blah, we can figure out exactly. Let's part with question for later. Permit has to be sensed, that's all. Yeah, okay. Next call we can see the true foundries but little CIC and deployment part. Sure. I will suggest one more thing. I'm flexible to do it this way that you suggested. The second call we do a true Foundry demo and third call we do like a use case evaluation. I think it will be mutually beneficial if you flip the order because then what we can do is we can actually give the true Foundry demo focused on the use cases that we think are more relevant for you. So that means the demo will be more meaningful as opposed to giving the generic plan platform demo by yeah, makes sense. So we'll figure out what and when the next thing. Yeah, take care. Can you stay? Also, I did not see that this was actually recorded. Okay, maybe use it for your own personal things and not shared with others. Oh, sorry. I think it sends out a message broadly sorry, but did not realize. If you want, we can also delete the recording realistically. Like this is only for our internal purposes. Because sometimes what ends up happening is we are giving demos. We don't end up taking notes. And this helps just to make sure that whatever we discuss that as long. As we don't share it, I think it's okay. We're not sharing it with anyone.