Can you hear me okay? Yeah, I can hear you. Can you see me? Yeah, I can. The video does not load for me. Let me read you on. Okay. Yeah. Hey, Vivik. Hey, yourself. I'm good. How are you? I'm doing well, thank you. Yes. Sorry, man. It is early for you at 09:00 A.m., right? Yeah, that's all right. I think even Nikkunj will be joining. Just a second. Someone is just joined. Hello? Hi, Yusuf. Hi. Actually, I'm having some difficulty seeing the video and that's why actually, I disconnected and rejoined yourself. Let me just take one more minute and I'll figure this out. What's happening. Okay. Let me also check with Abhisheki if he is joining on that part. You're based out of Toronto, right? That's right. Just bring them. To reward. But by the way, Nigong is the cofounder and the CEO of the company. He'll just be joining. I think he mentioned some problems. Hello. Sorry about this. Still having some challenges with the pipeline, so I decided to dial in from the phone instead of keeping you waiting. I appreciate you taking the time. I'll quickly set the context to Abhishek last night, and I also checked with him if he's joining today. But nonetheless, I'll go ahead and set the context on that part. So, Abhishek reached out to Yusuf when he was visiting Toronto last time for a meeting, which kind of got rescheduled to Bangalore. So he requested abhishek requested Yusuf to have an online meeting, and he has kindly agreed to meet us here. The idea was to get more feedback from his side to understand his ML pipeline, the challenges, the use cases that he's handling, the infra that he's managing from his ML pipeline point of view, and as well as getting to know him a little bit better, to help us in our journey where we are. That's the whole agenda. Like, two or three things. Over to you. Thanks for setting the context so you so I'll just briefly introduce myself first and then we can take it further. So, basically, I'm co founder and CEO here at Truefoundry. Before starting True Foundry, I used to lead one of the machine learning teams at Facebook, working in conversational AI. And prior to that, I was leading the ML team at a startup called Reflection, where we built out recommender systems and personalization algorithms for the e commerce industry. And there I got a chance to build a horizontal machine learning platform for the company. So we spent about a year and a half building a platform that would help operationalize the models across five different teams that were using machine learning. Right. And I think that is primarily one of the key insights that we learned where, like, you know, we worked on building a platform, and then we worked at Facebook, where we saw the platform was, like, just doing so many more things and so much more efficiently that we realized that building these platforms in house is actually not easy. So that was one of the key learnings. Anyways, prior to Reflection, I was doing my masters, went to UC Berkeley, and before that I was in India doing my undergrad at one of the It's. So that's a quick background about me, Yusuf, between Facebook and starting True Foundry, I did one more startup in the talent space called Entire, and that got acquired by one of the largest HR tech players in the world. And now at Truefoundry, the goal for us is to take our learnings from Reflection and Facebook and build out a platform which enables other companies to build and deploy and monitor their models faster. So that's where we are. I would tell you more about our journey, but would love to learn a little bit about you as well. And one more note, I noticed through your LinkedIn profile that you are a researcher at Must, is that right? Yeah, that's right. Yeah. So we met the founder of Must, Joy, also recently, because he's also from the same album at basically. Right, okay. I wish I had mentioned, I think when did you guys meet? Maybe about a month ago, like three. Or four weeks ago. So I have been associated with Must since maybe four or five years, although I'm not active at this point because I moved here. So Joy live in Hazabat, I think you might have met him there only. So I met him and we used to do a lot of stuff there, but then I moved here and since then I think engagement is comparatively less, but they have been associated with the team. Nice, lovely, awesome. I would love to hear a little bit more about your users. Yeah. Thank you, Nicole, first for briefing me. I think, before I start, one quick question before I forget. This horizontal eminent platform which you build, this was before Facebook or after Facebook? Yes. Are you talking about? True foundry. No, it was before Facebook. It was before Facebook. Okay. And then you joined Facebook, and then in Facebook also you build a platform. You worked on that or then Facebook. I did not build no. I think by the time I joined Facebook, I think they already had a very elaborate platform. The learning was more about as a user of the platform and what are the things that platforms like these can offer versus what were we offering. I think it's about that. Got it. All right. So what I do, I basically might have seen from my profile the data data strategy, data solutions, data intelligence for the credit. So whatever is related to data, I basically need it today in terms of data solutions, data intelligence maturity. We are at a position where we have intelligence exists in almost every stages of lending. So pay you credit as a lending lending business. Right. So we have intelligence exist in almost every stage of user journey now because this has been already matured in some sense. We obviously need platforms to scale all these solutions which we already have. Right. And for that we have recently for data science part, I would say we are quite mature for data plate form point of view. I would say we have recently started building. So we are trying to build ML plate forms, we are trying to build data platforms, data lakes and basically all that stuff, whatever is required to make this whole data solutioning stable. So that's what me and my team right now currently doing. Nice. We'll do the deep dive maybe. For sure. Yeah. And by the way, I'm one of the users of Pay use, so I'm recently familiar with the product. Okay, user as in test user or you're using it for something else as well? No, just like a regular consumer, I've used Pay You to transfer money and stuff like that. Okay, understood. Maybe they help you and Pay You, there are a lot of products, right? What you're talking about is Pay you payments and then there is Pay you credits where we disburse loans, where we give this buy now, pay later products if you are doing it. Lazy pay when you order food from Swiggy Zomato. Right? You must have seen that lazy Pay logo, right? By now. Pay letter, that is. We also have credit card, so I basically lead the credit side of the business. Understood. Okay, makes sense. Got it. Very helpful context uses. So one of the things that I want to understand from you is currently how large is the machine learning team size? Like the data science team size and people working on the platform thing anyways. And also what type of cloud providers are you currently using? Okay, yeah. So there are a lot of overlap, there are a lot of teams, I would say. But if I have to give you a number, you can assume the team size is somewhere around 40, 45, which includes data science. So just data science team. My team is around ten or eleven folks, right? And then there is data engineer Simile DevOps, basically people who are actually building this framework which I was talking about. So in total, somewhere around 30 45 people. Okay, I see. Okay. So that's a fairly large machine learning team. Okay, understood. All right. And then in terms of cloud, are you using one cloud, multiple cloud? No, we are mostly using AWS. I don't know what kind of follow up questions do you have on that and how many of them I can answer. But maybe let's try to try not to get into the system because for certain reasons but yeah, AWS is the cloud which we are. Okay. And so just a couple of follow up questions. Let me know if this is okay with you. Do you use sage maker? Initially, yes, we were using that, but then we realized there's no point when we can build something of our own internally. So we stopped using in general. Is the team like inclined to using like a lot of internally built platforms? Is that the culture as you said? Right. Initially we were using a lot of those tools, but as we have developed a lot of stuff internally, as we have created these intelligence in such a way that they are very proprietary or they are very specific to our business, our business model. Right. So we thought it's better to build everything internally. I mean not everything internally, but tools like Sage Maker and all that stuff is very much feasible inside, right? Yeah. Inclination is build the tools internally unless we get something which is very good, which we can't really build inside in house. So that's objective. Understood. Okay, got it. Now if you can share a little bit about your team's workflow of kind of building the models, deploying the models. Like are the data scientists typically using Jupiter notebooks to build the models most of the time? And are these Jupiter notebooks like locally hosted on their machines or they're like some central repository of jupyter notes? We have a quite big process for all the rest of DevOps and basically everyone is working on private AWS cloud for that. It's a fintech arena. So you can't really keep all that in your local machine. That makes sense. That makes sense, yeah. Okay, understood. So basically like the DevOps would spin up like a jupyter notebook server and give access to that to all the data scientists essentially call it. I see. And once the model is like do you all end up using anything like a model registry? Today. We are building one in house, but right now maybe I can't say yes or I can't say no. Understood. According to you, when you think about this in a model experimentation on Jupiter notebook, model training, actual training maybe, I don't know if you do any remote training generally and then deployments. I'm assuming some of your models are also real time given fintech make there's a lot of use case. Right. So like deploying the model is model actually deployed as a microservice today? Like is every model all the models? Okay, nice. So who does that microservice deployment of the models? And then if you think about the monitoring aspect of it, if you think about it in that pipeline, what are the areas that you are currently trying to work towards yousef? I think all of the points which you mentioned. We have very big monitoring framework associated with every model. Right. So whenever we already built a framework, in some sense a generic framework, very specific model framework, business specific monitoring framework. So whenever any model goes into production, these frameworks gets activated and we have to basically align. So the process is in such a way right. You build a model, you productionize it and when you are productionizing it, you need to make sure that all these two, three, four components are in line with that production. Right. So that's how monitoring is something which is obviously very important, very crucial for us. There was one other what was the other point? You asked like the real time deployments. Basically real time deployment. So for that, I think, as I said, we have data engineering team which is basically responsible for building these platforms. So we give the artifacts to these teams and they have the framework and. They just deploy it. The artifacts are usually a docker container or actually just a model file. Usually docker container, usually. But then we also have models which are a little complicated than that. Right. So maybe there's a model which is a hybrid of machine learning model and heuristics which are getting generated by certain other businesses or certain other teams. Right. So it depends. Most of the time, if it is a standalone model, then it would be a docker containerized model. If it is not a standard role model dependency, then we go by specific artifacts. So what happens? I'll tell you. Specifically for deploying a model, it doesn't matter, right? I mean, if you are going down that lane. But if you want to attach all these frameworks monitoring frameworks which you have, you need to give them artifacts separately. You want to give training data set to the monitoring framework. You want to give business definitions to other frameworks. So for that, we have not created a very connecting pipelines in such a way that one docker container would suffice. So that's why I said. From the. Platform point of view, we're not stable yet, but we are in the process of getting there. Got it. I see. So in this entire pipeline right now, for example, if you are handing over artifacts to people, to your data engineering team, you may have like different versions, different packages, et cetera, like running on production, different models might even use different versions, et cetera. Right. How is that part today handled? Again, in framework, this monitoring framework itself, we have created one champion challenger kind of framework which makes sure that every time we are deploying a model, we get a B test and all that stuff. So that framework existence in one form or another. I see. Okay, understood. Got it. And roughly how long does it take today to deploy a model? Like once you've built out a model to actually get deployed as an endpoint? There's no right answer to this. Again, it depends because as I said, it's a fintech and there's a lot of challenges we face in terms of audits approvals and all that stuff. But once there's all of these things done, considering that we have all the systems in place, it should not take more than a week or two weeks. But then again, it depends, right? For real time. Okay. What happens in any model building process we create features and then you train it and then you deploy it. Right. In a simple ecosystem, what happens when whatever experimentation, these feature pipelines building these things take place majority of the time in Python. Now, when we want to deploy this for real time usage, there is a possibility that Python will not support in the way we are expecting it to support. So for those scenarios, we want to basically convert those pipelines, put it in different language with Java or whatever. Now, when that kind of thing comes into picture, obviously the deployment pipelines deployment of the pipelines gets increased. Again, to answer your question, it depends and it keeps changing time to time. That makes sense. That makes sense. Got it. Okay, understood. I see. So overall, I think basically what you're saying is a lot of the tooling the team is building in house and you feel like most of the systems are fairly mature. Like you don't feel like any of the pipelines. There are particular challenges that you feel like you're trying to solve. Currently there are many. It's not like they are not challenges. Again, monitoring framework. Right. There will always be a chance of improvement when two person in house is building that framework, or 20, 30% in one company is building that framework. Right. You can't really compete or you can't really compare these two. It's just that maybe we don't know what we don't know. Yeah. Okay, understood. We have been doing this for us. We feel that we are good, but maybe you tell me that. Okay, so if you're not doing this, we have built something which can do this thing far better than what you are doing, then we are happy to explore. Okay, understood. Got it. Okay. So basically like it's a matter of unknown. Unknowns. Maybe I'm just throwing away the options. Right? There is a possibility. Right. Understood. Actually, how does infra allocation today work? If a data scientist needs some infrastructure to run their models, how would they request for that infrastructure? They just have to ask DevOps and. They do it for them. And how long does it take typically to ask and get the resources? Not much, I would say. I think mostly it's done in one day or two. Got it. Okay. So if you are going and asking for a GPU where you don't know what kind of GPU you want, and then you need a session with the person where you want to understand. So in that case it's different. But when you know which machine, what size you need, then it's a small task. Got it. I see. And like things like if you wanted to revert to a previous version of the model, reproduce some of the results that you wanted to do, track all your experiments, do you use? We have back population systems in place which are responsible through that if there is any requirement. Got it. I see. Okay, understood. So I think what might be interesting is that maybe if my computer works now for some reason my WiFi is not working. So if my computer works, what I can do is I can give you a quick two to three minute overview of the platform right now. Okay. And if you find it interesting, if you think that there are areas that you think could be interesting for the team and we can do a deeper dive on a follow up call. Is that okay? Yeah, that's fine. Okay, let me just give you a quick two to three minute overview then. Just 1 second. I'm going to drop up from here and join from the computer. Okay, sure. So when did you start with the. About a year and a half ago. We've been building this nice and in Facebook you were a machine learning engineer or what? Machine learning engineer, yeah. I was leading one of the conversational. AI teams and this team was what was it responsible for building data Intelligence or data platform or what exactly? No, it's a product team. It was building machine learning models actually using their platform at the learner. So we used to do a lot of NLP, NLU ASR type of work. And if you have heard of Portal, which is one of their video calling virtual assistant devices, that's the team that I was working in. Okay. So I would say more of science side of it, not engineering side of it. The way it works at Facebook is usually Facebook tries to get to this path of you build it, you own it type of model where if you are building a machine learning model like, you take it to deploy all the way to the end. But then people vary at different degrees. Some people are a little bit more model oriented, some people are a little bit more engineering oriented. Given my experience at Reflection, where I built models and built out a horizontal platform as well, I actually work both on machine learning and engineering side of things. So I actually took care of things really end to end. In fact, at some point I also built out a horizontal deployment infrastructure for specifically our team that connected to Facebook's main infrastructure, not the Facebook's overall platform basically. So I also led that up at about eight months. Okay, interesting. Yeah. Cool. So I'm going to show you the platform now. What are some of the capabilities that we have and I'm going to present more of a machine learning developers view of the platform. There is also an infra view of the platform, but I'll skip that for now. Okay. So starting with like what do you do on the platform when you think about deployments? When we think about deployments, we have three major areas like services, jobs and models. So services are basically anything that are like long running API, endpoint type of things. Right. So if you want to deploy a model as an API, you could do that. If you have a function that you wanted to expose as an end point, you could do that. It's like a pre processing function or post processing function. Okay. Jobs would be anything that is like you run once like a batch inference thing or a model training job. And models would be basically you take just the model file and nothing else and deploy that as an endpoint. I have to quickly check if I already have some endpoints deployed or they are not deployed. I'll check that. And by the way, like each of these things, you can do it using our UI. So for example, if you wanted to deploy a fast API using our UI, you would basically do something like this, where you just select whatever thing that you want to deploy. And then basically here you can actually just provide the URL of your GitHub repository where your code is, the branch where you want to deploy from, and the entry point of the function, right? That could be Python, Train, PY, it could be any function that you're writing, et cetera, et cetera. If you have a requirements TXT, you can provide that and then we have a lot of support. Like if you are trying to provide any environment variables, if you wanted to configure what resources you want to use. In fact, we have a lot more advanced options as well, like which type of machine to run this on, et cetera. You can provide those details. Can you hear me okay? Yes. So you can do these things directly on the UI. When you hit a submit, it will directly deploy from your GitHub repository. Right? So that's something you can do from the UI. Now, one good thing that I want to call out here is everything that you do using UI, it actually follows some of the best engineering practices as well. So it's not like you do it from the deploy UI and it's not version controlled. Like everything gets converted to this nice format and where we are tracking things basically on your GitHub repository. So if you wanted to switch back to like a previous version of something, it's actually very straightforward, like literally everything on the platform and I think there's something with my internet that's really not good right now. Everything on the platform is basically version control. So you can see that each deployment that you might have made has a version associated with it. So that's basically what I meant to say with that. And by the way, like the thing that I showed you from UI, you could also do it using your Jupiter notebooks, using our Python Library that we expose. Does that make sense? Yes, I'm just trying to understand basically you're doing it till you're putting your model artifact as a micro service and then exposing it and that's it, that's the end of it right now. No, there's actually a lot more. So either you can expose a model artifact as a service, or you can write any arbitrary Python code and expose that as a service. Right? Okay. You can run model training and retraining jobs on the platform. So for example, let me show this to you and you can compare them, basically. So, for example, if you have jobs running over multiple days and you wanted to compare these metrics and all, you could do those things on the platform. And there's actually a lot more beyond that. I think the platform is fairly broad. So, for example, like, if you're running your prototyping experiments right. Thing really sucks about my Internet today. Look like don't worry. So where are you based out of? Naked? Usually San Francisco. Right now I'm traveling to India to take to meet some members in my team here. Okay, and what about you? Banglad only. Nice. So you fit with yes, that's correct. Okay, let me show this to you. Yeah. So for example, if you were tracking running experiments, you can track each experiment as a table. Here you can look at the metrics, map, MASC, et cetera. You can look at the hyper parameters on the platform. Through UI. Or you have a code? No, we have a code. We have literally a code interface for deployments, monitoring everything that you do. For example, these are like all the experiments that you might have conducted. And I'll show you the code as well, very briefly, and then you can compare these runs as well. If you have plotted some things, you will be able to see the plots. In this case, I don't know how. Different it is from ML Flow integration. So this is actually like you can think of it as a supercharged ML flow. Like ML flow actually does only for scalar artifacts. It does not do like one click model deployments directly. It does not do. So, for example, you can't be logging plots with ML flow interactive plots. Those are the things that we do. But realistically we can think of it. As you can log, right? Yes, you can log plots, log, interactive plot. So basically log plot as an artifact, as an image. But what you cannot do is we just log the data and we will interactively plot. For example, if a confusion matrix you plot with us, you'll be able to interact with it, get which class, which section, et cetera. You'll not be able to do those things with them. Okay, so you're saying that let's say you run ten different experiments and those matrix you can basically try you can integrate key, either care on top of that pardon me? No. You're putting a visualization or interactive tool on top of the results of these experiments, right? Yes. For example, if you think about a model registry, we have like you can track all the model versions, we'll give you the usage, the schema, of the model, any metadata that you might have logged with the model, you'll be able to see those obviously model files, any model metrics that you might have logged. In principle, it's similar to ML flow, this part of the platform. But I just think that from a prioritization perspective, we've done a lot more work here and one other area that the platform covers, and again, I'm not going too much into detail of each area right now, but it's basically like the real time model monitoring. So, for example, if you were logging inferences let me just check here quickly. Yeah, monitoring would be area of interest, right? So if you were logging like, real time inferences, for example, I don't know. Which data is locked here. So basically what we do is you can do a hierarchical tracking of things from model summary to data distribution to raw data. Basically, I think this thing is. Let. Me just show this to you in. Production very quickly so that we actually have data logged. I think in the depth test. We don't have any data logged right. Now in this case. Like, for example, if you have certain number of predictions and actuals you can see your graphs here, I think here also, let me just go back in time. Yes, you see that? That's when we log the data in November time frame. So you can actually see your predictions and actual scrap. You can see your model performance metrics, like precision recall, any metrics that you might have logged across different classes. You'll be able to track that when things go wrong, you can actually look at the data distribution that is, how is your data doing? So here you're seeing like a prediction chart, an actual chart, and you see that these graphs are color coded. So, like, darker color means high log loss, lighter color means low log loss. So you immediately know that which section of your data is performing well and which section is not performing well. If you want it, you can actually add a comparison with like a training data set or something, and then you'll be able to track your data drift if you wanted to. You can track your feature distributions if you wanted to. So those are some other things that are supported on the platform. I know we are a little bit over time and actually we also have a next thing to run to, so I'll pause the demo here. Okay. I have a few questions around monitoring, but I don't think we have enough time to do that. Nicolen yeah, I think what we can. Do is have a follow up call where if you want, like, we can zoom into certain parts of the demo if that's more interesting, and we can answer more questions that you have around monitoring. Okay. So I think monitoring is one area of interest for me, so that I would definitely want to explore. Is there. Anything, any other USB you feel you have in this portal? Yes, we have an entire infra allocation layer as well and like a lot of working with your existing tools because I think the platform is built using with that as an insight. So we will share some of those details as well. We will follow up with you. I'll do that and I'll also set up a meeting as per the availability. Yes, let's do this. Just share me all the USPS of this bird and then maybe accordingly we. Can set up another call. Yeah, sounds good. Thanks so much. Bye.