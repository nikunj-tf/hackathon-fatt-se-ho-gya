Hi. Hey good, good. I had a difficulty click on the bottom because we are using Zoom not Google. Then I copy paste the link. So finally. Thanks Amish for taking out time for this call with us. I really appreciate the same. So basically Debug is one like a close connect from KGP. Like he was one of our seniors at Idi Krakpur and then when he was running Slintel, he was actually a customer to our first our earlier startup. So that's how we have been in touch and then when we started through Truefoundry, like he wanted to be an angel with us as well. So we are looking at talking to potential early customers and companies who have been doing and building in the data science space. So we thought it will talk to he mentioned about that. You leave the entire data science efforts there. So that was the context behind this call. Of course. Yeah, happy to help if I can. Awesome. Well I'll tell you what I had in mind for this call Samira and let me know if there's other things we could potentially cover so I can give a brief overview of what we are doing at Two Foundry and after that I'd love to hear a little bit more about how the data science at $0.06 is how you have evolved and during this evolution. What are the challenges you have seen and then we can also dive into a little bit more details of what we are building and would love to kind of hear your thoughts. Your feedback. Etc on what we are building and see if there's any way we could potentially take your help in this journey as an early adopter or something. Sure. Just to give context, you have been in the data science space for a long time. So the major goal like we are trying to build at Two foundries to make it very easy for data scientists and ML developers to take their models to the final state, which could be like being able to deploy our model to an end pin or deploy a bad job and do it in a scalable way. So right now what we have seen is companies have dependency on their infra teams because a lot of data scientists do not have background on the engineering side. They end up kind of doing things well in the local systems but when it comes to actually exposing the model to a real use case for the business. They have to interface with a lot of other teams and then sometimes there are delays and ultimately a lot of companies end up adopting like trying to build their own ML platform team kind of solve for this infrastructure to speed up the process. Our goal is if you can provide a platform that allows data scientists and developers to deploy as per their use case either to a test or a production environment and instead of building the platform. They have this from day zero and then it is not a black box for them. So basically everything is open and if they want to build on top of it, they can do that. So trying to not make it like very complex product that seems like a black box. So that is ultimately goals and here the few things we try and do is be able to give you monitoring out of the box, be able to give you scalability and other complex things that might come as the journey and data science evolved out of the box. So that is the third process behind building it. The inspiration is really coming from the kind of platform like Facebook has its own internal platform called FB Lender and both Amish and Nickman were there. So when we came outside and we were building our first startup in the talent space, we are trying to build a model for matching candidates and when we are productionizing it became a challenge. We thought that this thing would have done in 30 minutes in Facebook. So we thought why can't we bring that system to every single company? So that is the background of what we are doing. I'm happy to also by the way, since you're talking for the first time, like happy to share our introduction. I missed that part completely. Yeah, sure. So basically I graduated from it correctly of course. Me, I was shaken in different directions. I'll talk about myself. I spend my time working with a hedge fund called World Fund where we're using data to kind of build trading strategies and trading models for global markets. So spend time in India first and then between us and Singapore as a part of the CEO office and as a portfolio manager and then built the first status in the talent space. West was also a customer and then finally we sold it to Info Edge and then started building two countries. So that's like the background I wish maybe quickly you can also introduce. Yeah, I also graduated from 2013 from Computer Science and then worked pin Facebook for around five and a half years. Worked on different teams at Facebook like Distributed Caching System team was leading the mobile performance team and was eventually leading the beta organization there. And then I did and did quit my job to start our own venture. And then my journey is very similar to on Rug. So currently working on truth boundary. Nice to meet you. Yes, thank you. This is not the this is actually just for notes. Taking background as us. Then he went to Reflection which used to build recommendation system for ecommerce companies like et cetera. And then he joined also Facebook in their ML team. He was leading the development of Portal which is like the competitive device to Alexa. Yeah, so he brings background more in ML side of things. Samira and then apart from that we Karen team of like twelve other folks, mostly engineering. So small team right now? No, always a small team is better. Quick background about myself. I think I have 18 years experience in machine learning AI at the time that I started there is the name. The trend of the data science doesn't even exist. So we're talking about the datta mining. I think mainly when we wanted to talk about that and I had my own status when I was in Iran, so I based from Iran, I'm born there and then I moved to US working for a couple of companies and then joined Six Cents November last November. So since then I'm cool. Okay, I'm looking forward. I mean now you want to show me what you did. Before? I would love to hear a little bit samira around how the journey of $0.06 has been in data science. What are the use cases and how do you see that evolving? That will help us also get some context and also some of the challenges that you have seen along the journey, if at all. That will be helpful. And after that we can show you like how we are thinking of building and also maybe product. Before 600. I was working for PayPal. So as you know, you guys working for Facebook Editor Corporate is different from their startup. However, six months right now I would say there's a mid sign if you have an internal tool, his name is WG for deploying our models. But in general I talk in general because it's easier. So not necessarily I can tell you that what is important for me, when I wanted to build the data fines, let me not that much sure about the fixes because which part we have I don't want to put, but generally as a data science leader so when it started for me, it's important that team has a data science platform, has a good pipeline for feature engineering. It's very important, something that is missing always. So it's very important because everyone felt that, okay, data science is just you build a model, push that no, this is not important. It's more important you have a good platform that can do the feature engineering, do the simulation, pick up the most important feature. I think then building a model is 5% of the work in my opinion. So I would say this is the most critical part of the lifecycle. And then after that when you have this capability, depend on the use cases. For example, when you are in PayPal, SLA should be less than for example 50 milliseconds here. In six months you can run your platform for a day. So it's kind of switching the way that your future engineering event based on different use cases you are tackling. And then after that I would say that it's very important we have some sort of descriptive analytics before we jump to the model so we can see that what's the correlation have an understanding about the data which is I think I'm missing part right now data science become a fashion and everyone wanted ups to build a neural network before look at the data so then descriptive analytics and at that time so it's good that then you start to build a model and if you have a platform I remember when I was a student I was thinking about that what if there is a tool that look at the data and tell you which model is the best to use this recommend to you which one is this? Which I don't see it right now anywhere so what they do people just go to the libraries. Deploy all of that and then see compared to results and see the results so the other thing that is important for me as a leader so right now there are some sort of metrics in the market for example a UC curve and this kind of thing so it is good but not good enough at the very high level give you some insight but for example. I prefer to look at the percentile analysis of the model pick the threshold by more sophisticated logic rather than just look at the usage app and then I would say that okay. We deployed the model. We look at the metrics. We pick the model and then it's important to monitor this model so how you want it to monitor models? To understand again another trend auto training so I don't like the auto training honestly. This is a very commercial idea if your distribution of your input data is not changing why you have to change your model then your model is not good I always told my team right now for example we are building that we should look at the distribution of the input data if there is a change so you have to return your model otherwise the good model should be responsible auto training I'm not fan of auto training I think that it sounds like your first model is not good you need to auto train and then make a decision when is the best time to auto train. Maintain actually the model and then get the feedback between you and basically this whole cycle okay and I wanted to. Understand a little bit more on that piece around the deployment is that something that you feel is important from a data science perspective for the data science team to be able to do? What do you see mostly managing it and that capability can rest outside of. The data thing I will say usually in my previous startup and in PayPal we have a team of data engineering that handle that part so we do the simulation we actually even create our feature based on SQL Server or whatever test the model but whenever I wanted to go to the production so data engineering actually do that production like that in six sense DSL do that which I think that it would be good again. But pin six sense use case I would say because of the you don't have that much time sensibility, so it's easy. Okay, so even your query is not that much perfect. That's fine. But for example, in PayPal, even I remember whenever I want to push the model based on the SLA that we wanted, even I have to reduce some of my data preparation. I have to change my model that I meet the SLA. So again, that's become more actually challenging to how you write actually do the feature engineering, make your data ready for the model and running the model. I would say this is the time of use cases and the type of industry that you are providing that I would say probably if you want to target and targeting the one that SLA is more important, I think that is more useful to have more sophisticated quick platform. I see. I'm guessing somewhere you are mostly doing like now batch influencing the thing that you mentioned every day you run. So it can take. Actually use case. It makes sense. So you are not receiving every minute email. So in a day you receive an email and you have to respond to that you request for a demo and a week after you have to look at the demo, look at the use case. So it makes sense. Some of our pipelines even running weekly, but it still is good. I see. And what are the pipelines currently running on? Somewhere like do you use something like Airflow or something. On our side? I'm not sure that's exactly what we are using on that package. Because we have a team which is they do data engineering and everything there. Okay. But I know that my team write actually SQL and Hive which do the feature engineering. And then we put it in a model in a DaVinci and running the model, deploying the model there. Okay. DaVinci is an internal system that you have. Internal system? Yes. For example, in PayPal I had something we call it Tigress. So it's more sophisticated goals for solving the fraud problem. One other thing, I don't know if you're targeting all industry, all use cases or you want it to go specific to one niche market. That is something that Samira right now we Karen still doing very horizontal. But actually it will be worthwhile if we are able to get to one industry because it seems like reach out and being able to get the buying of the leadership becomes much easier. So we are open to that. But we haven't been able to narrow down this is the industry at all. I think the platform we have built is more from a horizontal perspective. But if need be, like we can actually have pre built models, et cetera there for industry use case and actually get that. I'd love to hear your thoughts when you kind of look at external platforms, et cetera. Do you look at particularly companies that are getting say specific use cases for. Example, for example Pin fraud domain we solve the imbalance classification is one of the hardest problems you have to solve and if you solve that you can solve any other classification but not vice versa. Because for example in paper I was solving the problem that has zero 1% label so there are lots that we can do there. So for me, I mean right now when I joined six months I feel that no one in my team can do that what we did. But this is our daily job there. Again, this is the use case. For example, my team said that this imbalance 10% of the label so the 10% for me is not imbalance at all. There are lots of use cases also for the imbalance. It's a good question. Honestly, I didn't look at it from that perspective. But what you can do, you can see that look at it from there for example, more technique perspective imbalance and then see that which industry have this kind of problem and then you package your tools in the way that they know that I don't know, you just offer the deployment part or the whole process. Feature engineering, you give the whole process or only one part of that. So right now we majorly focus on the deployment along with monitoring and there is a workflow for data scientists to also be able to track whatever they are building so like tracking of their experiments, et cetera and so on the feature we can connect to something but we don't have that built ourselves. So basically if you are using a feature store, then we can connect to it. My previous actually I was part of a status PayPal acquired article found there right now building a canvas AI, they start a new startup, they karen doing lots of feature engineering and look at that. I think the goal is that just build a feature and sell that. Yeah, and the other thing that I would say that for, that usually so feature engineering is very important that if you can give the solution that they can run it in their system, not need to send the data to you because then all the privacy and everything is right now. Do you have an early stage customer or not? Yes, we do have a few customers, summer who are kind of trying to use our system to deploy their models and also see monitoring. There's also one customer who's trying to use it primarily for their monitoring use case. So they have like structured data and they want to be able to monitor data drift, distribution drifts, et cetera between the training and the actual data. So one of them is in the real estate industry, there's one in the pharma industry and then there's one which is more like an tech kind of. A platform, you know better probably you look at the market. But I'm thinking, I think that especially if you are targeting the industry is not very tech oriented. It could be better because usually tech oriented, they have their own data science team. Look at which industry they don't have their own data science team and it's hard for them the maximum they wanted to actually hire one analyst or one data scientist that can handle everything end to end. So then your platform comes to the picture. They don't want to invest to build actually deployments because for example six months at this stage need their own actually otherwise at this stage. But I think that you have to look at the earlier stage more lesage and in the industry that the code is not tech. Okay, understood. And then even I would say that don't spend too much on the complicated model or want to just add the technical complexity or functionality, have it more smooth and more understandable for non technical person. I don't know if this is the target that you want it and again, what is in your mind? You want it to have many customers? Many customers or no, you want it to go to one industry and just it depends on what's your strategy. You prefer to have two to three customers, big customers or multiple industries? Multiple customers across the industry. I think initially it will be good to have a few slightly bigger customers because the needs are more evolved. So that way it also helps in the evolution of the platform that we are building and then overall it can be walled to kind of make it the smaller customers and make them use it in a cell. Yeah, what we are building somewhere like as of now at least you heard of the page maker and Vertex. Yeah. So something similar to that. Like a data scientist just write the boilerplate code like the predict function or something like that and then it's very easy for them in one clicks or two clicks they are able to deploy their thing. Track the progress. Locks everything in an autonomous way and it is cloud native like it runs on all clouds and it runs on their own cloud account and things like that. So that's the direction at least that we've been taking so far. And so why if I'm a customer, I ask you why I have to use yours rather than Sage Maker. Athletes is more popular, why I have to use yours? Yeah, so stage Maker is very difficult to use and it's like a black box, like the KPIs are not pin the learning. Sage maker is very high. Like when a company status a topic, sage Maker, it takes one month and there are companies that build platforms on top of Sage Maker. Sage Maker is supposed to be a platform but it became so complicated for data centers to use that companies then built another layer of abstraction over Sage Maker. Okay, I want to make that layer very easy to use. Number one is that number two is not like a black box like Sage Maker. If something goes wrong, you have to contact a s support like nobody else in the company can help you. We do it on top of that. Team already understands and if anything goes wrong, they can fix things on their own. So that is a kind of value property. I think it there are many companies right now doing the same thing. Am I correct or not? You know the market better than you guys have something that differentiates you from the. From the smaller company. The bigger players are there somewhere like the Sage Maker. These are the three big games. Apart from that, there are smaller companies that we know of where we have like a proper differentiation and things like that. But the bigger company, they tend to do everything. There is not a single, like a plus. You can say it also does feature engineering and everything. Like they say they do everything but don't do it in a good way. People don't end up using or doing custom things on top of it. So how much is the time of the onboarding on your platform? How much time you need to onboard the customer? Less than a day. Less than a day? Less than a day. One data scientists can start deploying in a matter of a couple of hours. I would say if I were you, I start to target one industry. Okay, make your package. Even build a use case on top of that. For example, for real estate, you can get it something that is easy for the person that real estate. Okay. Show them that. Okay. And this is the resolve. Let's distribute it to this industry. It looks like and even go with your strategy. Could be you want to just dominate in this industry. And then everyone in this, for example, right now at 6%, everyone in the market wanting to use our platform. So pick one industry and say that okay, we wanted to become an AR platform for this industry. And as you feel key, that would be a good value problem. Generally that will also have some use cases and models. So people can actually start getting models. But one thing Samira in that is how do you kind of find like suppose even if you have to build for one industry, you need like really production rate data to be able to actually build some really good models, right? Yeah, exactly. Because of that, I said that even you sell this platform if there is no data science team right now, whatever you are providing, if there is no data science team there to massage the data and get the good data, then they cannot build a good model. The garbage pin garbage out. Now I realize that I think that this is two actually different vision if you want to just stay as a deployment environment so this is a different vision what I'm saying is the difference yeah AI platform for one specific industry so you guys should make a decision which route you want to go so when you all want it to be basically your customer the first route that you wanted to be just deployment to your customer or data scientist okay but when you go to the industry your customer are real estate for example yeah the head status you have to make a decision which I would say that the real estate is your customer okay. Basically you're telling a business use case is better than just the deployment because. I would say that because I was in the same position that you were before so I had my own soccer so usually it become hard many papers which include lots of algorithms and lots of it is all my papers so which includes the next new model that even is not in the market okay so what I did at the end in my company I offer the customer segmentation for furniture pin the upper okay rather than all of my fancy model it was a very I don't know RFM model you are familiar with that recent frequency monitoring I picked that and extended everyone understand that so I did that okay but I do a lot of fancy things also you got my point so it's not hard when you wanted to sell only the technology but you know what always you can go to the real estate industry distributed there. Make your actual platform powerful and then it started to add and get distributed in the market. Is there an industry you feel similar where it's not easy to find like ML where it's not easy to find or build machine learning models themselves it's slightly more complicated where like some platform that is targeting a vertical use case could actually. Go to the market that after PayPal or not so if you can show the value of the model by dollar value. Everyone value okay this is a very tricky part I would say I'm going to the industry that money is there not too many actually clear there again I'm sure that real estate could be a good one honestly based on what you said and the data is clean also data is not that much complicated also real estate doesn't have uninstructional data so everything you wanted to forecast predict the market or I don't know you have to find it actually what is their problem and see also the connection is important so which industry you think for example at the beginning I had a connection with the furniture I was working for one furniture industry actually company that I do the data science work for them so I had a connection with them so it's much more easier I go there all these factors is important. Again, this is good. You wanted to sell the AR platform. It's hard. You have to go and sit and also you don't have any guarantee that what is input, what is output and then it's questioning yours. But if you go to one industry and have even predefined use cases, they are not a data scientist. So they will be happy. Okay, plug pin plug app. So I give you this so they Karen happy. Okay, I don't know what you guys need to change. One thing. It will be good. Like basically we have around 15 minutes. Maybe we can show you the platform from a user flow perspective to give you a more concrete sense of what we have so far. And any feedback on that would be a picture. If you think like any company in your network, like we could talk to like Data Science or ML heads or even any use case within six months. It would be great to kind of. Get your thoughts and talk to depart also about that because I think Depart also have a good sense of market. Yeah. Okay. Do you want to share and focus maybe starting somehow it will deploy and then from there. Have you used anything like ML floor and thing? Yeah, sorry, go ahead. Any of the tools that there take a note of tools nowadays in the MLS market, any of the tools that you have experience with or have used. So I'm very old fashioned. The last time I was coding, it was a long time ago at the time that I was actually do the actual coding. So there was a veco. We use veka. I don't know if you even know that or not. So it's open source. You can go white Java, add something to that or what's popular? Clementine. Did you see the Clementine drag drop 20 years ago? So the first search for that is Clement. I don't know if you heard about the Crisp methodology. Yeah, it's introduced by them. You can deploy that Crisp methodology. It has a very user friendly drag and drop. You can say that. Okay, I want to add this features, do this feature engineering. It was very user friendly. So the only platform I was used and then after that, usually I use Python to implement everything. Okay. For deployment, usually we have internal tools to deploy. Okay. I'll share my screen, so I'll start from the deployment. Okay. Basically we try to make it very easy for data scientists to deploy. So I go to the pipe how they can deploy. But this is basically the screen where they can see like okay, what are the services that are running? These can be jobs also. And they can schedule these jobs to basically run every day. For example, this is running everyone five minutes. You can schedule this to run every day or something like that. Whatever they want and doing it is as simple. As like basically you can do it by the UI also. So you just select where you want to deploy. So let's say I just want to deploy it here and then you just provide the name like simple, like I'll just do hello and then you can decide like okay, your source is on GitHub or Bitpocket or any source control that you're using. You can just choose the repository from where you want to deploy the branch and it already found a docker file there. If there's no docker file, that's also fine. Like we can automatically build it using you can just provide the past your requirements exe and that's it. Pretty much the moment you do this and you can just click on deploy and then the service would be up. And once the service is up, you will get an end pin using which they can query. So you can share this endpoint with the rest of your back end team. Or you can also analyze the results. Like if you're doing patch inferencing or something. Like you can also analyze the results that were published by this job. So it's pretty much like two minutes to do it. Did you select any model also? So model part is in the code. I will show you. That setting. What is this job is doing? Jobs have Python code to basically do the predict inside? No. I'll just show you the Python code. Basically a model that you don't want to run continuously, but you are doing it, say every day or every week. A model already deployed. You just call that model, put it in the flow, correct? Correct. You just load the model in your code and then you do model predict. Okay, but if for example, the input data that you are receiving, the type has a problem. So this is the assumption, help me to understand this assumption is that the data is ready. You just push that model so receive the input, give the output, correct? Correct? Yes. You don't actually have a responsibility about the input, so you are expected to receiving this data, correct? Yes, correct. What happened in that case, it will just throw an error for that rope, like only that and put it through an error. And then you can track like how many errors the model had. Okay, you tell them that. Okay, for example, this is mismatch. Exactly. Does anyone actually define the type of the input before you because the number could be also treated as a string. Yes, so we do that here. If you see we have a schema for every model. So you can add the features that are expected and you can associate the type with that. What is the type of the input right now is the exposure table. It's JSON input. We take JSON. Okay. I mean, you can convert we provide functions to convert like Excel to JSON and things like that. But this is predefined the user is responsible for. And it said that int. So if you are not receiving integer, you give the error that this is not integer, for example and this is not including also model training, correct? Model training you can deploy as a job. So I'll show you this, but I think this part will make it clearer. So like for example here, if you see this is a normal training job, an idea, a model training job that you're doing. You're just doing the five fit. And then here is where you can log the model to our model registry. Log the model and we can log the metrics. And this is basically your requirements TXT like simple Python. And then when you want to let's say you have written this code locally and you can run this code locally, right? But let's say you want to run your training every day. Or let's say you want to rerun the training thing and you don't want to run a laptop, right? Because you want to store save it and on the central infrastructure you want to run. So all you need to do is just write this Python code to deploy it. So you just write this Python code and then you just do job deploy. And the moment you do this, you will see it on the UI and you can go and trigger it from here. For example, your training job is you already saved your training job and let's say tomorrow you see that okay, the model is not behaving well or something like that. And there is a trigger button here. So I'll just click this and the job will start and your model will be trained and the new model will be automatically be saved to the registry with a new version. So your job is now running. You can go and see the logs also for this model, this thing does not print in lock now, but at the moment this will run. And then what you can do is you'll go here. So this is the model and this has these many versions every time you rebin a new version of the model first year. And for every model they can keep running all of this version, they can keep running. Yes, every model is saved and you can deploy like you can deploy version one, version two, version three, version four also together if you want. Got it? This is the scheme of the model that you sense. And this is basically the model strained with what the model was trained where the different requirements of the model, the pickle file of the model, the condo, ML, everything is saved in one place. Got it. Only I have a hard desktop at engine. So basically this is the main part and apart from that, all the system can be hosted on say a client's cloud. So we basically make it very easy to host on a client's cloud. So that anyone who wants to use this, they can easily do it. And I think we take a note able to do monitoring, but basically after the model is deployed, you can actually compare the data distributions and the drip as well, and you can push alerts to your, say whatever, like stack Gmail or whatever. And then based on that, if you actually want to retrain, then you can retrieger your model and retrain basically the functionality that is there somewhere. As of now, that was the thought process. But I think we will also think about going maybe particular industry with use cases. But what do you think? Like just the infrastructure side where Pin deployment monitoring is automated. Could this be useful for any use case that you see? Or. I would say that for the companies that right now, I realize that even you have to target the company that has a data science team but doesn't have a data engineering thing to deploy because you need some data scientists understand what they are doing. And then I think you need to find a company that in the status that they have someone to build the model, but they don't have to push it. I mean, let me be honest with you. So if I'm a decision maker, for example, I prefer to have that in house. Okay, perfect. But this is not all the market, for example, at the company side, look like six months, probably. If I joined the six months, for example, four years ago, it's different. Yeah. So only there are, for example, 50 people in the company. So no one actually wanted to give me a data engineering to build this pipeline. Yeah, but right now, I prefer that even right now, they wanted to put my monitoring system on a platform. I said that, no, I build the internal dashboard because I want to keep monitoring what's going on there. When the company has too many models, it's a big data science team. So I don't think so. It's a good target for you. Okay. We have a dedicated team. His name is DaVinci to build everything. I don't know if you use our platform or not. You have to look at the companies that doesn't have an actual data engineering team, I would say. Okay, a small data science team, not too many. And again, the Cofounders are not engineered because if the Cofounder Karen engine they sold that, they can build everything. Okay. Also, the first thing that we all built was the DaVinci. Okay. When you're talking about the AR to the computer, actually to the software people software engine, they feel about the deployment. So, honestly, for us, I think that we have more complicated and comprehensive data engineering compared to the data science. The data science was very small compared to what they did because they have a mindset of the software engineering and they thought that they have AI. But you know how modeling is not that right now. I'm working on that part. But we have a very good flow for deploying the model because they have the mindset that AI is that AI is not that only. That'S a very interesting point, Samira. I think it is something for us to think through a little bit maybe like it is possible that a lot of companies then become outside the segment because a lot of them actually feel they can then build it, right? Yeah. Don't target them just wasting your time and for them that this is AI. This is not AI by the way. But I know that in this case. So you have to look too many things, different parameters, but keep going. I think that it's not important that you Karen, trying to testing different ideas and then you find the place that you have to be there. Thanks Amish. I know you are also love to once understand your system at $0.06 later. Like if you have some time, like how you have the kind of models especially for this company. We wanted to see what are the different kind of models that they end up being which is actually applicable to a lot of other companies. Love to hear a little bit about that. But the type of the model that. You are using yeah, like the type of the models is it something common across SaaS industry and so on? Like a little bit. Like if we just target the SaaS companies, could there be similar kind of models that everyone is using and so on? Yes, definitely we can do that. But always, you know, simple model is the best. Cool. Thanks Amina. Of course. Good luck. Bye.