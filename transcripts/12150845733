Hey, Mike. Good morning. Hello. Hi, Manchu. Hi. Good evening. Can you hear us okay? Yes. Okay, nice. Awesome. Thank you so much. Hi Manchu, for making yourself available on a Sunday night. Not a problem. This works better for me because tomorrow I've got a very hectic schedule, so I had to change your time. I hope that's okay. It's an early morning for you guys. It works well so much. I really appreciate it. Where are you based on the East Coast? Yes. In Boston. Boston, okay. Is most of the TVs team in Boston? Not really. So we've got Boston, New York, Hartford and Rhode Island. Everything is on the East Coast, but spread out across two locations. Got it. I see. And by any chance, do you know Amarish? Yeah. What is his last name? Hello. That sounds familiar. Okay. Is he part of data science or data engineering? I think Amrash is an SVP on the he calls it digital digital technology site. I don't think he's specifically leading the data effort, but I think he works closely with the data effort folks, basically, or maybe like the data reports to him, but I guess he does not come from the background. Something like that. Yeah, I think that name sounds familiar. Okay. I'll give you a quick introduction about a little bit about us and why did we reach out to you and stuff. Some of this is clear in the messaging itself, but the idea being that here I'm Rag and myself are co founders at Crew Foundry. We have one more co founder called Abhishek who is not on the call today. And Bibik is a member of founder's office. Our team comes from Facebook and World Quant. Most of us used to work in machine learning in the past, and we saw some of the large platforms getting built at have you heard of a Learner by any chance? No. Epiler is one of the platforms, machine learning platforms at Facebook where they built out a lot of dev tooling to kind of speed up the machine learning model, deployment process, monitoring process and all. And we were inspired by the platform. We thought in a similar platform should be available everywhere in the world. And then what we did was we did a deeper dive into the use cases of enterprises and we realized that there is potentially a gap that we could fill with a platform like that. So that's what we are currently building. And we are fairly in early stages, dementia, so still talking to working with a few enterprises, but learning more and more about the problem and the intent today was primarily just learn from your experience at CBS. What is the state of machine learning? How is the team structured? What are the priorities and the problems today? I think that was mainly the intent of the call. Of course I can dive deeper into what we are building as well. I'm give me 1 second. Yeah. Okay, so tell me a little bit more exactly what stage are you in? Is the product developed? Like, do you have a POC or what part of the development are you in? Sure, yeah, we actually do have a product developed, himanshu. It's already getting used by a couple of enterprises. Right. So they are already like they have tested it out, is deployed on their cloud and people are using it now. That said, in terms of code and code feature completeness, I don't think we're there yet. We need a few more months of development effort to get there. And in terms of customers, we call them design partners. So the idea being that we are working very closely with them and we build out the platform to their specific customers, to their use cases as well because right now we are still in the product building journey. So we work with them closely essentially. Okay, and how big are these customers? They're fairly big. We have one customer who is $100 billion conglomerate across different we have another customer who is roughly $50 billion silicon chip, another one which is $300 billion like healthcare company, I guess. Yeah, we have three or four such large customers. Okay, and then how does this compare with the other technology? Sure. Have you worked with some of these technologies? Is there any specific technology that you are interested in knowing more about? When you say MLS, we use MLS in few different ways right. Depending on the use case. And right now the whole enterprise is moving into Google cloud and so there's a lot of focus around how we are going to leverage what's available in Google cloud. Right. But so far all our tooling is based around what we do in open stack, what we do. Can you hear me? Hello? Yeah, do you hear me now? Yeah, right now it is a mix of few different things that get used on our current platform, which is all in house. Meaning we are not using any cloud, but we are moving towards that in next couple of months the transition is going to happen or the migration. I'm just thinking out at a bigger level, how does this, what you are developing, how does this compare in terms of what is the differentiation between what you got versus what's already available, let's say as your or GCP? Right. So I think usually I will certainly answer that question and answer in terms of differentiation, but I think to be able to answer that question more appropriately, I will need to ask you a few more questions so that I can contextually answer, but at a very high level. One of the things that generally differentiates startups at our stage is the potential customization that companies can get working with us right now, this stage of the company, we are willing to work things that are really supporting the current use cases. If you're migrating from, let's say, one cloud to the other, or onprem to a cloud, or, like any major data lake, data warehouse integrations and stuff like that, we would essentially go ahead and build that out so that your at least your org's job or your job as a leader becomes a little bit easier that way. Right. Like you have another team supporting this entire migration effort. So that's one way. But I think there are other technical differences as well that I will highlight once I have a little bit more understanding of how the team is today, is structured and how are you all currently trying to build and deploy models. So if it's okay, I would love to follow up a little bit from the context that you gave that today. A lot of ML ops is in house and then are you all migrating towards GCP? From what I understand, CBS is heavily invested in GCP, right? Yeah, we got a few other assets in your as well, but GCP is the general direction we are moving in. Understood. I see. I would love to understand if you can get us started with explaining about how is the team structure, which is the team that builds the models, which is the team that deploys and maintains the model. If you can start with that and then I'll ask some more questions. I mean, it's like as you would see in most of the organizations that models get built in data science. And then we engage data engineering for putting those into production. Data engineering brings in all the knowledge around MLS, DevOps, Kafka and whatever is needed, basically, and CI, CD, all of that is done by data engineering. On our end, we would refresh our models. Let's say I haven't seen any model that could say let's get trained or retrained on more than a monthly basis. There's like nothing real time training that is happening. But then when things are refreshed, let's say on a monthly basis, sometimes quarterly basis, then those models get redeployed. Right now, I am not very much privy to all the nittygritty of that toolkit, of all those things that happen there. But from my standpoint, we are really focused like these days on model surveillance. Once the model is deployed, how are we monitoring its performance? Are we creating control groups to measure our performance against? Are we looking at things like data drift, the inputs, how are they changing? Are they still conforming to our regional input distribution? Right. Those are the things that we worry about in our side on data science. And there's always worry about, do we have a risk here in terms of creating false positives? And if we let this thing running for, let's say, a couple of months, and then if we realize after that that there's something that is completely changed in terms of the imports are completely changed in terms of regulations on how we should apply some of these. It's usually a combination of business rules as well with the model. So if the business rules have changed, things like that. And so we want to really catch those things early and then try to make any changes. Understood. I think I do have a couple of follow up questions on this part that you mentioned right around the entire model surveillance area. Just one thing is, is there like some specific type of models that your team manages? Like you mentioned, the Data Science team builds models and Data Engineering teams support the Data Science team to deploy the models. But are these both of these teams organized by different bu, different types of problems? If you can give some context on that. So in terms of bureau, different basically data Engineering is separate than Data Science, but they roll up into the same SVP. Okay. And so you still have different set of teams doing all of this. And then in terms of day to day functioning, it's more like a matrix relationship. I would have my team develop the model and then we would work closely with data engineering to make sure that they are aware of what we are developing, what's coming down there, their path in like a month or two, depending on the more lifecycle. And then they are set up that way. Now things might look different, let's say in a year from now when we are in GCP, a lot of this might be consolidated and compressed into, let's say a single team. Because then you would have at least what I'm hearing is more like a click button functionality to deploy your models. Now it might be an exaggeration. I mean, you guys probably know more of this, how much streamline is going to be, but I think it would be much more streamlined or simpler than how we are doing it today. For sure. Yeah, understood. And like himanshu, this SVP level that I guess manages both the Data Science and Data Engineering is part of one bu. And each bu in the company would have a similar SVP who manages their own Data Science and Data Engineering team. What do you mean? No, this is like a central log. Oh, it's a central log. I see. Data is central. Understood. Okay, so this one data Science and Engineering team combination basically serves the entire company, like all the machine learning models, anything related to data? Basically. Pretty much. Understood. Okay, I see. Very good to know. Okay, you mentioned that you all are moving towards using GCP. Do you know that in GCP, specifically, if you will end up using Vertex AI or you will build up something on top of, let's say GKE yourself? Is there any plan decided for that? I'm not aware of that. I know what x AI they are. They are conducting training on that. But are we going to use that versus something else? And that's not something right now. I see. Okay, understood. And then the next thing that I wanted to ask is about the type of models that your team is building. What are some of the focus areas for your team specifically Mancia? Like in terms of the business impact delivered through AI. There'S a lot of things, and primarily my focus is around operations and cost. And so how do we do things in a more automated fashion in terms of our claim processing, in terms of day to day functions, where we have a lot of manual stuff doing things right now. And so we are trying to automate small things. I see. Okay. So basically the CVS operations, how can you optimize that to save costs? Essentially, yeah. So think of the CVS is a big company, there's a few different lines of business. One of those is healthcare insurance. As part of the insurance unit we get tons and tons of claims for reimbursement every day. You can think of 1 million claims per day and the focus is how do we use machines, how do we use AI to process majority of those claims? Still some claims would drop for manual processing and all that, which is fine. But the more you can automate, the more you're going to save and streamline and be accurate and be consistent. Understood. Sorry, go ahead. No, that's what I was going to say. Yeah, that's like the best objective or arching that we are solving for. Right, got it. I see. So these claim processing himanshu would involve like computer vision would probably involve NLP, like extracting data from the claims and automatically processing them. Right. So we understand that now there are tooling specific to document processing that have come up. Right. So do you all end up using any of that tooling which are verticalized for this specific use case or is it more in house? So think of this as like a big enterprise system that has 50 different components. One of the components you highlighted is reading the images, reading the fax documents, the mail documents and right now that is being done by an outside vendor who basically collects all the Gmail, all the fax and everything. And I believe the vendor uses a mix of both OCR and then their manual staff to key in all the information. Basically they are keying in in the information and then a lot so this is one channel, a lot of the claims come from electronics channel directly into the system. So they don't need any of the pre processing. But once they are in the system that's the part where we are focusing is how do we automate the processing and try to not use or claim like human processors. Does that make sense? Yeah, completely makes sense. Do you all end up using a lot of actual like these human resource vendors? I guess like the more service model where you would give out a project to a team and they would build out these models and then the internal team focuses on the overall integration parts of it. Or when you said a vendor, you meant like a more of a software vendor? No, neither software nor a human staffing agency. These are more like have you heard of conduent? Conduent? Yeah. Basically they are I don't know how to describe I don't think they are like software vendors, but they are basically. Like the insurance third party provider site. Not the third party providers, but the approval site. Like the claims go to them, they'll look at it and they'll just approve it. Is it something like that? No. So convent basically bunch of people that would, let's say pick up the street mail CVS as a post box, like PO box where everyone is sending their letters, like let's say claim letters. Someone has to take those and do something with that. Like open the letter, put it somewhere, file whatever, scan it, fax, put it in the system, OCR, whatever, all of that is done by Conduit. Got it? Understood. Okay, this makes sense now. Okay, understood. And image one other question around the entire like you mentioned about the model monitoring data drift a B testing component of the model. Right? Now when you say A B test, I guess the first question is are most of the models that you're building batch models or are some of them real time models? So I'm not talking about the training, but the inference is good of it. Both. So in some cases we are just getting overnight files to process through their model and some other cases it's more like I would say near real time we would be getting a file every five minutes or so. Understood, got it. And in this context when you said A B testing, right? Like did you mean more for your batch models where you may be trying out two different versions of the models. What did you have in mind? Doesn't matter. Right? Basically the other objective is that you want to set aside some of your cases or some of your examples and use them as a control group, right? So let's say you are getting a million samples every day to go to the model and then you are going to say, okay, I'm going to keep aside maybe 1% of them. I would say 1% is still too much for a million, right? I mean it depends on how much, how big of a control you want to create. But that's the idea. So then for those you are going to record the decision that model would have made, but you still let them pass through like the model did not do anything and let the human make the decision. And then you are going to go back and check. And again, this is all part of the pipeline continuously you're going to monitor how did we do on those cases because the one that you already automated, you are not going to learn anything because no human is going to touch them. So there's no feedback loop. Makes sense. Understood. And for this use case, are you already using something? Are you currently trying to build something? What's the stage here? What do you mean? Like we have models production? No, I meant like being able to do this control group and then experimentation and then charting and stuff. Yeah. So that happens not on every model, but that is something that is becoming becoming high priority. So like, okay, we have to do this on every model that we are deploying right now. The ones that we are doing, it is set up, I would say in a very in house developed manner of, okay, what cases are we going to set aside? Right. And then we have got our own dashboard and tableau that kind of tracks the model outcomes and the human outcomes. I see. So basically you put like some flag on the one person data and then you run it through humans later and then you have a tableau dashboard which compares what would the model performance be like compared to the human performance, essentially. I see. And is this something that you specifically want to improve on this entire setup or you feel like it's working decently? Fine. Currently? I mean, this could be part of the whole model deployment setup to be able to set aside the cases easily. Right. It should be more like a switch that you can turn on and off. Right. Understood. Yeah, makes sense. So actually on this note, you mentioned about the model deployment set up, right. How to set up the model deployment. Is this something that the data engineering team typically decides which vendor to work with? What's the process of setting up the model deployment? Or is it something that data science team decides? Like, who has the ownership of that part? Data engineering. Data engineering. Would it be something like, okay, now we are going to use this type of deployment and let us migrate everyone to use this deployment? Like the data science team has a lot of they are actually close to the decision making process as well. I would say specifying the requirements. Okay, understood. And then I think it's more like what is possible, right. In terms of a current tool set. In terms of, let's say I think where you are trying to go to is who makes the decision in terms of what tool set is the right tool set. Right. That still lies with the data engineering. Got it, understood. So of course I have a ton of other questions, but I also want to be respectful of your time. We have two more minutes in our scheduled time, so I'll at least first take a couple of minutes to describe a bit more about building and ask your questions about some of the differentiations. And then if you have some time, I'll ask you a few more questions, otherwise we'll call it a night today. So, Himanshu, our focus area as a startup is towards model deployments, okay? And when we talk about model deployments, we mean like all real time batch deployments, retraining automation, model training, automation, being able to do like these AB test dark launches. I think what you referred to is commonly known as dark launch, where a model runs, but it is not served to the end user, it does not appear in user. And then you're able to compare the performance of this new model against an older model or a human being, basically, right? So supporting these dark launches, monitoring dashboards of how is the performance of one model version compared to the other model version. So those are some of the areas that fall within the scope of deployment for us, first of all. Any questions on that? No, that makes sense. Now, the approach that we are taking Himanshu is more of an enabler approach as opposed to just take the platform as this type of approach where we work very closely with the data engineering, or some companies call it a platform team who support the entire data science program. So we work very closely with them. We understand the requirements of a data science or like what are the type of interfaces that would be best for them and basically customize those requirements. Like some data science scientists may be more comfortable with Python and whereas some of us would be more comfortable with UI. So what are the best user interfaces that work for different data science? We try to build those out and integrate with the organization's end to end workflow. That's the approach that we are taking overall. And of course, in terms of existing tooling, we try to work with all the existing tooling within the enterprise. And for that we actually sometimes also go through an initial technical interview process where we just make sure that the tooling that the enterprise is using or trying to use is compatible with what we can support because it's not like everything under the sun that we will be able to work with. So we make sure that we only take up those projects where we can truly actively contribute, basically. So that's the general approach that we take. Now, in terms of the different cloud providers, we actually work very closely with all the major cloud providers. So if you are using GCP or Vertex, AI or basically Azure, ML, AWS, whatever, we'll work very closely with your team and integrate with those cloud providers as well. And you could be using them for, let's say, certain part of your overall development process. And wherever you think the truefoundry is adding the maximum value, you can use us for the Delta increment that you can make for the overall dev workflow, basically. So that's the typical mode in which we currently engage with enterprises and we are seeing that once we start doing this for a small use case like developers, like the actual end ML developers really like the interfaces that we provide. So they want to do it for more and more. So it kind of really organically helps us expand our footprint within the enterprise as well. Got it. Very much like a consultant based model where you're still going in trying to understand the problem, build a custom solution that you can probably then use somewhere else as well. Right, right. So we actually call it a product consultant consultant in the beginning, but yes, at this stage we are very open to building out more custom integration. So the consultant model that you mentioned, but we have a full baseline product which could technically be used independently. It's just that we put in more engineers to customize that product, basically. So literally, technically you could be using us as a product. We have a full Docs page, you can sign up, start using the product and stuff like that. But right now we take it like one notch ahead. And I guess it's likely because of the stage of the company, because we are early stage, we want to make sure that we are adding that additional delta value for our first few customers. Like the first five, six customers basically. So working with three right now, we will probably do the same thing for the next two to three customers that we work with. And likely the way this works in Manchu is we work with one data leader within the organization and try to work very closely with their to solve for their problems, basically. So ideally like that is able to deliver things hopefully much faster with our tooling and the integrations that we build out. And then after that, the goal is that once we start expanding, we take more and more of the product route and less and less of the consultancy route. That's the goal that we have. Hamachu. Good stuff. Good stuff. So you are operating both in the US as well as how many countries are you currently? Yeah, so we have presence in the US. In Europe and in India. But primarily our center of Mass is in India. A lot of our teammates are based in Bangalore, including the two co founders and Royal Khan. Abhishek and I operate out of San Francisco. Dimension, correct. Yeah. Good. So I noticed from your lingon you went to It Bombay. Which year did you graduate in? I was nine. Okay, nice. Yeah, nice. We are all from 2013 batch, the three of us, basically from Karakur. From Karappur, right? Yeah. I was just looking at good stuff. So one of the things that I wanted to call out was in this journey of us building out the product and us working with a few early customers, one thing that I realized is especially with enterprises, that is a segment that we are going after our team actually does not come from the same background, right? Like our team used to be at Facebook, we used to be at startups in the Bay Area, etc for so we don't come from this background and we have realized that getting to work with enterprises is a completely different ball game. You just need to be patient, spend like months and months of understanding the problems, identify this one sliver where you could potentially add value and then move forward from there. Right? And that to all of this in the best case scenario and these things are practically impossible to do sitting outside and not even coming from this world, right? So we are basically resorting to seeking help from folks from the same alma mater or people that we have worked with or people who have some shared context with us, right? And in this journey there's a lot of people who have joined us in helping us in semiformal or just informal capacities. Maybe some people have decided to put the small angels check. That okay, I'll work closely with you all, I'll help you wherever possible and I would love to participate in the potentially, hopefully financial returns as well. Right, so they would put in a small angel check and then get some equity. Some people join us as advisors where they get more of a sweat equity as we call it, right? And some people are like you guys are building something nice. I would love to be there to help and just keep things more informal. So there's take a note of these different options where we have worked with leaders in the past to seek their help in this journey. I was wondering if you have been working with startups in any capacity as well? Himachu not much so far, no. I mean, I've been advising a few folks out of It bomber in Tech Blue Learn so those guys are from Bits and then there are few other folks from It as well in formal capacity, I would say. But the thing I think you mentioned about working with large enterprises and making opening doors and starting once you start with them, I think once you establish your business, it's a very good business that can go on for years. If you become part of their ecosystem, then it's very hard to move you out. So definitely encourage you to go after it. It's lucrative, but it can also mean that, yeah, you have to put more effort right? And you have to be more patient. For sure, yes, that's one thing that we're definitely learning as we work with a few enterprises. Himanshuanshu one of the things that I would love to do is I had a lot of follow up questions in the context that you gave, if you are open to it like one of the evenings, if you're open to sitting with us and giving us some more insights into how things. Are structured, how things are built, where do you see potentially we could do something, potentially we could add value. Right? I would love to spend some more time with you, but I understand if this is a bandwidth issue for you, that's fine. Yeah, send me over, we can chat more. That's fine. Also a few things just to add is we were seeing through a little bit over the Internet and we saw that data like CVS Health uses databricks quite a bit. So love to get insight into which all parts of the pipeline databricks comes in. Also because that is one area I wanted to dive into. Second part was given the movement to GCP. Like wanted to understand a little bit more about is this primarily from an overall ORC perspective? Is this just an ML thing where you are moving to GCP and is there a primary reason behind the movement to GCP part? Wanted to dive there a little bit and then also from a use case perspective, while you said that your team is focused on the operations part, how are other teams like? What are the focus areas of the other teams? And then the tooling that you use between, say, the use case of your team versus other team, is it common, is it different? Want to also understand that part a little bit. Those are some of the teams wanted to dive into to be able to actually understand the overall organizational structure, especially for the data science. Your first and third question around tooling and data breaks, right? I would say there are two distinct groups. One is because CVS is a merger of two companies. One is like the legacy CVS, which was pharmacy, and then Aetna, which is the health insurance. Right. I'm on that side. I've been on that side since I joined in this part of the order. We don't use a lot of databricks, I think in the pharmacy side. Yeah, I've heard that they do use it. To what extent, I'm not 100% sure. But then GCP is basically happening for everyone. That is the data. It's not just for like ML, but for everything including storage, compute. Yeah. Okay. And the data is across farmland insurance, basically, correct? Yeah. They are becoming more and more integrated. The part of moving to GCP is going to bring more integration and more commonality. Okay, got it. Also, I'll take just 1 minute. He wants you to answer the earlier question which you asked is around the differentiation. So I mentioned about the customization thing. Few of the things that at least around the product based product that we have built, we have tried to do is one, we have tried to build it in a way that it's multi cloud. So from days if you adopt it, it's very easy to move from one cloud to another or say even support multiple cloud on top of the product at one time. So that is one of the things that we try and differentiate against the cloud providers where you are stuck to one of the major tooling that they provide. Second is in terms of use cases. Like suppose there are new use cases that come in or new features that you want in the product. It's very easy to build out those features like either as custom plugins or even tell our team to just build it for you. So the architecture of the base product is such that it does not get you binded to a particular workflow. So you can choose a workflow depending on your use case. And if there is a particular feature, particular need that comes in, the base product already supports the build out on top of it so we can build it for you. In certain cases, like if your team wants to build, they can actually build on top of it. So that's how the architecture of the product is built. And the third thing is around the developer focus. A lot of the decisions that we have taken, we have tried to make it very developer friendly. So even with very less knowledge of infrastructure or depending on different levels of knowledge of the infrastructure, people will be able to still take and deploy models in the very reliable and scalable way. So that's like the onboarding into the product is very less like it takes like an individual around 30 minutes to get started on using the product. So these are some of the basic differences. But yes, we are still trying to further make this differences more deeper by working closely and understanding the various use cases of the. Okay, great. Yeah. If you can send me a couple of pages on exactly what are the key value drivers that we would pitch to CTO or like SVP of Data. Because I think a lot of these things that you are describing, they're great. They are great for developers and ML engineers. They would love these features. But as a leader, I think you still have to prove how it's going to increase, let's say, the productivity of their team members right? And what value is going to drive for them. Yes, absolutely. So we'll follow up with him on you once. We'll love to kind of get on another call to understand this part as well a little bit more. And based on that, like we'll be able to kind of send you a better customized document as well in terms of the value add that could potentially. Be. Schedule another call. Yeah. Good. Awesome. Nice chatting. Thank you so much Amancio. Very nice chatting with you. Have a good night. We'll reach out and think of another call. Okay, bye bye. Sounds good.