Good. How are you doing? Thank you. I'm good as well. Thank you so much for setting up time for this call. I really appreciate this. Just give me one momentum energy. Are you based? We are based out of Bangalore, Bangladesh. Bangladesh? You are in your office, is that. What yes, we are in office. Okay. And how many people are there? So we are currently six people in the Depth team and four or five people into the business group. Okay, awesome. So again, what I would love to do in this call is can definitely share a bit of background for myself and my co founders and co founders. But apart from that, I'd love to learn a little bit about sense of would love to hear as to how did you kind of know about the truefoundry and what was the motivation in reaching out and then what are you doing with ML? What is the state of the inside this point? I know you all are also kind of getting started and it's not been a long journey. So here as to how the evolution has been and then if we can dive into few of the use cases or problems that you are facing, that will help give a good context for me. And then based on that, we can take this discussion forward. Like the goal will be to kind of learn as much as possible through this conversation so that I can actually do my best to try and help in whatever wayve for your ML. Right? Does that make sense? Yeah, that makes sense and from even our side. So definitely I'll start telling you about what we do at salesforce. So we are developing AI models into the HR tech industry. And the kind of offering that we provide, we are providing the pretrained models as of now and then we are providing the pipeline to train the models. So in pretrained models we have built models for resume passing, document extraction, basically parsing invoices or any kind of documents into the chart tech industry. And then we have developed a model for automatically matching any two documents. Like there are jobs resumes, job to resume, resumes to jobs, resumes to resource, any directional matching or basically any kind of text that occurs into the HR tech. We are able to get our use cases on top of that. So that's in short that we are doing currently. And I love to learn a little. Bit more about both of you as well. I'll give some context. Industry very close to Hart Has. We founded this form called Entire before our current form and it got acquired by Info and Pin that we also ultimately ended up building some of these models like the JD res job prediction and whatnot company name on what sorry, enthire. It's called enthire. If you kind of go I think it will redirect you to the northeast big step website because we got acquired by them just 1 second, sorry, we got acquired by them. But yeah, I mean what we were doing then was like any candidate that used to come to our platform, we used to help them get matched with companies after they have vetted through us. So for companies it was mainly that you get access to vetted candidates. So we had a standardized pool of interviewers who used to take interviews based on standard things like DSL code, system design and so on. And for software engineers, we used to kind of do the first filter based on the standardized interviews and then candidates used to be forwarded to companies and we used to ask companies to skip one round of interviews in that this matching used to happen. Then there was this interviewers which were like a pool of interviews from Google, Facebook, et cetera. And then finally the so that was the entire area. Awesome. Great to know that we are just building at the technology layer level. We are integrating with the people are integrating to the central system like Ads. We are directly catering to the ATSs HRMS interview as a service platform and what all sort of platform recruitment marketing platform and helping them build the AI based use cases for different matching purposes. They have either matching it through the database or the pool, or matching it live when the applicants are coming into the system or matching it to the questions for the interview platforms, matching it to the interviewer profiles in the interviewer as a service platform. So these are the various use cases that we are helping Ads and CRMs companies built to become an AI first platform. Absolutely. And you Karen, exposing this as ups or are you giving this how are you exposing this to folks? So currently we are exposing it currently on the API basis. Okay. Yeah. And so we started this 2018 December. I'm leading the technology part and one of my other co founders, professor he is based out of Austin. He is leading the business and he's looking at the growth business development and all. Okay. He's the core ML engine. And I'm curious, this industry like this tech earlier, a lot of these ABS or you take this HR software, a lot of these, they build it themselves, especially the big ones, if you take like a greenhouse or a liver and they'll try to build it themselves. So is the target market for you like this smaller size form for maybe non tech players, like big recruitment players would not have tech around them, but they want to do this what is really the target. Yeah. So the target market is like all the mid size HRMS ads, definitely there are bigger ones like SAP Respect, they use text kernel and there are these on the supply side, if we see there are legacy systems like text kernel, archery, dexter, these are there into the market for search and parsing services. Only and they have not extended their system to really get something out of the data which people Karen hosting there. So that's where we had come into the picture and where we are able to and even there is a lot of game on the accuracy level. Yes, people have built it into their system and secondly, there is a lot of things going around the buyers as well in the buyers it comes to hiring part, right? So even the EUC in US they are coming up with some data regulations and bills which will make it mandatory to check the bias in all the hiring specific models. So that's where we are training a generic AI generic on a global data set. That's what our plan is to build a generic screening algorithm and make it bias free so that everyone can take benefit out of that okay wayve. You seen this company, Eightfold by the way. Definitely, yeah, eightfold has done pretty good work in this buyers and diversity based hiding, right? Like they try to remove as much bias. Yes. We went as a talent intelligence platform and we pivoted last year when. We saw that there are a lot. Of platforms doing a similar kind of thing. But what we were actually building was the Aiplet and that's what we saw bigger chunk of market and how we can really make it to the people's system, right? That's where we thought we'll be pivoting as an API service company. That makes sense platform can ultimately become like this take as an enabler service like that. That is something that can get started very quickly. So I can understood. Let me give a quick introduction. I'm one of the co founders at Crew Truefoundry. Like my other co founders, nikkun Obese. All three of us were batch mets from IIT. Batch. And then we went into very different parts. I used to work for a hedge fund called WorldPoint where I was using data to build trading models and then I was a portfolio manager trading them in the market. So got a lot of exposure in terms of trading in US markets was able to maintain a portfolio of 600 million Pin assets. So that was a really nice experience and then worked very closely with the CEO as a part of CEO of this very label various strategic initiatives and at that time I used to invest in status as an angel like small amount by angels and also got very interested he wanted to build something of my own. And I amish Nickens were at Facebook at that time. They were very good friends from college. I wish on the software engineering site, nickel was on the machine learning site. We all kind of left our jobs and started building entire with them of like solving the talent problem. We felt that interviews are biased and a lot of companies are not conducting structured interviews. So our goal was can we unify the interviewing process and make it structured and actually make it very resumed bias free rather than skills. We kind of got a good number of customers, but we face challenges in scaling and at the same time like team's perspective what happened is we got more interested on the tech site as we are building these models. What we realized is he even built his models but deploying that is not very easy. And we were discussing about the internal system at Facebook where like literally at the click of a button anyone can deploy a model and we are like why is that not available to people around that? This is a more interesting problem. Can we actually simplify machine learning pin a way that anyone, whether it's a data scientist or a developer, they should be able to take their models to production by hosting them as entrypointtype video, expose that to the world and it should be very simple to get started, it should be simple to kind of use and it should be possible to scale. So rather than depending on like a learning curve of three, four weeks to kind of use platforms like a Sage maker or so you can get started in 30 minutes and if you want to expand, you get the best platform right from scratch. Like something like what Facebook has from day zero rather than having to wait for it. Because what we realized was ML scales, like a lot of companies start doing ML but they start failing when the ML starts to scale. Because the systems don't scale, their install does not scale and people are not able to revamp their systems and that's where ultimately they kind of have to be a laggard in ML. But if the entry is well built right from day zero then a lot of creativity can be taken up by the data scientist who can actually work on building models rather than focusing on the operationalization part. And that is where we kind of started. We provided to sold entire to info. It was a coincidence that at that point was trying to expand into the premium tech segment and therefore we found a good buyer and then that's it. I think that's the discovery process we are into and that's why we have taken decisions to expose our whole pipeline because we have made all the things from document extraction pipeline, pretraining the bottles, adapting it to the domains and then training it on further fight using tasks. So I think that's where we have got more interested pin how we can expose the pipelines and provide more insights or provide more flexibility to the companies directly to train their own models as well. But we are also building an approach where we are more domain focused and taking approach where we are going into a domain cracking view of the solution and then helping customers to build. And when apps I got interested when I looked at the proof on tree because I don't know much about that, but whatever I have understood based on. So we are working on a very similar frameworks like Hugging Face, Torch TensorFlow. And that's where I feel like how we can leverage through foundry to run all of our experiments, what's the capabilities that are there, what are the kind of instances because we completely run our currently we run on AWS, so it comes with its own pros and cons. Yeah. So that's where I would love to explore about proof on Tree and more about you. Definitely. Let me take some time to understand a little bit more about the system. So, how does the world pipeline look like? If you start from, say, a process where you have the data, where is the data inside? When the data scientists or ML engineer starts working on it for building models, like are they using their own models? Are they using AutoML kind of solution? Once the model is built, is there a testing phase, is there experimentation framework there? And then when you're exposing this, how are you doing the deployment as an end pin? How are you creating the entrypointtype video then? How do you kind of see the challenges in terms of scaling this or exposing this multiple customers? So it would be great to understand that pipeline a little bit and it will be good to also highlight a little bit of whatever goals you are using. And if you wayve faced the challenge somewhere or something has been a blocker for you, it will be really nice for me to learn. Sure. So I think there are no challenges on the deployment site much, but there are definitely challenges in tracking these experiments. Because as a data scientist, I will start with the first data set we karen using. And then from there we prepare the data, load the data into the training machines, all those processes as a pipeline which are already there built. And then when we run these experiments, I think that's where saving one model and then running multiple epochs, evaluating it again and then running performance evaluation and then retraining the models. Right? So that's where we need more tracking and visibility of what happens at each step when we are retraining the models or then finally checking, evaluating the model for the accuracy. And then case of the restructured unsupervised learning, maintaining the quality pipeline as well, maintaining the quality of clusters embedded and everything that we have generated from the models. So maybe this pipeline, the tracking of this pipeline and visibility into the past versions, I think that's where I feel the challenges for us are. Okay, got it. Understood. And currently you're not using any tool for tracking. Is it like say, have you by chance seen or looked at ML flow and weights and biases or anything like that? No, we have looked at those tools, but we were on the Sage Maker completely leveraging their reports and everything. But the one thing that I find missing there is like how we can compare the versions. Yeah, okay. If you Karen using Sage Maker, I think Sage Maker doesn't do a good job on the experimentation side at all. It probably does a decent job on the deployment side. Training side is good. You will be able to configure everything and do the training very seamlessly. But more on how we can compare between two versions and two set of experiments that we have. And then Sage Maker, where do you run this training job? Like do you run on local notebooks or do you run like hosted Jupyter notebooks or hosted Sage Maker notebooks? Hosted instances itself and mostly I'm guessing GPU instances, like even the type of models you are building. Got it. And then once these models are built, suppose who creates the end pin, like who in the team is responsible for kind of creating and then dockerizing it. I'm guessing you are dockerizing it and. Then yes, that's the whole pipeline set up. Like once the model is trained, the topper builds up and then we push it either to the lambda or ECS. You'Re using managed. Yes. And how do you decide between lambda and. So if there is any individual tasks which take a lot of resources right. Or need to be scaled back individually so that we prefer to and based on the cost as well, what's the kind of memory it is taking, how much time it is taking for the responses. Do we need to call it in parallel? There are multiple parameters around that based on which we take like the cost is good for lambda or the cost is good for ETS processing. And this training that you do, like do they run as a job or is it like offline training and then the model is built and then the model is pushed? Or is it like the training is continuously updating after a certain time and then the model has to be updated? Like how is that? Currently we trigger the training. It doesn't get automatically triggered to do it in a period of time because we are also building the quality pipeline. One of the biggest things when we are delivering the application to directly in the customer sense. Right. So the quality pipeline needs to be taken care of. That's why we prefer to do training ourselves and that maintaining the quality checking. Do you do any sort of for this quality thing? Do you do entrypoints monitoring or something? Currently? Yes. So all those are there for EKS. There are logs getting generated through which we have metrics dashboards, everything, lambda, API. Or are you pushing them to something like a grafana or something? Yeah, promises. Kaan we have planned to use because we have a user layer which is written on Java. So that needs to go on. Promises. Cafana okay, but right now where is the monitoring happening? Is it just cloud watch that you. Are using laptop tracking also monitoring. Okay, but this is only system level monitoring, it's not ML monitoring, right? Like it's not giving you performance monitoring or a drift monitoring? I'm guessing yes, not at this point in time. Okay, and is that like something that you plan on doing or that has not been a concern as of this point? We have not yet seen any performance lacks when we are running in parallel. So we are scaling it based on the resource, right. Whatever resources Karen required, we are doing that. But definitely on the performance side we are also experimenting with a lot of things on the torch quantization when X net all of these things. So that's how we are doing the optimizing, the performance. Do you end up using model servers for your optimization operating et cetera for performance optimization rather sorry, model servers like a TF serve or Pin dot Sense or something like that? Or do you end up using fast API plus KPIs entrypointtype video? Just. One more question. I'm sorry, asking just a lot of questions just to get the overview of the entire pipeline because it helps me understand as to the state of the ML and then helps me ask where do you want to get to as well. So one more question is suppose you deploy an end point, right? Like you have a model that is there, you have trained it and you have deployed it. Now you have a different version of the model that you wayve trained and now you are not sure which model is good. Let's say the model one or model and you cannot replace completely model two because you're not sure and real traffic tends to be different. So a use case that people end up doing is you kind of expose them to different percentages of traffic and then as the one model performs better than the exposure of the traffic to that increases and at the end like whatever is with something like traffic or you call it like shadow testing or Canary testing. Do you have a use case like that or have you seen or do you envision a use case like that? Not at this point. Pin time we definitely want to do a B testing but also we want to make sure that the quality of our end before. I think I get a good sense of basically at the truefoundry I'll tell you what we have built, okay? And then because we set up this call for a small time, I'll set up another call where we can go into a deep dive of some of this. But basically the way we wayve built roof on is to make it very easy for you to deploy models in a scalable way. And around that deployment there are other functionalities of the platform to kind of complete the flow of a data scientist or ML so the code is still a deployment which is built on top of Kubernetes, like you're using EKS. So if you cannot use our system, you can just connect to your ECS and then it works. What it does is key the deployment that you are doing right now. If you are doing as a fast API, right like you might be incurring performance and depending on how much request you incur, you might need to improve your performance. So what our platform does, it allows you to take a model and deploy it using choice of whatever you want. Like you can use a faster pair, you can actually select the model so that we actually give you an option. Okay, your model isn't fighter, so why don't you use this? Or your model framework is this, why don't you use this which is a more optimal way of deploying in case you are looking at a performance as one of the criteria. That depends on how many requests your model is entering and so on. So this is one major part that we do in deployment. We give you access to a real time model as well as a batch time. You can deploy very easily and then you can run it at your own schedule order front out, or you can manually trigger it for a batch model pin real time you get an API endpoint with a documentation and along with system monitoring that is automatically generated. So for all the entrypoints, you basically automatically get your Hana dashboard which already have the system log in there as well as the system metrics in there, so that you can compare and track ups to what is happening. So this is the flow. Apart from this, if you insert one line into your code you can actually see basic monitoring which is ML level like performance monitoring. Monitoring part is again like it's not something every company will use, but we do recommend some companies depending on what form of data they are using to try them because it helps them keep their models and the entire team integrates very easily with your CI CD pipeline and then label on as your use cases for traffic splitting, et cetera. It's also put it in a very similar wayve and this can happen over all three mediums. Like you can use directly from notebooks, you can use gamma method or you can use directly from so that's primarily the platform has been built. We make it very seamless for you to actually use the platform and generally because of the way we kind of used generally the cost that you end up pin court turns out to be lower than what you normally incurred through a normal system. Because we can integrate your training and deployment workload on the same service or on same box. So there is sharing of resources and everything that is enabled. That is one major benefit now because helping you track experiment in a very seamless wayve. So what you do is very simple. While you just log your model, you can log your data, you can log metrics, custom metrics and so on. And then it will take you to the Uri and you can compare between models and select okay, this is what is best in terms of the and then you can take the model and just deployed directly from there as well. And that functionality is there. So from what I understand, the major use case that you are looking for at this point is that experimentation we can show you a demo of maybe a model and how you can use the product platform in terms of tracking and comparing different versions, et cetera. At the same time, I would like to show you the deployment piece for the reason that it's simpler than what you might be doing on Sage Maker plus it makes it out from you. So you will be able to kind of do some things without having to worry about things like monitoring, et cetera. So even if you continue to usage, because that's fine. What I would love to really do is if there is few models that you can take and you can do a POC and you can just try out your platform so that you get a sense of how easy it is or how hard it is, that will be really helpful for us. We are very early stage so we need a lot of help from the ecosystem. I will take you through the demo and we'll give you access and you can try and play around with it and we can take it from there. Does that sound good? Make sense? Next week sometime will work. So can we do it post 630 sometime? I'll get my other cofounder to. Thank you so much for taking time. Really nice speaking to you all and I really hope you all do really well in this. Sure, we'll discuss more about this on the next call. I'll send you an invite. Okay, thank you so much. Have a good one. Bye.