Are you in midst of travel? Hello? It's very bleak. Like the voice is breaking out. Could you try once again? Is it better now? Yeah, it's much better, but there's some background noises in between. If I am on my way to office, I should be in office in 5 minutes, but we can get started. Sure. Maybe we can use that for just the intros quickly. Kalan. I can hear you guys properly. Okay, cool. So Kalan, just to kind of I think from the email thread basically we were connected by Adithta caldro who is actually one of our advisors and angels. So he connected us to Prasana, he used to be at FB Learner at Facebook and he built the entire ML platform at Facebook there. And we are trying to build in the ML space. So that's how he connected to prison. The goal was to kind of understand today's system set in mobile in terms of what the entire ML pipeline looks like, including the infrastructure, how is it managed, what are the things that matter and so on. Brief background about us. Like we amandrag one of the co founders at Truefoundry ML Infra. Nick Kunj is also one of the co founders. We started Truefoundry one and a half years back. Nick Kunj, Abhishek and myself, we are all bachelor's from IIT Kharaspur and then post that like we spent our parts in different areas. So I used to be with a hedge fund called WorldPoint where I was building trading strategies using a lot of data and then later on was doing portfolio management for them. Nick Coons used to work with a startup called Reflection in the Valley where they were building recommendation systems for ecommerce companies like Ebay up to a scale of 600 million users and so on. And then later on he joined Facebook and was a part of the team that was a lead ML engineer for the team called Portal which used to build competitor to Alexa almost. And Vishek used to be at Facebook. He led the videos team there and was also part of the intra redesign team. He was a senior staff software engineer. So all three of us basically left our jobs in 2020. 2020. We built our first startup in the talent space called an Entire, where we were helping companies had technically better talent and then we sold it to InfoAge in India. During the course of that time we were building models for matching candidates to resume and the next job prediction. And while doing so, while the build out of the algorithms was okay, like when we had to deploy it in a seamless when integrated with our website, it took us quite a long time. So that's why we kind of basically were discussing about the systems that existed at Facebook at that time and Vishik and Nicole mentioned about Epiler, which is their internal ML platform. And basically if we can bring a similar platform to other companies around the globe where the entire infrastructure is abstracted out for the data scientists, we can potentially help the ML deployments to be much faster. So that's how we started. Proof truefoundry. It's been almost one and a half year now. We have built out a basic product, deployed it in a few customers and so on. The goal is really to build a multi cloud system on top of Kubernetes, wherein it's very easy for data scientists and ML engineers to quickly take their models tested and then deployed in production in a reliable and cost effective manner. So we try to provide the Python SDKS and we try to provide the best practices, best desired practices across the pipeline. That's like a brief sort intro about us. Chinma is a part of the founders office with us. So the goal of the Call Kalyan was to kind of understand the entire ML system at inmobi what are the use cases and then also primarily from an engineering perspective, what are the things you all have built in house and so on. If you don't mind, can you bring. Got it. Cool, I can do that. Basically what we have built is like a platform on top of Kubernetes, basically because Kubernetes is hard to understand for a lot of folks. What we try to do is abstract away the difficulties of Cubanatis. And on top of it we have built a layer wherein first of all, you can come in and easily manage your Kubernetes clusters in an efficient way, including connecting to clusters from different cloud accounts and so on. From there, within each cluster, what we try to do is divide it into different workspaces, which is similar to say, namespaces in Kubernetes, which comes with a certain amount of resources and certain degree of limits as well. So suppose there's a team, you can basically allocate it to different teams or you can allocate it to different environments like dev, test fraud. This is the intra side of things, on the deployment side of things. What we try to do is like we have a Python library wherein you can use it to then take a model directly from say, a GitHub or any versioning, or you can also take the docker image directly and then you can basically deploy it with some things. Like you can put Auth behind it, you can put the right secrets management, you can connect to the base accounts and so on. So the deployment here does not necessarily have to always be a prod deployment. Anytime you are going and doing deployment, whether it's running your training code or running your inferencing, where you are using virtual machines, that's where the system comes in. So you could basically select like a particular instance type where you want to kind of post your model and then you will kind of just either through the UI or through the Cli or through directly from the jupyter notebooks as well. You can take it and then you can deploy it. You can also put like configurations in terms of resources, et cetera that you want to use. You can configure auto scaling and all behind the hood in terms of different pieces. And once you deploy, you can deploy to a test environment and from there the promotion flow is there where you can promote it to say production environment with the right A B testing and also the A B testing pattern. We are still building up. If you are doing like training jobs you can also kind of log your training metrics and compare it and then choose which metric is good and so on. Can I submit something like model output to your platform or what? Does that work? Yeah, you can use any model library. Basically it's built in a pretty much framework agnostic way. So we also kind of have a small concept of the model registry and you can basically register your model there and then we directly deploy on one click from there. And what we try to do behind the hood is depending on the same work we use, we try to use the right model server as well to optimize it. So you can deploy it as a fast API. You can choose to kind of then use like a python. So if you're using pythons and so. On got it ondra. I just leaked office. Give me 2 minutes, I'll join back. Sure. Can you hear me? Yeah, I'm able to hear you. Hey, sorry man, I got delayed in the traffic. Okay, cool. I think I got a fair idea, at least a high level. I do have many follow up questions, I'll ask them, but before that, just a brief intro on myself. This is Kalyan. I look into the ML platforms in mobile over here and at the same time I also work with one of our business units, demand side platform which is heavy, and data science in general. Right, okay. So we have in mobile I'm not sure if you guys have an idea of in mobile structure. In Mobile has the ad tech division and the consumer division and in the ad tech division we have multiple business units platform, supply side platform and affiliates is another one, so on so forth. Right. All of them use data science to a certain degree and some use more extensively, some use it moderately, so on so forth. Right. But we as a company are looking at all our problem statements to revisit to see where we can use MLA more and more and where it makes. Sense to use that. Okay, so that's about my current role in mobile. I used to be in Flipkart before. This is frankly, this is one space I have been interested in ML platforms, flipkart, there was a dire need of ML platforms and so forth as well. That's what I realized at least I realized back then in Flipkart, and then there was a platform team member platform team in Flipkart as well, but not so well funded and not really the capabilities for limited, so on, so forth. And then prior to that, I was in LinkedIn. LinkedIn probably is one of the company that has invested heavily in this ML platform area and they did have their own capabilities, platforms and so forth. So, yeah, that's about me. And that's how I've been in this. I saw in your Lingon, it's mentioned like the research side. So I just wanted to know, were you also in the part of the Lingon platform team for them, or was it like a different team for research there? Correct. So the way Lingdin is organized, right, there is a data science. It's not called data science, at least when I was there. It's called essentially it was under engineering itself. It's an ML or under engineering and people are called research engineers, right? Unlike I think Google and Facebook have a similar setup. Essentially, people own both the engineering and data science in those teams, right? They own end to end everything, be it modeling part of it, and beat the build, deploy, serving, so on and so forth, all those aspects as well. So back in those days, the initiative, this has started and in fact there were a couple of more bunch of pocket initiatives as well, right, in terms of building one ML platform inside this ML division for the company. And in India as well, we had a small initiative. There were different types of initiatives, right? There were one initiative in terms of just purely for model libraries, if someone wants to use probation tree logistic region, so on, so forth. There were these model libraries floating around and there were frameworks only for text models. And in India we also were using essentially some sort of automl kind of framework, right? We were trying to build automl framework with the human in the loop stealth frameworks. And then there is a bigger it's called Pro ML, if I remember correctly, Pro ML was a bigger framework ML framework that was trying to take care of everything end to end model pipeline, right? So that when I was there, it started under the ML.org. But if I remember correctly, it is not transition to a platform team. So usually the projects start under the ML.org and then they transition to platform team the moment, because again, ML.org is not really meant to take care of these things, right? So, yeah, that's how I was in the Mlr, but then I wouldn't say I started this or anything like that, but I was part of those discussions and meetings when the initiative was happening, right? And of course I was the user as well of some of these platforms. So, yeah, that's how my journey has been in this space. And I'll talk briefly about in mobile, right? Sure. In Mobile, our use cases are split it into offline and online. Right? Okay. If you ask me any external capabilities or tools that we use today. I. Would probably say only data bricks notebooks, right? That is primarily for data analysis also, any idea purposes? We use the databricks notebook, right? And ML flow, it's tightly integrated into data bricks on case by case basis. Some individuals use it, but it's not tightly integrated into our ecosystem where we can track the model performance regularly, so on, so forth. Right. So that we don't do overall from the outside perspective that's the data bricks is the only external capability that we at least as of now, right, as it stands today. But that's our external capabilities, right? That's the only one I think in Mobile over time has evaluated. I have been inmobi one and a half year in this one and a half year as well, we have evaluated and prior to that as well in Mobile has evaluated certain capabilities. But as of now it's primarily on data bricks in the internal capabilities, right? In terms of internal capabilities, there are two, right? That I would say that we have internal capabilities primarily we use something like Sigma. It was also hosted notebooks, but then that got replaced by data bricks. So I won't talk about that. That is sort of I would say we have deprecated it, we don't use it. Sigma, we no longer use the Sigma. But there is one called a ref framework. Okay. Ref framework. Okay. Ref framework. It's not a company wide capability. It's a capability that was built specifically for DSP demand side platform. That business unit essentially built it for the results. Right? All it does is essentially it supports two aspects, right? It takes a model binary and it knows how to serve it. Okay. That's all it does. Right? That's the first thing sort of serving part of it. It's a library. I mean, it's not a hosted serving or anything of that sort. It's a library which essentially takes the model by the end knows how to serve it and at the same time it takes care of refreshing the model as well at a regular basis, right? You can configure the refresh and it takes care of refreshing the model as well. So it has support for the challenge with that model is with that essentially framework is the kind of model that it support is limited. So it only supports linear models. And recently we extended it to Dictionaries and MLP is something in neural networks. MLP is something that we are trying out as we speak today. But yeah, it's not a scalable framework, I would say, because we have to figure out for every model type how to serve that particular model. Right. That's about the that is like open source or is it like your internal thing only? No, it's all internal. I mean, we are thinking of open sourcing. So ref is again, right. That's one part of it. Right. Ref is again only limited to DSP, so on, so forth. But the bigger umbrella in terms of initiative, ML Platform initiative overall, it forms under the bigger umbrella called Omega. Omega is our ML Platform initiative and that tries to cover end to end, right. Everything. It's tightly integrated with Databricks because our journey starts with Databricks and we have built a feature store on top of Databrick feature store. This is frankly probably six months, eight months old initiative, right? Or six months old initiative. We have started this and it's a work in progress as well. And we are building a bunch of capabilities around it. See feature store, we have evaluated good amount of feature store that is feast in the market. And this is a new one called Feather in the Market, the Microsoft one. So we have a Databricks one as well. So under one of the challenge with our use cases, right? This is a constant challenge for any of the components that we pick up right, in the cycle. Like there is EDA, there is feature engineering and then training and then servings and so forth, right. The online path is extremely critical for us. So we are not extremely high throughput and low latency systems. So even the remote call tends to be inexpensive for us most of the time. So that is one challenge for us. Most of the feature stores we had to discuss because there are latency right. That we need, they are not coming closer to that. What is the latency requirements? Is it like one to two millisecond or even lower than that? I'll tell you, I'll give you a brief. This one at least current or production, right? We have around 20 milliseconds to run all of our models, right. In a request request path. Okay? That's a bandwidth that we have and we end up at least 90 percentile if you look at it, we end up running close to 50, 60 models in that. Okay, so it goes almost in the microseconds, right? If you take a single model call, it goes almost in the microseconds. I mean, you can batch them up songs over whatever, but yeah, the single predict call or latencies usually are close to microseconds, usually. And which is where I mean, like again, in this 20 milliseconds there is the rest of the bandwidth is usually used to fetch whatever it is called the data stores and get the data, so forth. All that that goes in that. Which means if we want to do like the feature stores that we had, right? One of the challenge with them was most of them were expecting another external call. So adding any external call will end up adding more and more latencies. Right. So, yeah, bottom line, both for any of the features serving as well. Right? That's a big challenge for us. Again, I'm not saying remote call is remote calls are not possible. It's okay. We can make the remote calls as well. But again, probably per model call we won't be able to make a remote call then one batch call probably we can potentially make and then do that something we haven't done till now. It's been mostly within most of our models and within the same process currently. So yeah, that's about in terms of this is just the feature engineering on the serving part of it, right. Online part. And then there is a training yeah, in the training path omega we have, as I said, in house one Omega. We are trying to cover that offline pipeline as well. Essentially making adding the features easily and both adding features and consuming them in the online path. Right. Both of them making it easier, so on so forth. So we're trying to make that journey as well easy through this omega and our current focus is more around feature engineering, serving and a B testing. This is what our primary focus is around and then we plan to get into monitoring and other pieces as well. Okay, so yeah, that's about the current use cases, so on so forth. I hope I gave you a decent picture. Yeah, that's good. I know the call was set for 30 minutes, but do you have some. Additional time to learn like 15 minutes? 15, 20 minutes unless someone comes to the room. Great. Like to understand what is the serving layer built on top of? Like what do you behind? Do you kind of build it on top of Kubernetes or is it it's Kubernetes. Yeah, it's built. We are hosted in Nazir kind of. Just give me 2 minutes. I need to look for a different room. I think the room is. You are on mute, I think. Sorry. Yeah. You are asking whether we are in Kubernetes. Yeah. And then overall system like the training, I'm guessing it is also running on virtual machines and all. What is the entire workflow? We use Spark for all of our training and most of it is distributed. Okay, that's a pain as well. Today these distributed libraries do end up failing once in a while and we have to keep debugging them. Whatever is that this one, right? Be it distributed TensorFlow Flow or Distributed Light GBM, any of these things we do see once in a while issues with that. So yeah, that's also paying as of now today. Okay. Another question. Kanye mentioned that the models that you support in rep are mostly linear models, right? So are all the production models linear or you also mentioned like sensor flow, IGBM, et cetera. So are they like. So it used to be only linear, right. And last year we upgraded it to tree based models as well. So that's where production primarily runs on tree based models. And as we speak, we are a B testing neural network model as well. So this is all again the ref that got upgraded to these models, right? Essentially. Okay, just trying to understand. So you mentioned that supporting each new model takes typically a longer time. Most of this time goes in what exactly is it to build down the abstractions, to optimize the latency? What part of this pipeline is like the bottleneck which causes like the tiny. The bigger problem is we can't use these libraries as it is, right? Like if you take a TensorFlow, let's. Say. Forget about TensorFlow, right? Even Lgb light Gpm, if you say we can't deploy it in Python and then we can't make a remote call to Python from our service side. Firstly, the Python model entrance call tends to be expensive, whatever that runs in Python. And secondly, we can't afford to make these remote calls. At least we try to avoid making these remote calls. So yeah, more than remote call, I would say the Python being it doesn't give the latency constraints that we have, it doesn't satisfy. And most of these models have mostly Python wrappers and not many Java rappers that you would like to be. And again, some of them has C plus plus. Right? Again, that's again, it's a key area for us. We don't prefer integrating Jni libraries into our production systems. That historically we have seen some leakages, memory leakages and so forth with Jni libraries. Which means that we ended up writing Java rapper's for most of these ones, GBM. We ended up writing production quality Java for light GBM one. And similarly, even there is TF Java as well for TensorFlow models and there is on X and so forth. Right, but all of these use Jna calls, we can't use them as it is. So that's where the challenge is chilling. They use which code? Sorry, I didn't catch that. Jni, jni calls to the underlying is E plus plus. Right? Both abort TF java NX So they end up using Jni and Jni tends to memory is not in the control. Right. So we have historically seen some memory leaks and stuff 1 second in a meeting. Understood. One question. So Coleman, like you mentioned, so I guess your other micro services are in Java and because you cannot afford a network call, you have to load the model in Java and executed by the Jni call. It's not just about that. We can potentially afford a network call is probably we are making only one network call, right? We haven't tried that out till now, but we can potentially try that, right, that's not an issue. But model running in Python most probably won't meet our throughput and latency requirements. That's even if we discard the network call. Right, I see. These models, you can host it by the TF server, python service will run in a C plus plus back end as a separate service and then call them. Correct? That's one thing that we are trying out as we speak. Right? Again, the throughput requirements are not exactly what we want to be, but I think we will reach somewhere in the middle load there. That's something that we are trying out as we speak. So the game is of three things, right. For us, one is throughput. Throughput we talk about usually I wouldn't say millions of queries, but somewhere around millions of requests per second, right. Or half a million per request per second. Okay, that's a throughput. Right. And as I mentioned, these are half million requests per second. And then every request tends to have 40 50 model calls at least. The which needs to be completed in 20 milliseconds. Right? That's the second one. And thirdly infra cost is again also super constrainted. Otherwise you can probably horizontally just scale the systems. Right. Infra cost is also key criteria for us, especially in a tech. Infra cost tends to become like around 30% to 40% of the net margin if you look at it. So we can't yeah, just scale the systems horizontally easily as well. So we are bounded by that infra cost as well there. So, yeah, we these are the three key constraints that we look at usually before we take up any model, before we end up taking something to production. Right, understood. So what is the current setup today? Like, you have your Java microservices where. You load all this 40, 50 and. You just call them together in code itself mostly, and then you return the result. Correct. Think of these as model libraries, right? That we have serving libraries and we integrate them in our production service and then they end up calling it in the same process again. Having said that, we are evaluating the remote calls and one of the one that we are planning to evaluate for that is TF serving itself as we speak here. That is one effort that's going on. I think that can ease out a bit of because they also support these guys also support like batching microbicing and. All these guys are twocap is very. High and microbicing will really be correct. The batching should take care of that, is correct. Yeah, understood. So on the training side, you mentioned, Kalyan, that you pretty much use databreak notebooks, right? And then you submit jobs to data breaks for the training. Correct. It's essentially pipe park job and we submitted data breaks for the training. And then you get scheduled through Airflow regularly. Okay. You are using Airflow as the scheduler layer. Correct? Understood. And then the models are saved in you're using ML Flow to save the models data, like metrics parameters or no? Abbyshek, today it is the Omega ML platform that I mentioned, right. That is something that is getting built today. And we plan to onboard the clients, right? I mean clients in the sense that the platform clients Omega clients sometime in the next quarter and that uses ML Flow internally. Model registry, right, ML model registry as well. But today in production we use the ref. It's not a model registry. I would say it essentially goes to a model store Blob store and our machines sidecar essentially pulls from the model Blobs are regularly and then it gets updated in memory. So it's pretty much an internal solution that we have today which will mostly be replaced in the coming quarters. And in Omega you are building a layer over ML floor. Are you building custom UI also? It's just we are not building any UI. We are primarily one of the feature transformations, right? Especially in the feature store. Like when we evaluated, feature transformations is something that none of the existing plates in the market provided are providing. Some of them are providing, but then again, like they involve making a remote call for these transformations which again didn't sound completely something that works out for us. So feature transformations we are building and then we are also building on top of stitching the pipeline, right? Completely. That part is all built on top of ML flow offline feature engineering and training and then the serving pieces, right? Serving pieces that include local serving as well through our libraries that I just mentioned and then TF serving kind of hosted setup as well. Understood. One question here that you mentioned that for the transformation functions you cannot make an RPC call, but does that mean instead micro services are in Java? You actually are writing the transformations in Java also because you need to call them right from there. Right. Interesting. And this is something that we are debating a lot. Right. The funny part is we do use Pye Spark, right though it's all in Python, but most of it is distributed and then we end up using Bicepark, most of our offline use cases. So one constraint that we have put is the transformation has to be in scalar. So that way we end up using it in both offline by Spark and the same transformations get shipped for online use cases. I see. So you're writing it in like do data scientists are comfortable with writing? What I've heard is most data center is only no Python. So do all data centers write this Keller transformation? These are simple transformations most of the times. So that way it's not a major we are just expecting some sort of a lambda kind of thing right from them. So to that level they're comfortable but end to end scala they might not be completely, but yeah, small transformations they are signed with and our transformations are usually not to measure. I see. Even the transformation layer colon. Have you thought of separating it out like a sidecar running or something like micro service running next to it that you can quickly call or the RPC call will just become too expensive for you? Yeah, thing is that these are lightweight as well. Completely. That's the reason we didn't want to make another call just. For these transformations. Right. But we felt like essentially we are adding one more call to something that is not too heavy as well. So that's the reason we haven't explored that solution in terms of making another call beat through a separate service. Okay, makes sense. Go ahead. Asking on the cost side, because you mentioned cost is generally cost management is important. So how do the folks who are kind of building out these models when they spin up like this park clusters, how do you kind of set up limits, et cetera? Or is there a system in place for that? I wanted to understand that. Sorry. How do we monitor the cost? Is that the question? Monitor the cost and then even like data scientists and ML engineers were kind of also building out these models when they are spinning up spark clusters and running their training jobs and all. Is there some resource management that is controlled there to ensure that no one kind of consumes a certain beyond is the cost mainly the serving cost because of the amount of. Interest? I got it right. Interesting question. And in fact, this is the discussion that we are recently having with Databricks team as well. We don't have monitoring to that level right today, frankly, we don't do monitoring at a job level. We try to I mean, if we attach tags and stuff like both Azure and data breaks would give us some cost estimate, then but our cost monitoring at least currently is limited. We usually look at Azure gives a cost management for everything, including data breaks as well. We usually look at that at a monthly basis also to see how what that trends right? And sometimes at a weekly basis as well. But other than that, we don't have any fancy solutions in terms of if someone is, let's say, starting a job, can I get an idea of in terms of how much that potential cost of that job going to be or if any alerts in terms of like if a cost is piking up suddenly, right. Any of those things we don't have today, or any budget allocated for a certain I would say into the cost monitoring, it's been limited. Having said that, we haven't explored much as well. We haven't invested much in having a better cost monitoring for these things. So, yeah, we don't know. Probably there are solutions out there in the market. In the market in the sense that just with the data bricks itself, right, that we recently started a thread, so probably there are data bricks itself already available that we haven't explored as such. And that's about the offline part, right? Yeah, serving part. Frankly, we know the cost for the serving machines, but as I said, it runs in the same process, so we don't know exactly how much model is exactly contributing to the cost. Right. We can do some backup. Sorry, you're saying basically out of the 40 50 models that you mentioned that are called at once basically what is consuming, what cause that is not known as of no. That'S still an unknown as of now. Unknown in the sense that we can do some back up there on local regulations, right? And then we have some estimates on that but the exact number is we don't monitor the exact number, but we have some rough estimate, right? Rough estimate in terms of how much they must be contributing to the cost. And one more question that I wanted was because a lot of businesses use it for their ads like do the data is there like separation of models that are serving different businesses or data from one business that is being used to run that is also kind of used for others? Are there multiple copies of the same model for each? Sorry, adding a discussion, can you repeat that? Actually I wanted to understand from the attic perspective, does the data basically you would also be serving a lot of the clients like what they are using for their ad. So the data that corresponding for one of the clients, is that also being used for other clients in anonymized way or for the same model? Are there a lot of different versions that are there for different clients? That was the question. Frankly, at least the data sharing between advertisers, right, at least as of now hasn't been a major concern. We do train all the advertisers together and we use the data of all the advertisers together as well. I don't think I haven't heard any privacy shoes as well. There is at a user level we do have, right? In terms of we need to delete the rate after certain point of time so forth but we don't have at a cross advertiser level it's not a major use case as such. Having said that, we do have we are exploring use cases of in terms of having model for Saturn advertisers differently, right, so on and so forth. So the model that I like if I take a use case right, I mentioned 40, these are 40 model calls, right? But the model is same, the model is same, it's just that you get different advertisers so that's why we end up making 40 calls, right? But we are also exploring if we can potentially have probably a model for advertisers and so forth or at least for certain advertisers we can potentially have a different model. So that is something that we are exploring. I don't know if it answers your question. Yeah, got that cool. I think I get a better sense. Did you have any other questions also like oversighted the time to just want to be mindful there. Nothing from my side. Thank you Kalan. It's really useful basically love to I think you are also in Bangalore would be great to sometime actually meet and for you more like the system that we are thinking of building and even take your feedback as to how we can make it more useful. And if there's even an internal use case that you feel we can closely work with and help, that will be great for us. From a learning standpoint, we are still in very early stage of the journey to looking for help from the ecosystem as well and trying to work with some early customers as well. Basically that's the sighting. But the main thing would be to kind of still show you what we are building and then maybe even get feedback and discuss on a few interesting points that you also brought up. Yeah. Serving part is something that we would be interested in. Is there any documentation that you have for the capabilities or is it something in the early stage and documentation is not out? We have documentation design, I think undergo. You can send the documents and all that we usually send. Yeah, that will explain you a bit of the architecture. We can also show you like maybe one other call. We can show you what we have. Yeah, can you probably first appoint me to the documentation and then we can have probably a demo call also as well for the serving board. So serving is yes. Okay, understood. Coolen. Will do that. I'll send you the documentation. Also try to send you the architecture overall so that you get a sense of the architecture side also. And then it'll be great if you can drop your number or something and I can connect and we can set up something later on once you have gone through. Sure, makes sense. I'll drop my number in the chat. I can drop it in the chat itself. Yeah. Thank you. Jellen, this is great to know. One other question that I just missed. The ML is like a very core part of inmobi in terms of how you all kind of see it internally or just wanted to understand how many members are there that are on the ML side in terms of maybe data scientists, engineers and architects and even like intra folks who are involved. Correct. In fact, this year theme is sort of ML first for mobile, we currently have around ten to 15 data scientists. These are the core data scientists, right. And then there are a bunch of units as well that work with them. If I combine everything, probably maybe it'll cross 20 as well. Like the number and every division, as I mentioned, use the data science to a certain extent and for some divisions it's almost bread and butter, right? As I said, like there's DSP, there is a supply side and then there are other units like measuring, exchange, so forth as well. For DSP, it's almost like a bread and butter. And then SSP, there are a bunch of solutions that we are trying in the SSP side as well. And SSP is in movies, bread and butter and exchange. Or exchange. So that's a big in terms of the business, that's pretty big out there. Whatever the impact that we make out there that gives us a lot of benefit. And yeah, there is again a consumer division of In Mobile, right with the clans and repos and so forth. They also heavily used. And it's the central team. It's not like each division has their own. Usually look at the ad tech division. So we are primarily looking at the ad tech one. We do plan to expand this to the consumer division as well but make it more central for the consumer as well. But that's frankly that's more not in the current plans. Probably once we solve the problems of a tech that will potentially go with that. Cool. Kalan really appreciate you taking out the time. Thank you so much. I'll send over the documents and we'll connect and once you have looked at it we'll take it from there. Cool, cool, cool. Thanks. Andro it was an interesting call. I would look forward to the documentation and then we can thank you. Bye.