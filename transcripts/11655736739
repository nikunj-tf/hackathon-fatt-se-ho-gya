Hello. Hello. Hey. Hi. Kunal? Yeah. Hi Kunal. How are you? I'm good. So you have the fulltime. He'S also here. Always good to meet someone from KGB, obviously. Yes. Okay. So we also have Mohan actually, who is our DevOps engineer actually here with us. Actually, Amal also wanted to join, but let me come. On. Great to meet you. Where are you? Sorry. He's in Dubai. Okay, got it. Nice to meet you. You guys started office or still working from home? We are actually in office only. Hybrid model. Recently big like now it's almost a 20 member team. 15 are full time. There are a few contractors and third interns, most of them being on the engineering side. Including contractor. And 15 full time bucking contractors fluctuate. 15. Both vary teams. I think you mentioned someone else also joining me. Yeah, I talked to Amal actually. So Amal is currently the business head here, logistic actually. And he is also prone. Corrective. Very interesting. Is he joining? I pinged him. He said he will join actually. But that's okay. I think we can start actually. But I think for Mohan's context, we can give a quick introduction. Mohan basically, at Truefoundry, we are helping companies quickly deploy their machine learning models to production. We are building a platform for that. And we have also worked with companies where we are helping them to build like a developer platform where they can ship their software, engineering services, et cetera very quickly to production on top of either multi cloud environments or so on. So it's more like a pass on top of Kubernetes that we have built. So yeah, right now we are working with few companies and we are looking to kind of partner with more companies that we can work with in the early phase of the journey. Vishayk, myself and Nikkunj, like, we are the three co founders. We started it like last year in September sorry, 2021 September. Now it's no one last year and it's been like a little more than a year. And the team, as I mentioned, is now like a 20 member team. The goal is to work with good companies to try and understand their problems, help them add value to them and go along the journey. So in this call, basically I think a Vishek wanted to take you through how we have built the platform. Also give a live demo, also talk about the principles on top of which it has been built and some of the things that we are doing in future. And then answer questions from your perspective. But before that might be do you already have context on the current system or do you want to kind of get that context so that you can also reference the right points? I think Funal said last time, you have a decent idea, quickly give a two minute overview here for Android and also. Currently as such, we don't have any ML model actually. We have in plan actually to build a model for predicting the right career partner for any delivery. That work is still going on, where I think we are struggling to get the set up done for them. Because the ML team here, we don't have ML engineer or the data scientist here, actually. But Landmark Group has their own data lab team. And so we told them that okay, we want such a thing actually. And so they are working on that part. And earlier those guys were working on the Azure platform and so their model training, everything was on Azure. But we use the GCP actually. So we told them to use the GCP platform to build the model, train them and then provide us the API through which we can get the career partner at any point of time using the model, actually. But obviously and also that is not that much priority for us, actually. And because ML will come later on, very late in this. Right. And regarding the deployment side, actually, so we have three, four services actually, it's purely a macro services based architecture, actually. And those are deployed on the GCP using the Kubernetes and whatever the standard Kubernetes way is there. Actually. We use that and we have cloud build, right? Yeah, cloud build. So that's the ingo service in GCP, similar to Jenkins. So we are automated for the CITP purpose. Okay. Google provides the in house services, actually. That's the cloud build. So we use cloud build to build creating the build and then deploying onto the product. Or the Google case, actually. Yeah. So there is no Jenkins, right? There is no Jenkins. We don't need. Probably both. Is it Mohan who was speaking on the background? Okay. One question I had was like the current deployment of how is the overall mohan and his team does the overall deployment or is it like people who are engineers, they themselves kind of convert the services and how is it currently done? It's automated. So basically whenever a developer pushes a code automatically using the gig clothes, the trigger will happen and it will build and deploy to the respective clusters. How many clusters do you maintain? One as of now. Right now, we have three different environments. One is dev, sandbox and pro. And we are like different machine types of different nodes in these machine clusters. So private and public clusters. Okay. And how many developers are there, like, who are writing these codes and everything? Currently there are code, so the actual code, right? Yeah. Basically all the developers who are anyways writing. Twelve to 15. Okay, got it. How easy is it? Like how given it is set up and all, how is your, how difficult it is more like for you to do and do you find it that. One can answer, actually, since it's a startup, it was a great journey for me. We didn't directly go to production, right. So we were having a lot of flaws. We started developing the development environment. Then we went to Sandbox. So we identified our mistakes and how we can improve in the production and all we got to know and I'm the only person in the DevOps team so who is managing and writing all the managed side. So it was like kind of easy and difficult. So actually Mohan also has I think Mohan also learned himself actually the GCP and all the details and also he deployed and work on that part. Okay. So he can tell what the difficulty he faced initially. Actually in a previous company I was working on Azure and a baseball. So this was nothing anywhere. The public cloud is something like the naming convention only is different apart from the functionality of services. I used a normal kind of standard procedure where I'm using healthchart to take care of all the deployments and CSV pipelines to automate all the deployments. Okay, got it. And you mentioned you are the sole person in the DevOps team. So the Landmark Group, the other functions they run separately. Is it. Actually I think I told because in the Landmark Group is a large organization actually where there is more than 20 businesses out there. We have one way to handle it. But there is a separate Landmark group which does have the big engineering team and their deployment and everything. Okay. And where do developers see logs and metrics on today? Like for debugging and things like that. So we have Amal. How are you. From Kirkpoon? We are from it. You would know a lot of these people canov and all they can. Sorry, whom did you speak to in Capillary who started this one? This company called what is it? But I am actually telling about this company. Do you remember Vishek, which company which is in the development phase like Fayette Cloud? Basically we were actually talking about the True Foundry platform ML and basically we are trying to get in understanding from Mohan and Kunal on the overall current software engineering deployment pipeline and overview of ML. I think ML is not being used right now much maybe later on. But the General Software Engineering deployment platform and we wanted to kind of showcase the demo of how we have built it. And there is a potential way in which you could use our platform for testing and for deployment if it could help solve any of these. That was the main. You guys are doing some cool stuff. Let's see. Thank you. We need help from. If you have more question, I think you can ask but start demo. During the demo we'll say that. Maybe go ahead with the demo and in between like please feel free to ask questions like anyone who has like you'll directly start from the demo. You want me to kind of give the overview? If you want to give an overview then I started the demo that's fine. Okay. I think you can just start because I think everyone understands the part, the structure you can just call out before. I will just share my screen. Basically I'll go on to the more software side Kona, as you mentioned last time, or if I will highlight towards the end. So software like we start with like you start with cloud and then you'll probably start with the Kubernetes cluster, right? So this is where you can see all the Kubernetes clusters in the company in one place. We also support cross cloud today. Like if you have one AWS you want to move to GCP or you want to have both cloud. You can also do that very easily. So this will basically create we also have provision to create the clusters. Let's say you want to create a new cluster, so we'll automatically create the cluster for you on whichever cloud you want. And we also give you all the TerraForm code to create the cluster. So it's not like we're taking the code. So every code that you want will give you the code in all that gets repository, everything you like a lot of time writing the code to create the cluster and things like that you'll going to do with the telephone there. Then for each cluster you can basically you can see that this is connected, this is just connected. So this cluster is not connected to the main our control plane kind of thing, but this cluster is connected. So you can have four components and maybe one of them you are disabling or things like that, so you can mark them as disconnected. So first you create the cluster and then once you create it so it automatically gets connected with the cloud actually. But to connect with the cloud you have internal authentication system and how that will work for us. Yes, I have a credential and everything right, that set up need to be done. Yes. So if you already have a cluster with you, what we do is we give you a very small hem chat. So you just go and install that hem chat on whatever cluster you have and then there will start being connected here to start showing up here. Okay. And in a new cluster like the authentication basically in terms of users, I think I wish you can show like admin editor viewers so you can quickly. Mark like which users have access to what clusters. If you want to limit like some users can only access some cluster and things like that. You can do it very easily. Which cluster? Cluster. You provide all the details. Yes. Which type of machines are allowed? The machines is basically generally more used for ML machine learning, but you can also kind of provision okay, this kind of machines for this environment or this kind of machines are restricted for the other environment. So you can configure that here as well. You're seeing something. Go ahead. Then you can connect all your docker registries here. It can be any register, like AWS, ECR or Google Container, Wall Street, like whatever you want to connect to docker registries here. You can also connect your git repositories right here. Can be GitHub bit bucket or githlab. You can also connect your secret stores, like if you're using Google cloud, secret parameter store, whatever you want to connect. You can connect everything here. And once you do this, basically your entire infrastructure of Kubernetes clusters kind of come here. So you can basically see all the namespaces and all the clusters that you have together. And you can also see for each namespace how much is being used. So if there is 0.5 CPU out of two, you can limit the amount of CQ memory and storage that you are giving to each namespace. This will also help you limit the cost. Now, what is the. Workspace? Is the name of space on Kubernetes basically? No, in that cluster, right? In cluster we'll have multiple application rights. We have names. Namespaces are like smaller units of a cluster. You can think of it like a cluster machine, but you want to create smaller units of it, like two machines too much like that. Okay, so basically you are giving the name to the whole setup actually. So you are creating smaller workplaces where the limit is two CPU. You cannot spend more than that. So you can limit it even if you have a mistake, you'll not go beyond this, basically. So there are two ways in which kunal we have seen teams use it. One is like sometimes people set like one cluster and then within that they will create like a production workspace, a dev workspace, and whatever test workspace. So you can bifurcate in that way or you can have multiple clusters for this. And then within each cluster you can have workspaces that are located to different groups of developers. So for example, there's someone working on a particular business problem, then they can be assigned workspace one, the other one can be assigned workspace. That way you always are able to do like resource allocation, cost allocation per workspace as well, if need be. No, but how the cluster is associated with the workspace? One cluster has multiple workspaces. One cluster will have multiple workspaces. Here, if you see like workspace argos, the first one is on cluster Dfyctl EW, this one. Then there are some on different clusters. So this way within each you can create multiple workspaces. Okay. And that works space will have multiple application deployed. Yes, you can decide if you want to put one application for work. I'll just create one workshops for myself. I can just put this and I can just create a workspace. Where is the workspace basically? So you'll get a workspace basically. And we also plan to show the costs for every single workspace, every single application that you're deploying, you'll get to see a cost right here, like how much cost you're incurring. Yeah, and usage also, right? How many memory storage that we can also see. Yes. So that one I'll show you right here. Actually because work space will have already predefined memory state and everything. So you will know the cost, how much we are incurring but based on the usage actually cost should become lesser actually. But the overall we are paying higher. Exactly. So you see here, I provisioned two but I'm only using 0.5. Okay? So I'm only using 4000 MB. So this is basically showing me how much I'm using every place. Okay, so we kind of make sure that your Kubernetes cluster comes with the best security, best kind of practices. Everything is given to you from day one, like very high production quality. And then now that you have namespace, deploying is as simple as you want to deploy a new service. You go here, you select which workspace you want to deploy to just once. Again, sorry for interrupting so far. Whatever setup is then going to that's mainly that is handled by the DevOps or the intra team so they get a full visibility of clusters and workspaces this space, this deployment is more for developers. Obviously DevOps and intra team can have access to it but generally this is more free for using by the developers. And whatever you see here, like obviously it is going from the UI but the same thing can be done like from a CLI as well. And also the deployment actually, normally the production deployment is controlled actually that is done by the DevOps only, but normally at the dev environment level it is done. This comes in. So for every namespace you can mark, like who has admin access and who has edited access. Okay, so grants permission to deploy application. So for the production workspace will probably not mark your dev team as you will only mark them as viewer access but for the dev workspace you will mark them as editor access so they can deploy right? So that is there. And then basically for developers it's as simple as they want to deploy a service they don't need to like again and again bother the DevOps person and again so they can be pretty independent. So they'll just choose let's say they will choose which one they want to deploy yes, which workspace they want to deploy to. They will do this and I will just and they will choose which GitHub repository they want to deploy from. So I will just choose this guy. It will automatically figure out the branch, the commit, share the latest commit, there is a docker file, all the presents, it will automatically do what is for the service and then I will just click on submit. The moment I click on submit, this will basically start deploying now and automatically build the image and deploy and everything automatically. And for each service. You will get to see the logs and metrics for that service right here. How much CPU, how much memory it's being used. So that metric is at the workspace level, right? This metrics is an application level per application? Per application, yeah. Okay, so one any questions? Yeah, now you deploy this one failed. The development got failed. So. How do you troubleshoot like now. You showed me the logs, but the logs? Yeah. So this is some dummy application I deployed. I actually don't know what this application is. And this is also a dev environment that I'm showing you. So like one actually deployed things so that way at least you can see the running logs, at least some metrics so that it gives a better sense. Yeah, I'm trying to find the service. We just recreated this entire cluster. But I'll try to show you here. But in the meantime, this is for anything, right? Yeah, this is for anything. I'll come to the ML deployment also. So this is where you get to see the logs and everything and all. Our data will be in your application. All what? What? Consider this one all the application logs, which is very limited in Normand. So those logs we can see from two from the application? Yeah. So this thing actually more this thing we can deploy it on your cloud only it's not that they'll flow to our cloud. This entire application. Right? Like how many days we can see the locks in this environment. So we are using low key for logs. So the logs are all safe to s three bucket. So you can configure how long you want to keep them. And that s three will be present in our enrollment or in your environment. In your case it will be GCP like the corresponding equivalent. Everything is in your environment basically. Okay, so you can see the bid locks and everything for every single bind docs and everything will also show up. So this is where the docker image got built and everything. And you can also add secrets. So for example. For an end service, how that API endpoint can be called. And I think Mohan has more questions here. Yeah, so the API endpoint is generated for every single service. So this is the API endpoint. So this is like one of the service basically that is being deployed? That is deployed. So every single service has an endpoint that you can call the APIs and everything and to add a secret. Yeah. Your deployment, is it also handling the configuration? Actually yes. So we do handle configuration when add to the dev environment sandbox or the prod, they each have the different environment variables and the value themselves. This is where you can add environment variables and you can also directly use the secrets that you have created here. So you can add the secret right here. So the developers will never get to see the value they will just see the secret name, but it will be available in application when it's deployed. Okay. You can also mount files, by the way. So for each service, I can create the environment variable here. Yes. Suppose I will be creating database. And. I will tell the developers that for production you have to use this secret and they have to pass here. Yeah. So this is where you create the secrets and it can give access to developers to see only the keys. So, for example, if you see this, this secret, we have a concept of groups. So is kind of a group of secret. You can have a secret group per application or a secret group per DB. So, for example, this is a secret group that has two secrets. And I've added like only one person is admin, but I can go ahead and add multiple users as viewers. The viewers can only see the secret keys, they cannot see the van. Okay, so you can add all and we will also show you how many this will actually show you this secret is actively used in how many deployments. It's not like a separate secret thing. Like it's just whatever you are using. It's just the layer on top, like. Yeah, I mean, the actual values are not stored on our platform. The actual values are stored in Google Secrets Manager only. We just tie it up that this secret is being used by this deployment. So you can actually go and track that. Okay. If I am changing the secret value, I have to restart this deployment. So this is basically we try to manage then if the developers want to deploy jobs quickly, like a cron job or something like that, or if you want a daily report or something, it's very easy for developers. Like you can just go and select the workbook. They will again select the git repo, whether the job needs to be triggered manually or the job needs to run on a schedule. They can just select when the job wants to run and then they can click on submit. Then the moment you deploy, this basically goes ahead and creates the job. And the job will continue to run for each job. But this job will invoke some XM. Right? Where do we configure that? Actually, suppose I'm creating the job. So in the job I need to do something, right? Yeah. So that code you are importing here. Basically that code will have all the instruction actually. Okay, so you're importing the code that needs to be deployed as a job. Okay. But we'll coordinate running it and you can see all the metrics and everything. Of the job and their failures on the alert, on the failures, everything is. Automated alerts we are building right now. Alerts will come right here. What will happen is that developers can do something like this. So when they deploy a job, there is one more field that will come here, send alerts on failure too and they can mention the email or the slack channel where it will be sent. So this form will come right here. So that is something that we are working on. Okay. I think if you have to give custom command you can do that. So maybe we should focus that. Yeah, so I think this one does not happen. I'm not sure which thing has it. We actually build one cool thing where you can actually create a template for a job. Also I don't have it right now. Let me check in depth test if it is in production, if it is there. Okay. Now, Abuja, I have one question. I have one customized application and before the application is getting deployed in the production cluster, I need to run certain commands so I need to use some init container. Okay? So now how that use case will be solved in proof of it. Yeah so if you are planning to run init containers mohan so what we do is the service is actually powered by a hem shard behind the hood. Okay you can think of this is like you're making the values YAML so this can hook up to any hem chat. So in your hem chat you can provide the initial containers. So this is actually not hard coded. This is actually we customize you can have your own landmark service right here. But you told I mean like do we need to write handshats or by default, 200 will give us the option to deploy without writing lunchats we do. For standard use case. So this is a standard one will come by default but the moment you bring init containers or sidecars it becomes very custom, right? We don't know what requirements will come so that becomes a bit difficult. What we can do is if there are a few commands that you want to run, we can always have like commands run before and we can add one more field here for you. So you can paste all the commands here. So by default it will flow to the ended container. So you don't even need to write that. So that customization we can do when we ship to you basically. Yeah and if you are writing that custom chart, we can actually help you. If you have to write a landmark service customer chart, we can actually help you. So that it's very easy for you to kind of get this set up. Like for example, this is something we added for one company right now. So they said like we want to mount files at a certain location in the service so we just added this file. So they just paste the data like whatever the data they want and this can be mounted at this location. So slash app data CSV if you just submit then this file will automatically be available there. What do you do in it? Containers? Whether we want. Is it more database migrations that you run? No, ideally we run certain predefined commands for application to tell our part what to perform, give some max size and other things. Okay, yeah, I mean some of those things usually can be avoided. We can take a look at the exact use case that you have. One might not need any container in some cases, like a lot of the cases like people were using but a lot of the cases were not needed with this system basically. Any other questions? And we want to show the cost also by the way, the cost will also come. That is something you're working on right now. We want at the summarized view of the cost we are incurring to the infrared actually. Right now we don't have, but we can build it. So basically that is the whole point. Like few of the companies we are working with we want to also learn from you, like what you want and then we want to kind of generalize that and build like this cost thing. If it's an important requirement for you, we can think of it like a feature that you are helping us build out and you let us know and accordingly we will also build out as you start using the platform. Okay, because. You have the cluster, you have the machine and you know that usage and everything. So the one picture where it can say okay, this is the overall cost that you are wearing but here is the overall usage. Actually the hair is optimization that you can do so that will also because see the infrared obviously is costly, right? So the deployment is one of the things that we want to obviously automate but in the market actually this has been somewhat streamlined and until it becomes very large application or you have too many macro services then it requires such type of things but initially it is fine development environment, daily deployment, but broader automation. You are creating a cluster, right? And this cluster we have all infrared actually from the one system only. I can see my info level of cost, do the deployment and everything and will be helpful. The goal is to get that to Nal. When you do the ML modeling actually because you said that you also do that because in ML actually because you are fetching the data from a different source actually that also comes up with the cost actually. Yeah, I want to also know that for developing one model how much cost that I incur, what is initial cost and there is a recording cost that comes with the model. Okay. The training costs and all of those. Basically model volume because you asked for it basically Joe service job and the model is there. So machine learning model deploy correctly. It's as simple as you select the workspace. Then you select this model system and registered I'll show you where they. Registered so you can select any model you want to deploy and then you pretty much just click on submit and deploy it will create this, it will create the service automatically or your service create okay open API generate this you can make a request to the model right here. Those things basically come automatically. It's not that you put this is like 10% of where we want to be it's like a journey that we can build and prioritize based on what you need immediately and things like that. It's a long product building cycle. It should have product. Yeah obviously it takes time and answer what is the need in the market? Because Mohan does everything for us actually. Yeah it will be good like Mohan. Up they accept the platform and try Ganacha. We would love for you to try it out as well and then if it is useful we can then discuss next steps. Environment. But as I told we are doing other things also then stuff like site transaction and other things. And what about ingress and other file files? So do we need to write manually or available in your environment? Some things we do give I'll quickly show you a design that I have it is under progress now let me just find it out. Whenever I create any service, will I be having options to choose public IP or private IP? Yes, those are the private IP. Public IP. And for private also you can choose, I think, the number of authentications we added. One year if you face any challenge in the DevOps or the problem that you faced. So if you can ask if those easily available on you might be knowing now, actually. But during that time there was one thing right. Publishing the public IP duplicates on that? Yeah, that's what that time I created private cluster. Earlier we were not using private cluster, so there was no requirement for us. To reach a private. But then Ecom wanted a public IP. This is how one you can click expose and you can also customize the endpoint that you are given. So by default we generate like this endpoint you'll get by default IP I. Can create my own endpoint? You can create your own endpoint based on the ingress that you have given to the cluster. So when you create this cluster, you can mark what is the ingress map to this cluster. So if you see here, I'll show you so these are the domains I have marked to this cluster okay, so this is ingress domain and I'll also tell you how do we configure this part? So when you create a cluster anuraag truefoundry we will give you a git repo that has the entire code of the cluster. I'll show you quickly. Our deployment multicloud will support automatically, right? Yes malawi mera application GCP Madam and future suppose I want everything to be also going to the multi AWS how quickly that will be okay. We can move it cluster creation, we automatic. That is something that we are working on right now. The cluster creation. Create one by the service. Moving is as simple as what we did was clone. You just clone this guy. You choose any other workspace in any other cluster. Now, one more question I have. So consider I will deploy one application from your enrollment, and I found there is some changes required at that time. Since it's a production, consider it's a production. I don't want that time. So, again, I will do some changes and I will give some version. Do you guys have rolling update and roll back options? Oh, yes, we have rollback right here. Built in. So let's say this has only two versions, but I want to roll back to this guy. Okay. I can find a service with more versions. If I can find let me see. Yeah, so this guy has like so many versions and these were somebody was trying with it. It was failing. Failing because the docker image was failing or something like that. But then they succeeded. So if I want to roll back to, let's say, version six okay, as simple as click on submit and with. Zero down time, I can do this. Yes. You have to put two Replicas at least, though the only criteria for that one is you need to have greater than one Replica. Obviously that will remain. Otherwise. You can do whatever. So, final question for you now, if you guys want to access our application, our GCP public cloud, what kind of access I should give you guys? I'll explain to you the architecture a bit more. I think that will make it a bit easier. So you mentioned that you have three Kubernetes clusters running, right? One, yeah, correct. Let's say this is your cluster one. You see my screen? Was it too small? Yeah, you can see. Okay, so let's say you have these three clusters. So what we will do is we give a small hemcat called agent. We call it TFI agent. It's a hempot that we go and install on this cluster. Okay? And our control plane is hosted at anuraag Truefoundry.com. The moment you install this agent, you will start seeing when you go to Truefoundry.com and log in, you will start seeing these three clusters. It's not just agent chart. There is a little bit more configuration. I'm just simplifying a little bit. But we'll give you a script that you run on this cluster. You can see the content of the script. It just installs a few things. And the moment you do that, these all these clusters will start seeing here. And then you can start deploying directly from here, from Adbertrofundry.com. That is one option. You can also deploy this entire control plane if you want on your own cloud. So you can make this appointed.com to something like whatever, like platform Landmark.com. Okay? For this. I think I wish it will be good to explain like what all access? Because this is the question for Mohan. Like for this deploying on their GCP account. Then what all things they will need to provide to us. If you do this, you don't need any more access. Actually. Otherwise we don't want to share the coordinates access to anyone. I think he's asking for the GCP one we should not in our cloud. He's asking for their cloud. Can you just specify. The question is right. Correct. Now cluster built around I need to give some access to you guys, right? Okay. Then you have to give us a GCP access secretly which has permission to create clusters. If you are creating clusters through us if you need those features. So we need to have access to your Google cloud container. Google GCR manager. In which case you will call it like platform. I think Moan will have all the access. We'll not need any access. We can just work with Mohan SB, get this entire system up. Nothing actually flows out ever. Kunal and one like really nice and time for asking the questions. And I would suggest call me actually real service like it like Vishek and one more team member. They can actually showcase you and guide you to actually deploy it by yourself on that. And then if there are other questions we can answer and in terms of next steps after this then looks good. So we can discuss answers. Or you can take an access and you can check it out the platform. You can try deploying one of your different repositories or things like that. How useful it is for us. Thank you so much for the time. Bye.