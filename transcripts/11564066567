Hi. Nicole. Hi. Ravi. Hey. Hi. Jimmy. Hi Ravi. How are you doing? Good. Nicole. Hey, Jimmy. I see. Other than note taker, how are you guys doing? Very good, very good. Have you used this Notetaker or similar other apps? Ravi yes, with some of our vendors they do take notes as I'm familiar. It doesn't spook me out anymore. Let's just say that. Okay. We have been using this and it's serving us quite well since the last few months. So we are happy about it. I gather your India trip was great. Yeah, India trips are always fun. Where were you in India? I traveled all over. I am from Jharkhand, my wife is from Haryana, so those two places. And then we went to Bangalore with family in Delhi and Calcutta. Pretty much all over the place. Oh nice. You have family in Calgary? I do have some family in Calgary. Okay. I'm from Calgary actually. Oh, you are? Okay, thanks. So you grew up in Calcutta? I grew up in Calgary. I'm born and brought up in Calgary actually. Almost close to Bharabazar. Do you know show up as I. Have know a few names. I'm just a visitor in Calcutta. I don't know the geography very well. Okay. But they are close to Rawazar, like just a kilometer or so away from it. Yeah, I mean your last name sounds like a Marvadi guy. Totally. Yeah. One of the very stereotypical Maravadi guy who moved from Rajasthan to Calcutta. Yeah, I'm from the same plan, but we are from Haryana. Marwari is from Haryana. Okay. Jinma is feeling a little left out, so let's stop that part. Where are you from? So my family is from Bihar, but I've been born and brought up in. Common. I'm the Marvadi from Behalf. Nice. Awesome. Ravi thanks a lot for taking the time to speak today. I think it sets some context in my LinkedIn message. But I guess I'll just give you a little bit more brief about the purpose of this call. So basically we are building True Foundry which is a startup in the machine learning domain, helping companies build out ML platforms. So like a lot of what we are building right now is what we saw at Facebook where ML platform teams built out a lot of internal tools that made our machine learning developers super, super productive. Like being able to deploy many models updates like launch on frequent updates and kind of stay at the cutting edge of it. Right. And we noticed that doing something like that without a platform, getting that level of productivity without an actual internal tooling basically is super, super hard. So that's a problem that we are trying to solve a Truefoundry. And currently we are at a stage where we are working with a few companies like Early Enterprises that we are working with and cobain the platform basically. Right. So in that context it's super helpful for us to talk to a few folks who are building and like, practicing machine learning at scale in some of the largest companies because that's a segment that we are working with. And in that context, I had reached out to you primarily to understand what type of machine learning problems that you all are trying to solve, what's top of mind for you from a modeling and an infrastructure perspective, and getting any business outcomes from machine learning, basically. So just wanted to discuss that a little bit. Sure. I'm happy to give you what we do. I don't know if you work with any fintech. Fintech is a little tricky and I'll explain in more detail what that means in terms of we have a lot of limitations around how much data science you can actually deploy. You can analyze all the way to glory. Nobody stops you from that. But from protection, it gets very tricky and I'll get into that in a little bit. I can answer that question. So do we just jump in or do you have anything else you want to feed me before we jump in? I guess I can just give you a very brief background about myself as well. And maybe I can also set some structure to how some of the questions that I would love to get answers from. Of course you can. Also briefly about the fintech issue that you just described. Right, so my personal background has been in machine learning. I worked at a startup called Reflection where initially I built out a lot of ML models myself, like recommended systems, personalization algorithms. And then the last year and a half, I got a chance to build out an ML platform for the company because we had like five, six teams building out machine learning models. So it made sense for us to build out a horizontal platform. And then at Facebook, I did not do any platform work. I just did conversational AI, led one of the conversational AI teams, but I got to use their platform. And that actually opened up a lot of insights for me. I guess that how much Delta there was based on what we had built out at Reflection versus what I saw at Facebook. And the interesting part, one of the realization for me was while I was at Reflection, I never realized that all these interesting things can be built out basically. So I thought that what we had was amazing. Only after I got exposed to Facebook is when I realized that, oh my God, there could be so much more. That's what we're trying to bring. But that's practically my background. I would love to learn a little bit about you and then jump into understanding a couple of things, which is what are some of the machine learning problems you're trying to solve? How is your team structured from a machine learning data science perspective? And how do you like what's your process of putting the models to production. What are some blockers there? These are the three main that I would love to get an insight on. Chida is part of your team. May you want to also briefly introduce yourself? Yeah. So I've been part of the two foundation since the last four to five months. Before that I was working with McKenzie as a management consultant and I graduated from It in 2001. During college I worked mostly in natural language processing and machine learning, where I also got a chance to work on CD in the internationalization team. That's a bit about me. Based in India. I'm just trying to get a more context of where based. I am based in San Francisco. Shenma is based in Bangladesh. So you guys have offices in both places? We have a couple of folks operate out of Europe. Time zone, basically. Got you. Okay, so good background on me guys. I've been with LC for four years. I've been exaggerated probably when you were in diapers from It. I've been in the fintech industry mostly. Fintech industry itself is like ten years old. Before that I was in banking at HSBC and consulting for price for the house. And since for the last eight, nine years been very much in the fintech space. And I span mostly in between analytics, product and pricing, risk management. Those are domains that kind of move around in depending on the need of the company. Currently I delete the data science and I had fraud on my plate and then I transitioned that to somebody else. I have taken on more of a business role now. So that's my current role at Lending Club, jumping into your questions and on data science front. So when it's not fintech, when it's a pure big tech, you don't really have a lot of regulations in that space. So you can use any kind of personal information from a consumer perspective and build any kind of models. You could show a white guy versus a brown guy versus the black guy, different recommendations, and there is no issue around, hey, you're having this current treatment because you are responding to their needs and what they are looking for. When it comes to lending, any fintech product, insurance, payment, probably not so much, but mostly insurance and lending, you need to have a very clear explainability of your ML. And because of that, our hands are very tied to we don't go anything beyond the classical MLS like neural net or a gradient boost or a random forest, which is all classical ML. Right? ML has expanded to a lot more than that. But at fintech companies it's been a hard sell in terms of, hey, we have this AI we can use to improve the pricing or approval rate or blah, blah, blah. It's just not there. Like it's just not allowed by the industry because when you have a black box ML, it's difficult to explain to the regulators why I gave a certain Apr or why did I force somebody or decline somebody? So that makes deployment of ML very tricky in the fintech space, the sustainability part of it, right? So that's 1 second is if you think about the state of the art is you start with your historical data. You do a bunch of testing in Python across all these models you host, raise these three, four models and then you deploy a version of it in your Java code from production environment. It could be no, Java one of these languages. You just deploy the code and then you have to wait for three to six months before you can see performance in terms of responses are perfect one month. But credit quality of your portfolio, it takes six months because that's how long people take to charge off on the loan. So it's very difficult to charge off. If I give you a loan today, it's only after six months I realize you're going to pay me back or not. Right? Oh, I see. Okay. So that also increases the lead time of iteration, so to speak. But let's say you're at Amazon. You're recommending a blue packaging versus the red packaging. You'll see a response in two weeks. So you can really trade and do a lot more AB testing there compared to in a lending environment. So because of that, a lot of the lending companies, they keep talking about ML, but they don't really a lot of ML because the returns are kind of not there. Because around what you can do with it and how long it takes to iterate. Understood. Just a question on that, Ravi. Are there use cases besides the one that directly go into decision making, let's say something that you would need for additional data point in order to target your sales effort or something like that, or to push a certain product? Are there such models where the regulatory hold is not that great? Does this kind of use case exist? So, that's a good question. There is. When it comes to marketing, you have less amount of regulatory scrutiny because you're just telling people to come and apply for your product, right? And bringing in conversation AI could be one. If you have chatbots on a portal or on a third party website there we could apply the whole GPT AI, the latest ones, to say, hey, this is the right language to use, this is the right way of reaching a customer so you can increase their response rate. In that case, you're starting to do a little bit of like my team started to build again very basic models, right? Which is doing great in boost nothing because we don't even have an ML platform today. So we just do a bunch of work in Python and if it works, we just deploy it into our production environment. Like we've been talking to Amazon H Two to try to see if we could test their platform, but there's not enough use cases even on the marketing side today to actually deploy it. Okay. And is this considered like a priority for the organization in terms of its business impact or something? Or is this still on the sideline? It's mostly on the sideline because it's cute, but it doesn't really drive the business. They use the word cute. That is still physically that's how they look at it. The bread and butter is the risk models, which is mostly decision trees. Actually, it's not even a model. Right. So you have models that generate these risk scores and these models are mostly GBT models. That's about it. So it doesn't really go any deeper than that. So ML and data science in lending space and insurance space probably not. Like a buddy of mine from my badge who launched it's called Rocket ML. He might have based out of yeah. So the founder is my classmate and he and I have been walking, talking through he was able to get into one of the Amex vendors with something on the marketing side. But when it comes to actual lending decisions, it's not worth the effort because the Cam is not that big and the problem doing its statement doesn't lend itself to ML. Very well understood. I see. Understood. This is very helpful context. Ravi, one question that maybe I would love to understand is you mentioned about people building out these Python based models and then quickly deploying it to production, et cetera. Right. So, in this context, how is the team structured at LC? Right? Like, the people who are building out the models, are they the same people who actually deploy these models and maintain these models of production? Just like separate teams? No separate teams. Right. Because we have data engineers and then you have a data scientist. Right. We basically have people with stats, PhD majors who come in and basically build these models and build the next generation six months, twelve months out once they have more fresh data. So recency of data is just so the data quality is the biggest thing in lending as opposed to the model itself. So being able to generate the next version using recency of the data and recalibrating those basic models, that's the bread and butter of statisticians in many companies. So what we do is the build and the pass on the requirements to the data engineers. They do the deployment, the testing, the launch, and then the maintenance of there's only one version in production, so they manage that while the other group of data scientists, they just look for more data and keep refreshing the buyer in progress. I see. This is also very different. Right. If you talk to a data scientist who don't know how to deploy a model, if you talk to a data engineer, they won't know how to build a model. Makes sense. Yeah. And these models that you're talking about. These are the typical ML models. Are these for risk or this is for which use case? So it's mostly for risk. On the marketing side, we have just started to play with some of the basic modeling. So far it's been very brute force. It's also a function of lending them, not being the biggest lender. Like in terms of there's not a whole lot of optimization to be done. The way the channels work, they're very straightforward. But when you go to a big company like Amex or Capital One, they have economies of scale where even a 0.1% improvement helps them actually get that incremental topic up on the left. So for us, it doesn't make sense to invest that much on the marketing side and on the risk side, there's not a whole lot to do. Which is why ML is something which it always almost looked at is like a good to have. One area which we are exploring is alternative data, right? So how do we bring in non bureau data into the room and how do we pull signals out of it? So I'll give you an example. So we were just talking to a company who uses they haven't given us the signal, but they're using bank cash flow data to come up with underwriting models. Now that is one space where you could have some wins if you're able to pull out signals using ML from the bank transaction data. Okay? And that's one area which is very nascent. It's up for disruption. A lot of companies have tried it, but nobody has gotten a success. Like they were pitching a solution to us, but the runtime was like 45 seconds. So they said give us your bank data and we will be able to give you back a risk signal, but we will take 45 seconds to generate the signal. That's too long. Today's game was people are talking in milliseconds, right? Like one or 2 seconds in the worst case scenario. Interesting. Makes sense if you guys can figure out that could be one area I would love to stay in touch with. If you guys have a solution where you can say look, I'm going to take your cash flow data and I can generate a signal for you in 300 milliseconds or 400 milliseconds because patient time is not that much. I don't know where is it? Just maybe I don't appreciate the problem that much. Or maybe those guys are just behind the curve. I don't know which one it is. Okay, but is this like a processing problem or is this something else related to the pipeline? They haven't revealed that. I understand. Yes, they haven't revealed it and I don't know, but that if you're able to crack that code, that could be really big for learning investment. Makes sense. Makes sense. One other thing, Ravi is like you mentioned about this handoff of these models that are getting built by the data scientists to the data engineering team to get deployed. Right. And you mentioned that skills between these two teams are not very transferable. So I assume that given that the skills are not transferable, this handoff might be a little bit tricky, that people need to sit together or. Machine, because it's been happening for ten plus years now, all lending companies. So what they do is they come up with the productivity tools. So the way it works is you build your code in SAS, for example, right, in Python, and they have these converters, which converts that into their in Java code. And they look at the code, sanitize it and deploy it, and they come back with the results. And we do the validation, the test comparison of what I gave you and what he came back with. So that part is a very well oiled machine in the banking industry because it's been like that way for the last ten years, right? Like you have these two different groups of the other thing is your production environment is not suited for testing and sorry, for analytics. So you need to have a modeling tool like a Python or a MATLAB, where you can actually do all your coding and do the what if analysis and run your models. The production environment doesn't need that functionality. It's a waste of computational power. That makes sense. Yeah. And then you can't do your testing in Java and you can't do a production in Python. Makes sense. This is more statistics and analytics. This is more pure production and engineering. I see. And what is the team split between these two? Like, how many data science people do you have and how many data engineers? I would say for every data scientist, they probably have three engineers. If you have one dev, one, two dev and one QA. Interesting. I see. Got it. And one of the things is, let's say you had suddenly like two X the amount of resources, right? Like, let's say you need data science, you need fraud, and suddenly you have two X the number of data scientists. What are some of the interesting things that you would want to do with these data scientists, extra data scientists that you have on your team. So I think if you have additional horsepower, you will put them towards the marketing, marketing side of things to find ways for things like multitouch attribution. So you've done a bunch of Google Ads, you've done a bunch of Facebook ads, and you've done Direct Mail, you have done Gmail Now, which made the customer come to you. How do you find that out on the marketing side? If we can figure out multi touch attribution solutioning, that could be one area I'll put the customer base in. But again, not at Lending Club. It will be at other bigger companies. LCP doesn't add a lot of value, which is why the data science teams in these companies are not that big. In the last 15 years, it has not grown has been that much. It has always been like a team of five or six modelers for the whole company. Got it. Interesting. I see. And how frequently do they actually end up like updating the model itself? Is it like in the order of many months? I want to say like at least an annual view. In some cases it's every six months. On the marketing side, recently makes more impact. So it's more like six months. On the credit side, it's more like an annual view. Interesting. I see. Okay, got it. Okay. So basically the frequency of building new models and the frequency of pushing updates to the existing models is fairly low, basically. Yeah. That is the nature of the industry you belong to. Compared to that at Amazon, you're probably turning to models every month. Right? That makes sense. Yeah. And how about the customer service side of things? Like, do you have a lot of these chat bot and question answering that type of modeling? There is a lot of that, but I guess, again, it will depend on the size of the company. So if you go to bank of America, if you go to a cap for and if you go to US bank, those places, they'll have a lot more return for investing in like chatbots and conversational AI and streamlining your communication process. Compared to that at smaller companies like LC, it's not worth the dollar to spend there because they're not that big. I see. Understood. Also mentioned that there is a lot of testing, et cetera, that you guys have to do. How do you facilitate that? And also like the model explainability part, is that something you have tools for or is it like handled by some vendor? How does that work? Testing is straightforward. Right. So basically I give you a simple regression model. You deploy it and there's some results which you expect back from it. The results are given back to us from the engineers, our data scientists compare the results and that's how you test, basically. Go and deploy it in production. It's a standard, it's a mock environment. Okay, and then what was the second question? Yeah. For explainability of these models, do you use any tools or how is that. Going to be explainability? So because you're not using like black box ML models right. It's mostly like gradient boost m, so it's easy to trace back towards score deploy. What's the point? In production chipma is only data from the credit bureau. So using a credit bureau attributes, you come up with some scores. Using the scores, you build a decision tree and you end up in a decision tree node and those nodes have approved the client outcomes. Basically, it's very easy to explain it to go after the attribute. Basically. In general, like tree based models are like by design, they are explainable, right? Yeah, exactly. Got it. So do you guys end up using any external tools for your entire modeling pipeline from whatever your experiment tracking to deployment monitoring? Experiment tracking? Yes, we'll use optimized for our A B testing, for example. Right, for your session tracking. I think those are two key tools we use from a testing optimize the heap. That's about it. Everything is on the cloud. It's redshift. AWS, redshift is where all the right stores we have these production tools, which you need, but specifically for ML, it's not like we go and we have a big H two account or whose databricks account. No, that's not what we do. Makes sense. And how about model monitoring? Has that been a concern? Like, for example, tracking things like data drift, concept drift performance of the model in production, like in real time? Yeah, that's a good question. Again, depending on scale required, you might want to say, I want to use the tool outside in. But right now that's all done using the SQL queries, you know, the scores and the model. Right. So on a monthly basis, the same data scientist, what we do is look at the production output and see what is the drift every month. And if the drift is outside of your range, that's when you kind of say, hey, I need to do something about this. I see. Okay, got it. So actually being able to monitor this in real time when I say real time, not like real real time, I mean like near real time is not a concern that for whatever reason, let's say data format changed, right, and the model started breaking. You want to be alerted for that, right. Or some data drift happened because of, let's say, an event like COVID. Right. Being able to detect that and respond to those things, has that been not a concern? Like a big concern in general? Not really. Because the amount of testing which grows in is pretty huge. So once you put something in production and when you make changes to it, there's a lot of logic, which happens after that. And there are enough splunk logs and alert systems in place which trigger any kind of deviation, has enough root cause done by engineers. When event like this happens, more often than not, it's mostly a big bug in the port. So you're saying usually like these. So. I was just saying that in general, the degrading of the model has, because of at least manual errors, has not been a concern at all. Basically, yeah, and it's obviously not. There are nonzero errors. But the point is, there's enough feedback loops to catch them early enough that your loss of business is not big enough. That's a concern. I see. Understood. And the drift and all. Again, given that you check for drift, like every month, generally, you don't see data getting drifted in this business, like more frequently than that. Yeah. And then you have all the older instances of your code base. Right. So let's say you see something which is off, you suddenly revert back to the older version, deploy it and then go to the root cause in the meanwhile, because that sort of butter of the company. So you can't really afford to have errors, unchecked errors go on for too long. It really impacts the bottom line of the company. Got it. I see. Okay, understood. This is very helpful background. Jimmy, did you have other questions here? No, I think. Got it. Understood. We are also almost on time, Ravi, so we wanted to keep a few minutes to answer any questions that you might have for us as well. I'm glad that you guys are I'm just curious what made you guys jump off from the corporate and pick on this challenge? Sure. I think the story for me is basically having experienced the problem personally. So I guess the story for me is like a two part story. Like I had started up another company before starting True Foundry and that was because of the problem that I had experienced in the hiring space. I decided to solve for that problem. So I spent about a year building that and that company got acquired by one of the largest HR tech players in India. And while building that startup, I again came across this machine learning model building and deployment problem, which resonated quite a bit with the story that I told you about building a platform at reflection, not realizing. So those three things basically came together, the trigger point for us to jump and start building Truefoundry. Basically. That's the story for me. Actually, I'm also very curious to know your answer about how you decided to leave your cushy job at McKenzie and join like an early stage seed startup. Actually. Yeah, I always wanted to build something of my own. I had always been interested in machine learning. This was something. And having been at McKenzie, I got some view of the business side of things. I think this was the best of my interest, combining the best of both world machine learning as well as business aspect of things that I get to work on. That is all. That's my motivation. Awesome. Good luck. I know this has not been the most answers which you are looking for, but at least tell you what not to do, what not to focus on. Absolutely. This is very helpful, rabbi, I think. Always good to hear. Basically put guardrails around where you're moving it's. Great. Cool. There's other things which come up. I'll keep you guys in the loop. If we have needs from ML platform, things change right in the Fintech place. When you get more tech, that's when ML becomes interesting and useful. So we stay in touch. If things evolve, we can keep talking. Yeah, for sure. Actually, one thing that we can actually use your help in is at this. Point, I think. I'm sure that your peers, like from your undergrad days and stuff, are leading similar teams in a lot of other companies and startups and stuff right. Who are probably not in the fintech space, which is super guarded. So if you can maybe introduce us with one or two of your peers. Not asking for a lot of introductions here, but given that, you know, the problem that we are trying to solve and the background with which we are trying to solve this problem, maybe if you can make one or two introductions there, that would be very helpful to us. Yeah. So what you could do is if we can identify some of my LinkedIn network is public. Right. So if you can find people in my network who think might be useful to you, that would be easier way for me to because otherwise I'll just do. Guesswork that makes sense. Yeah. We can totally do that. So maybe we can take that action item, scan through the network of Ruby and then maybe send out a few folks, maybe four or five folks, from which Ruby can choose one or two introduction that he feels comfortable making. Yeah. So at least make a list of five or seven, because a lot of the leads are cold leads. Right? For sure. Yes. We have to talk to things undergrad, for example. I know. Yeah. From that, I'll be able to find a few which are little bit more warm, and I can make the connect 100%. Awesome, guys. Cool. Good luck. Thank you. Thanks. You have a good one. Bye. Talking to you. Thank you. Bye.