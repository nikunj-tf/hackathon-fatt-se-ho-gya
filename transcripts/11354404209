Yeah, hearing you. Hi, can you hear me? Yes, able to hear you. Is Mata also joining? No, Mata got a conflict today so he won't be able to join. Okay, cool, awesome. Thanks a lot for taking out time for the call, really appreciate the same. So I think from our side we wake and myself will be there, so we'll dive right in. So just for context, can I get. A bit of introduction before we dive in? Yeah, of course. I'll go first. So I'm one of the co founders at True Foundry. Prior to this, like I graduated out of IIT Corrupt 2013 batch. After that I worked with a hedge fund called World Fund where we used to build trading strategies out of global markets and then spend my time between India first four years and then three years between us and Singapore. That time I was a portfolio manager managing around 600 million in assets for them. At the same time, as a member of the CEO of Looking After Growth Initiatives I used to angel invest as well as into a number of startups. So I was able to make like 22 investments and then post that like in 2020 quit my job. Avishek and Nikon who were my bachelor from It correct. They were on the same boat, they were working with Facebook we Shake on the software engineering side, Nikon the machine learning side. We all started our first company called Entire where we were helping companies hire technically with a talent. We sold it to Info Edge and then we started to found a year back with the goal of helping companies speed up their machine learning pipelines. Sure, yeah. Go ahead, sure. So I work with Anada very closely with Anada, joined Roofonry I think five months back and before that I am into this Indian startup ecosystem for almost four years. Previously working with a startup and connected worker domain for Frontline workforce in manufacturing and then before that I was in consulting and graduated sometime in 2012 from Issamar. Work very closely with Anaracare. Nice. Actually Nikunj is not able to join. That is basically the note tickets for us to take notes. Here. And I basically had the AI from Pegasystem. So we are basically deployed at literally all 1400 companies. The AI for marketing, for customer service, for NLP, for process automation, etc. And before that I had a startup and the startup got acquired by Pega and that's how I joined it. Okay, what was it called? It was called Mesh Lapse. It was the NLP startup. We got acquired in 2015 or times. Okay, very interesting. Early days of AI. Been in this case for a long time now. For a long time. Basically the context is the following so I think we got connected by Ankur and he's one of the seniors from KGP. We are like very quick before diving into more about Truefoundry, we are working with companies to help them enable machine learning deployments faster. So basically when we were at Facebook, we saw internal platform that Facebook had which made it very easy for building out the models and once the model is built out, the testing and the deployment and the monitoring of models and when we came outside we saw that this ecosystem was quite broken. Companies were facing in internal tools, open source tools or building internal platforms themselves to kind of get this to happen. And we thought that it could be made much better and much seamless with a lot of flexibility for developers and the aid. And that's where the truefoundry comes in. We are a platform built on top of Kubernetes multi cloud from day zero and we enable our developers to kind of deploy models fast like whether it's a training code or whether it's like a production ready code and it comes automated with like monitoring so that you have debuggerlity right from day zero. And the main part is while this system is being built with making it easier for developers at the same time it has the entire right security framework etc. So that the entire team does not have to bother the right liability systems. So that is basically at a very very high level what we are building. And the goal was to kind of dive a little bit more into Pegasystems as to what the current ML pipeline looks like. How do you kind of deploy models, how do you test it out? If there's an internal tool we use, like we got some context that probably there's usage of Sage Maker like how do you manage multicloud? Because if you have Fortune 100 clients, I'm sure they'll be on different clouds. So when you ship it to them, like how is that? So I wanted to dive into that a little bit for today and based on that happy to also do another follow up call where we can share more about our platform. So you also demo and potentially see if there is a synergy in which you want to try it out or it helps in any of the use cases that you are trying to solve. Sure, yeah. So if you wanted me to talk about how Pegas AI infrastructure right? Yeah. So I think we can start maybe with the use cases. I know that Pegas solves for a lot of customer relationship like CRM and business process and AI on top of that. So maybe we can start there and then we can dive into the developer side of things. Yes. So the biggest use case for Pegai is marketing. So all these types that are there, whether using GPMC City, all the top banks, all the top insurance companies, et cetera in the world, they in Pega under the hood. We are the gartner leader in real time interaction management which essentially I use would see those annoying ads like you talk something and that starts popping up with Facebook, that happens with third party data, we do it with third party data. So essentially we connect with the CDPs customer data platforms, we have our own version of the CDPs, but we can connect with external CDPs. We get in the data, we get in the customer analytical reports, et cetera. And then sort of we have this model factory which is called the Adaptive Models. The entire intention being that zero data scientists are required. So I just had to look at proof and read that's very similar to like Domino Data Labs. That's a project that we sort of crack. But what we do essentially at this point of time is sort of data comes in, the model gets built literally every five minutes. The model picks up the best predictors. It's basically an online learning model. It picks up the best predictors, it deploys itself automatically, no help required from any data center. That's why is the market leader in this segment. Having said that, there are certain models like customer models, Churn models, fraud models, et cetera, which are basically not these online marketing models. From there we are looking at a couple of options. The first is that since we are required by Fortune 100 companies and Fortune 500 companies, they have their own data science team, right? And their data science team are using either Vertex AI or they're using, let's say Sage Maker, et cetera. And they have this entire pipeline ecosystem. You can experiment, you can build models, et cetera. Also has these what is this? We sort of integrate with that. Also we are partnered with H Two because H Two ML libraries which are lot of our customers and we are always looking to add partnerships with providers who are deployed in Fortune 100 or Fortune 500 companies. So we can end up process being that since the execution is happening at Pega, we care not so much about the experimentation side of things because for the marketing models are the main models that we care about. Many other models that you want to bring in the scores or you want to bring execute the model, et cetera. We want to sort of get those models either into there's a format called P MML. H two has their mojo format. For example, you can bring in Python models as a PKL file and we'll deploy it using our MLS pipeline there into a stack and we should be able to execute it. So that's what we do currently, right? And the tools that we have mainly encountered out there are basically Sage Maker, Vertex AI, H Two for a lot of places. All of these are basically building out this entire experimentation pipeline model, building pipeline, monitoring pipeline, notifications pipeline, so on and so forth. And they are also investigating these days integrations with data robots, domino Data Labs, et cetera. And all this. But looking at your website, it's a lot similar to Domino Data labs that we have. Yeah. So I'll give you a little bit of context there. So the website is not like the best representation but I'm just not focusing. On experimentation and building out and all those things. Right. Data science pipeline, the ML Ops for the data science process to the deployment and the monitoring the renter cycle that we talk about. Yes, primarily there. But the more focus is actually outside of experimentation. So basically once you have built out the models so while building out you might need distributed training or you might need resources for training. So we kind of enable that and then once the models are built out the deployment in a scalable way. So you start deploying say with 1% of the traffic and that scales 200% of the traffic or if you want to do batch real time, all of those. But the experimentation part we do less of actually. So that's more like a support system, but primarily more on the deployment side, making it easier there. We can dive into that a little bit. So I just wanted to understand so I'm guessing you would be also building a lot of these models internally and testing it out before it goes into the client side. So there will be an analog pipeline. Yeah, we are starting to do that. So this is one use case. The other use case that we have is in customer service where again we are the sort of partner leaders there. If you see let's say customer service salesforce is one, but they guys do and there as part of the AI also we have the highest we even beat out salesforce on the AI. Okay, I see there we basically have NLP and we have Cushioning. So there we have models like the search voice. So if you're talking to the call center, the data gets converted into text and anything model kicks over recommendation and then from there yeah, so these decisioning models come in, the predictive models and then they start giving offers back to the customer. So that entire loop is there from voice, from Chat, from emails, et cetera. And since Pega is a business process automation company, we are also into process AI. Process AI is basically optimizing processes. It's a very unique template. All these are basically done mostly with internal models, models that our data science team builds and then that's basically a recipe that we deploy with customers and then go okay, but we also want to activate customers this thing. So from there, that perspective we are also building out, we are also connecting to different sources. Okay, so most of these will be deployed on customers. Cloud is it like no, it's Tiger cloud. Yeah, customer cloud instance. But we basically we call in Pega Cloud which is essentially a customer instance. You have to remember these are large companies. Yeah, banks are data sensitive, there's a lot of governance rules and all those ensure they'll probably give you an instance. And then you will deploy the system on. XYZ compliant. Okay. So then they are fine with it. Okay. Understood. Okay. Got it. One more question. So when it's your cloud, like, do customers tell you that, okay, we only want GCP or we want AWS or. Is it fine, we are targeted both on GCP and AWS. Okay. What about no azure? We don't. Okay. By the way, GCP you as a startup now, what is happening in a lot of companies is basically Google is having these budgets given to GCP, targets given to GCP. So we are basically like for example, a large telco provider in UK just spending $50 million just from shifting from AWS to GCP. So those kinds of things are essentially happening. So yeah, you can target GCP and say that GCP is giving a lot of room to companies to actually switch. Right. A lot of budget is basically getting pre allocated because Google is helping them. Interesting. Now, love to understand like how big is the data science team or is it like the AI team in paper? So it's basically engineering and data scientists together. Yeah, together it could be around 50 people. 50 people primarily. Okay. And the decisioning got it. When you are deploying on this customer, these models are hosted on your interest. The ML Ops is something that you look at primarily the entire taking the. Model, we don't care about the experimentation of the model. Yes. The operationalization we look at primarily because we need a latency of seven milliseconds on this. Fair enough, that makes sense. Because for example, City runs at 6000 decisions per second. That's just not the models, that's the entire so again, there is a leader in decisioning as well. So the entire decision piece comes in where basically all the customer logic of who should see what offer, et cetera, and all those things comes in. And basically the model is called at the last stage it recommends an offer, then it closed back up. So that entire thing should happen within 200 milliseconds. And even though we connect with Sage Maker, right? The latency there itself is around 200 milliseconds. Right. So a lot of usage is not happening for real time marketing use cases. But the point is, because we are deployed in Sage Maker, we are trying to see if we are deployed in the US. I mean, we're trying to see if we can sort of provision Sage Maker into a VPC and then have that latency reduced down to sub ten milliseconds. But that's a limit for most of her, basically. One quick question before that, like the data scientist, do they do the deployment themselves or do they pass it on to the engineering team to do the deployments? Currently, yeah. So, all good questions. So essentially, as I said, we are 90% of our algorithm is basically this online learning marketing model, right? So that model essentially is being built in house, right? And basically they're getting deployed into the ecosystem. That's 10% of the model. So just for pure predictive model, we have a very outdated stack, I would say. So we are looking to modernize that stack. But if we modernize that stack, that essentially would be something similar to something similar to H two, I would say for example, because and that might be a directional guidance for you also because we really don't see data scientists deployed. Yeah, so we want to move data scientists honestly out of the picture. Yeah, that is true. We want to basically sort of enable business users to say that this is my data, this is what I want to predict. And the UI is actually that simple. And the data set is coming in from 500 different sources, all getting collected in Pega, the master of record. And they just point and say for this part of my journey, they use the journey, this is what I want to predict and this is the data. Right? And what we do internally is we basically go down the data, figure out the best predictors and then deploy the models, basically build it up. Now that works sometimes well with an online learning mode, but we are also trying to build out an offline learning mode where we are basically partnering with each two at this point of time because we don't have enough time to build out that pipeline. But we will go towards the AutoML kind of a solution. Fair enough. Yeah, it seems very much like an AutoML kind of route that should be there. Yes, because I agree data scientists are important. But if you want to have I'm just talking from the perspective of your startup. If you want to have a lot of growth that is coming in, it would be better to target business users because data scientists honestly speaking, in my experience of last 1015 years of a stumbling block, they essentially are the ones who take a lot of time. They essentially are the ones who basically build models which don't get operationalized, right? So probably a bit better moving towards a business user saying this is my data, this is what I want to this is my business problem. Forget the data, this is my business problem, this is what I want to do. This is the data sources, this is the output that I want you to predict. You sort of run under so many other site AutoML. There's salesforce transform, we have so many online open source libraries that are available out there that can help you build an AutoML model. And then the game is basically to figure out once you operationalize those models, you monitor it, right? You just monitor it for drip data response, all the different monitoring that you do, lift, blah, blah. And then you sort of start getting notifications and then you sort of trigger a new AutoML bill, let's say if it doesn't meet the yeah, if there's. An alert and you kind of just go to the retraining loop or something in case of AutoML, AutoML will automatically do for you, is it? One major question that I have is you are using Google for AWS and GCP and potentially maybe later on you might have to use as your or something depending on the clients you are serving. So why is the stack you have built off the deployment around Sage Maker? And the reason why I'm asking this is because Sage Maker is tied to the AWS cloud, right? Like you cannot use it over multicloud. And I was seeing that internally Pega uses Kubernetes. Right. So why not build this ML deployment layer over Kubernetes? Was there a decision making there as to why? Or was it more like because Sage Maker was very easy to use and therefore wanted to move to switch Maker and then later on the plan is to build this deployment layer over Kubernetes. So that's the thing I was trying to explain. So our models are typically internally built. But as I said, Fortune 500 customers that we deal with have their models, right? Yeah, their models are where we also need to connect. Right. So probably 50% of the guys are basically in Sage Maker these days and they expose endpoints to us, which we try to call or they give this post back to us, et cetera. 20, 30% are basically in age two and we are seeing GCP pick up rapidly, very drastically at this point of time. Okay. It is to enable the data scientists on the customer side of things that we actually connect to these tools. Right, but that is purely execution. Right? That is execution. They give us execute this, get this inference in and then we will sort of inject it process, flow or decision and then make it work. Right, but what about the internal part? Like the internal models that you are deploying on, where are you deploying those? So currently we are building out the entire gradually not in our priority roadmap, but we are basically building out the entire end to end ecosystem for model building, model deployment, experimentation, et cetera. I see. But not in our priority because as I said, 90% of our models is a better and better online learning models, which is our marketing models. We have also very good set of models for NLP. We are shifting that to the deep learning infrastructure. But the main thing that I'm trying to communicate is Pega does not give data scientists too. No, that is well understood. Right. So we have our SAP, you use our if you have a better model in your site, you're welcome to challenge, you're welcome. If your model is better, we don't care. Right? Yeah, but yeah, what we try to do is we try to eliminate this out of the picture. No, that is completely well understood. Fair enough. That is well understood. While I understand that, my question is see, I'll tell you, I think I'll give you a picture and then maybe the reasoning or the question will become clear. So we do not hear whether the model is being built by a data scientist or is being built through say an automated process. Our system, what it does is it just takes a model and it deploys it reliably a built model, deploys it reliability with the right scalability, with the right latency requirements and ensuring that the cost is minimal. So now suppose your system, whatever like the team or the AutoML process build like 50 models which need to be deployed. Then either you will be deploying it on Sage Maker or you will deploy it on a Vertex or ML or you will build your own system and over Kubernetes and deploy it. The fourth option is what we are taking currently. The fourth option, yes, exactly. So that part that you are building, what we are saying is instead of building that, what we provide is a platform that's already built and that is not a black box. So Domino Data Labs. The difference what comes in. Domino Data Labs is a black box. You cannot build on top of it. Our system is built on top of it. It connects to your Kubernetes cluster, it basically sits on your cloud. You get access to the entire code base and then there is a UI layer on top of which your models can also be deployed. You get a full visibility and then on top of that further, if you want to do anything, you can do it. So it's basically the internal development process that you will do to deploy your models. Instead of building that platform, we kind of make it easier for you to do it via our platform. So that is the point I was getting at key I'm sure like given you are deploying it, you'd be building something internally. So that is where our platform can come in. So I would love to kind of maybe schedule one call where I can walk you through details of the platform because that will give you a much better picture. And if by chance that is something that there is a plan or there is a roadmap, it might be worthwhile to even just try it out for one use case where you use this to kind of deploy if you like it, rather than trying to build it out. But that's just kind of a potential route. But yeah, what I would suggest is we would basically be anyways doing this in house, right? Because the team that is there. So if you want to schedule a demo, you can sort of look at it with respect to maybe taking feedback from us that can be done. But from our perspective since we have, let's say so many compliance issues because we have to be compliant with, I think 32 these things across us right there's model governance rules and all these things. We typically don't go for anybody who is not basically falling within Pega cloud, meeting these things for Pega cloud etcetera. And then second of all, even if you want to do this, we'll probably acquire somebody if you want to do this, this thing will probably acquire a startup president partner because we don't want our clients paying additional license fees to somebody else, right? Basically it would probably go in that direction. So I think that if you want to schedule a demo, I can sort of give you a feedback of if, you know, like what we have seen across our Fortune 500 clients, what they are doing, how your system might work, might not work, maybe some tips on how to improve those things. So that definitely I can help you out. That will be helpful. I think we can do that actually. We can show you the platform. We can take feedback and even learn as to what can be done to enable it or like, even try to understand a little bit as to when you are building it, what are the challenges you are facing, which might be the challenges others might be facing. That would be great to do. What's your growth story that you have told the investors as such? So for investors primarily it's the following a lot of companies to build internal platforms to kind of deploy ML models. Ideally it takes like a good time, like a year of a few developers to kind of build scalable platform and then every company is not able to do the best job because ultimately they'll not be able to cater to all the use cases. So what we are building is a platform on top of Kubernetes that allows very quick deployments along with the flexibility for developers so that they can build on top of it and with the right security and other things for the Internet. Because we already work with a few clients. Like, I'll give you a few names that we work with, like a global silicon chip design manufacturer, a multi counter, $100 billion plus company. And then another company in the healthcare, which is again, $50 billion plus company that we are working with. So we kind of make it so we are not like a black box. I think what we do is we provide you something that works and then on top of it you can do your own customization. So that way you reduce the time to value for your ML models and you are able to get a system that is scalable. Because we have seen this at Facebook, we have already built it at Facebook and Facebook has a very scalable system like the things you talked about like latency and all. We already take care of that in the system so that you don't have to worry about it and it's very easy for anyone to use. Getting started on it is like 30 minutes or so. We try to make it sure that it easily deploys on your infrastructure so there is no data leaving out of the system. The entire compliance is met, VIP testing and other things of the system is done and then it comes with a very good UI that is something that gives visibility to the intra team as well as the MLT in terms of what cost is going in, what resources are being used. So at no point they are overshooting costs. So building such a system internally is generally not easy. We hope that a lot of companies, instead of building internally, will potentially use. But I mean, obviously there'll be companies that will want to build this internal and that's completely fine. Yeah, so I think if a sales process is you shouldn't target companies like Pega or Salesforce or these kind of because they have a lot of internal tools that probably like a chip manufacturer or a health manufacturer, et cetera, which are basically not hardcore It companies. Right. I would be able to target them and sort of see that there are model deployment process. But I'll sort of bring in my job to save you costs. And if you ever grew up big enough, then essentially companies like Salesforce and Peggy would be very interested to partner with you and then sort of say that okay, you have 100 clients on our side. Right? On your side, basically. Right. And that's what we do with Ishto, by the way, Isto is a joint partnership that we have, but Ish two is a $1 billion company at this point, but still so this is it. You have so many clients on our side. I have so many clients on my side. Let's sort of do a synergy and say, but for the initial growth stages, I think you should sort of go and sort of partner with companies who basically are not at the level of, let's say, salesforce or a service now, or these kind of companies which are basically selling software to other companies. Right. So probably that would be the best way. And the other thing is, seriously think about getting data scientists out of it, which your growth story will be much, much better because literally just hiring a data scientist. Just getting a data scientist, I hear, let's say, the CEOs of these big companies they voted on, and I'll cry about it, like literally hiring a data scientist, managing the data scientist in their expectations, ETCA. It's basically a long drawn out process. Right. So a lot of people are saying that if I have a business requirement, maybe you will not give me 99% of accuracy. That's my model. Maybe your accuracy will be ten to 15% later. But yeah, that's offset by the cost by not maintaining a data science team on my side. So basically you're. Sort of pulling the experimentation pipeline and all those things. If you have another pipeline parallel coming in this is my AutoML pipeline and you are enabling business users to basically run there and then your operational story basically makes a lot of sense in that. Let me connect to your database noflick whatever. Right? And we basically get the data out and we love the model and then we operationalise it for you. Then your entire story of Kubernetes and cloud agnostic and new monitoring and we will trigger the next model. You just leave the worry out to us. Right? Yeah. Fair enough. I think I think that's a fair point. No, I think that makes sense. Right. I'm definitely interested in any demo that you have. But again, probably the feedback perspective because partnership is not possible with a company like that at this point of view unless you grow a bit big. Sure, no problem. Let's schedule a demo. Let's take you through the product. I hope you're it will be good to kind of learn, get feedback and maybe it just presses your mind like if there is a need like that later on and at least not to find it. Yeah, sounds good. Thanks. Have a good one. Bye. Bye bye. Thank you.