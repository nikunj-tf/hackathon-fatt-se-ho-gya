I'm good. How are you doing? Good. Is he coming for the first? Nice. Where exactly? This is like a small town. A very small town nearby. Any town. Let me check with our motor patient. You're joining. Hello. Hello. Hi, Amar. How are you? Hi, Nicole. I'm good. How are you? Doing very well. Doing very well. Good to connect again. Yeah, good to connect with you. So you have some buzzer, is it? Yes, just chatting with Vivek, actually. I have a newborn. He's like five months old, and we came to India for the first time, and our family, like, with the newborns, there's like a bunch of that happened. So we are all in ajustam right now and just squeezed out a time for a couple of meetings. I guess that's it. Okay. I hope I'm not taking away your personal time off. No, it's all good. This is totally all good. How are you doing, Amar? Yeah, I'm doing great. Just working from home today. Okay, understood. Nice. Awesome. So basically I just wanted to pick up from where we left off last weekend. Yeah. So I think the idea was that I think you told me a couple of things that you all are trying to solve as a problem. And in that context, I wanted to dive a little bit deeper. I think I have a sense of the problem itself, but I don't have a sense of the current state so far about what have we done to solve the problem, what's the severity of the problem per se. I would have to understand that a little bit and then maybe in the context of that also show you some parts of the platform that I think might be relevant. And then we figure out the next step from there. Is that okay? Yeah, sure. So you guys have the platform ready, right? Yeah. You have a trial version or something? Sure, we can totally do that. Okay, sure. So should we do the demo first or how do you want to do it? I can do the demo also. I just have a couple of questions and then I can do the demo. And then through the demo we'll ask you more questions. Is that okay? Okay, sure. So one thing is, like, I know Murray had mentioned about this running of different model versions across different data as one of the more manual tasks that the team ends up spending a couple of days on every week. Right? Yeah. So one thing I wanted to check is and I think you mentioned that you are not using Sage Maker currently, right? Yeah, we are not using it right now. Right. So I just wanted to check for that problem to automate that problem. And you also told me that it's running on local machines currently. Is it like those jobs? Yeah. I see. So would you actually intend to run it on a cloud machine or do you think like, local machine itself is actually good enough. Actually, I guess the point is what kind of savings are you trying to make there? Do you think it's like the manual effort that you want to save? Is it like some processes that are breaking that you want to change? Like what's your main goal in that task? I guess basically some of the work that we do, it's like quick prototyping. For that we don't need a heavy server. Right. So for example, we do have a couple of servers in house where we actually run some of the overnight jobs as well. If you have to test it on large amount of data that we run on the service, but most of the work that we have, that's not dealing with big data. Right. And we can run it locally, but ideally we would want somebody to actually save all the results, interim results and everything. So we would want to move on to the cloud. Yeah. Which is secure because we are dealing with healthcare data. Right, right, I see what you mean. Okay, so in that case, for example, if it's about saving the data on the cloud, then would it make sense that the job can still run on the local machine, but then you have the output of the job getting dumped to like an S Three location or something? Yeah, that was feasible. But then we won't have to use the compute of the server of the cloud machine. We just need use the local compute and save the results in a remote thing. That is right, yeah, that could work as well. Got it. Okay. Because if that's the case, then honestly, any platform, including ours, would basically be an overkill for this use case, which is like basically just you're new AWS currently, right? Yeah. So for AWS, I think just being able to run the code on your local machine and saving the data on S Three might already be a sufficient solution. I feel like I will still show you what we have, but I just want to make sure that I'm not randomly trying to find a use case of the platform where something simpler can just work, basically. Nikon, can you give me a minute? I'm getting a call from office. Sure. Nikon. Sorry. Actually. Yeah, no problem. I was just saying that for this use case that you just mentioned, it might actually already be a sufficient solution to literally run the job as you're running today locally and just dump the output on S Three. Okay, yeah, but doing it locally and then saving the output, wouldn't that be a manual process? Do you recommend using Sage Maker? Because I'm actually not very familiar with that platform. Do you recommend doing that again for. The use case that you mentioned? I actually feel like Sage Maker or whatever. Nikunj. Nikunj. Truefoundry.com think both are an overkill, to be honest. I think Sage Maker is way too bloated for the use case that you mentioned. And even Pronounce has more features than it's required for this use case. Literally, the idea is to save the because that's why I initially started with asking that what do you want to achieve in terms of reducing the effort for this job? We have multiple use cases. For example, one of the use cases, of course, organizing everything and then sharing the data across different members of the team and all of that. And secondly, we also want to automate some of the model learning processes. For example, we have to iterate and tune the hyper parameters locally, which if Sage Maker provides that feature, then we would be happy to use that, actually. Yeah. So I think Sage Maker provides that you would be able to tune your hyper parameters on Sage Maker. You can also do the same thing on Truefoundry. To be honest. When you say that you want to run hyper parameter tuning, how frequently do you need to run this job? A month. Like your team and I say you I mean your team needs to run this job. And what size of these like, how much time does each of these runs take? Like they have a parameter tuning run. It's a low frequency activity, actually, because we run it maybe you can say two or three times a month whenever we get new data, we need to optimize the hyper parameters and all that, but it's not a frequent job. Right. So right now we use Optun hyper parameter tuning for the exe boost model. But if there's something which can automate, then why not? Right? Because there are two couple of projects which depend on hyper parameter tuning. So for that you have to do it locally. But it's not a very time consuming job. That data center spends about a day to finish it off. Oh, the data center takes like one day to run the above tuning job? Yeah, it's not even one day, it's half a day. But we allocate about a task for one day to finish it off. I see. And is that one day like mostly just a computer running on a local machine? Is that most of the time or is it like mostly like a data center? You're tweaking some code, where is the time getting spent? Mostly around tweaking the code and all. So the computer is fast. Yeah. I see. Okay. So then I think there is very little saving from any platform that you will get. Right? Because if the job is very fast, then actually in my personal experience, when you can run something locally and it's infrequent, I think it's an overkill to work with any platform because then somebody has to learn it, somebody has to maintain it. There is chance of accidental cost that is associated. Right. So there's a lot of things that can go wrong when the benefit is not enough to be reaped. So that's my personal experience. One thing that you did mention, which was around running the long running jobs overnight, you have some large machines, right? Can you describe a little bit more about that operation? So that is some signal processing code that we run to generate. Like the data that we get is basically a signal from the BCG sensors that we have placed under the mattress. So that signal we apply some Fourier transforms and Hillbilly transforms to get the HR and RR. And we have to test it continuously as we tweak the code, we have to test it continuously on a lot of reference data sets. So that is something we run overnight. And typically that takes quite a bit of time to do. And this is done every day? No, not everyday. Every time we tweak the code. Right. There's always optimizations, some error cases that we have to handle, and then we have to back test it on all the data sets. I see. Understood. So one question here is, like, the machine that you're running this job on, how large is that machine? I don't remember the exact configuration, but it's about 32 gig Ram and about Terabyte hard drive, but it's not a very high end machine. It has the GPU support as well, but we don't use the GPUs on that. Is that machine from the cloud? No, we have rented a server for doing that. Oh, I see. Okay. No server. Okay, understood. So you don't actually rent any machines from the cloud. We use this Google collab for doing some of our GPU heavy tasks, doing the image processing, and running some of the code on that. Understood. Okay, got it. Okay. So based on what you're describing, to be honest, I think I think your use cases right now at Dozee are a little too early to be able to benefit from a platform like ours or, to be honest, like any other platform. Okay, that's my reading. Based on the use cases that you're describing, let me take anyways, like maybe five to ten minutes to show you a demo of the platform so that when your use case is mature a bit, or if you just feel like this might be applicable to some other use cases that we have not discussed. We can obviously always consider setting up like a trial account or something. Or whenever you feel like you're ready, we can always set that up. But that's my reading of the situation. Okay, yeah. All right, maybe I'll just show you demo the platform quickly. Yeah, let's do that. Okay. And in this case, I'm also not covering the full platform because I think the full platform itself is like it has a lot of different parts that you could use. I would probably just focus on one part of the platform that I think is most relevant in this context. Please let me know if there's something else that you would be interested in seeing. Okay. Okay. Yeah. So the way I'm going to give the demo is imagine that Dozee just wanted to run one of its hyper parameter tuning jobs, okay? Someone data scientist at Dosi wanted to run their hyper parameter tuning jobs or train a particular version of the model with some particular version of the data. That's those are the couple of tasks that they have on hand, and they wanted to run it on Truefoundry platform, let's say. Okay, okay. So that's the context in which I will give the demo. And here I'm going to assume that you are not dealing with any infrastructure whatsoever at all. Your data scientist directly deals with their Jupiter notebook or whatever, like Python scripts, and they are able to quickly run that script on our platform, basically, as opposed to doing an entire infrastructure setup and all. Okay, so are you able to see my screen now? Yeah, I can see okay. So maybe like briefly start with a quick documentation here as a starting point, and then we will get to so this is training the model itself. This is great, actually. Let's use this as an example. Okay, so imagine that the model that the data scientist is working with is actually a simple ScikitLearn model. Yours is an Xg boost, but I think it's fairly similar, right? So the way this works is very similar. You write your training script or whatever, the model evaluation script or whatever in plain Villa Python. So you have some imports. You load your data, right? And this data could be read from S Three or local machines or any databases. In this case, we're just calling simple functions. No problem. Then you have your model code. So it's a simple logistic regression model. It could be an XGBoost model. This is all vanilla Python code. And last, it is the model eval code, right? So, like, you run the model predictions, you get the predictions. And in this case, you would probably want to save it in an S Three bucket or something. That's it. So this is like your vanilla code of running and evaluating a model. And there is Nikunj truefoundry specific here. This is like exactly how your code would look like if you were not using Truefoundry. Right? And then I must show you how you would deploy this on the Truefoundry platform. By the way, this entire thing can be on Jupyter notebook as well. Right now it's Python script, but the same thing works on Jupyter notebook. Now the deployment actually looks like the following. Okay? So, like, this is a small script. Deploy that file that somebody will write to deploy that code. You just have a couple of imports. Like, we have our clientside library that you would import, you build. You just tell us that, here's the file that I want to run. Pretty much that's what you're doing. Train pi. If you wanted to, you could specify some dependencies of the file that is xgboos, psychic, learn, NumPy, scifi, whatever things that you want as a requirements, you could specify that. And lastly, you would basically set up a job because this job is going to run that train PY script that you have written on a remote machine, basically. So you hit a job deploy and that's it. This is kind of the seven eight line script that a data scientist would write and as soon as they hit a job deployed, okay, they will see that a new machine is getting spun up for that job, okay? So it will look something like this 1 second one thing I'm just trying to pull up the better demo because it's service, that's why. Yeah, so basically once you hit the job that deployed that I showed you right here, okay, as soon as you do this, you would see that you have a dashboard, there is a new run of the job that will get spun up. So what happens here is you will have one machine that spins up, runs this train pi that you have just provided to us, save the output in whatever s three bucket that you want and then kill itself. So that's one thing that can happen. Some of the other interesting things happen which are particularly relevant in the example that we discussed is you actually log a lot of data and visualize a lot of data practically for free. So for example, if you have been having multiple jobs, like multiple models that you have run with the same data set, imagine each of these nine items are basically one model each. You can actually compare the different models here, different parameters and all that have been logged and you can track the metrics as well, that okay across different models. How has been the performance of this particular data set essentially. And if you wanted to, you can also track it over a period of time as well. That's how has my accuracy changed over different runs that have been running essentially, right? So this way, like tracking different metrics, tracking different run outputs, etc. Become very easy and you can also visualize the results as well. Now, one good thing that happens from an infrastructure perspective when you do this, and it is particularly useful when you have like these large jobs that are running is the machine itself is not always up and running. You literally hit the job that deploy. That's exactly where a machine is spun up. As soon as it finishes running the job, it will actually kill itself. So you only literally pay for the infrastructure for the time duration in which the job is running. So this in itself actually ends up saving a lot of cost for most people and there's a lot of other things that come for free by doing this. So for example, like your remote logs are available for you to visualize right here in the dashboard itself. In this case, we probably have not logged anything. So saying no pass logs found. But if you had logged some things, you will be able to see your logs. You can see your system level metrics as well. That is how is your CPU usage, memory usage. You can see those graphs right here. Okay. And then everything that we do is also version controlled. So you can see the different versions of your deployment, like 45678, 910. So just like on GitHub, like, people try to keep their code version controlled. Now you can keep your status of your job running also version controlled. So that if you wanted to switch back to, let's say, 9th version as opposed to 10th version, it's literally one click. So I think these are a few things that you get pretty much when you run the job using the platform without needing to manage an infrastructure. I'll take a pause here to see if you have questions. No, actually this is good. Actually. Does it also support, like, if you want to have the computer locally? Yes, you could do that also if you wanted to, instead of spinning up machines here, if you wanted to just run jobs on your local machine and want to track the results on our dashboard, you could do that as well. Okay. Yeah, fine. So I think this is mainly for some of the bigger jobs that you run. So not only as you mentioned, it's not really a very good fit for us right now. I really don't think that you need to onboard almost anything new for the current use cases that you mentioned. Yeah, exactly. Even though it's like literally getting started with this is very straightforward. You like to install a library and hit this API call if you already deploy things. I still think even this is an overkill. I think you just run it locally, get done with it quickly. I think that's the right thing to do. Yeah. That's why we have not invested in any platform so far. But as we scale, we might have to do that. Right? Exactly. And I think that would be the right time to do it. I think anytime introducing any level of new tools adds complexity to your process that before the time has come, I think it's just an overkill. Yeah, exactly. Okay, sounds good. So I don't have any more questions. I won't take more of your personal time off, but thank you so much. This has been very helpful. Thanks a lot, Amar. I hope you have a good evening and at any point in time. Amar, for us right now, the idea is not to like, hard sell the platform that we're building. We are also being fairly selective about the kind of use cases that we are working with because we want to make sure that you're adding value immediately. Right? Yes. So if you feel like the use cases are maturing and you would like to try out the platform, please let us know. And besides that, if you feel like you just have questions about the space, different tools you want comparisons for Sage Maker, XCI, whatever you need just to brainstorm your ML of architecture, just let me know because I think we have been working in this space for some time now. So we have built an understanding or overview of this space. I'm happy to share that as well. Yeah, sure. I'll let you know. Definitely. Thanks so much. Great. Have a good evening. Bye bye. You too. Bye. Take care. Bye bye. You can stay by for a minute or so? Sure. I think it's chilled out only I think would not benefit anything from our platform at all. There's no point hard selling for the Rasca call that we have. Have you talked to Abhik already? Do you know if she will be taking those calls? So I spoke to Anurag. Anurag said either he or Abishek, one of them will be definitely taking the call. He's currently not aware of his timing. Once he meets the person that he's supposed to meet, then only he'll know and accordingly he'll let Obishik know whether he'll be able to attend. Tell me one thing, Artis. What's the level of importance of these calls? So this sugar CRM again is not a big company? Of course it is. From the revenue point of view and from the size of the ML team point of view. But the guy said he does not need it right now. There is no absolute need for it. Then why are we doing the call? We want to have the sense, right? You mentioned last time that you would want to talk to understand the space because it could be like knee jersey's reaction that I don't need it right when I'm trying to ask for something. And when you talk to someone, then you realize maybe there could be a need. I think if that's the case, usually selling about pointing I think learning in a jar, that's a different story. The SaaS learning that we talked about. So I think that's okay. Usually we sell hokani at that point, but other relationship building or learning a thing, I'm totally cool for it. How long is the company relevant? Company is totally relevant learning a point of view. And he also agreed because of that only. Tell me one thing, Nicole. In that scenario there is very few company which says yes or very few company which he's even looking for a solution, right? A lot of companies either already have onboarded solution like databricks very strong like or something they have build of their own, right? I think somebody very strongly says that I'm not looking for a solution. Then I think Ma'am selling your Paige mostly because it's very hard to convert at that point. Usually I don't think it will be true. And if that is true, then I think we need to rethink, right? If you are saying that most companies in this segment have either already built or bought something and they're really, really not looking for anything external, then we're not dealing in the right place actually. Right segment basically. Those who have not responded, I don't know their data point, right? Say. For example, I'm talking to OutSystems. So OutSystems has a person who has built everything ground up and he has mentioned that in his description on LinkedIn. And the person is telling that hey, our system has issues, right? Or we are facing some challenges, so then it could be a good fit. The point is we ask people, people say that okay, either we have some challenges or we have learned something that we want to share or something that we have bought is really not working for us, et cetera. That is a good fit, basically, right? But if somebody is saying clearly that hey, I'm a safe maker, state maker is working great for me, I really don't need anything at all right now, then I think it's not a good fit. If somebody says that, hey, I've really built out something, I absolutely don't need anything. If somebody is strongly saying that I know what I'm doing and I don't need it, then I don't think it's really a good fit. From a sales perspective. After that, if you really want to, if you feel like this, what they have built or their experience in general might be relevant to learn that's okay, but then we don't go in the car at all with the sales motors. It does not become part of our deal pipeline and our deal one or lost pipeline basically at that point. That's what I'm trying to say. Logo where they have already mentioned in their description we should approach them, right? I think it's debatable. Should we approach them or not is debatable. If I could if I could, I would get more calls where there is an opportunity to get a business and if somebody is saying so you can approach them, but to understand like what you have, do you have a problem or not? In that we approach them? Not with the mindset. Basically, I don't want seven out of ten of my calls with people who know that they don't want to buy. Basically, I may have one or two out of ten calls where I know that there's no business here and I'm still taking because it's super relevant. I really want to spend more of our time in trying to find business and like validating or invalidating a thesis in some way. And after doing some big reach outs, if you realize that for some reason people in a certain segment has really already built our bot and are absolutely not looking for a solution, then that segment is not right. I think we need to arrive to that conclusion. Also, in sales segment, I've got more nos than yes. More nos than yes is fine. More nose than yes is okay. If some yeses are very relevant yeses in that segment. So the number itself does not matter too much. The quality of the yes matters a lot. Like out of 400 people, if you find three or four yeses with a very similar problem, then I think it's still good. But if you don't have that everyone is saying no, then yeah, it's not a good fit. Then. Let'S say in this situation right when you're talking to this person and versus we have not ever spoken to this person. Right now there are two things tomorrow for sure this company is going to need it. If they raise new funds, they will scale up. They'll need some platform and immediate approach would be us. Right. It's like you're building a pipeline for a year down the line. Yes. Correct. That's how enterprise sales ideally work. You start building relationship first and then six months down the line you start the sales page. Right. Now when you tell me that you would not want to have this kind of a conversation that kind of restricts a lot of things in enterprise sales. Right, right. Because no one will tell you that I need the solution. Come and sit with me. It has happened only with yes. I don't know. For some reason I did not approach the decision maker. I approached the gatekeeper and he kind of spilled the beams. Right. I'm sure yxtk decision maker, except we need invite and he would have not said anything to me. Right. From my experience, I think these kind of a call has to happen. It's just that you take it or someone else take it. Right. But these kind of a calls will necessarily help you scale up when you are in a growth stage. Honestly, I'm not of the opinion. So 30 minutes. I just understand that your time is very valuable right now and you would want to have those calls where the intent is there versus the intent is not very clear. I need someone to take that call. Right. So Vivek, I think what I'm saying and how you're reading are two different things. I am saying that where there's a clear no, not saying that I absolutely want a platform. That's not what I'm asking for. So basically, Ambivalence quote I'm not saying we should reject Ambivalence, okay? I'm clearly saying that where somebody is saying a clear no and that too. I'm saying the number of such calls cannot be more than the number of any intent or neutral intent call. That's what I'm saying. So how will you qualify this call? Dozee. So Dosie was good know, initially when you talk to Dozee, Dozee was like, yes, I have a problem, I'm looking for a solution. Then I dive deeper into it and I'm like based on the problem. So he does not understand the space. Anything at. All right? He's like, I have this problem. Automation went a little bit deeper. What are you trying to solve? It turns out that he needs something extremely simple and our platform is an overkill. And I just tell him that, hey, this solution will work for you and you just go use it. This I don't mind at all. Right. But if somebody has clearly told me that, hey, I absolutely don't need a platform, and if those are the only costs seven out of ten calls that you're taking, I think that's not a good use of our time. Yes, we need to invest in building our future pipeline, but most likely right now, the segment that we're targeting in that case is not right. That's the point I'm writing. Then all that makes sense. So these kind of a calls are okay, not very great, but okay to take. But the calls like, let's say sugar CRM where the person said he does not need it, or high radius where the person said he does not need it. These kind of a call, I did not push for getting them on call. Unless you think it's such a good material that just relationship building with them in the long term is very helpful. That's why I pushed for sugar CRM and did not push for high radius because I know high radius. That's good then, right? So for example, Dozee, if he had clearly said a no, I don't need it, I think it would have been a good idea to skip them because they're also small, right? Yeah, but those did not happen that way. But like, for example, like a merck or a synopsis. Even if they say that, hey, no, I don't need it right now, I would talk to them to just build a relationship at that point. Like a month type of sale. Anyways happen over like nine months, one year time frame. Yeah. Cool. Makes sense. For newsletter. Can you give me pointers? I'll write them. Which one? I'll try to write right now. So I'm trying to write the one that you mentioned around drift and I. Don'T think you'll be able to write. Okay, I think that one is too complicated. What do you think I'll be able to write? We don't have any other blogs that are up and running yet that we can do. We do not have any draft or anything in the pipeline. I've asked free for a model server while a blog. No, sorry. I was free for this Sage Maker blog. What is day or should I know? For model server? How can they do it? So I don't think they have bandwidth. In fact, the last blog or newsletter, he just gave me four pointers and I wrote it from there. I can try writing model server on my own. Again. I think that will also be a little too it needs a lot of fun. I cannot write, for example, what a. Server generic because I read a few blogs and I thought I think it. May not be surprised. The point is it may not be able to add value. The blog itself is like if you or I write, I don't need to be a very strong value add for the readers. Basically makes sense. But these people were building it, right? They will do a much better job, I think between you and me. Right? Yeah, of course. Let me see that rip one is also very common. I think honestly, the drift one at this point, probably only me or maybe Nickel can write off nobody else in the company. Okay. I was hoping to make it a little bit more relevant by capturing this chat GPT. Right. You see so many posts what chat GPT is doing. You can pick a few good Twitter threads on what they have done it and plug it into a blog post and take letter half of the article. Yeah, you can write one additional blog based on that. Like some trendy blog kind of thing that I think you can pick up and do as well. Again, I'm not sure, I mean if you have interest and time to do this otherwise I'm not sure if it's like a very critical thing to do. I guess that's what I'm just that. Make it more instead of just one blog post, you have few more elements to it to make it more readworthy. Sounds good, that's fine. I'm just checking some other blogs here. 1 second. I think one thing you can do vivek this time is send three, two or three use cases block basically and that I think you can pick up yourself. So let me actually show this to you and you can work with Genoa if you want on this one. Who will be able to tell me can maybe the right person. Right? I think there is a blog that has been written here. Hello? Hi, I can hear you. This is OCR text summarization as a team use cases. You can just say like whatever community related use community developed, use cases, et cetera and just make sure that these are not sent already. You can send this out basically. How does that sound? Good. I think that is doable. You already have blogs? You just have to get so many of these blogs. You have to just send that out. Makes sense. I think this works. Okay, bye.