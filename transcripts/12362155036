Go ahead Nikunj. Oh Hi, hello. From immortal. Hello. If you are audible, how are you doing? Great. Awesome, great. They're great to connect a new. Where are you based? Current Lemon Bangalore. Bangalore. Okay. Nice. Nice. Mmm. So first off, thanks a lot Anu. For taking the time, I was actually quite excited about this this call, maybe I'll just briefly introduce myself and also the purpose of the call. So as I reached out to you like an overland that  we are building out this, we are building out a startup in the In the ML infra domain True foundry and currently we are in a process where like, we have a product that's decently built out and we are collecting talking to more like, you know, ML leaders data science leaders in the industry and getting more feedback on a few specific parts of our hypothesis. Okay so so that's what I wanted to discuss with you in the context of like us and learn a little bit from your experience there. Quick background on myself. I come from a machine learning background. Used to be at Facebook, where I build out a lot of conversation AI models for them Prior to that. I led the machine learning team at a startup called Reflection that we were building recommended systems for the e-commerce industry. I've been in the Bay Area for about 10 years. Now, moved there from a masters at UC Berkeley. And before that I was doing my undergrad here at IIT Kharagpur. That's where I met my now co-founders and Ragana Abhishek as well. Okay, so so yeah, that's a little bit about myself before starting true foundry. Did one more startup in the talent space that got acquired by Info-h which is a parent company.com and and yeah I think I think that's that's pretty much a chinmay maybe if you wanted to introduce yourself. Yeah, I know this is Chinmay. I also guys from IIT Kharagpur during my time in college was mostly doing mln. NLP post that, I joined McKenzie and Company as a management consultant and join true foundry about six months back working in the founders office. I mostly helped looking at product sales and customer development. Okay, great. Amazing. I know it would be great if you can also give us a little bit of your background as well. Okay, so I did. I did my indicated process graduation. We take plus Mtech five, year course, from Tripoli Tea Gwalior. And from college itself and working in ML. I mean, my B tech thesis and practices my research paper, and everything is in ML and NLP. And after that, I started working in AI as an ML engineer. And then further, for last five years, So I've been in like us for three years or so. I have been working mostly in all on, like only in AI Domain ML, AI domain, training model, and everything. Let's talk. And then I probably I might be able to answer few questions or may or may Yeah. not. I'm not sure. I was just overall broad agent. I have in mind. I'm not sure how much I'll be able to help, but I'll try my best. Thanks a lot. Thanks for being flexible ticket. But, you know, one of the things that would be very helpful and by the way. Yeah, I will, I will generally be prompting you with some questions specifically here. So So the idea being that like, you know, one of the things is, would be interesting is to learn the structure of the team, Okay? How, how is the machine learning team structured within Zycus? That's, that's question. The second is like, the, the clouds set up as well. Okay. That are the models getting deployed on, like, you know, one cloud like AWS. Or is it on multiple clouds, right? So that's the second question that I would love to dive into and the third key. The ML Development Workflow, Okay? From a developer who's building out a model, what does if you to take that model to production again? And the last one would be key just specifically, What are the types of models, right? And then we don't have to go any specific order, but for overall areas, Your team structure the Cloud setup, development workflow, and the type of models, okay? okay, so in Zycus we have Like four AI teams when I say. So, there's one big team that, which we call Merlin, And it handles, all the AI work of Zycus. So in that we have AI teams Java teams UI Teams, QC and deployment team. Okay. Well, okay. So we have a very big team. So,  let's say. And then there is a product managers and data analyst team as well. So and so this is overall structure, so we do all the appeals. So we usually get problem statements from our product managers. And and also, if we see that, I see my product, and I've seen my product for three years, I know what are the pain points. I can also suggest that this can be a next project so there's two ways. And then I mean I think you're interested in this that not how teamwork and all that, just the structure based Yeah, I think I think I will have a few few follow-up questions on this as well, but maybe maybe like, you know, maybe maybe one thing we can start from asking is key. What is the cloud setup within the Zika? Okay. Yeah. So for for lower environments like QC environment and release management environment lower environment. We have our DC, a Mumbai, maybe have our network Mumbai DC. We use for higher environments. That is staging and production staging is one level down production And production is final that the one that is give into tenants finally clients that they use and staging is where we do uats and demos and somebody wants to test out our product before, you know, actually signing the contract, that is staging environment. So those two are are on AWS only and, and then, development workflow would be probably so it even if Area before, before we get another flow, just a couple more questions on this. So, say mentioned that a Dev QC, release process, environment etc is on your own data center in Mumbai. So do you know why, why is like, you know, the Dev environment separate from the like you know, it's not cloud crossed. God. Okay, I'm just First and cost is only problem in between. You also keep on seeing if anything is not occupied, will release it and all that, that process also in place. So mostly it's the cost. understood and you know how large offer data center? This is like what, how many nodes in the cluster like do you have to use and stuff? This. I'm not aware but GPU we have two in-house GPUs. I see, okay. Okay. a two, you know, when you say you know GPS it's on the same day doesn't or Local. They are not in their money. Yeah. Bangalore Data Center, but it's same. I mean, you can call them same. Understood. Okay, got it. Okay. And the second thing that in this regard, that I wanted to understand was key like a lot of this model training like the actual training of the models happen on this, this data center as well. Yes. So we have two kinds of training one is then we call it an offline training is when you know when there are bigger models and we need to little have a little manual process involved, we where we check the output and images and all that. Is that happens on the dev environments. Okay. And there are when there are offline online training modules, where as then where the everything is automated, as a data comes in model is going to train and then it is going to get released the follow the release process, they'll go to some repository location and then there's going to be around that so there's offline online training as well. So as of now, we do not have any Infrastructure. That is gpu-based all all our product predictions or predictions happen on CPU itself. For training module, we use our GPUs but then we convert those into Cpu-based training model and then they go, we lose on time. I mean it's twice the time prediction time but that is something that at this point in time we can live with however in future we might reconsider, you know, getting a GPU VM DP based here I see. Okay, so basically when you're prototyping you would you would run the tray model training on GPUs as well. But before you push it to like your online training or the Cicd type of thing. Like you would change the code to CPU based training and push it out there basically. And other models are remain, similar. Random for, they don't need GPU and all only specific modules that we need a GPU for But okay. I see how is the beta access and all that happens in this in this regard annu. Like because when you're training your monitor running on your local data center, right? So like is the data still stored in AWS and then you access the data from your local data centers. So the two, the two ways. One is a while, we are on our dev environment. So there are certain kinds of data that that client has given as a, you know, permission to access or to get it in on the local web environment when I say, debit is below QC or in my local in my laptop. So those kinds of data is something that we can get it from our DBS. We just run some query and we need to get some, you know, from Mongo from some, We have our own location like storage from there. We get it. Otherwise Data is in S3 S3, say it streams into an easy to instances there, everything happens. And as invent training is over, we release those 60 to instances. It still is in history, it remains in the S3 bucket, for which we don't have a permission to get it into our local systems. Oh I see. Okay so then our child so like the main data stays in S3 but for prototyping you have some data access, there are still more dB or something basically. Mongol or SQL or we have our own data lake where the data is stored, there's one team, which maintains a data lake. Testing. So then like you know during a training time you don't you're not able to access a lot of data because basically like the prototyping models you don't have access to a lot of data that is stored in S3 like that you can only you can only train on stuff that is on MongoDB on SQL today.  not not correct. We we do we might not be able to get it into a system but we're able to see it said their PDFs. There are XML files and there are, you know, Jason files and all. So they are all on either on DBS or on like normal dumped somewhere location profile system. Files. Just So we are able to see because once we log into a productions and words or whatever, we can see what it is. And also, and then there are a few like, for my team. It is possible. There are invoices, we do invoice extraction, it is possible but the other team which extract data from contracts. So contracts are something a very You know, sensitive documents. We won't be able to get it into our local systems. It becomes a issue. I mean, it's a security issue, I mean, data security issue. Think that's makes sense and you all don't use Sagemaker today, right? At this point in time, we don't in Zycus. However, I've used earlier in my previous company for training but there's a deep learning based model training, you said makeup. okay, but it okay so you know there's any reason why the company let's say does not use Sagemaker and uses more in-house tooling I mean, I believe one, one important part was Till now, I mean, in my previous company we were we were a startup. So we did not have this in-house GPU purses. It's like, Okay, let's try AWS and then we will see in here. To be honest, we did not feel the need of going to it. AWS at all GPU. We have in a local GPUs. Now we are, we want to go but then we want I want that kind of flexibility. And I want a fresh easy to instance, I would want to treat it as my machine say Maker is gives you already pre-created environment and all that. I don't want all this. I need my own environment so that's why it was my call to either go to EC2. You want to go to Sagemaker. So are being you know they're not boasting but being an experience developer AI developer, I give me a fresh machine. I'm going to set up it as my own. I don't want to wait money on Sagemaker and all Thanks, complete. Awesome. Understood. The separation on that Annu. So like How is the ML ops then manage? Like How do you guys like If you do a local training like there would be a resource constraint etc on that, right? So you do you just like train with dummy data and then like ship the model to like the server. How  no. We we do not train on dummy data. We train on Big Data itself. So our debitments have. So, if it is a good, they're two kinds of constraint either. The we will be using a deep learning module. So for that, we have GPUs, we so now the constraint is coming up so that's why we are moving. We do we cannot keep on purchasing bigger GPUs. So we are moving to the it explodes. It is easy to engines as for lower one like just for the sake of, you know, random forest or something. Some normal training, we have bigger dev machines. The data is that in that machine, we train it and we train it on actual data and after that, there's a process of shipping the model. So we put it into our next as a repository we maintain the model version and as and when the release happens, that model version is maintaining some configuration file. It goes into our actual instances where it has to be used and before you even be decide on, you know, which model and how and all we do some kind of a benchmarking where we see, okay, 4gb44 or 8GB 8, code would be good. Good enough for that model and let's say if for a given model and we have the load that we are expecting and then accordingly be scaled. Included deployment itself is happening on an EC2 machine, right? Like you create an Yeah, deployment happens itself on ice it. Yes. and for the training part, Let's say there are some standard practices like like maintaining the model metadata logging. These models etc, like Do you use some tool like a ML fluid etc. How does ML ops usually work on your like own self-hosted cluster? So as of now, we do not use, let's say we have to maintain, there is AI to be very honest. There's no ML ops per se in our company, there's an ML of steam, we do, we do whatever we wish to do. So Let's say, there are TENSORFLOW models, be there, heavy models. So we serve them, we make a server Understand. TENSORFLOW server. So, all that matter data is maintained in a ZIP file, all the models and the metadata is maintained in ones, zip file and that asset you can call Asset. Forget model that asset. It might be anything related to run related information related to run the model be maintain as AI engineers. It's at the job to maintain that. And then again, we push it to us centralized location model version. Like a particular version is maintained and that comes into that VM and that is where our environment file and all that. It's is there it gets created, it's normal. There's no normal deployment. If you remove AI from that, it is a normal python code. Also, what all libraries are needed, those will get installed and those models are going to come there. Slowly now and now Abby we are moving to Docker Docker and Kate. So that is where probably all these ML Ops things might come into play. But as of now, we do not use it. Reason being there is no expert in our company, who can, you know? Who can? Start off with this. Editor. human start off with, like the containerization for the docker and all Now this we have done, I mean the the all the, you know, standard ML of stool, where, you know, you can see all water training is going on and all that UI and these kinds of tools that we do not use. But I see actually in this one, a new like What's the biggest opportunity area? Like you You mentioned that like you know the team does not necessarily have like you know, Mlogs expertise already if let's say you had the expertise. What is the most important pain point that you would want to solve in regards to have a loss? I would want to, you know, there's a lot of manual work that is involved. As of now, we have to maintain every model version every model metadata and every in everything and anything. So, if that could be taken care by a tool, that would be the best, I mean I should not be once I train the model. I say, Okay with this I can see and this kind of a data, this is the best, my work should end there but I still, you know, have to work a lot in getting that model to production so that client can use. So that gap that gap could be filled, whichever way, I mean, you want to doctorize it, you want to put it in a gate cluster, you want? I should not. I mean, there's a level of benchmarking that I can can do, but I should not be spending too much of time in optimizing this in that If, if it could be done, like, see we are using OCD pipelines now as in when Docker came in, so many things got, you know, resolved for us so many tensions gone. So as then when these kinds of You know, tools if they come in they help us. We don't we don't have to take care of, you know, if what, what happens in the load starts coming, and it should auto scale in one cluster. And if they one more BM, get autoscale another cluster, how they communicate, I don't have to care for now. I have to care. I have to be very, very careful. Let's say, for instance, My my easy to AI is instances are host on Australia region or let's say US region and production starters, there's another tenant from Australia region so I have to get a permission to get those sports open so that these two can communicate, so I don't want that headache as well. So if somebody could do that for me, that would be the best. Oh, I see. Okay. Well, I have active so many follow-up questions, but I'll take a pause here to see like enough Chinmay has questions. He was asking, and I interrupted something good Chinmay. Yeah, no. I was also trying to understand if you can this black briefly walk us through the steps, so I understand that. So where does your development work? Like, Do you control connect to a remote ssh, current terminal or something? Like How just trying to understand? Like How is your dev environment? Then like How does it raining actually was like, You get a problem statement? So where do you develop, Where do you keep track of things? Then how do you demo or like feedback from shake older? If you can just walk us briefly through the journey? Okay, let's say there is one one scenario, where we are allowed to get data into a machine and the scenario where we are not allowed, Okay? First scenario will be allowed to get the data into machine. Let's say there's a problem statement. So first thing is that we get the data data, analyst or user has the access to the data. They give us the data on a local machines in my laptop office laptops. I do all my experiments. It's about to machine network experiments. And, and then it is a bigger machine. I mean, it's not, I mean, it's a big machine, so I do all my work and then once it is done, if I have to demo it to my product manager. So first level is this is accuracy. Really. Yes, or like notebooks and all. What's like the Pengary? I prefer I prefer by German vs code. If somebody wants to do notebook, they are free to do notebook during the experimentation. However, once the experimentation freezes it has to be a proper production level API. Okay, let's I'm doing it on a notebook because if it's a bigger data it's better to if it then CSB and all it's better to have on a notebook. And then we give the demo to let's say If my the personal reports it to me we talk and will discuss and then when it is done, we go to a product managers. And they if they agree with that number of numbers, I mean, accuracy numbers, depending upon what problem we are solving. Sometimes you are okay with 70. Sometimes we have to be 1995. So when the demo is done, that is then that is our side is done. Then we make that an API depending upon what the situation it can be using kPa or else in KPI. And then we use a flask APIs and then we make organic on app, out of it. And then once we produce then there has to be one input that comes from our Java, Integration link, Java is going to send us that input, and we are going to predicted the output goes back to the Java layer, the cash that output, and then, there's whatever they have to do. They do. And then, finally, it goes to AI team. Then they show it on UI. So this is the flow, but after development, what happens be, once we make the API, we have to, you know, benchmark key. How much infrar do we need for this to work? Okay. 4gb for core is enough. Then that is when we go to our scene, AI, I mean, VPs and all they will give will present key sir. We need this kind of intra, they give it to us. They give it to us on. Environment are QC. RM staging production on qcb we have OneNote purse API other. That other than that, we have two notes per API. If needed, we can have four node or how many clients or tenants that we have to deal with. So we decide beforehand. So this is one point. I don't want to decide beforehand, it should not be my headache. It should be able to I mean somebody should be able to take care as and when the load starts coming you have to spin off another instance. Understood. So once then, then deployment process happen. Go see the I'll talk about, ghostly deployment Paracord, goes into, Bitbucket, our models and assets, goes into a shared location. We call nexus. It's also an S3, bucket and configurations of how this model and how this board are going to connect goes into a gitlab repository. Well, let's say model version or which what kind of logging do, we need on QC? We need in for level on hand and we need a level logically. All those kinds of small little kind of code board, goes into our gitlab and then in big bucket and a Ford, we maintain those flower braces, kind of a placeholders as and when the code is released, all those configuration comes into my code and the models come and sit into my VM in the model folders, which we know that it is. To level outside the code. So this way at the end it is all integrated into the VM. That's it is a production VM. It is integrated into production here. And then the process happens and all of these are connected via HA where the healthcare keeps on happening. And no mad where you can, you know, see if your API is up or not and all that and then dinate raise any LQ that maintenance already happens. If it if it is going beyond a certain 90%, I think they're put, if there's too much loader, is coming. 90% Memories saturation of CPU, saturation happens. They start to sending alerts to us at p3p3 P3, saturation saturation, and it's, it is in a headache. It should also be managed by somebody. I should not be, it should not come back to any engineer that you're VM is getting You know, getting a lot of getting out of memory and such getting memory saturation. Understood understand, that makes a lot of this. Thanks a lot for working us through the entire journey. Feeling safer. Nikunj, you want to ask something? Yeah, this is, this is very, very helpful Annu. Thank you so much for like, you know, sharing this background. Now, I did want to switch gears a little bit and ask you, ask you about the team structure that you were describing, right earlier. So, I mentioned that you have this Merlin Org, where you have, practically all the components that are needed to maintain a machine learning model, right? First of like, Is this, like, a central machine learning or is this embedded within a product or basically, right? And are they like multiple different products within cycles? It is a later part, it is not, it is a. So before Merlin, was there invoices still used to come to us to like us and they used to be some AP person. Used to sit and extract the data out of using voice. Okay, our product, maybe we have just Wow. written Merlin is now. We now some person will not have to do. He will, as in, you know, it's our job as an AI engineers to do extraction. Even if we do it up to a 90% of accuracy, let's have a hundred quiz Ninety fields are extractor of the invoice just between the verify those 80 and then they just have to enter the 20. So it takes, there are other products in, Merlin we fit in somewhere in Merlin. Sorry in zaikus. There are other friends like us and modeling fits in At one place in Vegas. So we are kind of a USP in our procurement domain. I see. So basically Zycus has a company has multiple product lines Merlin as a platform serves different products. Basically, one of them is the invoice parsing and stuff. Invoice extraction. What, what are the other other products? The one would be, you know, the supplier recommendation and then, you know, so in invoice process flow there has to be in supplier a requisition and purchase order in an invoice. Okay, so one of the team, what it does it, it reads the contracts, you know, and find out the entities from the contracts. just, Other team says What are the risks involved in the contracts? You know, if let's say this Turkey, Syria earthquake was there? So we need to populate that in UI. Okay, this has come up and this possibly might affect your areas of work. And then there's one team which, you know, keeps on information about extract information about the suppliers. So, so they send their original, Let's a bank check. So the extract the account number from there, you know, so that they the payment goes to that account number. And then there's invoice, where we extract all the information from the envoilers and other team which extract information from requisitions and pos. And, and at the end, all the processes stitched together. This is another team which I handle is in analytics team. So once the invoice cycle or a requisition cycle, or the contract cycle is over, we try to analyze. What are the pain points? Where is the delay? How we can, you know, reduce the delay, which factors are causing the delay? Such kind of analytics also happen. Interesting. Very nice to know this so I know how many people are there in the Merlin team today. Oh, the red questions, you guys. I don't have a counter around 100 plus people are there. So we have five AI teams each team. Some. Some with eight seven, you take and then AI. This is Java QC. UI Product Managers Directors. All are there. It's a big team. I mean 100 plus people. Okay. And each of these AI teams that you mentioned are basically like, You know, one team is doing, let's say OCR type of work. One table is doing Let's say like actually is that how the team is structured like computer vision? We're under 40 etc. If it's not in the basis of AI expertise, it's basis on the product rather use cases to be used to use the correct word use cases. And depending upon what problem comes in, it's up to you to decide which technology would want to use. So, I've used, you know, simple correlations as well. NLP CV, Deep Learning, Everything Ocr we use Abbey, Find Reader. So we don't do our own Ocr. We use this enterprise levels here. Alright, okay, so now that you have to five teams like this, like the, you describe this entire pipeline, right? How models are built and deployed, etc, etc. So, do you know? if all the five teams follow very Here, it's around. similar pipeline or different teams, follow different pipelines of So, there's one Devops team for all five AI teams. So it's same The property is same. I see. Got it. Okay. I see. So and how large is the Devops team? Never stream. Also, they are around 10, 15 people. Got it. So how models are built and deployed like that, like water new processes that you want, introduce that's a decision with the Devops team and then the ML teams kind of cooperate with like, you know, whatever is the suggestion that comes from there? It's not their decision per se but we drive it develop it. Let's say, so now, we are having this docker, we are moving from go CD to Docker. So we so the, the need came up so Yeah. that till now we were having You know, one model for every customer want every tenant. So now we want, we wanted to build tenant specific models, that is where they're their standard, no matter. Their standard ghost, CD is going to fail. So then the solution came in, Let's use Docker. So, one, two member from their team to member from our team. So we were the first one to introduce, We go with them. So, scenarios that there go series is going to fail. So two members from HDMI sat together, we decided that we are going to do and then experimentation. We did Poc was successful. That is when To, you know, on the higher level how the whole Docker and Kate is going to work. I think they hired one consultant and then they talked the senior members of Devops team talk to those consultant and they got the idea and their mind and then we developed it. So when is a consultant these are external consultants. Okay, outside like this. So like what are the areas where you work with external consultants like like you know, this this example that you skip to in short, I would say the new technologies that at this point in time our team is not aware of So AI team usually are okay with you know, experimenting with new technologies because the experimenting stays with them and as and when it finalized it goes out but these Devops team. Some, all of them might not know about Kate, or what are the security risk involved and how the connection is going to happen. So that's when they It is a gate like Kubernetes. Yeah. Which are like. So like when you said, when you said like this dockerization, now, like the models are going to Kubernetes. Yes, still. Now we are the best. We are in production. We are still at Docker. We haven't gone to Kate now. So Docker plus nomad. We have already worked nomad is our, let's say you can say load balancer now, but still don't balancing is happening manually. So now we are moving to K8 Cuban. It is, is that is where the experimentation at this point in time going on. So for example they shift to Kubernetes right? Who internally? I understand that you're working with consultants like Is this like a large team of consultants that you're working with like basically of this project basically? So it is a two people, two consultants. And to one or two, not more than that. I've always seen two people in the call. I see, okay. Got it and like, you know, from internally who would be driving like, you know, this this engagement about Kubernetes because somebody needs to understand Kubernetes to be will drive this even with consultants, right? So they they are they are our devops heads. I mean their leads. Okay, but Understood by the way, we are a Is. little bit over time, a new like, but this like you this college being very very helpful, I think you're being super nice and helping us with this in this journey. Is it okay? If like if we extended by five or ten more minutes? And you give me a second, let me just check Yeah, I want to make sure that we are not over running into any other few meetings. No, they won't be any meetings. I keep I don't keep meetings in first time because I have to work but Okay. something should not ping, that's it. Yeah. One second, let me log in. But none of my seizures are pink juniors at the beach. Yeah, okay. Yeah, just question. Thanks. I know. So is the Kubernetes movement happening only for like the ML team for Merlin? Or is it like for the entire world? Like the other software etc? Are they already doing Kubernetes? Are you aware or is this effort just for like the It is going to be for all all the team. They are completely going to move from nomad to gate. Both but starting has to be from. Like we are the one who would need it first. And so we are moving Merlin team at that point. Merlin team. Specifically my team is moving invoice team. Slowly, we will pull in others. So now it's like how they care cluster, I'm going to communicate with outside VMs that also has been finalized, just that slowly. You move every every other team into Kate. Make sense. That's the end goal. Yeah, Understood so Annu. Here, like you mentioned about your team being the invoice team like and you never five, five machine learning teams, right? You manage one of the five machine I managed two of them, two of the learning teams is what you're saying. team fully and the other teams, you know, brainstorming calls and discussions. And then after, you know, after they build model and all the architecture discussion is somewhere. I get in what? Got it. So these two teams are invoice and Analytics Team Cognitive Analytics. I say, okay. And there are how many member in Invoice and the analytics team. Invoice. We have eight and analytics. 4 4. Wow. That's a recently large team. Yeah. Understood. Got it. Understood Chinmay. You had any problems on this? Not exactly. I would want to understand like the Like, for this movement and for, like, presenting your requirements. Like Are you orchestrating, like what kind of ML tools would you require, when you like Move to this new Devops and like coordinate with, is it with the external stakeholders or is Thank you. there? Someone internal Devops also driving like this conversation for So it usually all the the driving factor has to happen from our end and then we have to convince their head. I mean Devops head their director team. Okay it is needed so we there's some they're meetings called. Yeah I'd be. Where are VPs and my director, I Devops director and their senior person since we sit and we see, can you solve this? This is my requirement, let's say, multitenant models, I want, can you solve this with your existing pipelines? If you're able to solve well in good, otherwise, we'll have to move to the next technology. Okay, so personally, I've been pushing it for last three years. Like, I we should go move to Docker, Docker Docker, but then since it was not needed, every problem would get solved with Go CD, whichever pipelines that we have, but now it's time, so it's like a long process of hand, hammering them. Okay, now we should shift to doctor. We are staying back, back in technology and all that. So now it was the need. So my car, my work is to Annette Wala. Bucky they don't know, I won't be able to do all the dog all the. They have to make on the release Yeah. pipelines and downtime really easy release. But the fine ones are final call is taken, we obviously work with them, you know, they cannot, they don't know our Python code. So we dockerize them. We give them the docker file. And then all that work happens in collaboration. Understood. Yeah, if you were to get one of these like, you know, model management tools, right? You mentioned that model version management. Would you just get a tool by yourself but once again, one second, let me or would you just Yeah. Yeah. My maid is very protective, she doesn't leave until I lock the house. Oh nice. That's good by the where in Bangalore Might feel. do you live or no? White field. Okay. Actually, just just one one side note on that. So as I mentioned that I'm generally based in San Francisco, but right now, I happen to be in in Bangalore To. itself. Like, I'm traveling to meet my team, like, so, we are in Kolamangala. If regular while I'm in the city, I would love to have many ways connecting with a lot of data leaders, machine learning leaders. So like if you're up for it we can even meet in person. I'm happy to make my way closer to White Field if that's easy. We can meet up, no problem. Just need to decide the time, that's it. Take a likely sometime next week, but but like I'll try to coordinate with Okay. you whatever time works best, okay? Okay, so I know basically hello. Tom Hardy management. You're talking about? So just wanted to ask about like a model management. Let's say If you want to adopt a tool like improvise this side of the pipeline as well. So how would that process look like? Would it is it something that you can just introduce within your team or you will have to work with your devops team? What would happen here? It will depend on how much it falls into our. It is a it has to be both because you know one side push the model to a certain location they have their integration and scripts ready to get the model to required VM after release. So it has to be both and There's something that I can pitch in and drive, I mean, like, introduce and go and talk to people around. I mean, I've been in longing teams, so they listen, probably. But then they better, it will be finally used or not. It's a call that my league, my head, and my VP, and their VP will take. David. Okay, again, this would I see a chato like that? There is a VP at the level of all the five machine learning teams, basically, like somebody who manages the entire 5 machine learning teams. So there's no vp percent. There are directors five, there are five teams and there are three direct directors per se, and then above our director, we have a senior VP and then our CEO, Oh, so this director is senior. Director is still within Merlin. Basically like it. So, my team has so there are three directors of managing, five teams, and then, and then then I am the senior results. And after that, there are all people reporting to us. Understood. Okay, got it. And I don't like you. Right, it doesn't. I mean, it's just for the sake of, you know, who approve, your attendance, and all otherwise, it's one. Huh, I feel good. Somebody go. I know hejo to consultants that you are working with. You are working with them before you joined or like it's a recent phenomena. Consultation, I'm not sure if before, I don't think. So, before we need it consultant, because we are the one who started giving new things. I'm not sure it was before but then when we moved to from Cicd, when we moved to go CD, is there when we needed a consultant, he used to talk to us. And then for Dockerization as well, there was one consultant who trained, the Devops team on drization things, but we all knew, I mean, yeah, Python authorization we know but how they may win and Java and all the other, and all the UI one, JavaScript, and hold. All those would be authorized for that. The consultant was needed. And then for now Kate, another set of consultants are needed. So, I have seen in last four years, for three and a half years, three, three, consultants, six. People you can say, Oh, these consultants are basically embedded within your team itself. No, they come from outside, they talk and then they leave, they are on. I mean, I think hourly basis they hire Oh, I see. These are like, hourly basis size consultants. Caught it. Okay and desire decision. That the devopsy makes you like right now. I just want to hire with this one. Hire this one etc. Haha, if they are capable, if they have that. You know, they have that knowledge and somebody is ready to take that up. And if the timelines are not strict, probably they don't higher up. Otherwise it Got it. Okay. Understood, Okay. I know I think this is very helpful. I think we've taken a lot of your time to be. I will try to wrap up this call as well, but a couple of things anuto is abena. We are building a product in this domain, okay? And one, other things that I wanted to do was kind of like, you know, get some feedback from you on the product as well together, that's number one. Number two, you mentioned a few problem areas that you all are trying to solve take care, I would love to go a little bit deeper into that as well. Okay, and number three, just like in, also, median person and connect while I'm in the city, right? So if maybe like, you know, we can try to set up some time next week, where we can meet in person and like, in a block, whatever one and a half hours where we can just go deeper into this fine to meet in a cafe, find to meet in in the office area. Whatever works out best for you. Anything is fine. And then we can we can figure something out. Ticket. So what would be the best way to coordinate this on this annu? Like would it be like Do you use whatsapp? Can we coordinate on Whatsapp? Also. He can. So if you ping me on Whatsapp number here, on the chart, I will follow up with create a group, which in my, and a couple of other folks on the team and then we'll set up a meeting next week, Hannah, If you like we lost or no, Think it okay. I think there's some idea. Yeah. Okay. I think Yeah, your video was frozen but like No, hello. I've just messaged, I got the number. Yeah. So, we might plan. Take care. Okay, I know. Hello. Next week, your planning. Happen next week I'm here though. Next week if you are available we can meet. But that next week I think there is all the holy. I don't know that changes things for you. It does not change things for me. We are planning to go home. I'll let you know. Okay. To take up you, please let me know. Of course, like whatever you prefer like next week or either next one next week is also okay, by the way. Okay. But just, you know, whatever works out. If it's not next week, then it would be. After a month, we may have been Okay. planning to go back to Kerala. So likely I might just go back if it's a month. So like I think if we can coordinate something before, actually, if you can tell me like in a whatever like even Monday would work. Whatever. Like you just tell me whatever date works if you know tentatively you can tell me now so that we can plan it out. Need to check you just planing to go this weekend itself because we just have to you know, drive to Kerala. It's not a problem. We'll finish we'll finalize and then I'll let you know. We are yet to finalize. Take it by the way, would it's tomorrow an option? Tomorrow this week not possible because this weekend is that production release. So you need to you know work with the team itself. I'll let you know if at all it is possible. Otherwise anyways, we can connect on Google meet, not a problem anywhere in India. Yeah. Take okay. Sounds good. Take care and please let me know corner with you on Whatsapp ticket. Okay. Okay. Very nice meeting. You know, thank you so much. Thanks a lot.