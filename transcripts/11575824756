Hi, Alex. Good afternoon. I'm good, how are you doing? Well, nice to meet you. Very nice to meet you. Where are you based? So I'm in Fenton, Michigan, which is about 40 minutes north of Ann Arbor. That's where Ground I think headquarters is now. Officially remote now, so I'm not sure we have official headquarters. I'm sure we have a PO. Box somewhere. How about you, too? I'm generally based in San Francisco. Okay. Right now I'm traveling to India. Oh, yeah? You're at the airport? Like, mid travel? No, I'm in one of the cities called Udaypur. Oh, okay. Yeah. Great to connect with you. Thanks a lot for taking time. Yeah, for sure. Definitely interested in hearing about the platform. Yeah. Just a little bit about myself, I guess. I'm a data scientist here at ground speed. Do a little ML engineering, also a little ML apps. Also kind of in all three camps, really. Just end to end sort of model development, conceive it, develop it, deploy it, monitor it. We use some stuff like ML flow in the past, like limited scope. And I kind of get the ML flow vibe a little bit off of a True Foundry. So I guess just excited to hear what's being offered. If it could be framed from how it deviates from ML flow, maybe, or how it's similar, how it deviates. That'd be a great framing for me personally, just because I'm used to ML flow. Sure. Absolutely. I can give you some context on what Truefoundry is doing compared to ML flow. Right. So can certainly do that. Let me maybe start with a brief introduction about myself. Yeah. And then I can tell you a couple of things. Just an overview of True Foundry and how it differentiates from memory flow and all. And after that, today, the majority of the call I would love to spend talking to you about the type of data science problems that you are trying to solve. What's the machine learning operation Stack that you have built out over the last few years that you have spent at Groundspeed? How is the team organized and structured and what are the next few challenges that you are trying to solve? I'd love to spend some time understanding trying to understand that. And in that context, after hearing what we are building a True Foundry and what problems you are trying to solve, if you feel like there's, like, relevance, then maybe we can set up, like, a follow up call where we can do a demo that's personalized to the existing problems that you have. Does that sound a reasonable plan? Yeah, sounds like a good plan. Cool. So a little bit about myself. I come from a machine learning background myself. I used to work at Facebook where I led one of their conversational AI teams. Have you used heard of a product called Portal? Portal? I don't think so. You know about Alex or Google Home. Yes. So Portal is like an equivalent voice assistant device, virtual assistant device that comes with a video calling functionality. So that's the device that I was working on before they launched it. Prior to Facebook, I spent three and a half years at a startup called Reflection where I built out a lot of recommended systems for the ecommerce industry. So there I got a chance to pretty much do what you're doing now, where take a hat of data scientists and build the models. Take a hat of an ML engineer and deploy the models, and then take a hat of an ML ops engineer and build out a horizontal platform for the company, basically. And between Facebook and the starting True Foundry, I did one more start up in the HR tech space that got acquired by the largest HR tech player in India called Info Age. So that's a little bit about my background. And at True Foundry, our goal is to basically build a platform that acts or supports the internal platform teams within companies, right? Internal ML platform teams within companies help them operationalize their machine learning models from training to deployment to monitoring. That's the part of the stack that we are focusing on now coming towards ML Flow. So the way I think about ML Flow is it's actually very useful. Like ML Flow's core strength is in experiment tracking and project packaging, right? So the core strength is in when you're doing the prototyping of your models, you track all your metrics in one place. You put those models in a model registry, right? I think that's where the core strength of ML Flow lies. They have a little bit of a support of directly deploying a model as an endpoint, but that's not like a flagship feature yet, more like yeah, it exists as well kind of thing. For us it's the exact opposite. So we do support some level of experiment tracking where you can track your metrics, track your experiments and prototypes and all of that in one place. But we focus as the core on the deployment bit of it, which is you can actually run real training jobs. And by the way, we manage infra. So like ML Flow is a Akshay, Truefoundry is a platform as a service, right? So we actually manage your infrastructure. So we will actually manage an entire Kubernetes cluster where you can run your training jobs, we manage clusters where you can deploy your model as an endpoint. So we do all of that in from management as well. And then we take it all the way to the inference monitoring side of things, right? That is, once you deploy model to production, how do you make sure that the model is working well and stuff like that? So from that perspective, actually we are fairly different from very different from ML Flow. Basically. A better comparison then after hearing that is probably more like data bricks or something more managed services that leverages something like ML flow as the centralized landing pad. Right? So I think data bricks would be a little bit closer than ML flow itself. Right, but actually, even with data bricks, what ends up happening is data bricks started by Spark folks, right? And by the way, data bricks is super close to me because like, I was at Berkeley, so these folks who started Databricks are practically the same lab and stuff, right? So Databricks, it's built on top of Spark, right. So it really excels in your data pipelining jobs, model training jobs as well. But I think once you get to the online inferencing side of things, I think that's not necessarily a core strength of database, but they're like excellent in your data management and your job management. For us, we have a core focus on job management as well, but we really treat online services as a first class citizen. So if you really had to think about the closest analog to what we are building, a truefoundry it would be something like a Sage Maker from AWS or Vertex AI from GCP, that would be the closest analog of what we are building. Okay, yeah, I would love to learn a little bit. Maybe I'll start with asking this one question, which is a unique thing that I noticed in your profile is you have actually played a role of a data analyst, ML engineer, and a data scientist at ground speed. So what's the role difference and how is the team organized between across these three role titles? Right? Like is there generally a data science team and engineering team or how is that structured? Yeah, data analyst was just entry level that I wasn't in for too long, but there was a lot of shift going on because that was early stages of the company. So there's probably only three different job titles at that time, so that was just an artifact of that. But as far as the ML engineering and data science goes, currently we have a healthy ML engineering team. I think that's probably like eight people on a couple of seniors and then MIDN level ML engineers. And those teams are focused on supporting our document processing models. So we have two families of models that live in our pipeline. What lives earlier up is taking these insurance documents, doing OCR and then doing named entity recognition or different ways of labeling the data points in these documents. And then after that we have enhancement models. So say that something was missing from the actual data found in the document, maybe like the coverage type of the insurance policy was missing. We have models to sort of impute that type of stuff and those are data science concerns, the sort of enhancement stuff, whereas the name entity recognition and the extraction using various models from the documents as our ML engineering team. So that's sort of how that's set up. If that answers your question. Got it. The ML engineering team is focusing on the OCR and Er type of models, and then the Data Science team is working on enhancements. But actually, I did not quite understand that. What kind of enhancements are we talking about here? Yeah, so a big one is just imputing things missing from the presentation of the document that we ingested. So if someone sends us a bunch of insurance claims, for example, and depending on the client, it's coming from the presentations very drastically. So there could be a field in that document that is Coverage type, or it could be something very similar to Coverage type, or we could think Coverage type is just completely missing from this page, and it's something our clients would like to see. So we take all the other data points on the page, all tabular at this point because it's post extraction, and then train models to impute these various fields using the context of the other fields. So, for example, there's a field called AI Transcription that appears in claims a lot, which is a verbose description of the accident. So based on the reading of that, you can infer, okay, this is probably an automotive claim, or this is a general liability or something like that. So it's basically an enrichment on top of the broad data that was extracted. Understood. Okay. And this is the data science role. On top of that is like different, like, regression models for trying to predict claim outcomes and things like that, but basically things on top of what could be extracted from the documents. I see. And how large is the Data Science team? So it was larger. Right now, it's just one person. It's me. Oh, I see. Okay, understood. Got it. So I'm curious, why is there this, for example, these Data Science models that you're building? Does it get deployed to the end client like this filling things? Or is it more like an internal batch processing job that will fill in the missing data? How is this consumed? The way it works, like, operationally, is we have all these SLA contracts with our clients, what we call flow. And these SLA's are like under an hour. So they'll send in a few documents, and then these documents will have the data points extracted. And this is all like in an event driven system we have in Kubernetes and also using AWS step functions and lambda. So it's very much like an online sort of flow environment where the document comes in, goes through the OCR part of the pipeline that notifies the next part of the pipeline to pick up those results to extraction. Once the extraction is done, then the enhancement pipeline picks it up, runs these various models, which are all hosted as services on Kubernetes. We use like SQS and SNS to kick off those events so that the data is ready. Perform your function on the data, publish your results back up to the redis queue, and then once at the end of the pipeline, that's turned into, like, an extract, which is based like a CSV, and that's what kind of gets sent to our clients. I see. Okay. What did I see? So basically this is actually part of the actual, like, the end deliverable to the client, basically, these imputation models that we're talking about. I see. In that case, is it hosted as, like, an API endpoint or does it still run as a bad job? Yeah, it's an API endpoint because the clients have a few different options for getting their materials to us. There is an API endpoint and then there's various email triggered. I'm not exactly sure how they work, but like, an email gets sent to an address and then the attachments get scooped out of that email. But it's very much like an automated process where there's some sort of API in all cases. Got it. So you mentioned about AWS. Is that the only cloud that Groundspeed currently uses or there are multiple clouds that are getting used? We're working on getting away from GCP. We still have a foot in that for a few things, I think, maybe just OCR leveraging their Doc, AI, Google Vision. But AWS is our primary thing. A lot of our services are in Kubernetes, and then things that aren't in Kubernetes exist as lambda or step functions. But the models I'm talking about specifically usually are hosted as services on Kubernetes. I see. Okay. And who manages this? Is it do you yourself manage it or the ML engineering team has set up something like the entire Kubernetes management. Right. Say that again. Who manages these models? I guess it depends on the definition of manage. Like, the environment is all managed by DevOps. We have a helm repository where we have all these helm charts, and there'll be like a helm chart template for these enrichment models because they're all fairly similar. So we'll have a template for a helm chart to just deploy one of those models. And that's all managed by DevOps. I guess you could say they manage the deployment process and the infrastructure, but that's it. The Data Science Engineering team is actually making the PRS that then go into that home repo. Oh, I see. So basically the models that you're building is each model eventually deployed as a separate endpoint and then hosted on top of Kubernetes? Is that how it is working? Yes. Got it. I see. And are you all using any containerization, like dockerization of every model or something? Yeah, everything's containerized and then deployed on Kubernetes. Okay, understood. This entire pipeline of containerizing, the model, creating the hemp charts, et cetera, still falls within the scope of the Data Science Family team? Basically. Yes. So we're working towards a place where, for example, one thing we've been doing recently I've been doing recently is developing my models as we use an internal pipe repository for certain things. So just having the code of the models be like a pipi package and then those get installed by sort of like a service template basically that just installs them and knows there's a predict method for this library makes a call. So it's kind of similar to what you were trying to get to somewhere where it's similar to kind of what you're describing for ML flow, but where they have this sort of loose definition of a deployment and you can kind of package it there. But yeah, we're trying to find a way to get rid of these pain points where we just have to concern ourselves with here's our model, it works, we tested it. Now we just want to wrap it and then deploy it and have as few sort of like pain points and touch points along that path as possible. Got it. What are these pain points that we are talking about here basically today? I guess once you build a model, how long does it take to deploy it and I guess what pain points do you experience in that process? I guess yeah. So the biggest pain point was, and this has been sort of alleviated by this pipe approach, but the biggest pain was what we used to do is develop the model. Data Science team would have a notebook. We would develop this model using Sklearn. Here's our SK. Learn pipeline. We fit the model, we evaluate the model and then we would use to just hand it over to another team and they would take that and deploy it. But what that ended up meaning was ripping code out of the notebook and translating it into PY files and then putting that code into the service. So we tried to put as much of that code into objects as possible, like the fitted objects. So like trying to pickle a whole class which corresponds to the model so that they would just have to unpickle that and then incorporate that into this SQS service we are using that we call that the model and deployment. So that's the pain point I would say is getting from we're developing the model locally, like in a notebook or whatever, and then getting that wrapped up into this actual service that gets deployed to Kubernetes. And a lot of this is probably maybe not sound quite right, but a lot of this has to do with the fact that there was, like, this infrastructure put in place for delivering, like, typical software stacks or typical applications that we sort of I don't know if it was the right fit for ML models, but we tried to shoehorn it into that sort of paradigm where we have these services deployed to Kubernetes and then that was causing these sort of pain points. So I think that's kind of where the pain comes from is the fact that we were taking this approach that was being used for other types of applications and trying to deploy ML models using this paradigm. Got it. I see. And this approach that you described about, you have these models in Jupiter notebooks that you want to package and deploy on Kubernetes. You would hand off the Jupyter notebook to someone who would create like a PY file and actually understand the code and stuff like that. Right. So how long was that process taking and how long is it taking now that you have switched to this Python approach? Basically like the library approach. Yeah. So the problem with the previous approach was just it would get done and it wouldn't take too long, but then we would run into all sorts of problems because of the different environment. So assumptions were made in the notebook that were probably no longer necessary true. In the production or development environment. So the way the preprocessing functions were, there's differences that then had to be diagnosed by us. Eventually we would throw it over the wall and then have to go back over the wall and figure out what was wrong after they tried it. So we figured the best way is to handle all the logic stuff, the preprocessing of the data, the inference, any post processing on the inference that needs to be done, have a very clean API that we can then interact with these teams that are responsible for deploying it. So now that we have this sort of pie pi approach, it's a lot faster. Because once we ensure that this library is working and returning the correct predictions and stuff, and then we put it on this internal pipe repository, then it's just a matter of rebuilding the service image, because now it's just pulling the new version of the model, installing it, running some tests and then deploying that. But the main, the difficult part was repackaging the model from one environment to the other. So there was some background noise, no problem. Understood. So when you say that you're creating this PyPy repository, are you also packaging the model itself or is it like the post processing logic and then model reside somewhere else? Yeah, so you're right. It's sort of the like we take in the input data, the pre processing, post processing, but then the actual fitted model. So right now those are being stored using the ML Flow model registry, but we just use those to sort of publish up to S Three. And then once the model is being built in this sorry, the pipe repository holds the source code, which is the preprocessing steps, post processing steps, and then the actual source of the model class, we'll say that has like, the predict method and everything. The artifacts are being stored on ML flow using the S three back end. When the model service is being built, which imports the library. At that point, while building that docker image, we reach out to S Three and pull in the fitted objects that then get baked into the final service image. I see. All right. I see. And, like, if you had to change the model version at this point, what would be the processes for doing that? Or if you had to create a new feature or change some feature which requires a change in the PyPy version, et cetera, how would you test that out? Basically, like, these two things I'm very curious to understand. Yeah. Then I would just go to the repository on GitHub where it contains this library. We'll say model library. Yeah. I would just go into that model library, make my changes, do my tests just on that library. And then we have a Jenkins CI CD pipeline. So once I merge my changes for that library into main, it'll just automatically redeploy that pipe. I package up, but the build for the actual service is pinned to a specific version. So I'd have to then rebuild. That. Service after updating the library version. Right. So it'd be two builds. Yeah. That makes sense. And how about updating the model version? You know what I mean? You might have a separate training pipeline. Right? Right now we're talking about the actual inference pipeline. Yes. So you're saying if I wanted to train a new model with that process? Looks like, yeah. So let me give an example. So imagine that you have one model that keeps getting retrained with new set of data coming in, basically, right? The model remains the same, the hyper parameters remains the same. It's just new data, new version of the model. So nothing in your code is changing, but you're getting upgraded version of the model every single day, let's say. So how do you make sure that you're using the new version of the model? Do you handle it? Using ML Flow latest or something? Oh, no, we don't update our models on that sort of cadence. It's less often than that because before we want to deploy a new model, we don't have annotated data sets. We don't have new annotated data sets always flowing in. We have to make an effort to decide what records we want to annotate and then retrain the model on that and see if there's an improvement relative to our previous evaluations. And then we would consider updating the model. But it's definitely kind of flipped in that regard. I see what you mean. Okay, this is actually an important question. How many models would you get are currently in production, and how frequently are these models updated? So there's a lot so they're not updated very frequently. And there's two reasons for that. One is we need to improve our monitoring, our sort of like, unsupervised monitoring to get an idea of when we're experiencing drift and things like that. But right now, we basically only retrain things when we know there's an issue, and a lot of that is driven by client feedback, unfortunately, at this point. So right now, we don't retrain things unless we see that there's an issue just because of our limitations. Currently with just capacity. But as far as how many models, I'd say somewhere between ten and 20, depending on how you define them. A lot of them are like the same model, but fine tuned on specific cases and stuff like that. But somewhere between around 20, I would say. I see. Okay. And do you end up using primarily, like, the classic ML types models like Cyclone and XGBoost, etc. Or do you end up using a lot of Pythons TensorFlow type of models? For a lot of our stuff right now, it's just ScikitLearn and then, like, Heuristic stuff, too. For example, our ML engineering team to extract some stuff out of some documents. These are something called Sensible, which is almost like an API with a sort of high level regex language around it, where you basically kind of just very descriptive and like, okay, if this word is to the left of this word, you label it as this word sort of thing. But yeah, we're experimenting with some stuff, like doc, AI question answering models and some stuff like that. That's more deep learning. But, yeah, for right now, it's all classical ML. Got it? Okay, understood. And then I guess one thing I'm very curious about is what are some of the current top challenges that you're trying to solve? Put in other words, why is this call interesting to you? Sure, yeah. So we've tried a few different things, and it's just hard to ML flow, and some people might use it personally, but we struggled to get because it's not, like, centrally supported. It's hard to you can't just call somebody and ask for ML flow support. There's plenty in the community and stuff. So we are definitely looking for more of, like a managed, more holistic platform for that reason. But I like MFL because I like the centralized logging as opposed to something like DVC, where everything's stuffed into your GitHub repositories and it's very distributed across all that. I like the idea of central logging. And then we definitely need some sort of solution for monitoring. And that seemed kind of from where I forget where I read this, but I thought your platform was mostly focused on post deployment and the monitoring stuff. I mean, it seems like you do a lot of stuff. There also. Model pipeline. What's that we call it post model pipeline. Okay. So we focus on our once you have built a model, like, on a jupyter notebook, how do you deploy that? How do you monitor that? That's the part that we focus on the most. Yeah, I think that's we've kind of found solutions to that. But, yeah, we're sort of looking for the holistic solution, but we want to find the right workflow to fall into sort of for all these things. And it seems like this platform kind of hits each of those points we're interested in. Got it. I see. So here you mentioned about. I think. The idea here is that you tried ML Flow and because ML Flow was not managed, is that why you face the problem? That was part of the problem. Just because then we had to lean on, like, DevOps and other parts of the company that are also super busy and then it's just hard to yeah, just lack of support, I guess, because I guess when you go to something like that, you're assuming someone internally is going to support you, right? That makes sense. That makes sense. And then you also mentioned about monitoring solutions. Right. Have you seen any monitoring solutions like ML? I'm assuming here you mean by ML model monitoring and not like the data dog type? Not like data dog. We want, I think, weights and biases maybe does some of this, but like, we want to get good indications of, like, drift in our confidence scores. We want to know we want outlier detection on what we're sending to the model, what we're getting out of the model, ways to sort of empower that sort of model monitoring, getting indications of like, oh, we're starting to see things that are considering outliers. Maybe we want to get proactive in retraining this model and target some of that data so that we can extend to it before getting the ticket from a client saying, we're starting to see poor results on this new segment we've been sending you. Got it. I see. Understood. How, like, if you had to rate these two problems, right? One is your workflow for deployment, right? Like, how do you get it from your prototyping to your end deployment stage? And then you think about your monitoring pipeline. While I understand that these two are not really separate, like, there is some interconnection between the two, but like, your model monitoring, if you had to rate the current problem intensity on a scale of one to 10 for each, what would that number be? Though? The monitoring stuff definitely is more of an issue just because we barely have any sort of solution in place for that. We're very reactive at this point. So I would say definitely that's a ten. And then the deployment stuff that's slowly getting better as we go to this as we separate concerns more. So I'd say that's four, probably. Okay, I see. Okay, understood. Got it. If monitoring itself is like, ten or ten right now, are you actively evaluating other platforms right now to figure out which monitoring solution might work best? What are you looking for? Let's say if I gave you a magic wand, that okay, get whatever you want for monitoring, what would you pick? One thing I liked about ML Flow is or one thing I didn't like about ML flow? I saw this addressed on your website somewhere, but you could publish all these, like, atomic or scalar metrics to. ML flow, and they supposedly had this sort of way you could dive into those metrics and group them and basically perform analysis on top of them. And I found that very limited, and I think this is even said directly on your website, but you don't need to publish the PNG image of your confusion matrix or classification report. You can just back that stuff out from the atomic data points. So I like that a lot, not having because what we found with ML flow is that we wanted to publish this new type of metric or do this new sort of analysis that involved coding that up so that now the model is publishing that every time and then having to distribute that to all the models. So it would have been nice just to be able to just dump all this atomic raw prediction information, then be able to do analysis on top of that. But I liked the API sort of there and how dumping the Maxwell and stuff actually worked. I just didn't like the level at which you could do the analysis. So that's one thing I would say. And then even just further, I guess, just more analysis on top of just atomic data points. Then if you could do outlier detection on your ins and outs from there and that sort of stuff, I guess maybe that's approaching probably a new model that you would develop internally on top of that. But I guess that's sort of the direction I would I'm hoping to go towards. All right, I see. So how soon are you looking to solve this problem that we're talking about, which is like an outlier detection and just being able to do more analysis on your model predictions? Yeah, so there was a pretty big push. We started to get concerned about this earlier this year and then some priorities shifted and I think we're targeting like Q One next year as we're really looking to evaluate how to move forward with this and then start putting in solutions towards the end of Q One, I think. So we're just now sort of picking back up on looking at what it's out there as far as more managed services go and Vishal had recommended. We speak to you guys. Got it. Okay, for sure, yeah. And what's the general philosophy at ground speed? Do you all actually end up using a bunch of external tools to solve for these problems or the tendencies typically to build some things in house? What's been your observation there? Yeah, it's a lot of in house stuff, especially for the ML teams, just because it's for historic reasons, I guess, but these deployment systems existed for the standard software stack, and then we found ways to get the ML tools also deployed in that fashion. Okay, understood. Which is what I'm hearing as well. Based on the stack that you described, it sounds like the team actually intends to build a lot of solutions in house. Right. So why for monitoring specifically, like model monitoring specifically, you are currently looking for solutions outside. Is there any specific problem that was harder to solve internally? No, I don't know that we've, I guess necessarily tried to solve this internally. But I guess the assumption was that there are probably great tools for this specifically out there, so that maybe we shouldn't focus at first anyway on developing an internal tool for this. Got it. I see. Okay, understood. And one other thing is even in model monitoring, you can think of it as in two different directions, right? One is where you mentioned about being able to compute things like confusion matrix from your raw data, right? Like model predictions and actuals that you are logging. And then the other direction could be that you figure out and by the way, which is an analysis on the output of the model. Right. So you have model predictions, you have actuals and you compare that and you do something there. But then you could also do things like drift tracking, which is frequently done on the input of the model, which is the features, are your features drifting and stuff like that. Right. And then there could also be just a general model performance monitoring. So like tracking your F one score of the model over a period of time, or accuracy, precision recall, whatever you want to track. Right. Between these three, is there a certain thing that catches more of your attention right now? I guess they all do, but for the same reason we want to get out in front of drift and not being reactive to clients in any way, we can sort of detect that signal that's occurring or about to occur. That's where my interests lie. Immediately tracking performance is definitely important, but we don't have a great way to we need to implement some sort of like we have human in the loop for some things, but for these model predictions, these specific ones, we don't so we don't have a regular feedback loop. We can't track our accuracy enough one scores unless we do these sort of batch audits every so often to sort of get a gauge on this. So that's why I thought the next best thing I guess is to sort of look for these outlier detection and the input signal and that sort of thing so that you can sort of get this secondary indication that this is occurring. So I guess that's why I was bringing that one up. But yeah, a lot of this has to do with probably our processes and needing something I've been asking for, but this sort of feedback loop for that to track performance. Got it. Understood. Okay. So I think this gives me a pretty good understanding of the major problem that you're trying to solve. And it sounds like in the absence of the label data set frequently, like the direction that the model monitoring would go into would be the unsupervised model monitoring as you initially called out, that is being able to track your feature drifts, maybe concept drift, maybe using model predictions and stuff like that. And outlier detection, of course. Right. So that's the direction in which things can go. And also some specific focus on just kind of solving the scaler, only logging for ML flow. Right, like solving for that and being able to visualize more like custom vector features or graphs and plots and stuff like that. So that's like the primary focus area, maybe as a next step like some deployment workflows that if you see something interesting, you may want to optimize for that. But that's not like a burning problem right now. Right. My summary of the discussion, is that a fair understanding, Alex? Yeah, I think so, for sure. One thing, this is a little out there also. But getting back to the lack of annotated data and that feedback loop, one thing this sort of outlier detection is part of this actually is. But we want to get into sort of an active learning workflow where we're able to identify with some confidence, like the records we should be annotating so that we can get those in front of an annotator. And then we have a way to label the high value records that we think are going to be most important for improving the model performance going forward. So that's probably a use case on top of being able to identify the records that are more outliers, but just wanted to throw that in completely. Makes sense. Yeah, understood. Okay, I think this sounds great. We have a couple more minutes in the call right now, so I just wanted to use this time to kind of figure out the next steps. Alex, should we set up a follow up call where we can discuss more about this entire monitoring side of things and let you see a demo of the platform? I'll probably try to COVID a little bit about the deployment as well just in case it adds value to your discussion sometime next week. Yeah, that'd be great. I meant to try this out earlier, but do you have any software that is demoable? I don't know if you have an open source component to your stuff or anything like that, but is there any way I could just try it? Yes, for sure. So after I show you the demo of the platform, what I will do is I will actually create an account for you that I can share with you and I will share a link to our documentation so you're welcome to try out the platform as well. So I will certainly do that. Yeah, that'd be great. Yeah, I think a lot of this too is us just finding something we think works and then just running with it for a bit. Totally. Yeah. A lot of the software is like that so completely relatable to what you're saying. As an engineer, I understand that concern. So absolutely, you should absolutely try it out. And we as a startup always love it when people want to try it out. Try out the platform hands on, because it works out best for us. In both cases. People love it, they want to use it. Great, we got a new user of the platform. If people hate it, like something is not working, they give us good feedback and we improve the platform. So either way it works out very well. So what time should we set up a call next week? Like, which day of the week works best for you? Let's see here. Monday or Tuesday would work. Okay. Anytime after 01:00 P.m on Eastern time. So that might be 10:00 A.m. There. Yeah. So I can do Tuesday. Tuesday, 10:00 a.m PST. Works well for me. Can we set up a call of 45 minutes for Tuesday then? Yeah, that works. Cool. Awesome. I'll send out an invite to you and I look forward to connecting then. Yes, sounds good. Enjoyed this. Thank you. Yeah. Really fun chatting with you, Alex. Talk to you on Tuesday. Bye. See you. Bye.