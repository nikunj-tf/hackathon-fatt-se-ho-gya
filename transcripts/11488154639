Okay. Hi, Visheik. How are you doing? Well. How about yourself? I'm assuming? Good. It's going well. Good. I'm sorry, where are you? I'm out of Boston. Massachusetts? Yeah. Yes, it is. It's getting cold, but it's raining too much, so we're not getting snow yet. So it's basically like one of the worst times of year. How's the weather out there? This is good. I am now in India currently. Nice. This is like Bay Area weather here, so it's good. Oh, awesome. That sounds great. It's the same weather throughout the year. Nice. Yeah. I used to live in New York earlier. Pretty much the same weather as Boston, I guess. Yeah. My sister lives up there now and she's experiencing the same thing. So we're just getting through this kind of nasty weather time and waiting for winter and spring. Okay. Yeah. Cool. Awesome. So we were just getting introduced, if you can give a short introduction. Yes, Jonathan. About me. I graduated from college in India in 2013, computer Science, and then worked in Facebook for around five and a half years, worked on different teams there. Like the Distributed Caching System team was leading a mobile performance team and was eventually leading the business organization there. And then early 2020, I got to my job, traveled around for a bit around six months, and then finally started. We did another startup before this, which was on the HR tech domain, but currently we're working on Truefoundry, so that's roughly about myself. Nice, that sounds great. Yeah. So a little bit about me. So I've been working in the industry now for about six and a half, seven years, so hi. Actually. So I started at Wayfare, located here in Boston, Massachusetts. They're an ecommerce furniture company. I started on their marketing engineering department, which kind of got me in the machine learning space and the data science space. So I worked on marketing engineering for about two and a half years. I moved to the Wayfarers Python platform team about halfway through my tenure and started supporting Python at Wayfarer. And then about four months ago now, I switched to CarGurus as their principal machine learning engineer to try to help their data science team remove some of the barriers that they're facing to getting their data science models in production and actually used by stakeholders and some of the process and infrastructure they need to be able to achieve that. I see. It's interesting. Jonathan, what does marketing engineering do? Is it like deciding strategies? Which marketing strategy will outperform each other or something like that? Kind of, yeah. The marketing engineering arm is basically taking all of the users data on the site, everything they do, pumping it through a bunch of ML models, and then determining, well, what ads should we show them and when should we show it to them? So it's like we're optimizing for click through rates, making sure people buy stuff based on if we show them a couch versus like, a chair based on the data that we have and the output from the model, are they clicking through to the couch ad more likely? And then is that converting or do we have less clicks on the chair but more people are converting after that to actually buying products? So that's kind of the domain of marketing engineering at Wayfare. Interesting. Yeah. The trajectory that you mentioned is pretty interesting going from there. Science. Yeah, it's a typical progression for sure. Did you end up learning machine learning? To some extent, like the algorithm side of things? To some extent. I'm still not great on it. Great. At the data science side of things. I'm more about making sure Data is in the right place, making sure that they can train their models, setting up the infrastructure around that. Even. Right now, I would say that I was better informed about the data side of things when I was working at Wayfare than I am now. Right now I'm the only machine learning engineer at CarGurus, and so I've got a lot on my plate, so I don't have a lot of time to kind of like, dig into the Data Science portions of things. But I can tell you basically, like, what a linear regression is and things like that, but my knowledge in the Data Science space is fairly minimal. Understood. And how is the team structured, Jonathan? Like you mentioned, there's only one Emily that's you how large is the Data Science team? Are there like multiple teams or it's like one unified team that runs across the hog. Yeah. So at CarGurus, kind of the scale that we're currently at Data Science has only been around for, I think the past like two to four years. So it's a fairly new department within CarGurus. So I have my direct manager. She is also the manager for all of Data Science, and so she's managing me as well. And then I work with one other data scientist who is going to transition into a machine learning engineering role, this 2023. But right now he's doing Data Science stuff and then I work directly with 12343 other data scientists, two principals and one senior and then two Coops who are doing internships with us that ended December. So our department is currently pretty small in terms of Data Science. And yeah, we're looking to grow that. We're going to be hiring one more machine learning engineer. This 2023 quarter as well. Understood. And this is a central Data Science team that serves all the use cases, right? Yes. Okay. Yeah. There's no one else at CarGurus. At Wayfare. It's different at Wayfair depending on kind of like, what sort of information you're looking for that may be useful for truefoundry, I could go into that as well, but whatever you guys want to know. So I wanted to understand, Jonathan, how are you laying out the stack? You're the only person I can imagine like there's a lot of stuff that you need to do. So how are you thinking of planning on doing it? Laying out the stack or making it easier for the data scientists? What is the stack now? So right now we're fully leveraged on AWS platform. So Amazon's platform, the data scientist, before I came in, they started doing a proof of concept of Sage Maker. And so at this point, we've got two of the four models that the data scientists work on, just fully on, boarded on the Sage Maker. And essentially in that whole pipeline, we have our historical data in Snowflake. There is no real time data right now and that's a project that I'm currently working on is to make sure that we can populate that from our event stream. So we're basically going to be using Snow Sage Maker for all of our machine learning, training and ingesting and processing needs going forward. We're going to be serving that with Sage Maker as well. I have mixed feelings about that. I was less than impressed with what Sage Maker provided. I thought it would provide a lot more and I thought it would do a lot more, but it works well enough. There are definitely pain points we ran into, but at least for our use, cases are fairly simple right now. So I don't see any problems going forward with using it. So we're using Sage Makeup for the models right now. We're building out essentially a real time feature store. We're going to be doing that in Flink, Kafka and Kinesis and probably populating to a data store like Redis. In the short term, we're doing some new things that the data scientists want to do, like aggregating features at event time as opposed to at inference time, which is not a typical pattern I've worked with, but I have seen it in the industry. So I'm kind of learning as we go on there. And then a large portion of my time is spent just helping them debug issues with docker, helping them debug issues with out of memory issues when they're using DVC on AWS or anything like that. So yeah, I do a lot of different things. It's really hard to find one common thread of focus and that's what I'm hoping to do is two more machine learning engineers, I'll be able to focus on a few high level projects and be able to bring those two conclusions, but that's what things look like right now. I see you mentioned Jonathan, that you tried Sage Maker and you didn't find it. I'm very impressed with it, so I wanted to get a bit of an idea on what did you not like about Sage Maker? I thought it would do so coming from wayfare we use Google Cloud over there and I was fine with the Google Cloud was great. I have no problems with it. It was easy to use, easy to set up. Amazon's a little bit trickier to set up. I have had some growing pains with getting everything set up and figured out, but onboarding onto it wasn't the worst thing in the world. But the Sage Maker ecosystem in space, I thought they would provide more clearly defined paths. It seems like to me, they essentially give you a wrapper, kind of a wrapper for a docker image or like a wrapper that says you need to expose this endpoint and will be able to swap out models for you that you store in S three. You can specify training pipeline with our Python SDK, but all of this, it's a lot of work. Like, it's a lot of custom code that we've had to write around their APIs to interface with Sage Maker. So like the boto three Python API, we essentially have this package that we wrote that kind of abstracts away a lot of that for our data scientists. And I kind of expected for as far along as Amazon is at this point that they're offering for Sage Maker and machine learning would be more robust and I feel like it's not we are not using Sage Maker Studio, so that may be one of the portions that we're missing out on. And we may find that maybe all of the tools we need are there. But at the time when I came into the company, they had already decided, well, we're not going to use Studio, we're just going to use kind of like Sage Maker and then custom roll our own code. And Amazon has been very, you know, they've been great. We meet with them every we were meeting with them every week to ask questions every two weeks. I guess I'm a little surprised at how much custom code we've had to write to really leverage their system and kind of like how many things aren't as well specified as I would imagine. One that comes to mind is the realtime feature store they have. We looked into it briefly but decided very quickly that it wasn't going to be a good solution for us and it just seems like it's not going to be a good solution for a long time. It's missing a lot of things that I would expect out of, like, well, a real time feature store for data science features. I see. Interesting. And for wayfare. Jonathan, did you guys use Kubernetes on Google Cloud or we're using Vertex AI. Let's see. So we were using Kubernetes, but yeah, we were using Kubernetes on GKE and we put everything with Helm templates and it worked great. Okay, and this time you didn't decide to use Cooper it is because Sage Maker is already there. So yeah, Sage Maker is already there. And once again, I'm only one guy I know well enough as anyone at this point because of my time at Wayfare, but ideal world is we would be using Kubernetes. We would be spinning up something like amazon's version of Kubernetes and just deploying everything to that. But once again, I'm not on the infrastructure side of things that car drivers. And so we're making use of the infrastructure that we have available to us. So right now that just means spinning up Sage Maker instances and resourcing those appropriately. Okay, awesome. And you mentioned that sometimes you also have to solve for the memory outages, et cetera, and that take a lot of your time. What are the usual processes that end up taking a lot of your time when working with Sage Maker and where are you facing the most issues? I just want to understand that. Yeah, so I think probably the largest time sync that I found in my own day to day workflow in helping data scientists is not so much with the production of or the deployment of applications. That seems like a fairly paved path. And you can always get around out of memory issues by resourcing more hardware, even if it's more expensive. Right? And so there's ways to kind of like mitigate that in production, even if they're not the best solution for the time you can get it working. The problem comes in defining good local development for data scientists. Because they work with a lot of data, they need that data to be able to ingest, train, you know, their models. And when there are multiple data scientists with multiple different CPU architectures on their local machines, whether that's Apple Silicon or intel or some other CPU architecture, you need a consistent development environment so you're not bogged down all the time by help request, by like, oh, this docker image is not working because NumPy doesn't support Apple Silicon yet. So a large part of my time when I first came is getting docker running, docker development environments running for my team locally, because everyone was just developing locally on their local machines. Basically the dependencies were all a mess. It was very difficult, there was constant problems. So now we've got our development flow dockerized. So it's like a two second spin up, basically just run, docker, compose, build, builds all the images and a new co op can onboard onto any project basically within half a day. Because they don't have to go through tons of guides to install various things and then have various avenues that they go down that may or may not work for their architecture because it's all dockerized now. But within the data science realm, that does come with some problems. So because they work with so much data, their development can be very slow. I recently went through an issue with one of our data scientists where we dockerized her entire model. She was trying to ingest something like it was like a year or two worth of data to her local machine. I had to debug a bunch of docker out of memory issues. And ultimately what I found is that they just weren't understanding that you need to stream the data to disk. They were trying to load the entire thing into memory and running the out of memory errors. So once we stream their data to disk in their application, we resolved that. But it took a lot of my time to figure that out in the first place. Mostly because I wasn't sure if the issue was coming directly from dockers memory management, which is quite a bit different than I expected, or from some problem with the local machine's memory, which I found out it wasn't happening that way. So I think ultimately most of my time is spent trying to figure out ways to make the data scientist local development faster and stable and consistent. There's a couple of avenues I've explored, like maybe spinning up EC Two instances and then just provisioning those quite well. And everybody SSH is into like an EC Two instance or a box on AWS, but that comes with its own fair share of problems. So for now, everything's dockerized. Everyone's got a local development flow and we're not having the same dependency issues or kind of the same workflow issues that we were before. So you don't use the Sage Maker notebooks or anything like that? They develop locally? Yeah, they all develop locally. They don't use the Sage Maker notebooks yet. So that's something that we're going to be working on in 2023, is like trying to leverage I mean, they use Jupyter notebooks, but not the Sage Maker notebooks. We've not leveraged anything from Studio yet. Okay, so the Jupiter notebooks, but that's on the local. It's not like you hosted a Jupiter hub or something like that in the company that they can go and visit and run, right? Correct. Okay, understood. And not using Studio, is that like an onboarding issue or just like a price consideration? At this point, I think it was. Mostly an onboarding it was a mix of both. One, it was onboarding because the avenue that the data scientists were taking was to try to leverage as little of Sage Maker, like basically as barebone Sage Maker as they could just to get an idea of how it worked. And then when I came on, we started discussing kind of what Studio provides and so they wanted to complete at least one into in model deployment with what they have currently going in Sage Maker, and then we could discuss in 2023 potentially leveraging Studio. Understood, that makes sense. You mentioned that this memory outage and loading the dependency and the larger data set, is that relatively solved now or it's still like an ongoing issue? No, that's solved. Yeah, those are all solved problems at this point. Those are just issues that I ran into when trying to get everyone on because we were wasting so much time just individually pairing with data scientists to try to get dependencies installed on the machine that worked correctly for their chipset. Or we were trying to I was spending too much of my time pairing directly with data scientists to debug individual issues. And so Dockerization seemed to solve all of those. But at least in the past two weeks, it came with its fair share of problems. But we've got most of those resolved now, and I don't find myself pairing with a lot of data scientists regularly. At this point, which is good in the Docker composed. Jonathan, you put the Jupyter notebook also as a service. Jupyter notebook is there and a bunch of dependencies that should be there. Is that what you do? Okay, got it. And if somebody needs a new dependency, then they will go and update this Docker composer file and then the image there, you have built a CI that will push the image and then everybody will just pull the new image and then run it. So if they need to update a dependency, they can just update the dependency, run Docker compose, build so that they get it locally, then they push it to PR. Once it gets approved, then everybody can have it on their local development environment. But yes, that's the basic flow. Understood. Cool. Are you using anything for experiment tracking sort of thing, Jonathan? Like the artifacts, log the model, where are you logging the models? Is it all Sage maker currently? Yeah, it's all Sage maker. Right now we're using Cloud Watch dashboards to track kind of like all the Sage Maker metrics that are output. We have kind of like a game plan for essentially just leveraging more AWS infrastructure for all of that. But at least for this time period, it's all going to be monitored through Sage Maker and then just like Cloud Watch log filters and log metrics. So anything we want log from our running applications, we're just going to output and capture that way. You have used ML flow by any chance? Not no. Okay. Yeah, it's more for just logging the metrics during training, like, what was the precision recall and things like that for every model so that you can go and revisit things like that? Yeah, we'll take a look at it. Anyways, I think that's all the questions from my search. And you mentioned that one of the projects that you are now taking up is live inference kind of a service, is that correct? What kind of roadblocks are you facing in that? Is there something new custom code that you're having to develop since you're on top of Sage Maker? Or how is that fairing out for you? Let me think about that for a second because most of the issues that I'm running into with this one are existing human resource issues. There's a couple the technologies that we've chosen to build this thing are fairly straightforward like Apache Flank, Kinesis, Apache Kafka, and just outputting our features to a register, which we can then serve through some like, Python web service. It's a fairly easy, straightforward pipeline system for each feature. So that portion not a huge deal. I'm not really having any problems with onboarding or learning it. I think if there was one thing that I would find super useful is it seems that a lot of companies like CarGurus or Wayfare, they seem to rebuild the same things over and over and over and over. So I'm rebuilding the same thing that I built at Wayfair, just with a slightly different flavor here at CarGurus. And so it seems like every company has different every company shapes their data in a different way. They're outputting from different event streams. Like here at CarGurus, we're using something called snowplow that outputs event data. At Wayfare. We had a custom internal build system that would grab event data from the site and put it somewhere. So every company has different data, but the things that we do with this data is all fairly the same, right? Like you aggregate it. You want some time windowing on it? You want to populate it into some kind of, like, feature set so that a data science model can train on it. And yet I find myself writing the same code over and over and over again, building the same systems over and over and over again for all of these different companies. CarGurus, Wayfare. I'm sure at some point in the future, I'll build it again. And it seems like a waste of time to me. It seems like a waste of time to have these meetings at CarGurus where I'm like, okay, well, we're going to architect it this way and then having pushback on that and then trying to figure it out. I feel like the industry should have a solution for this. At this point, that is fairly standard. Give us data. Label it appropriately. You can put it through the series of aggregations. You don't need a programmer to actually do the aggregations. Maybe you have a graphical UI and you just say aggregate on some time window. Make sure your data looks like this, and we'll output it in a feature set like this. All of the pipelining stuff. You don't have to worry about that. You can hand it off to analysts. You can hand that off the data scientists who don't need to know anything about programming to do it. And so, you know, there's something to be said about custom build solutions. You got a lot more control there, but they take a lot more time. They require a lot more support and maintenance. So it would be nice if out in the industry somewhere there was something that was able to fit kind of this general case of accomplishing this task of I have data and I want to aggregate over it in some way and I want to output it somewhere. But each solution that I find like we've looked at feast. Feast is an open source python future store. Each Solution Seems Kind Of Like Overwrought, kind of complicated and it seems like it may take just as much onboarding time as just building the solution as we find it ourselves. So it seems like there's a ton of options out there. It's very hard to discover which option would fit well. And I feel like personally, as Machine Learning Engineer at my level at this point, I'm doing a lot of the same tasks that I feel like should be solved, that I think I shouldn't have to do over and over and over again. Are you writing a Sequel Queries for Transformations, or are you actually loading the data and writing Python functions? I mean, like, either or, I've done. So at Wayfare, we loaded the data and we wrote Python functions to aggregate over our data out of Arrow Spike, which is a pretty performant data store. So that was one way we did it. This time we're doing it differently. We're essentially running SQL queries in Flink and Aggregating over event data. It's the same thing, but like, the data we're aggregating over, it's the same in every company, right? It's like, how many times has a user viewed this page in the past two weeks? How many times has a user seen a red Chair in the past month? Right? So the features and the things you need to do to aggregate them, they're all super similar. This is all like, really standard stuff. And yet the amount of time that we have to spend, like actually building out the infrastructure to support aggregating over these features or aggregating over this data, or making sure that our data looks the right way, it's just a lot of wasted time. In my opinion, there should be a pretty standard template at this point. And I figured something like Amazon. Really? When it came to CarGurus, I was excited to come to CarGurus because I knew they were using AWS, and so I was excited because I was like, okay, Amazon's been around a while. They're going to have all this cool stuff and it's going to be super easy to use and they're going to have this all figured out. And then when I get here, I find out that's not the case and I'm just doing the same thing. I was doing it way for over again. So I think that's my biggest pet peeve with kind of the MLE industry right now is there's just so many different solutions, but none of them seem to be a good catch. All. Understood. Makes sense. And in terms of use cases, do you find the team stuck up in terms of deployment? Are you facing any particular issue with the deployment bit per se? Like, is there more support from the team that is not being able to be able to deploy? You mentioned most of the use cases are batched this time, right? Yeah, right now it's mostly batch inference. I mean, like, in 2023, we're going to be putting two or three models into a real time inference pattern. So we'll be serving endpoints for those. I don't think deployment is so much an issue. I set up CI pipelines, like, as soon as I got here. They didn't have that running. And so once we have, like, real time inference like, once we have kind of real time inference needs, adding a CD pipeline to that is not going to be terribly difficult to do. So deployment issues aren't something that I've ever had a huge problem with or huge issue with. Yeah. You're currently using portfolio? Yeah. At CarGurus right now we're using Team City, which is not a great solution, but it's the solution that they have available. Yeah, team City, CI, and then there will be a CD pipeline that just talks to AWS and just uploads the new docker images whenever they're available. Yeah. I like to build, so I use buildkite previously at Wafer, and I like that a lot more. I thought that was a much easier, kind of, like, more feature rich solution. And Team City is a little very jetbrainsy. Understood. I think we are running out of time, but I just want to understand one thing. Like, in Sage Maker, you said you have to write a lot of custom code. Can you please would it be possible to tell which parts or which aspects you have to write this code for, like, some simple use cases of yours? Yes. So essentially, we wrote a package that allowed us to essentially define all the steps in the Sage Maker pipeline and all the steps in the Sage Maker pipeline. And then we're just able to wrap kind of like it's a package that essentially wraps whatever code you want to execute out of the Sage Maker pipeline. But I would expect that to be already existing. Like, I would expect something like, sage Maker pipeline step run this code, sage Maker pipeline step run this code. But we didn't really find that. So essentially now we have this package which can slot in any docker image or custom code that you want into any number of steps that you define in this Python SDK. And it just essentially provides a template. It allows you to kind of, like, logically think about your code as a series of Sage Maker pipeline steps and just slot in your custom code into those steps. So something like that would be really nice if Sage Maker provided some sort of template like that or some sort of package that allows you to just slot in, like, custom docker code or some custom Python script even. But we've had to kind of, like, build out sort of the bones of this skeleton application of this wrapper. So that's one use case. And then I'd have to talk to the other data scientist who was on one of the primary projects. But let me see if I can pull up some of the other code that he. Wrote. Did you try using any other solution or you had it in mind that you're just going to use Save Maker and build everything else? They had already decided to use Stage Maker more or less when I came on, but when I came on, they essentially wanted me to go, does this look like it could do what we wanted to do for the next few quarters in the next few years? And I said, yes, it does. But I have these concerns. The only other technology that they had used is up to this point. It's called converge, I believe. Okay. Yeah. Converge. IO. And they were not super happy with it. I have no experience with Converge. Zero. But they used it for two years and there are numerous problems that they just didn't like about it. And they're happier with Amazon system for sure. Understood. I think that helps us a lot. Sorry, if you have time, I can just give you a short brief of what we're trying to build. I think you're well over, so you'll get it right away. Yeah, sure. So Jonathan, we are trying to build this platform on top of Kubernetes. You can think of it like page Maker on Kubernetes. So you get much more flexibility, like if you need. And kind of making this cobalt is easy to use. And as you mentioned, you are the sole engineer. So it should have taken you time to set up and everything. We kind of try to do that in an hour for anyone. So I'll share my screen. So basically this thing that you are seeing currently is our product. Okay. And this can be installed on your cloud itself so that you can take this entire thing and put it on your cluster. Basically this comes with a chart that you can install. And this is where you can connect like all your Kubernetes clusters. So if you are using like two, three Kubernetes clusters, you can just quickly connect them cluster. We provide an agent that you install into each of these clusters. And the moment you install those agents, this is showing as disconnected because it's not yet the agent is not installed here, but in this case, the agent is installed and that's why this cluster is connected. So we call this the control plane. And each of the clusters that you're connecting to become like the data plane. Does that make sense? Yes. And you can connect the same cluster to itself so you can deploy in the same cluster. Okay. So the moment you basically connect your cluster, you can see all the namespaces right here. So let me just do this. So you can actually see all the namespaces here. And then what you can do is you can lock namespaces to different teams or different people. They can have access to it. So this person, like for example person can only see this workspace. They will not be able to see anything else. So if you want to do that separation of concern, you can do that. You can also limit the size. Like if you're giving something to data scientists and all, you can limit the size that they cannot go beyond four CP and eight CPR Ram and things like that. So this is you can decide the space of the work space that you're giving to them, who is adding to it. Then you can also do a few things. Like for example, one team needs access to GPU while others don't. So you can give them access to which machine types they have access to. And then you can also do something like service accounts. Basically you can give them access to like let's say one team needs access to two s, three buckets where the data is kept, but you don't want to let them touch the rest of the infrastructure. Like you're dealing with all of access key and secret key. So you can just attach the service account to this workspace to this namespace here and then these people, the people who you have added to this namespace can only play with that setup infrastructure. Like those are three buckets within this limited space. So no matter what they do, they're not able to screw up the rest of the things. And once you do till this point any questions? Jonathan yeah. So true. Foundry so you could connect a Google deployed Google's Google GKE cluster or you could connect Amazon AWS GKE cluster from the same exactly. Okay, so it seems like what you're doing is you're abstracting away a lot of the kind of difficult setup that you would encounter with both Google Cloud or I don't know what the other ones are, like ocean azure, any of that because like the Im role stuff stops. Right, okay, cool. Continue. Yeah, yeah. So this is where you lay down the workspaces and you divide the cluster between teams or people or things like that. And then what you can do is you can basically for creating a new deployment like this is where data scientists can do deployment themselves. Like you don't need to teach them. Like we try to make it super, super simple. So let's say you want to run a service. Service is something like an API that runs 24/7 with auto scaling and things like that. Job is like something that will run and shut down. And model is if you want to deploy model file directly. So like for service, let's say they will only be able to see the workspace where they have access to they will not see everything on the admin. That's why I'm seeing all the workspaces. So they will see here they want to deploy something, enter the name of the service, they will integrate the git repositories. So if you want to deploy from a certain git repository, they'll choose a git repository, the branch, if you already have a docker image built, you can give us the path through the docker image also. Otherwise you can give us like the repository name, the branch, there's a docker file. Great, we'll automatically pick it up. If you don't have a docker file, we can also generate the docker file for you. So you just have to give the path to your requirements TXT. This will automatically tell you if we are able to find it or not find it and things like that and then you give us the port, your CPU and memory limits and then you just click on submit deploy. The moment you click on submit basically the service like it will automatically build the image, push the stuff to your docker history and I can go forward to show so this is how you integrate your docker. This can be docker hub, Google cloud, AWS, doesn't matter, this is how you integrate your repositories. So GitHub bit bucket of GitLab and then when the moment you deploy it automatically the image it will deploy it and the service will be live on this link shared. Yes. So the service will basically be liquid on this link and you can also set username password. I think the service is set with I'll just quickly show the service I think the service is set with username and password. So the API link you can also provide username and password so the basic auth credentials so you can also set username password so that nobody else can access the API if you want to. So this is how you basically and you can see the logs and metrics right here. Basically of all the services and everything, the logs will also show up. I think this service does not have any logs as of now. The logs and metrics will show up. And one more thing and for the model I'll quickly show job is similar. So job you can also do a job deployment. Again you choose the code, this thing, whether you want to take out the job manually or you want to run it on a crown schedule, you can choose that and rest is pretty much the same. You can inject environment variables also if you want to inject secrets and all like values that you don't want the data scientists to see. So I'll show you like this way they are entering the path to the secret but they don't know the value of the secret. So they can use those environment variables in their code but they'll never get to see the values. And one more thing was if you want to deploy models, it is as simple as let's say the data scientists build a model and they have saved the model somewhere, right? So they will just give the name here and this is where they can see all the models that they have saved in the system. So let's say you want to deploy this, you just choose this, give us the resources. Do this. If you want to put it on a certain instance type, you can also put that right here. And then you can just click on submit and it will become a service. And each service basically comes with an open API by default that we generate. So you can actually query the model right here. That's awesome. Does that use fast API? This is using model servers behind the hood. For TensorFlow. We use TensorFlow serve. For Pythons. We use Python Serve. For exe poos psychic learn, we use Ms seldom server. So you're guaranteed to get the best performance out of the box. And you guys are pretty well optimized for that individual. Any feedback that you have, Jonathan? I understand it's a very high level overview. What about the use case of, I have multiple model steps, I need to ingest data, pre process that data train on that data output model. Show me, are you guys planning on adding something like that, like a Dag or something that you can say, this step runs this step, then it needs to adjust data from here, that kind of stuff. So the pipeline support is the part that is missing right now. Jonathan we don't have pipelines now. We have jobs that are single jobs, right? But we don't have pipelines yet. So that is something we do plan to add. Where you are giving a bunch of it's a series of jobs that you are giving us. You can run them in parallel, things like that. As you mentioned, you'll give us either for each step, you give me the rocker image or the Python script, and you tell me the edges, how it's supposed to run. And then we'll run it and show you the logs and metrics right here. Nice. That's awesome. This looks like a really good product. It seems like it's got everything that I would have hoped from Sage maker Amazon's product. It presents it in a simple and clean way. What about being able to modify manifest files directly? Do you use home templates? What about the extended use case that isn't covered by your kind of control planning plane UI? This was the UI router that I showed you. Everything that you can do on the UI can be done. So I'll quickly show you this. So go here. This is the YAML Spec. If you want to do it the YAML way, you can send us this YAML spec and you can deploy this YAML spec and it will service will be deployed. So as you're filling up on the UI, we show you the YAML spec right here. And let's say you want to do it on Python because it's data scientist first. Sure. This is how to do it on Python. So, I mean, this is your code and deploy Fast API service by Python. So this is your standard Fast API code that you've written so that we've really tried to make sure Jonathan is we don't try to put our library inside your main piece of code, right? So that they lock in. So this Fast API code that you're writing, it remains as you're writing like that. Sorry, not this one. This is not the fast API code. This was your standard fast API code. This is your standard fast API code. And to deploy this piece of code, if you want to do it by Python, you have to write this deploy PY where you basically tell us how to build an image. So the image is the Python build, the command is this, the requirements TXT is this. And then you get service objects, service equal to service name, image, ports, resources, environment variables. If you want to give and then you need to be just called service deploy behind the hood. This guy also goes to the YAML. The UI also goes to the YAML. That's awesome. This is great. This looks like a great product. This looks like the kind of product that I would prefer. We would use it at CarGurus, at least from what you've shown me so far. I'm sure there's stuff that I would run into that I don't like about it, but at least from the time that you showed me, it looks like kind of the right mix of like the information that you need, the things that are difficult to do and set up correctly, you can do it quickly. Yeah, so far this, this looks, looks really good. But I mean, like, ultimately we would want some sort of pipelining solution, however that looks like. Cause I think out of everything, I think making sure the pipelining solution works well for the data scientists, that seems to be kind of where they at least working with the data scientists. I work with the pipeline, like setting up their pipelines. If that works well, that's smooth and it's easy to understand, it's easy to log from, it's easy to get monitoring and metrics out of it. If that works well, the rest of it doesn't need to work quite as well from feedback that I've got with the data scientists, because they work so much in that space of just ingesting data, trying to train it, try to analyze it in some way, and setting up this pipeline solution to do that. So, yeah, that looks great. I'll certainly put it on our list of technologies to keep an eye on. Cool. We are also building Jonathan, so we'd love to keep you updated of the progress and everything. Like the standard is one of the major things that you need. So once we have that, we'll love to get feedback from you on that. On the Kubernetes side of things, if you ever want to give it a shot like Jonathan, in January, we plan to do something like we'll give you a script and it will spin up this cobalt displuster in your own cloud and set up everything in 1 hour. Maybe you can do that. Just give it a quick spin and see if it works out for you. Yeah, that's my work email. So just keep me posted on my work email and that way I'll be able to see it. And if I like what I see, then potentially we could I'll bring it back to the team and see what everyone else thinks. But from what I've seen so far, good job. Keep at it because it looks like it could be a really cool product that doesn't do what I've seen. I haven't seen what you're doing from any other product that we were looking at when we were kind of like, seeing what else is out there. That's cool. And currently, Jonathan, which pipelining solution do you use? The sage maker one. Yeah. So sage maker, then? Previously we were using Converge, but once again, if they didn't like things about Converge, I don't know much about it. Pipeline. It's sage maker. We have to write so much custom code, it's kind of silly how much custom code we have to do to really leverage their system effectively. And yeah, I don't know if you guys have ever used things like Airflow or anything like that, but once again, those are perfectly fine pipelining solutions. But all this stuff, it seems like there should be simpler solutions to these problems that just allow you to stuff your code into, like, a series of steps right. And get metrics and monitoring out of them. It seems like a lot of over complications on a very common pattern, so yeah, definitely. Keep me posted. Let me know how the product develops. Cool. Awesome. Thank you so much. Awesome. Of course. Yeah, this has been great. Thank you for your time. Jonathan, would you want to try out the platform if possible, or we can share an access with you if you want to just play around with it. Yeah, definitely. 100%. I mean, I may not use it for CarGurus stuff, but also for some personal projects I'm working on, so absolutely happy to try the product. It looks really cool. Okay, then I'll do that and give you both awesome. Back to you in January once Jonathan with the updates. And then maybe we can give you the credentials, also try out and everything. Okay, cool. Sounds great. Thank you. It was nice. So much. Thank you. Thank you so much for your time. Of course. Likewise.