Hello? Hey. Hikarsh. Hi. Can you hear me? Yeah, I'm able to hear you. Are you able to hear me? Yes, I can hear you. Thanks a lot in time for the call and very Happy New Year. Vikas. Yes, Happy New Year too. And I'm not sure if Nickwind is on the call but Happy New Year to Nicole's too and your team. Nick, Andrew is not able to join today. He's on the road unfortunately. I'm taking the call in place of him. No worries. Yeah. So Ricashi, I can maybe give a quick introduction for myself as well and just to also set the agenda. The purpose was to learn from you as to how AI functions in healthcare where you have a lot of experience and specifically with respect to what I would love to know about the current ML stack, the use cases, the challenges you are seeing or what exactly you are aiming to achieve in the next six months. Also will share a brief background of what we are building at Cook Boundary. Sure. Yeah, I'm looking forward to hear about that. On my side, I can probably accordingly, as you said, can do the introduction and can share a few stuff where we are probably looking forward in three to six months. And even here probably just given some of the focus area we are looking at. Sounds perfect. Cool. So just to introduce Amanda, one of the co founders at Tru Foundry. Nikkunj and Avishek are my other co founders. Nikkunji is based out of the Bay Area generally and Vishek and myself, we are in Bangalore in India. So basically our history goes back like 13 years because we were bachelorettes at college in AIT Kharagpur and then Nikon and Abhishek went towards different path. Nikkunj was on the ML side for like seven years with his last team at Facebook and Abhishek on the other hand let the software engineering and the systems team at Facebook also let the videos are there. So he brings experience on the infrastructure side of things. I on the other hand, used to work for a hedge fund called Worldwide. It's a US. Based fund. So initially I was in India office building trading strategies across global markets using a lot of data. And fintech is another area where there's a lot of compliance and other things just like what you see in the healthcare domain property. So have some experience on that side and then went to us as a part of the CEO of the various strategic initiatives as well as was leading portfolio management for them for one of the asset classes. And then in 2020 we all left our jobs. We wanted to build something together, came to India and we built our first startup in the talent space which we sold to Infoge which is one of the big talent companies in India. And during that time we were building models and when we kind of built models the productionization of the models was very difficult. Like we had to assemble a lot of open source, learn about the cloud and so on. And it took us almost 30 days to reach a good stable state of production with reliability. And we were discussing as to the kind of infrastructure Nikunjana Vishak saw at Facebook and that is when, you know, we thought that if we can bring that to every company that will be great. And then after selling entire we basically started True Foundry with that goal that will help companies move fast with data in terms of testing out and deploying models to production as well as monitoring them in the right way without much efforts needed from developers. So that's like at a very short level as to my background as well as a brief about Truefoundry, happy to answer questions. Oh Ivan, that's great and a great journey. Looks like you guys have gotten some good experience to actually lay out for the True Foundry. And probably I'm hoping that the company is doing well and you guys are able to attract some of the kind of right users and customers for the product itself. Thanks. We are in very early stage, like a team of 20 members and we are looking to actually work with companies like we are working with around five to six different companies across them and one on the healthcare side itself. And we wanted to kind of talk to folks like you to learn more and potentially even see if there's a possibility potential to even work together in solving some of the problems that you are. Yes, I'm sure probably about healthcare. It can go beyond 30 minutes, but hopefully I can reflect back a little bit on what some of the specific challenges on healthcare are. Okay, I can go ahead with my introduction quickly. I'm at Advertiser. It's a healthcare machine learning company. The focus for the company has been in multiple areas, but right now it is primarily around pre operative care and revenue cycle management, which is the post operative care. So these are making about what should happen with a patient before any service or care is given as well as once the service or care is given. In us especially, that problem is like how account receivable, which is basically how the payment for the patient has to happen and that the entire area is called revenue cycle management. So it is not necessarily a clinical decision making, but much more from a CFO finance perspective of how all the money can be recuperated. So that is where Advata focus is from my perspective. I am leading the research and machine learning.org at Edwater. It is actually five years now. Joined Kensai, what we used to be known as in 2017, located in Seattle area. I am also based out of Seattle area and as part of that experience, of course, healthcare is very wide and deep both. So I have worked in both the pre operative care, the post operative care, as well as intra, which is like what this needs to be taken while a patient is in the hospital or given a care. So given all those different journeys right now my focus has been around mostly the joint replacement as a domain area and especially thinking about what and how we can actually enable the clinical decision making to be better for related to joint replacement. But over a period of time, as you can imagine, similar problems, similar challenges and whenever we talk about ML, we face the same challenges. But healthcare comes with its own unique flavor and the flavor especially includes one. The diversity of users is very high. Like as you can imagine in healthcare, the depth of knowledge for a particular domain is usually pretty good. The users are pretty well read, pretty well acquired, whether it's a math side of the thing or whether it's the literature side of the thing or including the patient people perspective. So clinical side of the world is pretty well aware of all those challenges as well as the depth that is needed. The problem that happens is that communication or the language is not the same. As we would like to communicate data in terms of tables and let's say even through GitHub codes for reviews, it does not work with the users who still have to review certain data elements to ensure that the clinical insights are true. One cannot send it to a doctor hey, can you review this code in a GitHub? So it creates unique challenges on how actually one can create a platform. In other words, journey of data from input to output in a way where it is both well managed but also well validated because validation is very important part of the data. It's not a consumer data, it's not something we can understand either. There are multiple codes that get used in healthcare, very domain specific parameters that exist and those are the specific, I would say unique characteristic of health care that presents very unique challenges and over the last few years have been working on various challenges and now I think we are at a pretty good position. But still I would say far from saying that there is one flavor of healthcare ML platform that can work, it is still far from there. Yes, apart from that, just before I joined what was known as Kensi before so we got acquired and hence the renaming and rebranding happened. I completed my PhD at the University of Minnesota in the area of recommendation system. So I had a chance to work with all the kind of multimedia domains music, movies, images, geosawan actually was one of my first open data that I worked with along with the Savan team that used to be based in the Bay Area, actually in Mountain View. So yeah, that is where my recommendation system journey as such it started and it was very beautiful. I enjoyed it. But at some point I was like, healthcare kind of got exciting. So I kind of moved a little bit of application area into healthcare. One question and one point before that. Nicole actually would love having conversation with you. I think he actually specialized in the recommendation system side as well when he was working with Reflection, which is an e commerce company. And they were trying to build recommendation systems for a lot of different ecommerce and kind of worked on ML for data to like 600 million users. So I'm sure you'll have a lot of good things to do. And prior to that, I was in India. I was not very far from Karakpur campus. I was in NIT Rawkilla, my bachelor's there. I remember we went to Raw Kila. So in 2007, I believe, when String came there as a band to perform incorrect. We went to see that band in your culture fest and whatnot I'm not sure if you guys were there at that time or not. Not in 2007. We used to call it SpringFest, by the way, the culture I joined 2009. Okay. But I'm sure it was a very popular thing. Every January. At least the east side of the thing of India. If spring fest are happening in Karakwood and if some big celebrities coming, it would usually attract a lot of distributors around there. Yeah, yeah, that's it. I would love to hear more on Advertise and the kind of use cases you already mentioned. But when you mention from the challenge perspective, I do understand a lot of challenges leaders to the model building side of things. How about the downstream? Like in terms of once the model is testing it out and then deploying it and monitoring it, how does that happen? A lot of it is still through. Like you can imagine, versioning of the code is pretty well and as you can expect across the industry of the code and the testing of the code is pretty well done because GitHub allows that to do. But the burdening of data and the testing of data is where I would say we are in like, let's say if there are three stages and stage three is a very mature stage, then we are in stage one. And the industry is probably, I would say in health care. I don't think I have seen anyone beyond stage one either. Stage one is where basically foundations are being led by associating certain quality of data by after certain preprocessing processing and whatnot. But it is still very non standard. Like I said, everyone has their own way of doing and dealing with the data. But the code side, as you can imagine, using GitHub as your pipelines and GitHub workflows together along with Docker and Kubernetes, you can get all the versioning testing, deployment pretty much in place. I saw the Truefoundry video and I was just looking at the documentation side on the website, it definitely makes sense given how probably easy it is to integrate it. It gives me a little bit of reminder of how ML flow generally has been designed. So I believe that is also I'm not sure if you would say so, that it is very close to what the ML flow looks like. So I think just to add there like ML flow basically primarily what I have seen at least with us, and correct me if you have seen something else, that people use it more from the experimentation perspective. Basically building models like comparing different versions of model while they are still in the training stage and seeing which metric is good and so on. Accordingly thinking that this is so our platform is more focused on the deployment side. Like as you said, you use probably Kubernetes for deployment. So for example, a data scientist or an ML engineer, if they are building a model or training a model, how do you orchestrate that training very easily over Kubernetes clusters and you have actually completed the training, then obviously comparison of the experiment is also there. But then how do you take any version of the experiment and then deploy it in a testing environment and from there move it to the production environment while having complete monitoring. So we try to build that layer where everything is orchestrated over Kubernetes. And a lot of the things that you will need to worry about on the infrastructure side are taken care of by the platform. And all you need to do is write like few boilerplate code lines and that's it. And that is very standardized and it works for pretty much all the frameworks. So we are more travelers on the site, but would love to hear your thoughts. I probably will have to of course continue exploring or understanding Truefoundry a little bit more. And if you have anything to share, demo or any other documents to share through email, I would love to hear it. And I'm saying this especially, and this is something Nikkunjo also wrote on the message when he connected with me on the LinkedIn. The reason this is interesting is I know these are the very common problems and many people are approaching it like every other day there is some message that you receive on your LinkedIn or an email that someone is reaching about ML ops, right everyone is trying to solve. So I think one best way I have figured out is to first of all see and understand where everyone is kind of understanding is going from and seems like the foundation understanding is same across the industry, that deployment is hard, tracking is hard and actually deploying multiple persons is even harder. It's definitely, I think, in line with what a product of such goal is expected to do. I think I found it very much aligned. At the same time I haven't used it so I can't comment a lot about it. One thing I can say about from the unique perspective like let's say from a data science perspective where as you can imagine a lot of time gets to spend on the experimentation. And the last part when the deployment happens, if the foundation was laid out right then foundation of deployment is not big knowledge gap. Once you have the pickle file then there are thousand ways to actually deploy it. And as of now, what we are using is KF serving along with Kubernetes. So just deploy it as a model as an endpoint for the KF serving and deploy it in a Kubernetes server. Have a security layer, have a user access layer and your APIs are good to go to be accessed by your users, right? But the problem that happens is when let's say API based models which is a stateless model versus model. Let's say that is based on certain data that gets pre processed every day. And then it kind of like imagine a case of a hospital where someone wants to get length of the state prediction. Then we are not talking about stateless model, we are talking about the data that comes every day about the entire hospital. You preprocess it, you prepare it and then from the trained model now you actually define like you build the features for the model from the data. So it's not like a model that has a very specific feature API that one can use. There are like 1000 features and that is where healthcare becomes a little bit unique. It's very hard in certain cases to define stateless model because models usually end up with large number of features and if it is not large number of features then the person on the other end is clinical. Let's say they are not going to say like okay, I mean one needs an It team basically on the other side and not every customer has a strong It team. Which means that at the end of the day the kind of the pipeline you are talking about is data comes in and data goes out irrespective of what is happening in the blackboard. So that is how we have seen the industry. It's very hard to suggest that hey, we don't take your data. Here is our model specification API. If you just request it, you will get the responses. Very good to have such thing. Problem is in healthcare on the customer side, you rarely talk to It teams because It team is managing healthcare infrastructure, their hospital infrastructure. And the conversation that you are having about most of the machine learning is not the infrastructure team on the healthcare side. They are the clinical team or they are the finance team and they are like oh, are you saying that we have to go and engage our It and infrastructure now into this they're like okay, this looks like a two year project to us and it just doesn't work from a company standpoint perspective. And then you are like okay, well, don't worry about it, give us your data. So data in, data out rather than feature in, prediction out. So that changes a little bit of a scenario and that is where we have found that a lot of ML ops become harder to adopt as is similar to ML Flow. We have tried ML Flow in the past for of course tracking purposes but it did not give us the leverage in the way that we wanted to have because that part of the deployment is very small part of the entire journey. So the leverage is not that huge. Got it. I would love to understand from these are great pointers and the point you mentioned about stateless models versus the non one. Curious, how do you currently implement it? Love to know about the overall pipeline today like starting data scientists using data. I'm guessing you're using a feature store or something for storing features but I'd love to hear how the thing starts from there to training to testing to actually deployment to monitoring. If you can give an overview because that will be really helpful. Yeah, if you follow the very traditional way. I know we are working on something much more different for our upcoming months and year but what we have traditionally worked on is as we all probably know you have either Python code or notebooks to explore and understand the features. You write two types of code, especially. One is for the training purposes, which results into the experimentation and one you write for the coding purposes and for each of the code and given if you have approached from a feature based models versus like, let's say deep learning models, which in some cases we also have like, imaging based models, so we don't really need features, definition and whatnot. So that differentiates like how those pipeline are looking like. But if it is feature based model then you define feature metadata, you define actually explicitly what those features look like. So similar to a feature store internally we call it feature bank and then you define and store all the features, their definition, their ranges, their expected values and whatnot. And then that becomes a foundation or as a metadata across your pipeline to be utilized and using those feature metadata which of course during the training time or during initial phase of learning or development metadata will keep updated very frequently. But let's say once you are in a stage of the end of a model development then they will be rather much more fixed for probably several years also at times. And that then results into either data bricks specific which where we have most of our feature based models which means we get the data from customers in a data bricks. I mean of course that part is managed by our own infrastructure which basically gets the data turns up the data bricks, runs the infrastructure, puts the right user access and the data is available in data bricks. You write your codes in the databricks to basically convert that data into models and then you write a scoring pipeline in there itself in the databricks and the data bricks. Along with Azure. We use Azure machine learning a lot. So along with Azure data factory is what is that pipeline? If we are looking at non feature based which is like let's say image based deep learning models, then we don't use database, we use directly Azure ML pipelines in our case. And Azure ML as you can imagine also provides a lot of similar kind of constructing pipelines, similar structure of burgeoning data, experiment tracking, including scoring itself. Because Azure ML itself has all these features available very in a standard way, so it's much more cloud native in a way, but it is still very zoodependent rather than independent of specific cloud. So that's how most of our pipeline is. At the end of the day, once the training is or training of the models are done, we usually end up with a pickle file or a job lift file which we use as either as a data factory if it is related to that kind of model. I was saying where we get data in, data out, not necessarily featured in prediction out then it is usually Azure data factory where the pickle file is with just one part of that data factory somewhere or on the other hand it is stateless model. So we do have some stateless model and stateless models are supported by KF serving and Kubernetes service right now. Got it understood. And this entire infrastructure is it managed by the infrared team or like yeah. So everything that is needed for the training as well as the scoring purposes if just the resources are required. So of course it is terraformed for respective customer if it is needed or respective even development environment that is needed. And then there are certain deployment automation for the KF serving Kubernetes if needed be and similarly on the scoring pipeline. So we generally separate the training and the scoring pipeline or in other words development and production. And each one has the infrastructure that is available as a code, can be deployed multiple times and can be of course utilized multiple times with the multiple customers. It's not as probably easy or simple as I am suggesting. It usually still requires some manual work just because of the differences that we start observing customer by customer and data by data. And that's what the messiness of health care is. The same data like let's say blood pressure as an input can differ from one customer to another just because of the metric they use or the formatting that they use. And that creates a lot of messiness, that creates a lot of customization which becomes harder and harder. And as you can imagine what is the right word? I'm looking at like chaotic way of keeping all the code where multiple versions of the truth starts existing and which is something we have been trying to address. Got it. And I'm curious to know like in this entire pipeline, once the pickle file is made and you have to deploy, who does the dockerization and everything? Infrastructure team or basically there is an ML engineering team. So we have infrastructure which does the Terraforming helmet, all those management and deployment. And then we have an ML engineering team who does most of the packaging for the models and deploying of the models against KF serving right now or the data factory if it is a kind of a batch processing rather than a straight place. So one question you got like all of these models are serving internal use cases or are there also models that actually have to be taken and deployed on customers infrastructure or mainly everything is deciding on your infrastructure. So until it is a stateless model, everything is actually deployed in a customer environment. And that's why infrastructure as code is required. Because data in healthcare cannot come out of customer environment. Right? So we go to their resume, we deploy all our resources in their Azure. Once everything is set up, run it, execute it, produce it, get out. Okay. And if they like, let's say relationship, as long as exist, as long as the contract exists, the interest exists. If the relationship does not exist, in some cases when it has, we either choose to leave the infrastructure as is or we destroy it if they don't want any of it. Sorry. Go ahead, go ahead. What happens if a customer is not using SEO or you are using Kubernetes for some deployments? What if the customer is not on Kubernetes and using a zero? Then you ask them specifically or do you kind of generally support multicloud? So in that case we provide the infrastructure then because we kind of create a specific infrastructure environment on our side, like tenant on our side and then allows the customer to be part of that. Now the foundational layer behind this all is because we are ideally to certain extent a Microsoft partner and hence we have a lot of Azure exposure as well as Azure infrastructure in place. And the moment you say to a customer that hey, we will help you get an infrastructure on Azure, more than often either they are not in cloud or they are a customer or someone else, then Microsoft also gets interested and it is Microsoft often who is willing to COVID the cost of it. Because as you can imagine, you are bringing a partner to a Microsoft as your right. It's a bigger Microsoft story than our story actually, let's put it that way. It's pretty amazing to know. Do you have anything to monitor these models from a drift perspective or from an explainability perspective? Yeah. So of course, most of the models that are deployed, whether it is in the data factory side of it, which means data bricks, pipeline or the stateless side, they all have certain monitoring properties around it, including the Drifts and we use Grafana for any visualization of those monitoring. But we use just actually trying to remember if we have used specifically on the Kubernetes something else. But on the Azure side we use most of the Azure logging metrics and then Grafana is connected to it and it picks up all those metrics and then it shows which includes both the system performance as well as model performance. But model specifically health perspective or model monitoring I'm not sure I would still trade even across the industry I don't think it is yet there to say that it is a matured way because it's generally about hey, model generated a prediction, it's up and running. Here is the prediction and it generated five previous day, five next day. It is not telling us like probably the qualitative differences of the monitoring. It's probably a lot more focus is still now on quantitative and this is probably true across the industry on the model monitoring side, maybe a few models that we may have from some of the bigger ML focused companies, but I am seeing at least mostly it is still quantitative monitoring. Got it. And when you try to like you have seen other ML platforms in the past, I would love to know if there's anything that from a goal perspective that you would want to meet if you would want to ever adopt an ML platform. And the reason for this question is the following it will help me understand where you might potentially benefit or where you might potentially think of even a two like two Foundry. And what I would like to do with us is also do a follow up call, post this where I can actually dive you through details of the platform, also show you overall demo as to the overall pipeline as well as for the Use case you are telling maybe also showcase that part in more detail and learn your feedback and learn from the system you have built at sure. Yeah. So I'm also realizing time again, thank you for taking the call. I think it's midnight for you already. I should be grateful for you to take the call. One case that as I was saying, what happens is multiple versions of the truth starts existing and that is a major challenge. It's not about that hey, whether we are able to deploy monitor and version models, that is solved. The problem is they are being done in multiple ways. So that's where when I see things like Proof Foundry or any other ML ops platform is a question of like hey, is this interesting to us because this will help bring consistency in what we are doing. Rather than, let's say if someone. Is in Data Break, someone is in Azure ML or using Data Breaks, using Azure ML or using ML flow, they all end up with different ways of doing the same thing. So idea is how we can bring consistency in both not only engineering side of it, but infrastructure, the quality, the quantity, monitoring everything around it. So that's where I see Truefoundry or any other ML Ops platform to be such. And 2023 is something we have discussed as a focus for that to kind of expand on, especially around the ML Ops and bring some consistency around it. We do have like as you can imagine, we do have a good number of ML engineers who has worked to provide the consistent framework. But imagine like let's say a five engineering team working to provide consistent framework versus I mean just even think about Kubeflow itself, 100 engineers supporting it. The number of features that keeps updated is very high. So we are debating that, hey, why we are building on our own, we should adopt something and then actually expand on it if we need to. So that's the discussion we are having and that's the consistency we are looking for. Got it understood. That is very helpful. And this is exactly what we also try and optimize for. A truefoundry like even if you are using tools, how can you standardize the overall flow so that even a new person coming in can use the same flow. If anyone leaves, then no one has to worry about knowledge transfer and everything is in one place, which is like a base layer that can sit on top of other layers if at all exist or make a common workflow as well. So that's very helpful. One more question I would like to know. Can you hold on? Someone is knocking on the door. Let me just take the call. Hello. Thank you for waiting. Okay, what we are talking about asking. About this entire pipeline because you mentioned about what will be there for adoption. So I wanted to understand one thing, like in this entire pipeline, which would you rather prefer? Like one tool doing that entire floor, you'd want it to be composed of different tools along the journey. And one more question is around Kubernetes. How comfortable is the team using Kubernetes as the main layer? Like everything orchestrated say on top of Kubernetes. The team managing and deploying Kubernetes is very comfortable with it because it is not new to us. We have been using Kubernetes from its beginning actually. So about three years now. So the team is pretty comfortable in deploying Kubernetes clusters and services on it. For example, the learning phase is already done. In a way, I would say from angle of adopting Kubernetes for stuff, we are at a pretty good stage. The difference is adopting Kubernetes for ML development is not there. Adopting Kubernetes for jobs, for data, for pipelines, because there is a lot that happens even before model kicks in. A lot of that is pretty well managed, deployed and versioned. I would say the ML side of it is not yet there. Got it. So for example we don't use yet cubeflow in our production pipelines. In other words we don't orchestrate models that is like deploying on multiple Kubernetes for training purposes. However we of course deploy models on Kubernetes for scoring purposes just to provide it as a stateless API. Got it? Understood. I think this is super helpful because I know we also set this meeting for 30 minutes so very of your time as well. But what I love to do is, based on this information, I'd love to kind of take a next step wherein I'll show you the overall platform as to how we have built it. And along the way, during that journey like discuss about some of the use cases that you think and your thoughts and at the end love for you to act as a champion for us. Like, if you like the platform, then guiding us on what part would be essentially useful within advata or how we could approach it. And if you feel really it's worthwhile, then we would love to work with you and your team potentially in order to take this forward. Sure, yeah we'll look forward to that and let's figure out some time for a follow up and then we'll accordingly follow up on that. I would say that let's figure out a time. I was going to say that probably by the end of the January we'll have even a better picture overall. Happy to have a follow up. But again, we'll have to be both mindful of each other's time given the outcome that we are expecting. And if the outcome is expected, then I would say right in the January we can have follow ups but we may not have outcomes yet, which will happen probably a little bit later in the year just because of how we are aligning with our OCRs and whatnot right now. Absolutely, I think that is helpful and thanks for sharing. What I would love to do is get your time to actually do this follow up, tell you more so that you are also aware of the platform and outcome is probably an end goal. It does not have to happen as it is but I'm sure it's a journey for both of us. I want to kind of have that conversation with you and then over a period of time as and when the use case is there we can obviously think about but more interested in a follow up call to kind of get your feedback and take it from there. Yeah, okay, sounds good. Do you mind dropping your WhatsApp or something so that I can communicate and maybe set up a follow up call maybe towards next week sometime or sure. I'll drop in my US number here in the chat and then. Thanks so much. No worries. Thanks so much. I really appreciate your time and lot of really good insights, especially from the health industry that we generally don't realize. So this is super helpful. Thank you. Yeah, no worries. And look forward to seeing a demo in the next. Awesome. See you. Bye.