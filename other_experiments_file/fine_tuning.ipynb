{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "56483e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f27d4ed",
   "metadata": {},
   "source": [
    "# How to fine-tune a GPT-3 model for specific prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b56a7b0",
   "metadata": {},
   "source": [
    "I'm constantly looking for ways to automate the work with support requests. An idea has been to fine-tune a GPT-3 model to answer common support-related questions.\n",
    "\n",
    "**Here's how you can fine-tune a GPT-3 model with Python with your own data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f831ab",
   "metadata": {},
   "source": [
    "In this walkthrough, we'll fine-tune a GPT-3 model to answer common support-related questions.\n",
    "\n",
    "Detailed step-by-step intructions for this repo in this blog post: https://norahsakal.com/blog/fine-tune-gpt3-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b102018b",
   "metadata": {},
   "source": [
    ">### Disclaimer\n",
    ">This guide walks you through fine-tuning a GPT-3 model in Python, shown in a Jupyter notebook.\n",
    ">If you're looking for the steps of fine-tuning right in a terminal, [OpenAI has a great guide for fine-tuning in your terminal](https://beta.openai.com/docs/guides/fine-tuning \"fine-tuning in terminal\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698f669e",
   "metadata": {},
   "source": [
    "# Define OpenAI API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dcaff120",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"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\"\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8f76ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlassian import Confluence\n",
    "import html2text\n",
    "\n",
    "confluence = Confluence(\n",
    "    url='https://truefoundry.atlassian.net/',\n",
    "    username='nikunj@truefoundry.com',\n",
    "    password='ATATT3xFfGF06V07kWfgHna6u3_qrZXaqC8Nfu3tk8JsSmLSv_6t1NIVpVzhNr41gViiHBVBwGAdZU3ATFJaaWjfNn5DHRQHoXZaSskmhRXQwmOX8SUNRObT0wPkKJbo1kgSlGwh1tp-0TMw7h-cHJn95qoDpUJdC8cIBZnq2VUUIRy4_DV9lO8=D51E469A',\n",
    "    cloud=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e846b208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_spaces = [elem['key'] for elem in confluence.get_all_spaces(start=0, limit=500, expand=None)['results'] if elem['type'] != 'personal']\n",
    "engineering_space_key = global_spaces[3]\n",
    "all_page_ids = [elem['id'] for elem in confluence.get_all_pages_from_space(engineering_space_key, start=0, limit=10000, status=None, expand=None, content_type='page')]\n",
    "len(all_page_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9df19b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 0 pages\n",
      "Finished 10 pages\n",
      "Finished 20 pages\n",
      "Finished 30 pages\n",
      "Finished 40 pages\n",
      "Finished 50 pages\n",
      "Finished 60 pages\n",
      "Finished 70 pages\n",
      "Finished 80 pages\n",
      "Finished 90 pages\n",
      "Finished 100 pages\n",
      "Finished 110 pages\n",
      "Finished 120 pages\n",
      "Finished 130 pages\n",
      "Finished 140 pages\n",
      "Finished 150 pages\n",
      "Finished 160 pages\n",
      "Finished 170 pages\n",
      "Finished 180 pages\n",
      "Finished 190 pages\n",
      "Finished 200 pages\n",
      "Finished 210 pages\n",
      "Finished 220 pages\n",
      "Finished 230 pages\n",
      "Finished 240 pages\n",
      "Finished 250 pages\n",
      "Finished 260 pages\n",
      "Finished 270 pages\n",
      "Finished 280 pages\n",
      "Finished 290 pages\n",
      "Finished 300 pages\n"
     ]
    }
   ],
   "source": [
    "# confluence.get_all_spaces(start=0, limit=500, expand=None)\n",
    "all_text_content = []\n",
    "for idx, page_id in enumerate(all_page_ids):\n",
    "    if idx % 10 == 0:\n",
    "        print(f\"Finished {idx} pages\")\n",
    "    page_html = confluence.get_page_by_id(page_id, \"space,body.view,version,container\")\n",
    "    html_content = page_html['body']['view']['value']\n",
    "    text_content = html2text.html2text(html_content).strip().replace('\\n', ' ')\n",
    "    all_text_content.append(text_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "233d2da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45986\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import json\n",
    "    \n",
    "    \n",
    "# Define a function to generate prompt and response pairs from plain text\n",
    "def generate_pairs(text, prompt_length=5, response_length=10, min_response_words=2):\n",
    "    pairs = []\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split('.')\n",
    "    # Remove any leading or trailing whitespace from each sentence\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "    # Remove any empty sentences\n",
    "    sentences = [sentence for sentence in sentences if len(sentence) > 0]\n",
    "    # Remove any sentences that consist only of punctuation\n",
    "    sentences = [sentence for sentence in sentences if not all(c in string.punctuation for c in sentence)]\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Split the sentence into words\n",
    "        words = sentence.split()\n",
    "        if len(words) >= prompt_length + min_response_words:\n",
    "            for i in range(len(words) - prompt_length - response_length):\n",
    "                # Generate the prompt\n",
    "                prompt = words[i:i+prompt_length] + ['\\n\\n###\\n\\n']\n",
    "                # Generate the response\n",
    "                response = [' '] + words[i+prompt_length:i+prompt_length+response_length] + ['\\n']\n",
    "                # Check if the response meets the minimum number of words\n",
    "                if len(response) >= min_response_words:\n",
    "                    # Add the prompt and response pair to the list\n",
    "                    pairs.append({\"prompt\": ' '.join(prompt), \"completion\": ' '.join(response)})\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "all_pairs = []\n",
    "for text_content in all_text_content:\n",
    "    pairs = generate_pairs(text_content)\n",
    "    all_pairs.extend(pairs)\n",
    "\n",
    "print(len(all_pairs))\n",
    "\n",
    "with open('all_prompt_pairs.json', 'w') as f:\n",
    "    json.dump(all_pairs, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f08dd7",
   "metadata": {},
   "source": [
    "# Create training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f0c1f",
   "metadata": {},
   "source": [
    "Make sure to end each `prompt` with a suffix. According to the [OpenAI API reference](https://beta.openai.com/docs/guides/fine-tuning \"fine-tuning reference\"), you can use ` ->`.\n",
    "\n",
    "Also, make sure to end each `completion` with a suffix as well; I'm using `.\\n`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10cb45e",
   "metadata": {},
   "source": [
    "# Save dict as JSONL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6ed465",
   "metadata": {},
   "source": [
    "Training data need to be a JSONL document.\n",
    "JSONL file is a newline-delimited JSON file.\n",
    "More info about JSONL: https://jsonlines.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4e4bc0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"all_training_data.jsonl\"\n",
    "\n",
    "with open(file_name, 'w') as outfile:\n",
    "    for entry in all_pairs:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf169e",
   "metadata": {},
   "source": [
    "# Check JSONL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7a5fe452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 45986 prompt-completion pairs\n",
      "- There are 1527 duplicated prompt-completion sets. These are rows: [1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 2675, 2676, 2821, 2822, 2823, 2837, 2838, 2839, 2845, 2846, 2847, 2861, 2862, 2863, 2869, 2870, 2871, 2885, 2886, 2887, 2893, 2894, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 3060, 3061, 3062, 3063, 3064, 3279, 3280, 5256, 5257, 5258, 5259, 5260, 5276, 5277, 5278, 5279, 5280, 5296, 5297, 5298, 5299, 5300, 5316, 5317, 5318, 5319, 5320, 5336, 5337, 5338, 5339, 5340, 5356, 5357, 5358, 5359, 5360, 5376, 5377, 5378, 5379, 5380, 5396, 5397, 5398, 5399, 5400, 5416, 5417, 5418, 5419, 5420, 5436, 5437, 5438, 5439, 5440, 5456, 5457, 5458, 5459, 5460, 5476, 5477, 5478, 5479, 5480, 5496, 5497, 5498, 5499, 5500, 5516, 5517, 5518, 5519, 5520, 5536, 5537, 5538, 5539, 5540, 5556, 5557, 5558, 6717, 6718, 6719, 6720, 6721, 6722, 6723, 6724, 6725, 6726, 6727, 6728, 6729, 6730, 6731, 6732, 6733, 6734, 6735, 6736, 6737, 6738, 6739, 6740, 6741, 6742, 6743, 6744, 6745, 6746, 6747, 6904, 6905, 6906, 6907, 7085, 7086, 7087, 7131, 7505, 7506, 7507, 7508, 7509, 8201, 8202, 8203, 8204, 8205, 8206, 8216, 8217, 8218, 8219, 8220, 8435, 8436, 8437, 8438, 8439, 8440, 8441, 8442, 8443, 8444, 8445, 8446, 8447, 8448, 8449, 8450, 8451, 8452, 8453, 8454, 8455, 8456, 8457, 8458, 8459, 8460, 8461, 8462, 8463, 8464, 8465, 8466, 8467, 8468, 8469, 8470, 8471, 8472, 8473, 8474, 8478, 8479, 8480, 8481, 8482, 8483, 8484, 8485, 8486, 8487, 8488, 8489, 8490, 8539, 8540, 8541, 8542, 8543, 8642, 8643, 8644, 8645, 8646, 8647, 8648, 8649, 8650, 8651, 8652, 8653, 8654, 8655, 8656, 8657, 8658, 8659, 8660, 9013, 9014, 9015, 9016, 9017, 9018, 9019, 9020, 9238, 9239, 9240, 9241, 9242, 9243, 9244, 9245, 9246, 9247, 9248, 9249, 9250, 9251, 9252, 9253, 9254, 9255, 9256, 9257, 9258, 9259, 9260, 9261, 9262, 9263, 9264, 9265, 9266, 9267, 9268, 9346, 9347, 9348, 9349, 9350, 9351, 9352, 9353, 9354, 9355, 9356, 9357, 9358, 9527, 9528, 9529, 9530, 9531, 9532, 9533, 9534, 9535, 9536, 9537, 9538, 9539, 9540, 9541, 9542, 9543, 9544, 9545, 9546, 9547, 9548, 9549, 10174, 10175, 10176, 10177, 10178, 10179, 10180, 10181, 10182, 10183, 10652, 10653, 10654, 10655, 10656, 10657, 10658, 10659, 10660, 10661, 10662, 10663, 10664, 10665, 10666, 10667, 10668, 10669, 10670, 10671, 10672, 11424, 11425, 11426, 11427, 11428, 11468, 11539, 11540, 11541, 12080, 12081, 12082, 12083, 12084, 12085, 12111, 12112, 12113, 12114, 12115, 12116, 12117, 12118, 12119, 12120, 12121, 12122, 12123, 12124, 12125, 12126, 12127, 12128, 12129, 12130, 12131, 12132, 12133, 12134, 12135, 12136, 12137, 12138, 12139, 12140, 12141, 12142, 12143, 12144, 12145, 12146, 12147, 12148, 12149, 12150, 12151, 12152, 12554, 12555, 12556, 12557, 12558, 12559, 12560, 12561, 12562, 12563, 12564, 12565, 12566, 12567, 12568, 12569, 12570, 12571, 12572, 12573, 12574, 12575, 12576, 12577, 12578, 12579, 12580, 12616, 12617, 12618, 12619, 12620, 12621, 12622, 12623, 12624, 12625, 12635, 12636, 12637, 12638, 12639, 12640, 12641, 12642, 12643, 12644, 12645, 12646, 12647, 12648, 12649, 12650, 12651, 12652, 12653, 12654, 12655, 12656, 12657, 12658, 12659, 12660, 12661, 12662, 12663, 12664, 12665, 12666, 12667, 12668, 12705, 12706, 12707, 12708, 12709, 12710, 12711, 12712, 12713, 12714, 12715, 12716, 12717, 12718, 12719, 12720, 12721, 12722, 12723, 12724, 12725, 12726, 12727, 12728, 12729, 12730, 12731, 12732, 12733, 12776, 12803, 12804, 12805, 12806, 12807, 12808, 12809, 12810, 12811, 12907, 12908, 12909, 12939, 12940, 12941, 12942, 12943, 12944, 12945, 12946, 12972, 13061, 13062, 13063, 13064, 13065, 13066, 13067, 13068, 13069, 13070, 13071, 13072, 13073, 13074, 13075, 13076, 13077, 13078, 13079, 13080, 13081, 13082, 13083, 13084, 13085, 13086, 13087, 13088, 13089, 13090, 13091, 13092, 13093, 13094, 13095, 13096, 13097, 13098, 13099, 13100, 13101, 13102, 13103, 13104, 13105, 13106, 13107, 13139, 13140, 13141, 13142, 13143, 13705, 13706, 13707, 13708, 13709, 13710, 13711, 13712, 13713, 13714, 13715, 13716, 13717, 13718, 13719, 13720, 13721, 13722, 13723, 13724, 13725, 13726, 13727, 14948, 14949, 14950, 14951, 14952, 14953, 14954, 14955, 14956, 14957, 14958, 14959, 14960, 15824, 15825, 15826, 15827, 15828, 15829, 15925, 15926, 15927, 15928, 15929, 15930, 16229, 16230, 16231, 16232, 16233, 16234, 16235, 16236, 16237, 16238, 16239, 16240, 16241, 16242, 16683, 16684, 16685, 16686, 16687, 16704, 16705, 16706, 16707, 16708, 16709, 16710, 16711, 16712, 16713, 16714, 16715, 16716, 16717, 16718, 16719, 17389, 17390, 17391, 17392, 17393, 17394, 17395, 17396, 17397, 17398, 17399, 17400, 18083, 18084, 18822, 18823, 18824, 18825, 18826, 19648, 19649, 19650, 19651, 19652, 19653, 19654, 19655, 19656, 19665, 20250, 20251, 20252, 20253, 20254, 20410, 20411, 20412, 20413, 20414, 20415, 20416, 21858, 21859, 21860, 21861, 21862, 21863, 21864, 21865, 21866, 21867, 21868, 21869, 21870, 21871, 21872, 21873, 21874, 21875, 21876, 21877, 21912, 21913, 21914, 21915, 21916, 21917, 21918, 21919, 21920, 21921, 21922, 22544, 22545, 22546, 22547, 22548, 22549, 22550, 22551, 22654, 22708, 22709, 22710, 22711, 22712, 22713, 22714, 22715, 22716, 22717, 22718, 22719, 22819, 22820, 22821, 22822, 22823, 22824, 22825, 22826, 22827, 22828, 22829, 22830, 22831, 22832, 22833, 22834, 22835, 22836, 22837, 22838, 22839, 22840, 22841, 22842, 22843, 22844, 22845, 22846, 22847, 22848, 22849, 22850, 22851, 22852, 22853, 22854, 22855, 22907, 22908, 22909, 22910, 22911, 22912, 22913, 22914, 22915, 22916, 22917, 22918, 22919, 22920, 22921, 22938, 22939, 22940, 22946, 22947, 22948, 22949, 22950, 22951, 22952, 22953, 22954, 22955, 22956, 22957, 22958, 22959, 22960, 22961, 22962, 22963, 22964, 22965, 22966, 22967, 22968, 22969, 22970, 22971, 22972, 22973, 22974, 22975, 23000, 23001, 23002, 23003, 23004, 23005, 23006, 23007, 23008, 23009, 23010, 23011, 23012, 23013, 23014, 23015, 23016, 23017, 23018, 23019, 23020, 23021, 23022, 23023, 23024, 23025, 23026, 23027, 23028, 23201, 23202, 23203, 23204, 23205, 23206, 23207, 23208, 23222, 23223, 23224, 23225, 23226, 23227, 23228, 23229, 23230, 23231, 23232, 23233, 23234, 23244, 23245, 23262, 23263, 23264, 23265, 23266, 23267, 23268, 23269, 23270, 23271, 23272, 23273, 23274, 23275, 23276, 23277, 23688, 23689, 23690, 23691, 23692, 23693, 23694, 23695, 23696, 23697, 23698, 23699, 23700, 23701, 23702, 23703, 23704, 24664, 25056, 25057, 25058, 25059, 25060, 25061, 25062, 25063, 25064, 25065, 25066, 25067, 25068, 25069, 25092, 25093, 25094, 25095, 25096, 25097, 25098, 25099, 25100, 25101, 25102, 25103, 25104, 25230, 25231, 25232, 25439, 25440, 25441, 25448, 25470, 25471, 25472, 25473, 25474, 25475, 25476, 25477, 25478, 25479, 25480, 25481, 25482, 25483, 25484, 25485, 25486, 25487, 25488, 25489, 25490, 25491, 25492, 25493, 25494, 25495, 25496, 25497, 25498, 25499, 25500, 25501, 25502, 25503, 25504, 25505, 25506, 25507, 25508, 25509, 25510, 25511, 25512, 27517, 27518, 27519, 27520, 27521, 27522, 27523, 27524, 27525, 27526, 27564, 27565, 27566, 27567, 27568, 27569, 27570, 27571, 27572, 27573, 27574, 27575, 27576, 27577, 27578, 27579, 27580, 27581, 27582, 27583, 27584, 27585, 27586, 27587, 27588, 27589, 27590, 27591, 27592, 27593, 27594, 27621, 27622, 27623, 27624, 27629, 27630, 27631, 27632, 27633, 27634, 27672, 27673, 27674, 27675, 27676, 27677, 27678, 27679, 27680, 27681, 27692, 27693, 27694, 27695, 27696, 27697, 27698, 27699, 27701, 27702, 27703, 27704, 27705, 27706, 27707, 27830, 27831, 27832, 27833, 27834, 27835, 27836, 27837, 27838, 27839, 27840, 27841, 27842, 27843, 27844, 27845, 27846, 27847, 27848, 27849, 27850, 27851, 27852, 27853, 27862, 27863, 27864, 27865, 27866, 27867, 27868, 27869, 27870, 27871, 27872, 27873, 27874, 27875, 27876, 27877, 27878, 27879, 27880, 27881, 27891, 27892, 27893, 27894, 27895, 27896, 27897, 27898, 27899, 27900, 27901, 27902, 27903, 27904, 27905, 27906, 27907, 27908, 27909, 27926, 27927, 27928, 27929, 27930, 27931, 27932, 27933, 27934, 27935, 27936, 27950, 27951, 27952, 29439, 29440, 29441, 29442, 29443, 29444, 29445, 29446, 29447, 29448, 29449, 29450, 30507, 30508, 30509, 30510, 30511, 30512, 30513, 30514, 30617, 30618, 30619, 30620, 30621, 30622, 30623, 30624, 30625, 30626, 30627, 30628, 30629, 30630, 30631, 30632, 30633, 30634, 30635, 30636, 30637, 30638, 30639, 30640, 30641, 30642, 30643, 30644, 30645, 30646, 30647, 30648, 30649, 30650, 30651, 30652, 30653, 30654, 30655, 30656, 30657, 30658, 30665, 30666, 30667, 30668, 30669, 30670, 30671, 30672, 30673, 30674, 30675, 30676, 30677, 30678, 30679, 30680, 30681, 30682, 30683, 30684, 30685, 30686, 30687, 30688, 30689, 30690, 30691, 30692, 30693, 30694, 30695, 30696, 30697, 30698, 30699, 30700, 30701, 30702, 30703, 30704, 30705, 30706, 30959, 30960, 30961, 30966, 30967, 32358, 32359, 32360, 32361, 32362, 32363, 32364, 33937, 33953, 34005, 34006, 34093, 34094, 34095, 34096, 34097, 34098, 34099, 34100, 34101, 34149, 34150, 34151, 34152, 34153, 34154, 34205, 34206, 34207, 34208, 37439, 37440, 37441, 37442, 37443, 37444, 37448, 37449, 37450, 37451, 37452, 37453, 37454, 37455, 37456, 38432, 38433, 38434, 38435, 38436, 38437, 38438, 38439, 38440, 38441, 38442, 38443, 38444, 38445, 38446, 38447, 38448, 38449, 38450, 38451, 38452, 38453, 38454, 38455, 38456, 38457, 38458, 38459, 38460, 38579, 38580, 38581, 38582, 38583, 38584, 38585, 38586, 38587, 38588, 38589, 38590, 38591, 39164, 39165, 39200, 41191, 41192, 41193, 41194, 41195, 41196, 41197, 41198, 41199, 41200, 41201, 41202, 41203, 41204, 41205, 41206, 41207, 41265, 41266, 41267, 41268, 41269, 41270, 41271, 41272, 41273, 41274, 41275, 41276, 41277, 41871, 41872, 41873, 41874, 41875, 41876, 41877, 41908, 42027, 42028, 42029, 42030, 42031, 42032, 42033, 42034, 42052, 42053, 42054, 42055, 42056, 42057, 42058, 42059, 42060, 42061, 42062, 42063, 42064, 42065, 42066, 42067, 42068, 42069, 42070, 42071, 42072, 42073, 42074, 42075, 42076, 42077, 42078, 42079, 42080, 42081, 42082, 42083, 42084, 42085, 42086, 42087, 42088, 42089, 42090, 42091, 42092, 42093, 42094, 42095, 42096, 42097, 42098, 42099, 42100, 42101, 42102, 42103, 42104, 42105, 42106, 43412, 43413, 43414, 44089, 44090, 44091, 44092, 44093, 44094, 44095, 44096, 44097, 44098, 44099, 44100, 44101, 44102, 44103, 44104, 44105, 44106, 44107, 44108, 44109, 44110, 44111, 44112, 44113, 44114, 44115, 44116, 44117, 44118, 44119, 44120, 44121, 44122, 44123, 45531, 45874, 45875]\r\n",
      "- All prompts end with suffix ` \\n\\n###\\n\\n`\n",
      "- All completions end with suffix ` \\n`\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Recommended] Remove 1527 duplicate rows [Y/n]: ^C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!openai tools fine_tunes.prepare_data -f all_training_data.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923b02a",
   "metadata": {},
   "source": [
    "# Upload file to your OpenAI account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fa6f8267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<File file id=file-0nQJ5M8CEJpOh3CAncVUbV0B at 0x150712590> JSON: {\n",
       "  \"bytes\": 6215647,\n",
       "  \"created_at\": 1678777896,\n",
       "  \"filename\": \"file\",\n",
       "  \"id\": \"file-0nQJ5M8CEJpOh3CAncVUbV0B\",\n",
       "  \"object\": \"file\",\n",
       "  \"purpose\": \"fine-tune\",\n",
       "  \"status\": \"uploaded\",\n",
       "  \"status_details\": null\n",
       "}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_response = openai.File.create(\n",
    "  file=open('all_training_data_prepared.jsonl', \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")\n",
    "upload_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fb4254",
   "metadata": {},
   "source": [
    "# Save file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "93469f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file-0nQJ5M8CEJpOh3CAncVUbV0B'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_id = upload_response.id\n",
    "file_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1007a6",
   "metadata": {},
   "source": [
    "# Fine-tune a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e2da9b",
   "metadata": {},
   "source": [
    "The default model is **Curie**. \n",
    "\n",
    "If you'd like to use **DaVinci** instead, then add it as a base model to fine-tune:\n",
    "\n",
    "```openai.FineTune.create(training_file=file_id, model=\"davinci\")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "16bb42a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTune fine-tune id=ft-2AHEeTXrtThrdjE2ssLmHj1a at 0x1507126d0> JSON: {\n",
       "  \"created_at\": 1678777772,\n",
       "  \"events\": [\n",
       "    {\n",
       "      \"created_at\": 1678777772,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Created fine-tune: ft-2AHEeTXrtThrdjE2ssLmHj1a\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    }\n",
       "  ],\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"hyperparams\": {\n",
       "    \"batch_size\": null,\n",
       "    \"learning_rate_multiplier\": null,\n",
       "    \"n_epochs\": 4,\n",
       "    \"prompt_loss_weight\": 0.01\n",
       "  },\n",
       "  \"id\": \"ft-2AHEeTXrtThrdjE2ssLmHj1a\",\n",
       "  \"model\": \"curie\",\n",
       "  \"object\": \"fine-tune\",\n",
       "  \"organization_id\": \"org-ojH41IdW0UR2VlysxKUx8AjA\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"pending\",\n",
       "  \"training_files\": [\n",
       "    {\n",
       "      \"bytes\": 6560975,\n",
       "      \"created_at\": 1678777772,\n",
       "      \"filename\": \"file\",\n",
       "      \"id\": \"file-nXKcQKc4Uwlz1P1IYst8NbDQ\",\n",
       "      \"object\": \"file\",\n",
       "      \"purpose\": \"fine-tune\",\n",
       "      \"status\": \"uploaded\",\n",
       "      \"status_details\": null\n",
       "    }\n",
       "  ],\n",
       "  \"updated_at\": 1678777772,\n",
       "  \"validation_files\": []\n",
       "}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tune_response = openai.FineTune.create(training_file=file_id)\n",
    "fine_tune_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059b2b3",
   "metadata": {},
   "source": [
    "# Check fine-tune progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7fda90",
   "metadata": {},
   "source": [
    "Check the progress with `openai.FineTune.list_events(id=fine_tune_response.id)` and get a list of all the fine-tuning events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4cf062ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject list at 0x15083aef0> JSON: {\n",
       "  \"data\": [\n",
       "    {\n",
       "      \"created_at\": 1678777772,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Created fine-tune: ft-2AHEeTXrtThrdjE2ssLmHj1a\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    },\n",
       "    {\n",
       "      \"created_at\": 1678778298,\n",
       "      \"level\": \"error\",\n",
       "      \"message\": \"Fine-tune failed. Fine-tune can not exceed $15 during free trial\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    }\n",
       "  ],\n",
       "  \"object\": \"list\"\n",
       "}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tune_events = openai.FineTune.list_events(id=fine_tune_response.id)\n",
    "fine_tune_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f5bf3",
   "metadata": {},
   "source": [
    "Check the progress with `openai.FineTune.retrieve(id=fine_tune_response.id)` and get an object with the fine-tuning job data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f01831be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTune fine-tune id=ft-2AHEeTXrtThrdjE2ssLmHj1a at 0x150856680> JSON: {\n",
       "  \"created_at\": 1678777772,\n",
       "  \"events\": [\n",
       "    {\n",
       "      \"created_at\": 1678777772,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Created fine-tune: ft-2AHEeTXrtThrdjE2ssLmHj1a\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    },\n",
       "    {\n",
       "      \"created_at\": 1678778298,\n",
       "      \"level\": \"error\",\n",
       "      \"message\": \"Fine-tune failed. Fine-tune can not exceed $15 during free trial\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    }\n",
       "  ],\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"hyperparams\": {\n",
       "    \"batch_size\": null,\n",
       "    \"learning_rate_multiplier\": null,\n",
       "    \"n_epochs\": 4,\n",
       "    \"prompt_loss_weight\": 0.01\n",
       "  },\n",
       "  \"id\": \"ft-2AHEeTXrtThrdjE2ssLmHj1a\",\n",
       "  \"model\": \"curie\",\n",
       "  \"object\": \"fine-tune\",\n",
       "  \"organization_id\": \"org-ojH41IdW0UR2VlysxKUx8AjA\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"failed\",\n",
       "  \"training_files\": [\n",
       "    {\n",
       "      \"bytes\": 6560975,\n",
       "      \"created_at\": 1678777772,\n",
       "      \"filename\": \"file\",\n",
       "      \"id\": \"file-nXKcQKc4Uwlz1P1IYst8NbDQ\",\n",
       "      \"object\": \"file\",\n",
       "      \"purpose\": \"fine-tune\",\n",
       "      \"status\": \"processed\",\n",
       "      \"status_details\": null\n",
       "    }\n",
       "  ],\n",
       "  \"updated_at\": 1678778298,\n",
       "  \"validation_files\": []\n",
       "}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_response = openai.FineTune.retrieve(id=fine_tune_response.id)\n",
    "retrieve_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2dac2d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['curie:ft-truefoundry-2023-03-09-07-03-13',\n",
       " 'curie:ft-truefoundry-2023-03-09-07-07-42',\n",
       " 'curie:ft-truefoundry-2023-03-11-07-50-58',\n",
       " 'curie:ft-truefoundry-2023-03-11-08-11-15',\n",
       " 'curie:ft-truefoundry-2023-03-13-03-39-11',\n",
       " 'curie:ft-truefoundry-2023-03-15-11-19-29']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tune_list = openai.FineTune.list()\n",
    "# [elem.fine_tuned_model for elem in fine_tune_list['data']]\n",
    "[elem.fine_tuned_model for elem in fine_tune_list['data'] if elem.fine_tuned_model != None and elem.object == \"fine-tune\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e846ed",
   "metadata": {},
   "source": [
    "# Save fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca86136",
   "metadata": {},
   "source": [
    "### Troubleshooting fine_tuned_model as null\n",
    "During the fine-tuning process, the **fine_tuned_model** key may not be immediately available in the fine_tune_response object returned by `openai.FineTune.create()`.\n",
    "\n",
    "To check the status of your fine-tuning process, you can call the `openai.FineTune.retrieve()` function and pass in the **fine_tune_response.id**. This function will return a JSON object with information about the training status, such as the current epoch, the current batch, the training loss, and the validation loss.\n",
    "\n",
    "After the fine-tuning process is complete, you can check the status of all your fine-tuned models by calling `openai.FineTune.list()`. This will list all of your fine-tunes and their current status.\n",
    "\n",
    "Once the fine-tuning process is complete, you can retrieve the fine_tuned_model key by calling the `openai.FineTune.retrieve()` function again and passing in the fine_tune_response.id. This will return a JSON object with the key fine_tuned_model and the ID of the fine-tuned model that you can use for further completions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a071cb90",
   "metadata": {},
   "source": [
    "### Option 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e760b",
   "metadata": {},
   "source": [
    "If `fine_tune_response.fine_tuned_model != None` then the key **fine_tuned_model** is availble from the fine_tune_response object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0616dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine_tune_response.fine_tuned_model != None:\n",
    "    fine_tuned_model = fine_tune_response.fine_tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34613188",
   "metadata": {},
   "source": [
    "### Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3635655e",
   "metadata": {},
   "source": [
    "If `fine_tune_response.fine_tuned_model == None:` you can get the **fine_tuned_model** by listing all fine-tune events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "77d2f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine_tune_response.fine_tuned_model == None:\n",
    "    fine_tune_list = openai.FineTune.list()\n",
    "    fine_tuned_model = fine_tune_list['data'][0].fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8bab66c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'curie:ft-truefoundry-2023-03-09-07-03-13'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d0d25",
   "metadata": {},
   "source": [
    "### Option 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13537bb",
   "metadata": {},
   "source": [
    "If `fine_tune_response.fine_tuned_model == None:` you can get the **fine_tuned_model** key by retrieving the fine-tune job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "35e6203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine_tune_response.fine_tuned_model == None:\n",
    "    fine_tuned_model = openai.FineTune.retrieve(id=fine_tune_response.id).fine_tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8601df11",
   "metadata": {},
   "source": [
    "# Test the new model on a new prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654268c",
   "metadata": {},
   "source": [
    "Remember to end the prompt with the same suffix as we used in the training data; ` ->`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37cfb2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = \"We started with Daily Stand-ups and have been through several iterations \\n\\n###\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "021b6d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bee69cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  => Went from daily to three times a week'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = openai.Completion.create(\n",
    "  model=fine_tuned_model,\n",
    "  prompt=new_prompt,\n",
    "  max_tokens=10, # Change amount of tokens for longer completion\n",
    "  temperature=0\n",
    ")\n",
    "answer['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5c99bebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n###\\n\\n###\\n\\n###\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = openai.Completion.create(\n",
    "  model='curie',\n",
    "  prompt=new_prompt,\n",
    "  max_tokens=10, # Change amount of tokens for longer completion\n",
    "  temperature=0\n",
    ")\n",
    "answer['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469f7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58fdbdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f10d668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "# input text\n",
    "text = \"\"\"\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc at purus euismod, porttitor dolor vitae, mattis odio. Fusce dapibus enim in lectus fringilla fringilla. Vestibulum id euismod urna. Nulla facilisi. Nulla facilisi. Donec molestie, est ac lacinia dictum, tellus dolor malesuada lectus, sit amet consequat velit nisl a odio.\n",
    "\n",
    "Sed tincidunt leo in quam volutpat, sed lacinia elit venenatis. Integer consectetur, nisl nec gravida semper, arcu nisi tincidunt dolor, ac bibendum quam felis at odio. Vivamus nec lorem luctus, vulputate augue sed, fermentum nibh. Ut tristique diam ut diam vestibulum euismod. Donec id metus ac nunc venenatis dictum.\n",
    "\n",
    "Praesent quis mauris at magna auctor bibendum. Suspendisse nec aliquam massa. Mauris vel fermentum dolor. Sed ut tellus turpis. Ut consectetur risus nec lectus porttitor, vel facilisis velit pellentesque. In malesuada nulla et nisl pharetra, eu cursus dolor ultrices. Nunc bibendum eget sapien a egestas.\n",
    "\n",
    "Nullam vel sagittis mi. Curabitur sed nulla purus. Nulla facilisi. Nunc in urna a elit fringilla placerat. Fusce nec sapien eros. Donec interdum eu sapien non efficitur. Pellentesque ac tortor lectus. Nam at odio ut velit bibendum dapibus. Nulla tempus, arcu sed hendrerit feugiat, augue lacus fermentum orci, at elementum eros ipsum sit amet augue.\n",
    "\"\"\"\n",
    "text  = text_content\n",
    "\n",
    "# regex pattern to match pairs of prompts and responses\n",
    "pattern = r\"(.+?)\\.?\\s*(?:(?:And|but|or)\\s+)?(?:however|meanwhile|therefore|moreover|in addition|on the other hand|by contrast|likewise|accordingly|conversely|in fact|otherwise)?\\s*(.+?\\.|.+)$\"\n",
    "\n",
    "# find all pairs of prompts and responses in the input text\n",
    "matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "# create a CSV file to store prompts and responses\n",
    "with open('data_confluence.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['prompt', 'response'])\n",
    "\n",
    "    # write each pair of prompts and responses to the CSV file\n",
    "    for match in matches:\n",
    "        prompt, response = match\n",
    "        writer.writerow([prompt.strip(), response.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca5904b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = [{\n",
    "    \"prompt\": \"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"mickey ->\",\n",
    "    \"completion\": \" nikunj.\\n\"\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "371c5fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. Hi Nikunjow. Are you? Hi, Nachi Hum, I'm good. How are you? How do you pronounce the name? Yeah. Actually. Yeah, Nachi that's right. Okay. Yeah, very nice to meet you. How are you doing? Yeah, good. Awesome. We based on the Bay Area. I'm in the Bay Area, where are you? I am generally based in the Bay Area, San Francisco and I'm right now, I'm traveling to India. So a lot of my teams here, some meeting them here in Bangalore oh, how long? I don't get to go too, often. Actually it's pretty rare. Okay. I went once left here and that was a first trip in about a decade. How? Wow, okay, I see you were asking sorry I think I got you off. Oh, I was just asking how long you're going to be there for. So I expect so this two possibilities either I can come back to the bay coming Monday. So like just three days from now or I might be here like four to five weeks and then come back to bay. So one of the two Okay, great. Yeah. Where the day do you live? San Francisco Main. So I live on Market Street Market in 10th Okay, okay. Great. How about how about you? Yeah. I'm at Washington and Van Ness. Okay, I see not too far. Yeah. Not too far. Yeah. Nice. Yeah. Awesome man. How do you know Nihal? We're doing Niall for a long time. I worked for him when I was in high school. You. Oh, wow. Yeah. Yeah he was a he was a pen. He was a couple years ahead of me and somehow got set up on a project where he was the manager. And then I worked with him again and then just sort of stayed. Until I mean, the halls amazing sort of stayed in touch with them. yeah, I I hear From 25. Years ago. Yeah. Yeah. Long time. Long time, nice. I hear that from so many of my house connections that, like, just like this person who like a social bee, right? Like he, he knows how to how to keep friends with people for a really really long time. Like maintaining long term relationships. Yeah, how about yourself? Yeah. So I work with Hadley, do you know, Hadley as well? I've met I think I've met him once or twice but I know. Yeah. So basically any aiac invested in us Hadley is the main partner that we work with and of course like you know through hardly like have worked with Nihal decently closely. Yeah. and when do they make the investment is this recent or So, it was 2021 September. So it's been about a year and a half. Now, Okay. All right, yeah. I'd love to hear about the the company, your background. I'll tell you about. I'll tell you about some of the stuff we're doing and yeah, I'll take it from there. Sure awesome. Yeah let me let me share about my background to begin with. So nachi, I I come from a machine Yeah. learning background. I worked at Facebook in deep learning research. At one of the conversational AI divisions. So we launched the product called Portal, which is like Facebook's version of Google Home, Right? Or Alexa. Yeah, I remember that. Yeah. Yeah. So so that's what I did there Prior to that. I was leading the machine learning team at a startup call reflection based out of San Mateo and there we build out recommended systems for the e-commerce industry. So reflection, I got a chance to work both on the modeling side of things and building out the horizontal machine learning platform for the company. So which we scale to roughly about Okay. 600 million users. Now between quitting Facebook and starting true foundry. I did one more startup called reflect called entire that was in the talents case and that got acquired by Info H, which is like the largest HRD prayer in India, right? Like owner of marketing. Basically I moved to the US from a masters when to UC Berkeley and basically since then have primarily been in the bay and did my undergrad at one of the Its Like It kharagpur. That's where I met my now co-founders and ragin abhishek as well. So they're my batch mates and hallmates from Kharagpur so that's nice to build the company together with them. And their background is gonna Abhishek was at Facebook for last six years and Rock was at a hedge fund called World Quant. Operating out of the organ. Singapore offices. So that's roughly about the team and quickly about true foundry. So, basically Nachi, I'm taking some of my learnings from reflection where we build out this horizontal level platform and from Facebook, where we saw like, you know, I've been learner predictor which is like their internal ML platform, right? Capturing those learnings and trying to build out a platform that can help other ML teams, operationalize their models better, right? So think How do they deploy their models? How do they monitor their models? Something is going wrong, how do they identify, which are the problem areas in their data set. So those are some of the things that our platform enables people to do. And in this journey, we are fairly early a year and a half. As I mentioned, working with a few enterprises few large startups. That's the typical market that we're targeting, but but call them design partners. Still don't call them customers. Yeah. Okay. So, that's Do you have right now? Part of me. How many clients do you have right now? yeah, we are working with like eight clients so far for very large For them. companies like 50 billion Dollar Plus and for Oh, For like you know startups three to 10 billion dollar. Three to five billion dollar range maybe. Yeah. Okay, and then how much have you guys done in a fundraising? So when Nihal Evan Hadley and all invested we are raised, like, roughly 2.3 mil back then. But recently, we opened up a small safe to to add some value, added to get some value added angels. And that like, you know, like 200k thing expanded to 1.2 mill. So roughly 3.5 mil so far. Okay, and then how big is a team right now? We are 17 folks, strong full time and like four. Five interns contractors, etc. Okay, and mostly in India. Mostly in India. We have one person in Paris a couple of us in the US and that's it. Yeah. Okay, that's incredible. So so yeah, this journey is amazing. Always look forward to like, you know, making new connections, especially folks who are working in the in the domain. And by the way, I go from the context Yeah. of the email that like nothing right now is interesting to you all and Yeah. that's okay. I honestly just wanted to learn about. And by the way, this space that you all are working in is very new to me so I was to learn How machine learning is done at a startup like Korea. Therapeutics is working in this, like, in a completely new domain that I'm not aware of, I guess. Yeah. Yeah, yeah. So yeah, I'll give you a little bit of history into it and it's actually very interesting. Machine learning computational applications are just going to be broadly used across multiple industries and I think the applications into biological systems is going to expand more and more, we specifically work on a type of gene therapy called AAV, gene therapy. So, gene therapy is the delivery of a gene of interest into the body somewhere and that gene has to get there somehow. So we put that inside of viral capsid, in this case, it's called AAV agno, associated virus, This is really just sort of a small basketball and you're it's gonna have pores on the outside and you're going to through a mechanism inject into it a DNA sequence and create the virus with the DNA that you want. That virus, will then infect us so that you're interested in going into. And it will inside the cell on the viral capsule and coat. And that DNA sequence will allow you to create protein of interest that you want to. So that's kind of the big picture idea of AAV gene therapy. Where some of the computational approaches have come in are, how can we improve delivery that is site-specific? So let's say I want to create a therapy that is specifically for the heart. I want to inject it straight into the heart. If I can't, inject it into the heart tissue, then maybe I can give it into a vein and then it makes its way to the heart. So one big problem is, How do we get cell specific transduction transduction? Just means getting into the specific cell type. And the reason this is important, is a lot of these genes can be toxic to in other parts of the body. So they have to get to the correct cell and produce in that cell, only for safety and toxicity reasons. And you can also give higher doses if you can stay in one cell. So over the last five to ten years, there's been a bit of movement on how can we create viral capsids that our cell specific and this has led to You know, just going back a little bit historically because it is somewhat. Interesting. There there are different serotypes. Here are types are just different types of aavs that show different things on their surface different antigens on their surface that allow different types of binding. There are a number of different stereotypes and people started taking all these different stereotypes you can imagine like maybe you have 20 different soccer balls and 20 different colors and different companies made the soccer balls and you're going to slice and dice them and repeat them together so you're getting soccer balls from different pieces. So that's called it's called it's called DNA. Space. Shuffling DNA says, you break up the DNA sequence of the viral capsids and you shuffle them around and whatever sticks because the pieces where the correct pieces and the edges, they come back together and they perform this new virus and nobody knows how that new virus will perform. You don't know if that's going to be better or worse. If it's even going to be stable. And but the idea is that if you start with maybe 10 of these different viruses and you put them through this process where you shuffle them around and they reassemble and maybe if you did this a million times and you do this in a sample of 1 million and you screen for it, only the ones that actually form would stick around That's historically how people did what's called capsa discovery. As they just sort of throw everything into like a meat grinder, and then to see what sticks together. Now, obviously with computational approaches things have become a little bit different. There is another major approach that people did which is there are certain areas on the surface of the viral cops that are known to cause change in their ability to interact with other molecules And these were surface exposed domains. They're kind of like, you can imagine. Like, the soccer ball now has a little keys sticking out of it and so we know where the keys are and we know that if we change the shape of the key, then that's going to interact differently with different cell types. Now, we don't always know what that shape should be. So Another approach that became popular in the last 10 years was random insertions and random mutagenesis over these areas. So insert amino acid sequences that are completely random, make libraries and millions, and then just see which ones get into a certain cell type. You throw the whole the library at the At the cell and you see what you find inside the cell and you sequence what's inside? And you know what happened? So both of these together kind of led to this idea. Well, we have an idea of what some of these keys look like, but we don't know very well. We have no idea of where these surface exposure regions are And we know that this rant totally random shuffling is very inefficient. Can we do something more intelligent? And so there have been a number of strong efforts. The one that is in the has done The best job of PR is dyno therapeutics the big ones in this area. Dyno therapeutics capsida biosciences And shape therapeutics. They're probably the three biggest encapsid and machine learning based capsid engineering right now. What they're doing is they're finding this specific region and they're thinking of what they can learn from the previous iteration of a random insertion to help steer the direction towards the correct key shapes. And so, that's kind of the big picture. There's a lot of there have been a lot of thoughts towards generative modeling very similar to like, This crayon and chachi pt the idea of being. You know, we have this series of things that we know work, can we create other things that belong in the same series, but we don't already have? And then can we use another machine? Learning model to determine if this item truly belongs in the series or doesn't? So I think I think of it as a two-part problem.  so we were working on this. We actually stopped a lot of our machine learning based caps and engineering approaches. We put we shelved them for a little while.  mostly due to some of the, you know, competitive landscape and where we want to direct a company. So we're doing a little bit less in that area right now. But that's kind of you know, just for big picture. I think that that's a and those types of problems. That's the problem of getting into a cell, the type of problem of Creating a promoter that drives the correct gene expression in certain cell types, is another one that lends itself well to machine learning. It and I mean modification of DNA sequences in general. I think there's I think there's a pretty wide range there. so, I think Understood. Yeah, and then we're also doing some image segmentation and image modeling, so we're moving. We're moving out of some of those DNA sequence and I personally, I was a programmer for a long time. So I wrote some of the code for some of what we did for DNA sequence modification, and we're still using it, actually every program that we take towards clinic as has been modified by code that I wrote But now we're kind of moving a little bit away from those DNA models to image. Image, Segmentation Models. I'll give you an example of things, things that we're thinking of, obviously, I know you want, you're not going to share what we're doing, but also, I'll give you like sort of an example. There's a We think a lot about ophthalmology, we think a lot about diseases of the eye and inflammatory conditions in the eye. And you can put a scope against the eye. It's called a fundoscope and you can look back straight at the retina and you can look at what the surface of the retina looks like. Or is there vasculature coming out of Is there increased areas of like fat it? deposits or protein deposits. There's hemorrhaging and so you can basically go back and you can see what's what's going on there. This is an area that I think is also right for machine learning, or alternatively image. Segmentation Type of Models. The problem with machine learning for this problem is a there aren't large sums of data. There are the database of Images to train on is a lot smaller, I think and that's actually something we're dealing with right now is trying to understand where to get good images. But the big, the big picture in general. Is we look at actually I'm not quite as good at Google, but let me see if I can share screen quickly and I'll show you an image of it from this. Let me know if this is a interesting or for going down a rabbit hole. For sure. I mean this is where I'm learning a completely new thing. I do have a couple of other questions for you as well, but would love to understand this part. Yeah. Yeah, yeah. Let me, let me see if I can See if I can share screen. Oh Open System Preferences. oh, I think you might start if you don't have the permission Yeah, let me see if it let's If you want, you can bring me the link and I can share my screen. Okay. Yeah, let's do that. Yeah, yeah, right. It's gonna make me restart. I was, I was just yeah. Let me, let me spend the link. I just pinged you on the chat so that there should be like a little button somewhere in that little notification somewhere. Yeah. I mean, I just went into image because Google images since I've been fundoscopy image, just to like look Yeah. at what? Just so you have an idea and then maybe if you just blow up, any any image Yeah, yeah pick. Yep, make one as big as possible and Yeah, this is not seem like the right. yes, to Yeah, perfect. So In general. You know this is a type of image that we're getting there. They're slightly slight variations. There's one called faf images, Auto florists fluorescence image, which highlights things a little bit different, it's in grayscale. But what you're looking at here is one problem, is identifying what everything is? So, the brighter area is the optic nerve. There's something called the macula and the phobia and you can typically identify these based on where they're located. Like yeah. Right. Right. Where your magnifying glass was a second ago to the left a little bit to the left and there's like another sort of central area, that's a little bit darker, that's the Phoebe. The fovea is where you get them. The majority of your vision from and then the macula, as the area around that it includes the phobia, and that's the retina. And so, when you're thinking about retinal diseases, trying to understand what changed in this area, and these images will look very they'll be very different. There will be deposits of accumulation of like fats and proteins and certain disease processes where you can tell that there is something that is going to be problematic in the future here because it's going to obstruct your vision. And so what the current pathway is you have an ophthalmologist take a look at this, they take a picture of it and they can store them, your record. They'll say, I believe that there is this type of pathology disease. Here it is this, you know, far along. But ultimately I think that this is a problem that will lend itself well to So, yeah. Yeah. But So we're thinking about that. I mean actually the easier ones are chest. X-ray is a in healthcare chest. X-ray cat's gonna head like these are all imaging problems but they have different implications. So if you if you do a chest, X-ray problem and a physician relies 100% on it and something goes wrong, you have a big problem. Whereas you know, really depends on what the application is as well. Understood. So one one question here nachi like from an from a machine learning standpoint Do so like I think you talk phenomenally out some of the applications. They're like you know you see machine learning could be could be interesting, right? When and and you also mentioned that you are reducing the ML efforts overall within the company. But I guess when people were doing machine learning have you heard of any specific challenges that people were trying to solve that? Oh I'm having problems in a getting more data. I guess is always a problem. But then anything beyond that, how to deploy, how to monitor anything, operational have you heard of anything like that from folks? I think I think there I think I think you're right. One is getting more data getting good quality. High quality data. Another one is the model itself How do you create the correct model to identify the right areas or, you know, code code. It's a correct way because I think that's a large part of getting this. Right. Also Data management is obviously a big issue, but these large pieces of data, how do you properly manage them to when we were doing a higher like larger scale machine learning problems? I was, I was not coding for those so the team that was doing that we have and some of them are still here. We're still doing some. We do something called Genomic analysis. So we sequence inside cells to find out what's inside and this creates a large sums of data. So we still have a computational team here. They're just doing currently they're doing a little bit less machine learning. Yeah data store. So there's about three there's two to three of them right now. Data storage remains a big Problem. But, you know, I think for our team, they're kind of build, rather than by We've actually been approached in the I've been approached by other people that I'm friends with also to see if we wanted help. And I'm, you know, I've talked to them, previously, they're, they're in the build phase. And and we're very, we've been very customs in terms of our solutions for different problems. So we haven't had sort of a large-scale framework either. That's that's a large scale framework. That is modular across different projects. I see. Okay but it's so basically you're saying that you all tend to build more because you need a lot of custom stuff that are typically not available in the other external large platforms. It's that and it's also sort of the personalities of the people you hired there. You know, they believe that they'd rather build which I think you know I think it really depends on the person. Some folks will say this takes up a lot of my time. I'd rather hire someone some folks say. I can do this and I'm good at it and so please don't hire someone. And then because it's also learning Understood. curve, there's maintenance when you hire externally. For sure, yeah, yeah, I guess I guess to be honest, I think that's this like frozen cons, both side like this maintenance, sometimes externally there's some maintenance internally, but I guess realistically boils down to the the personality of the people. Like if you're a person who wants to like own stuff, build stuff, etc, you probably would just prefer building. That's the DNA that you have seen in the in the team. Yeah. And then with some of the change in projects in the last six months or so. I think there's there's actually just less overall machine learning problems right now. They're mostly supporting our genomics team right now. I see. That may change this image. Segmentation problem is one that we're going to probably think more about but it is I don't think the data piece is the big piece. I think the the complex part of this problem is getting the algorithm, right? Understood. Okay, that makes sense. Yeah. And, and organizationally Nachi, like, How is the team structured like engineering? Like, Does machine learning fall within the scope of engineering reporting to like a CTO or something? How is that structured? Yeah, things have moved around a little bit recently. We did we did some like restructuring and so currently our Machine Learning Bioinformatics group lives in our genomics team, which lives a under our research and development team. We're we're at therapeutics company. So we're about a hundred. Yes, I don't think I gave you that much context on us, but we're about 170 people. We work primarily on building drugs, to get to clinic. most therapeutics companies don't have anyone on the machine learning bond from attic side. So with this, this is actually a very, very small pocket of the company. The majority of the company is You know, trying to get into clinical trials. it and I see okay understood what it so one thing that maybe I can learn from this call is like Companies. So you would imagine that most companies that work in your domain would likely have very little and very custom applications of ML that it's unlikely to be served by an external platform like ours. Basically, it's that is that the Yeah, I think that's, I think that's, space? I think that's pretty fair. I mean, the three companies that I mentioned to you are the big, the three big ones and capsid engineering. All three of them are going to do everything. Internal, like I don't even think it's worth reaching out to them Thanks. because they're Like there's these are such custom problems and people, you know, they pay a lot of money for their employees and and they hire smart people and these people do, you know, they build it internally. I think on the, You know, I think on probably in the diagnostic side, there might be some more interesting stuff on the diagnostic side. I see on this. Call it so maybe like, I'll ask you for another. I I'll ask you another question this domain. So like and by the way, just to give some context, amongst the customers that we are working with, We're working with Merck. The in the former domain we are Oh great. working with a largest farm medicine. Delivery company called Netmeds and then we are working with a very small startup called Neurobit. So, like in the healthcare domain, these are the three companies that we're working with currently. But, and another thing that I'm realizing is healthcare, has this unique combination of not being extremely advanced with ML, operational stuff. But now ending a lot of ML use cases Right. and having a lot of money to be honest. That's that's what that's the thing that I'm finding out in the healthcare domain. Could you help me, understand which areas in healthcare like, you know, according to your experience, could be right for a startup like ours to reach out to basically. I think. I think, in general, the The drug development side, the side that I'm on is generally less exciting. We did some of this sort of R&D like novel vector design in the past which do you use some ML? But therapeutics companies in general, I think are not especially small ones murk, isn't it? Murk isn't a slowly different bucket. They have a lot of money and a lot of different types of needs. I think anyone in like a A health tech spaces. Probably more interesting. I mean, you know, people working on a problem like reading extra reading X-ray is going to be solved by ML and the next 10 years. Probably Tend to tend to 30 different companies will try it and somebody will come out ahead, reading CT Head, These are going to require large sums of data image data, and we're going to require me machine learning support. Are you doing more of the machine learning? operation support or more of the machine learning programming itself for the algorithms to The Operations Less, let's program it. Algorithms here. Yeah, so I think I think any of these companies that are going to have large sums of data, I think, also at academic institutions, maybe a good to reach out to like like, Stanford over here, you know? If they they may want help and structuring and organizing their data to make to prepare it for research and analysis. Makes sense understood and by any chance in your close network, Do you know, some folks who are in a similar domain that you, that if that you would feel comfortable making an intro, I understand if you, if you would not like, you don't know me enough. Oh, no, it's not that at all. I mean, I don't know how so, very Okay. good. But yeah, let me think about, I don't, I can't, I can't think of anyone right now, stop my head, but I'll keep it in mind. Yeah. And the study It would be very helpful Nachi. If like, you know, remember someone who is using ML, who could potentially benefit from what we are building At this stage. I think practically all the customers that we are getting are like, you know, through the network and friends and people who are helping out at this point. So really appreciate, if I can get Okay. some help from you. But I understand like nothing comes across Did you have like a sales like PowerPoint Duck or something that goes through what you offer? So I can't do anyone right now but if I if I do then that would be a good Yep. way to introduce your company. Sure. Absolutely. So I, I do but like, if there's like a delay in like, you know, you think that it could take up a couple of weeks to get a, hold on to someone things at a startup scale change so fast that we keep updating our sales material. So, like in the moment you have something, if you just ping me, I'll send out the most recent one to you. I think that's probably the most Okay. useful. Yeah. Literally like every single week Okay. we're developing new features and we keep adding that to the to the direct basically. so, Yeah. The blurb does not change that much, but I think the actual handout that we have changes very frequently. Yeah. And and by the way, like Nachi so by the thanks a lot for helping me, understand some of the genomic sequencing problem, and all that you all are trying to solve. I, at this point I'm confident that I don't, I'm not in a position to help in any way. But at some point if you feel like I don't know, like hiring or anything that you might need any help with, please do reach out to me. I would love to be able to help in any way possible. Yeah. Yeah, of course I think probably in a year or two. We may be re-expanding the ML platform it. We're very Dependent on some of the like clinical results that you know, as things move forward in the clinical side, then the pool of money gets bigger. And you know, we think about doing more R&D Awesome, cool, awesome. Good luck, good luck. With the company, building, Nachi great, great connecting with you. Yeah, yeah, great meeting and you know, maybe sometime when you're back will catch up in the city say hello. Absolutely. Would love to meet in person. Take care. Bye.\n"
     ]
    }
   ],
   "source": [
    "# curl \\\n",
    "#    -X POST \\\n",
    "#    -H \"Content-Type: application/json\" \\\n",
    "#    -H \"Authorization: Bearer 1356da62-552d-4cec-a8e7-e4449f9d4ec3\" \\\n",
    "#    --data '{ \"query\": \"{ transcript(id:\\\"X0n3OrREM4ulna0e\\\"){ title date sentences {text }} }\" }' \\\n",
    "#    https://api.fireflies.ai/graphql/\n",
    "    \n",
    "    \n",
    "def read_fireflies_data(transcript_id):\n",
    "    # API endpoint\n",
    "    url = \"https://api.fireflies.ai/graphql/\"\n",
    "\n",
    "    # Your API key\n",
    "    api_key = \"1356da62-552d-4cec-a8e7-e4449f9d4ec3\"\n",
    "\n",
    "    # Construct the request payload\n",
    "    payload = {\"query\": '{transcript(id:\"%s\"){ title date sentences {text }} }' %transcript_id }\n",
    "\n",
    "    # Set the headers and authentication for the request\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    # Send the request and get the response\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "    transcript = ' '.join([elem['text'] for elem in response.json()['data']['transcript']['sentences']])\n",
    "    return transcript\n",
    "    \n",
    "print(read_fireflies_data(\"X0n3OrREM4ulna0e\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "797be1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-6v21VE6ovsSIM7KF6AJtbEaM8MxSn', 'object': 'chat.completion', 'created': 1679050493, 'model': 'gpt-3.5-turbo-0301', 'usage': {'prompt_tokens': 505, 'completion_tokens': 276, 'total_tokens': 781}, 'choices': [{'message': {'role': 'assistant', 'content': '\\n\\n[{\"prompt\":\"Why is ChatGPT not enough in itself for these vertical applications?\",\"completion\":\"Models like ChatGPT are trained on massive dataset but all that available on the clear web. So you can’t ask it a question whose answer depends on anything on deep web like your email or private docs.\"},{\"prompt\":\"What is the difference between clear web and deep web?\",\"completion\":\"Clear Web- publicly accessible web pages indexed by search engines. e.g. wikipedia, books, social media posts. Deep Web- part of internet that is behind an authentication system. e.g your email, or SaaS platforms. This constitutes 96% of the web.\"},{\"prompt\":\"What is fine-tuning and how is it used in the context of ChatGPT?\",\"completion\":\"Fine-tuning is a powerful technique that allows us to leverage the knowledge and learning of a pre-trained model like ChatGPT to improve the performance of the model on a new task by training it on a smaller, task-specific dataset.\"},{\"prompt\":\"Can you provide an example of how fine-tuning can be used in a specific scenario?\",\"completion\":\"Example, let’s say you want to build a Question answering system for your internal company docs stored in Confluence. You can pass all the text content from your confluence and fine tune a GPT model with them.\"}]'}, 'finish_reason': 'stop', 'index': 0}]}\n",
      "[{'prompt': 'Why is ChatGPT not enough in itself for these vertical applications?', 'completion': 'Models like ChatGPT are trained on massive dataset but all that available on the clear web. So you can’t ask it a question whose answer depends on anything on deep web like your email or private docs.'}, {'prompt': 'What is the difference between clear web and deep web?', 'completion': 'Clear Web- publicly accessible web pages indexed by search engines. e.g. wikipedia, books, social media posts. Deep Web- part of internet that is behind an authentication system. e.g your email, or SaaS platforms. This constitutes 96% of the web.'}, {'prompt': 'What is fine-tuning and how is it used in the context of ChatGPT?', 'completion': 'Fine-tuning is a powerful technique that allows us to leverage the knowledge and learning of a pre-trained model like ChatGPT to improve the performance of the model on a new task by training it on a smaller, task-specific dataset.'}, {'prompt': 'Can you provide an example of how fine-tuning can be used in a specific scenario?', 'completion': 'Example, let’s say you want to build a Question answering system for your internal company docs stored in Confluence. You can pass all the text content from your confluence and fine tune a GPT model with them.'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "def generate_response_given_text(context):\n",
    "    # API endpoint\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "    # Your API key\n",
    "    api_key = \"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\"\n",
    "\n",
    "    # The number of responses to generate\n",
    "    n = 1\n",
    "\n",
    "    # Messages\n",
    "    prompt = 'Break down the context below to sentences and from each of them generate the relevant questions and answers in a list of well formatted jsons with exactly two keys and no new line or space characters. First key being a prompt which should have the generated question and second key should be a completion with the answer to that question. The data would look something like this- [{\"prompt\":\"question1\",\"completion\":\"answer1\"},{\"prompt\":\"question2\",\"completion\":\"answer2\"}]. Here is the context- '\n",
    "    text_content = prompt + context\n",
    "    messages= [{\"role\": \"user\", \"content\": text_content}]\n",
    "\n",
    "\n",
    "    # Model\n",
    "    model = \"gpt-3.5-turbo\"\n",
    "\n",
    "    # Construct the request payload\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"n\": n\n",
    "    }\n",
    "\n",
    "    # Set the headers and authentication for the request\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    # Send the request and get the response\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "    # Print the response\n",
    "    generated_response = response.json()\n",
    "    print(generated_response)\n",
    "    return generated_response\n",
    "\n",
    "\n",
    "with open(\"content.txt\", 'r') as f:\n",
    "    content = f.read()\n",
    "    data = generate_response_given_text(content)\n",
    "    prompt_completion_pairs = json.loads(data['choices'][0]['message']['content'].replace('\\n', ''))\n",
    "    print(prompt_completion_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424de764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
